[
    {
        "id": 20000000,
        "doi": null,
        "title": "The Model Arena for Cross-lingual Sentiment Analysis: A Comparative Study in the Era of Large Language Models",
        "abstract": "Sentiment analysis serves as a pivotal component in Natural Language\nProcessing (NLP). Advancements in multilingual pre-trained models such as XLM-R\nand mT5 have contributed to the increasing interest in cross-lingual sentiment\nanalysis. The recent emergence in Large Language Models (LLM) has significantly\nadvanced general NLP tasks, however, the capability of such LLMs in\ncross-lingual sentiment analysis has not been fully studied. This work\nundertakes an empirical analysis to compare the cross-lingual transfer\ncapability of public Small Multilingual Language Models (SMLM) like XLM-R,\nagainst English-centric LLMs such as Llama-3, in the context of sentiment\nanalysis across English, Spanish, French and Chinese. Our findings reveal that\namong public models, SMLMs exhibit superior zero-shot cross-lingual performance\nrelative to LLMs. However, in few-shot cross-lingual settings, public LLMs\ndemonstrate an enhanced adaptive potential. In addition, we observe that\nproprietary GPT-3.5 and GPT-4 lead in zero-shot cross-lingual capability, but\nare outpaced by public models in few-shot scenarios.",
        "chunk-id": 1,
        "chunk": "Sentiment analysis serves as a pivotal component in Natural Language\nProcessing (NLP). Advancements in multilingual pre-trained models such as XLM-R\nand mT5 have contributed to the increasing interest in cross-lingual sentiment\nanalysis. The recent emergence in Large Language Models (LLM) has significantly\nadvanced general NLP tasks, however, the capability of such LLMs in",
        "authors": [
            "Xiliang Zhu",
            "Shayna Gardiner",
            "Tere Rold\u00e1n",
            "David Rossouw"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:38:45+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19358v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19358v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000000,
        "doi": null,
        "title": "The Model Arena for Cross-lingual Sentiment Analysis: A Comparative Study in the Era of Large Language Models",
        "abstract": "Sentiment analysis serves as a pivotal component in Natural Language\nProcessing (NLP). Advancements in multilingual pre-trained models such as XLM-R\nand mT5 have contributed to the increasing interest in cross-lingual sentiment\nanalysis. The recent emergence in Large Language Models (LLM) has significantly\nadvanced general NLP tasks, however, the capability of such LLMs in\ncross-lingual sentiment analysis has not been fully studied. This work\nundertakes an empirical analysis to compare the cross-lingual transfer\ncapability of public Small Multilingual Language Models (SMLM) like XLM-R,\nagainst English-centric LLMs such as Llama-3, in the context of sentiment\nanalysis across English, Spanish, French and Chinese. Our findings reveal that\namong public models, SMLMs exhibit superior zero-shot cross-lingual performance\nrelative to LLMs. However, in few-shot cross-lingual settings, public LLMs\ndemonstrate an enhanced adaptive potential. In addition, we observe that\nproprietary GPT-3.5 and GPT-4 lead in zero-shot cross-lingual capability, but\nare outpaced by public models in few-shot scenarios.",
        "chunk-id": 2,
        "chunk": "cross-lingual sentiment analysis has not been fully studied. This work\nundertakes an empirical analysis to compare the cross-lingual transfer\ncapability of public Small Multilingual Language Models (SMLM) like XLM-R,\nagainst English-centric LLMs such as Llama-3, in the context of sentiment\nanalysis across English, Spanish, French and Chinese. Our findings reveal that",
        "authors": [
            "Xiliang Zhu",
            "Shayna Gardiner",
            "Tere Rold\u00e1n",
            "David Rossouw"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:38:45+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19358v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19358v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000000,
        "doi": null,
        "title": "The Model Arena for Cross-lingual Sentiment Analysis: A Comparative Study in the Era of Large Language Models",
        "abstract": "Sentiment analysis serves as a pivotal component in Natural Language\nProcessing (NLP). Advancements in multilingual pre-trained models such as XLM-R\nand mT5 have contributed to the increasing interest in cross-lingual sentiment\nanalysis. The recent emergence in Large Language Models (LLM) has significantly\nadvanced general NLP tasks, however, the capability of such LLMs in\ncross-lingual sentiment analysis has not been fully studied. This work\nundertakes an empirical analysis to compare the cross-lingual transfer\ncapability of public Small Multilingual Language Models (SMLM) like XLM-R,\nagainst English-centric LLMs such as Llama-3, in the context of sentiment\nanalysis across English, Spanish, French and Chinese. Our findings reveal that\namong public models, SMLMs exhibit superior zero-shot cross-lingual performance\nrelative to LLMs. However, in few-shot cross-lingual settings, public LLMs\ndemonstrate an enhanced adaptive potential. In addition, we observe that\nproprietary GPT-3.5 and GPT-4 lead in zero-shot cross-lingual capability, but\nare outpaced by public models in few-shot scenarios.",
        "chunk-id": 3,
        "chunk": "among public models, SMLMs exhibit superior zero-shot cross-lingual performance\nrelative to LLMs. However, in few-shot cross-lingual settings, public LLMs\ndemonstrate an enhanced adaptive potential. In addition, we observe that\nproprietary GPT-3.5 and GPT-4 lead in zero-shot cross-lingual capability, but\nare outpaced by public models in few-shot scenarios.",
        "authors": [
            "Xiliang Zhu",
            "Shayna Gardiner",
            "Tere Rold\u00e1n",
            "David Rossouw"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:38:45+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19358v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19358v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000001,
        "doi": null,
        "title": "Annotation Errors and NER: A Study with OntoNotes 5.0",
        "abstract": "Named Entity Recognition (NER) is a well-studied problem in NLP. However,\nthere is much less focus on studying NER datasets, compared to developing new\nNER models. In this paper, we employed three simple techniques to detect\nannotation errors in the OntoNotes 5.0 corpus for English NER, which is the\nlargest available NER corpus for English. Our techniques corrected ~10% of the\nsentences in train/dev/test data. In terms of entity mentions, we corrected the\nspan and/or type of ~8% of mentions in the dataset, while\nadding/deleting/splitting/merging a few more. These are large numbers of\nchanges, considering the size of OntoNotes. We used three NER libraries to\ntrain, evaluate and compare the models trained with the original and the\nre-annotated datasets, which showed an average improvement of 1.23% in overall\nF-scores, with large (>10%) improvements for some of the entity types. While\nour annotation error detection methods are not exhaustive and there is some\nmanual annotation effort involved, they are largely language agnostic and can\nbe employed with other NER datasets, and other sequence labelling tasks.",
        "chunk-id": 1,
        "chunk": "Named Entity Recognition (NER) is a well-studied problem in NLP. However,\nthere is much less focus on studying NER datasets, compared to developing new\nNER models. In this paper, we employed three simple techniques to detect\nannotation errors in the OntoNotes 5.0 corpus for English NER, which is the\nlargest available NER corpus for English. Our techniques corrected ~10% of the",
        "authors": [
            "Gabriel Bernier-Colborne",
            "Sowmya Vajjala"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:48:46+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19172v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19172v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000001,
        "doi": null,
        "title": "Annotation Errors and NER: A Study with OntoNotes 5.0",
        "abstract": "Named Entity Recognition (NER) is a well-studied problem in NLP. However,\nthere is much less focus on studying NER datasets, compared to developing new\nNER models. In this paper, we employed three simple techniques to detect\nannotation errors in the OntoNotes 5.0 corpus for English NER, which is the\nlargest available NER corpus for English. Our techniques corrected ~10% of the\nsentences in train/dev/test data. In terms of entity mentions, we corrected the\nspan and/or type of ~8% of mentions in the dataset, while\nadding/deleting/splitting/merging a few more. These are large numbers of\nchanges, considering the size of OntoNotes. We used three NER libraries to\ntrain, evaluate and compare the models trained with the original and the\nre-annotated datasets, which showed an average improvement of 1.23% in overall\nF-scores, with large (>10%) improvements for some of the entity types. While\nour annotation error detection methods are not exhaustive and there is some\nmanual annotation effort involved, they are largely language agnostic and can\nbe employed with other NER datasets, and other sequence labelling tasks.",
        "chunk-id": 2,
        "chunk": "sentences in train/dev/test data. In terms of entity mentions, we corrected the\nspan and/or type of ~8% of mentions in the dataset, while\nadding/deleting/splitting/merging a few more. These are large numbers of\nchanges, considering the size of OntoNotes. We used three NER libraries to\ntrain, evaluate and compare the models trained with the original and the",
        "authors": [
            "Gabriel Bernier-Colborne",
            "Sowmya Vajjala"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:48:46+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19172v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19172v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000001,
        "doi": null,
        "title": "Annotation Errors and NER: A Study with OntoNotes 5.0",
        "abstract": "Named Entity Recognition (NER) is a well-studied problem in NLP. However,\nthere is much less focus on studying NER datasets, compared to developing new\nNER models. In this paper, we employed three simple techniques to detect\nannotation errors in the OntoNotes 5.0 corpus for English NER, which is the\nlargest available NER corpus for English. Our techniques corrected ~10% of the\nsentences in train/dev/test data. In terms of entity mentions, we corrected the\nspan and/or type of ~8% of mentions in the dataset, while\nadding/deleting/splitting/merging a few more. These are large numbers of\nchanges, considering the size of OntoNotes. We used three NER libraries to\ntrain, evaluate and compare the models trained with the original and the\nre-annotated datasets, which showed an average improvement of 1.23% in overall\nF-scores, with large (>10%) improvements for some of the entity types. While\nour annotation error detection methods are not exhaustive and there is some\nmanual annotation effort involved, they are largely language agnostic and can\nbe employed with other NER datasets, and other sequence labelling tasks.",
        "chunk-id": 3,
        "chunk": "re-annotated datasets, which showed an average improvement of 1.23% in overall\nF-scores, with large (>10%) improvements for some of the entity types. While\nour annotation error detection methods are not exhaustive and there is some\nmanual annotation effort involved, they are largely language agnostic and can\nbe employed with other NER datasets, and other sequence labelling tasks.",
        "authors": [
            "Gabriel Bernier-Colborne",
            "Sowmya Vajjala"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:48:46+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19172v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19172v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000002,
        "doi": null,
        "title": "VideoMambaPro: A Leap Forward for Mamba in Video Understanding",
        "abstract": "Video understanding requires the extraction of rich spatio-temporal\nrepresentations, which transformer models achieve through self-attention.\nUnfortunately, self-attention poses a computational burden. In NLP, Mamba has\nsurfaced as an efficient alternative for transformers. However, Mamba's\nsuccesses do not trivially extend to computer vision tasks, including those in\nvideo analysis. In this paper, we theoretically analyze the differences between\nself-attention and Mamba. We identify two limitations in Mamba's token\nprocessing: historical decay and element contradiction. We propose\nVideoMambaPro (VMP) that solves the identified limitations by adding masked\nbackward computation and elemental residual connections to a VideoMamba\nbackbone. VideoMambaPro shows state-of-the-art video action recognition\nperformance compared to transformer models, and surpasses VideoMamba by clear\nmargins: 7.9% and 8.1% top-1 on Kinetics-400 and Something-Something V2,\nrespectively. Our VideoMambaPro-M model achieves 91.9% top-1 on Kinetics-400,\nonly 0.2% below InternVideo2-6B but with only 1.2% of its parameters. The\ncombination of high performance and efficiency makes VideoMambaPro an\ninteresting alternative for transformer models.",
        "chunk-id": 1,
        "chunk": "Video understanding requires the extraction of rich spatio-temporal\nrepresentations, which transformer models achieve through self-attention.\nUnfortunately, self-attention poses a computational burden. In NLP, Mamba has\nsurfaced as an efficient alternative for transformers. However, Mamba's\nsuccesses do not trivially extend to computer vision tasks, including those in",
        "authors": [
            "Hui Lu",
            "Albert Ali Salah",
            "Ronald Poppe"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T08:45:31+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19006v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19006v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 20000002,
        "doi": null,
        "title": "VideoMambaPro: A Leap Forward for Mamba in Video Understanding",
        "abstract": "Video understanding requires the extraction of rich spatio-temporal\nrepresentations, which transformer models achieve through self-attention.\nUnfortunately, self-attention poses a computational burden. In NLP, Mamba has\nsurfaced as an efficient alternative for transformers. However, Mamba's\nsuccesses do not trivially extend to computer vision tasks, including those in\nvideo analysis. In this paper, we theoretically analyze the differences between\nself-attention and Mamba. We identify two limitations in Mamba's token\nprocessing: historical decay and element contradiction. We propose\nVideoMambaPro (VMP) that solves the identified limitations by adding masked\nbackward computation and elemental residual connections to a VideoMamba\nbackbone. VideoMambaPro shows state-of-the-art video action recognition\nperformance compared to transformer models, and surpasses VideoMamba by clear\nmargins: 7.9% and 8.1% top-1 on Kinetics-400 and Something-Something V2,\nrespectively. Our VideoMambaPro-M model achieves 91.9% top-1 on Kinetics-400,\nonly 0.2% below InternVideo2-6B but with only 1.2% of its parameters. The\ncombination of high performance and efficiency makes VideoMambaPro an\ninteresting alternative for transformer models.",
        "chunk-id": 2,
        "chunk": "video analysis. In this paper, we theoretically analyze the differences between\nself-attention and Mamba. We identify two limitations in Mamba's token\nprocessing: historical decay and element contradiction. We propose\nVideoMambaPro (VMP) that solves the identified limitations by adding masked\nbackward computation and elemental residual connections to a VideoMamba",
        "authors": [
            "Hui Lu",
            "Albert Ali Salah",
            "Ronald Poppe"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T08:45:31+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19006v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19006v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 20000002,
        "doi": null,
        "title": "VideoMambaPro: A Leap Forward for Mamba in Video Understanding",
        "abstract": "Video understanding requires the extraction of rich spatio-temporal\nrepresentations, which transformer models achieve through self-attention.\nUnfortunately, self-attention poses a computational burden. In NLP, Mamba has\nsurfaced as an efficient alternative for transformers. However, Mamba's\nsuccesses do not trivially extend to computer vision tasks, including those in\nvideo analysis. In this paper, we theoretically analyze the differences between\nself-attention and Mamba. We identify two limitations in Mamba's token\nprocessing: historical decay and element contradiction. We propose\nVideoMambaPro (VMP) that solves the identified limitations by adding masked\nbackward computation and elemental residual connections to a VideoMamba\nbackbone. VideoMambaPro shows state-of-the-art video action recognition\nperformance compared to transformer models, and surpasses VideoMamba by clear\nmargins: 7.9% and 8.1% top-1 on Kinetics-400 and Something-Something V2,\nrespectively. Our VideoMambaPro-M model achieves 91.9% top-1 on Kinetics-400,\nonly 0.2% below InternVideo2-6B but with only 1.2% of its parameters. The\ncombination of high performance and efficiency makes VideoMambaPro an\ninteresting alternative for transformer models.",
        "chunk-id": 3,
        "chunk": "backbone. VideoMambaPro shows state-of-the-art video action recognition\nperformance compared to transformer models, and surpasses VideoMamba by clear\nmargins: 7.9% and 8.1% top-1 on Kinetics-400 and Something-Something V2,\nrespectively. Our VideoMambaPro-M model achieves 91.9% top-1 on Kinetics-400,\nonly 0.2% below InternVideo2-6B but with only 1.2% of its parameters. The",
        "authors": [
            "Hui Lu",
            "Albert Ali Salah",
            "Ronald Poppe"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T08:45:31+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19006v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19006v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 20000002,
        "doi": null,
        "title": "VideoMambaPro: A Leap Forward for Mamba in Video Understanding",
        "abstract": "Video understanding requires the extraction of rich spatio-temporal\nrepresentations, which transformer models achieve through self-attention.\nUnfortunately, self-attention poses a computational burden. In NLP, Mamba has\nsurfaced as an efficient alternative for transformers. However, Mamba's\nsuccesses do not trivially extend to computer vision tasks, including those in\nvideo analysis. In this paper, we theoretically analyze the differences between\nself-attention and Mamba. We identify two limitations in Mamba's token\nprocessing: historical decay and element contradiction. We propose\nVideoMambaPro (VMP) that solves the identified limitations by adding masked\nbackward computation and elemental residual connections to a VideoMamba\nbackbone. VideoMambaPro shows state-of-the-art video action recognition\nperformance compared to transformer models, and surpasses VideoMamba by clear\nmargins: 7.9% and 8.1% top-1 on Kinetics-400 and Something-Something V2,\nrespectively. Our VideoMambaPro-M model achieves 91.9% top-1 on Kinetics-400,\nonly 0.2% below InternVideo2-6B but with only 1.2% of its parameters. The\ncombination of high performance and efficiency makes VideoMambaPro an\ninteresting alternative for transformer models.",
        "chunk-id": 4,
        "chunk": "combination of high performance and efficiency makes VideoMambaPro an\ninteresting alternative for transformer models.",
        "authors": [
            "Hui Lu",
            "Albert Ali Salah",
            "Ronald Poppe"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T08:45:31+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19006v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19006v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 20000003,
        "doi": null,
        "title": "Sonnet or Not, Bot? Poetry Evaluation for Large Models and Datasets",
        "abstract": "Large language models (LLMs) can now generate and recognize text in a wide\nrange of styles and genres, including highly specialized, creative genres like\npoetry. But what do LLMs really know about poetry? What can they know about\npoetry? We develop a task to evaluate how well LLMs recognize a specific aspect\nof poetry, poetic form, for more than 20 forms and formal elements in the\nEnglish language. Poetic form captures many different poetic features,\nincluding rhyme scheme, meter, and word or line repetition. We use this task to\nreflect on LLMs' current poetic capabilities, as well as the challenges and\npitfalls of creating NLP benchmarks for poetry and for other creative tasks. In\nparticular, we use this task to audit and reflect on the poems included in\npopular pretraining datasets. Our findings have implications for NLP\nresearchers interested in model evaluation, digital humanities and cultural\nanalytics scholars, and cultural heritage professionals.",
        "chunk-id": 1,
        "chunk": "Large language models (LLMs) can now generate and recognize text in a wide\nrange of styles and genres, including highly specialized, creative genres like\npoetry. But what do LLMs really know about poetry? What can they know about\npoetry? We develop a task to evaluate how well LLMs recognize a specific aspect\nof poetry, poetic form, for more than 20 forms and formal elements in the",
        "authors": [
            "Melanie Walsh",
            "Anna Preus",
            "Maria Antoniak"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T05:36:53+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18906v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18906v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000003,
        "doi": null,
        "title": "Sonnet or Not, Bot? Poetry Evaluation for Large Models and Datasets",
        "abstract": "Large language models (LLMs) can now generate and recognize text in a wide\nrange of styles and genres, including highly specialized, creative genres like\npoetry. But what do LLMs really know about poetry? What can they know about\npoetry? We develop a task to evaluate how well LLMs recognize a specific aspect\nof poetry, poetic form, for more than 20 forms and formal elements in the\nEnglish language. Poetic form captures many different poetic features,\nincluding rhyme scheme, meter, and word or line repetition. We use this task to\nreflect on LLMs' current poetic capabilities, as well as the challenges and\npitfalls of creating NLP benchmarks for poetry and for other creative tasks. In\nparticular, we use this task to audit and reflect on the poems included in\npopular pretraining datasets. Our findings have implications for NLP\nresearchers interested in model evaluation, digital humanities and cultural\nanalytics scholars, and cultural heritage professionals.",
        "chunk-id": 2,
        "chunk": "English language. Poetic form captures many different poetic features,\nincluding rhyme scheme, meter, and word or line repetition. We use this task to\nreflect on LLMs' current poetic capabilities, as well as the challenges and\npitfalls of creating NLP benchmarks for poetry and for other creative tasks. In\nparticular, we use this task to audit and reflect on the poems included in",
        "authors": [
            "Melanie Walsh",
            "Anna Preus",
            "Maria Antoniak"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T05:36:53+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18906v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18906v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000003,
        "doi": null,
        "title": "Sonnet or Not, Bot? Poetry Evaluation for Large Models and Datasets",
        "abstract": "Large language models (LLMs) can now generate and recognize text in a wide\nrange of styles and genres, including highly specialized, creative genres like\npoetry. But what do LLMs really know about poetry? What can they know about\npoetry? We develop a task to evaluate how well LLMs recognize a specific aspect\nof poetry, poetic form, for more than 20 forms and formal elements in the\nEnglish language. Poetic form captures many different poetic features,\nincluding rhyme scheme, meter, and word or line repetition. We use this task to\nreflect on LLMs' current poetic capabilities, as well as the challenges and\npitfalls of creating NLP benchmarks for poetry and for other creative tasks. In\nparticular, we use this task to audit and reflect on the poems included in\npopular pretraining datasets. Our findings have implications for NLP\nresearchers interested in model evaluation, digital humanities and cultural\nanalytics scholars, and cultural heritage professionals.",
        "chunk-id": 3,
        "chunk": "popular pretraining datasets. Our findings have implications for NLP\nresearchers interested in model evaluation, digital humanities and cultural\nanalytics scholars, and cultural heritage professionals.",
        "authors": [
            "Melanie Walsh",
            "Anna Preus",
            "Maria Antoniak"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T05:36:53+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18906v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18906v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000004,
        "doi": null,
        "title": "Can we teach language models to gloss endangered languages?",
        "abstract": "Interlinear glossed text (IGT) is a popular format in language documentation\nprojects, where each morpheme is labeled with a descriptive annotation.\nAutomating the creation of interlinear glossed text can be desirable to reduce\nannotator effort and maintain consistency across annotated corpora. Prior\nresearch has explored a number of statistical and neural methods for\nautomatically producing IGT.\n  As large language models (LLMs) have showed promising results across\nmultilingual tasks, even for rare, endangered languages, it is natural to\nwonder whether they can be utilized for the task of generating IGT. We explore\nwhether LLMs can be effective at the task of interlinear glossing with\nin-context learning, without any traditional training. We propose new\napproaches for selecting examples to provide in-context, observing that\ntargeted selection can significantly improve performance. We find that\nLLM-based methods beat standard transformer baselines, despite requiring no\ntraining at all. These approaches still underperform state-of-the-art\nsupervised systems for the task, but are highly practical for researchers\noutside of the NLP community, requiring minimal effort to use.",
        "chunk-id": 1,
        "chunk": "Interlinear glossed text (IGT) is a popular format in language documentation\nprojects, where each morpheme is labeled with a descriptive annotation.\nAutomating the creation of interlinear glossed text can be desirable to reduce\nannotator effort and maintain consistency across annotated corpora. Prior\nresearch has explored a number of statistical and neural methods for",
        "authors": [
            "Michael Ginn",
            "Mans Hulden",
            "Alexis Palmer"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T05:17:04+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18895v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18895v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000004,
        "doi": null,
        "title": "Can we teach language models to gloss endangered languages?",
        "abstract": "Interlinear glossed text (IGT) is a popular format in language documentation\nprojects, where each morpheme is labeled with a descriptive annotation.\nAutomating the creation of interlinear glossed text can be desirable to reduce\nannotator effort and maintain consistency across annotated corpora. Prior\nresearch has explored a number of statistical and neural methods for\nautomatically producing IGT.\n  As large language models (LLMs) have showed promising results across\nmultilingual tasks, even for rare, endangered languages, it is natural to\nwonder whether they can be utilized for the task of generating IGT. We explore\nwhether LLMs can be effective at the task of interlinear glossing with\nin-context learning, without any traditional training. We propose new\napproaches for selecting examples to provide in-context, observing that\ntargeted selection can significantly improve performance. We find that\nLLM-based methods beat standard transformer baselines, despite requiring no\ntraining at all. These approaches still underperform state-of-the-art\nsupervised systems for the task, but are highly practical for researchers\noutside of the NLP community, requiring minimal effort to use.",
        "chunk-id": 2,
        "chunk": "automatically producing IGT.\n  As large language models (LLMs) have showed promising results across\nmultilingual tasks, even for rare, endangered languages, it is natural to\nwonder whether they can be utilized for the task of generating IGT. We explore\nwhether LLMs can be effective at the task of interlinear glossing with",
        "authors": [
            "Michael Ginn",
            "Mans Hulden",
            "Alexis Palmer"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T05:17:04+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18895v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18895v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000004,
        "doi": null,
        "title": "Can we teach language models to gloss endangered languages?",
        "abstract": "Interlinear glossed text (IGT) is a popular format in language documentation\nprojects, where each morpheme is labeled with a descriptive annotation.\nAutomating the creation of interlinear glossed text can be desirable to reduce\nannotator effort and maintain consistency across annotated corpora. Prior\nresearch has explored a number of statistical and neural methods for\nautomatically producing IGT.\n  As large language models (LLMs) have showed promising results across\nmultilingual tasks, even for rare, endangered languages, it is natural to\nwonder whether they can be utilized for the task of generating IGT. We explore\nwhether LLMs can be effective at the task of interlinear glossing with\nin-context learning, without any traditional training. We propose new\napproaches for selecting examples to provide in-context, observing that\ntargeted selection can significantly improve performance. We find that\nLLM-based methods beat standard transformer baselines, despite requiring no\ntraining at all. These approaches still underperform state-of-the-art\nsupervised systems for the task, but are highly practical for researchers\noutside of the NLP community, requiring minimal effort to use.",
        "chunk-id": 3,
        "chunk": "in-context learning, without any traditional training. We propose new\napproaches for selecting examples to provide in-context, observing that\ntargeted selection can significantly improve performance. We find that\nLLM-based methods beat standard transformer baselines, despite requiring no\ntraining at all. These approaches still underperform state-of-the-art",
        "authors": [
            "Michael Ginn",
            "Mans Hulden",
            "Alexis Palmer"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T05:17:04+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18895v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18895v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000004,
        "doi": null,
        "title": "Can we teach language models to gloss endangered languages?",
        "abstract": "Interlinear glossed text (IGT) is a popular format in language documentation\nprojects, where each morpheme is labeled with a descriptive annotation.\nAutomating the creation of interlinear glossed text can be desirable to reduce\nannotator effort and maintain consistency across annotated corpora. Prior\nresearch has explored a number of statistical and neural methods for\nautomatically producing IGT.\n  As large language models (LLMs) have showed promising results across\nmultilingual tasks, even for rare, endangered languages, it is natural to\nwonder whether they can be utilized for the task of generating IGT. We explore\nwhether LLMs can be effective at the task of interlinear glossing with\nin-context learning, without any traditional training. We propose new\napproaches for selecting examples to provide in-context, observing that\ntargeted selection can significantly improve performance. We find that\nLLM-based methods beat standard transformer baselines, despite requiring no\ntraining at all. These approaches still underperform state-of-the-art\nsupervised systems for the task, but are highly practical for researchers\noutside of the NLP community, requiring minimal effort to use.",
        "chunk-id": 4,
        "chunk": "supervised systems for the task, but are highly practical for researchers\noutside of the NLP community, requiring minimal effort to use.",
        "authors": [
            "Michael Ginn",
            "Mans Hulden",
            "Alexis Palmer"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T05:17:04+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18895v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18895v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000005,
        "doi": null,
        "title": "SSP: Self-Supervised Prompting for Cross-Lingual Transfer to Low-Resource Languages using Large Language Models",
        "abstract": "Recently, very large language models (LLMs) have shown exceptional\nperformance on several English NLP tasks with just in-context learning (ICL),\nbut their utility in other languages is still underexplored. We investigate\ntheir effectiveness for NLP tasks in low-resource languages (LRLs), especially\nin the setting of zero-labelled cross-lingual transfer (0-CLT), where no\nlabelled training data for the target language is available -- however training\ndata from one or more related medium-resource languages (MRLs) is utilized,\nalongside the available unlabeled test data for a target language. We introduce\nSelf-Supervised Prompting (SSP), a novel ICL approach tailored for the 0-CLT\nsetting.\n  SSP is based on the key observation that LLMs output more accurate labels if\nin-context exemplars are from the target language (even if their labels are\nslightly noisy). To operationalize this, since target language training data is\nnot available in 0-CLT, SSP operates in two stages. In Stage I, using source\nMRL training data, target language's test data is noisily labeled. In Stage II,\nthese noisy test data points are used as exemplars in ICL for further improved\nlabelling. Additionally, our implementation of SSP uses a novel Integer Linear\nProgramming (ILP)-based exemplar selection that balances similarity, prediction\nconfidence (when available) and label coverage. Experiments on three tasks and\neleven LRLs (from three regions) demonstrate that SSP strongly outperforms\nexisting SOTA fine-tuned and prompting-based baselines in 0-CLT setup.",
        "chunk-id": 1,
        "chunk": "Recently, very large language models (LLMs) have shown exceptional\nperformance on several English NLP tasks with just in-context learning (ICL),\nbut their utility in other languages is still underexplored. We investigate\ntheir effectiveness for NLP tasks in low-resource languages (LRLs), especially\nin the setting of zero-labelled cross-lingual transfer (0-CLT), where no",
        "authors": [
            "Vipul Rathore",
            "Aniruddha Deb",
            "Ankish Chandresh",
            "Parag Singla",
            "Mausam"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T04:21:59+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18880v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18880v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000005,
        "doi": null,
        "title": "SSP: Self-Supervised Prompting for Cross-Lingual Transfer to Low-Resource Languages using Large Language Models",
        "abstract": "Recently, very large language models (LLMs) have shown exceptional\nperformance on several English NLP tasks with just in-context learning (ICL),\nbut their utility in other languages is still underexplored. We investigate\ntheir effectiveness for NLP tasks in low-resource languages (LRLs), especially\nin the setting of zero-labelled cross-lingual transfer (0-CLT), where no\nlabelled training data for the target language is available -- however training\ndata from one or more related medium-resource languages (MRLs) is utilized,\nalongside the available unlabeled test data for a target language. We introduce\nSelf-Supervised Prompting (SSP), a novel ICL approach tailored for the 0-CLT\nsetting.\n  SSP is based on the key observation that LLMs output more accurate labels if\nin-context exemplars are from the target language (even if their labels are\nslightly noisy). To operationalize this, since target language training data is\nnot available in 0-CLT, SSP operates in two stages. In Stage I, using source\nMRL training data, target language's test data is noisily labeled. In Stage II,\nthese noisy test data points are used as exemplars in ICL for further improved\nlabelling. Additionally, our implementation of SSP uses a novel Integer Linear\nProgramming (ILP)-based exemplar selection that balances similarity, prediction\nconfidence (when available) and label coverage. Experiments on three tasks and\neleven LRLs (from three regions) demonstrate that SSP strongly outperforms\nexisting SOTA fine-tuned and prompting-based baselines in 0-CLT setup.",
        "chunk-id": 2,
        "chunk": "labelled training data for the target language is available -- however training\ndata from one or more related medium-resource languages (MRLs) is utilized,\nalongside the available unlabeled test data for a target language. We introduce\nSelf-Supervised Prompting (SSP), a novel ICL approach tailored for the 0-CLT\nsetting.",
        "authors": [
            "Vipul Rathore",
            "Aniruddha Deb",
            "Ankish Chandresh",
            "Parag Singla",
            "Mausam"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T04:21:59+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18880v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18880v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000005,
        "doi": null,
        "title": "SSP: Self-Supervised Prompting for Cross-Lingual Transfer to Low-Resource Languages using Large Language Models",
        "abstract": "Recently, very large language models (LLMs) have shown exceptional\nperformance on several English NLP tasks with just in-context learning (ICL),\nbut their utility in other languages is still underexplored. We investigate\ntheir effectiveness for NLP tasks in low-resource languages (LRLs), especially\nin the setting of zero-labelled cross-lingual transfer (0-CLT), where no\nlabelled training data for the target language is available -- however training\ndata from one or more related medium-resource languages (MRLs) is utilized,\nalongside the available unlabeled test data for a target language. We introduce\nSelf-Supervised Prompting (SSP), a novel ICL approach tailored for the 0-CLT\nsetting.\n  SSP is based on the key observation that LLMs output more accurate labels if\nin-context exemplars are from the target language (even if their labels are\nslightly noisy). To operationalize this, since target language training data is\nnot available in 0-CLT, SSP operates in two stages. In Stage I, using source\nMRL training data, target language's test data is noisily labeled. In Stage II,\nthese noisy test data points are used as exemplars in ICL for further improved\nlabelling. Additionally, our implementation of SSP uses a novel Integer Linear\nProgramming (ILP)-based exemplar selection that balances similarity, prediction\nconfidence (when available) and label coverage. Experiments on three tasks and\neleven LRLs (from three regions) demonstrate that SSP strongly outperforms\nexisting SOTA fine-tuned and prompting-based baselines in 0-CLT setup.",
        "chunk-id": 3,
        "chunk": "setting.\n  SSP is based on the key observation that LLMs output more accurate labels if\nin-context exemplars are from the target language (even if their labels are\nslightly noisy). To operationalize this, since target language training data is\nnot available in 0-CLT, SSP operates in two stages. In Stage I, using source",
        "authors": [
            "Vipul Rathore",
            "Aniruddha Deb",
            "Ankish Chandresh",
            "Parag Singla",
            "Mausam"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T04:21:59+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18880v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18880v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000005,
        "doi": null,
        "title": "SSP: Self-Supervised Prompting for Cross-Lingual Transfer to Low-Resource Languages using Large Language Models",
        "abstract": "Recently, very large language models (LLMs) have shown exceptional\nperformance on several English NLP tasks with just in-context learning (ICL),\nbut their utility in other languages is still underexplored. We investigate\ntheir effectiveness for NLP tasks in low-resource languages (LRLs), especially\nin the setting of zero-labelled cross-lingual transfer (0-CLT), where no\nlabelled training data for the target language is available -- however training\ndata from one or more related medium-resource languages (MRLs) is utilized,\nalongside the available unlabeled test data for a target language. We introduce\nSelf-Supervised Prompting (SSP), a novel ICL approach tailored for the 0-CLT\nsetting.\n  SSP is based on the key observation that LLMs output more accurate labels if\nin-context exemplars are from the target language (even if their labels are\nslightly noisy). To operationalize this, since target language training data is\nnot available in 0-CLT, SSP operates in two stages. In Stage I, using source\nMRL training data, target language's test data is noisily labeled. In Stage II,\nthese noisy test data points are used as exemplars in ICL for further improved\nlabelling. Additionally, our implementation of SSP uses a novel Integer Linear\nProgramming (ILP)-based exemplar selection that balances similarity, prediction\nconfidence (when available) and label coverage. Experiments on three tasks and\neleven LRLs (from three regions) demonstrate that SSP strongly outperforms\nexisting SOTA fine-tuned and prompting-based baselines in 0-CLT setup.",
        "chunk-id": 4,
        "chunk": "MRL training data, target language's test data is noisily labeled. In Stage II,\nthese noisy test data points are used as exemplars in ICL for further improved\nlabelling. Additionally, our implementation of SSP uses a novel Integer Linear\nProgramming (ILP)-based exemplar selection that balances similarity, prediction",
        "authors": [
            "Vipul Rathore",
            "Aniruddha Deb",
            "Ankish Chandresh",
            "Parag Singla",
            "Mausam"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T04:21:59+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18880v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18880v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000005,
        "doi": null,
        "title": "SSP: Self-Supervised Prompting for Cross-Lingual Transfer to Low-Resource Languages using Large Language Models",
        "abstract": "Recently, very large language models (LLMs) have shown exceptional\nperformance on several English NLP tasks with just in-context learning (ICL),\nbut their utility in other languages is still underexplored. We investigate\ntheir effectiveness for NLP tasks in low-resource languages (LRLs), especially\nin the setting of zero-labelled cross-lingual transfer (0-CLT), where no\nlabelled training data for the target language is available -- however training\ndata from one or more related medium-resource languages (MRLs) is utilized,\nalongside the available unlabeled test data for a target language. We introduce\nSelf-Supervised Prompting (SSP), a novel ICL approach tailored for the 0-CLT\nsetting.\n  SSP is based on the key observation that LLMs output more accurate labels if\nin-context exemplars are from the target language (even if their labels are\nslightly noisy). To operationalize this, since target language training data is\nnot available in 0-CLT, SSP operates in two stages. In Stage I, using source\nMRL training data, target language's test data is noisily labeled. In Stage II,\nthese noisy test data points are used as exemplars in ICL for further improved\nlabelling. Additionally, our implementation of SSP uses a novel Integer Linear\nProgramming (ILP)-based exemplar selection that balances similarity, prediction\nconfidence (when available) and label coverage. Experiments on three tasks and\neleven LRLs (from three regions) demonstrate that SSP strongly outperforms\nexisting SOTA fine-tuned and prompting-based baselines in 0-CLT setup.",
        "chunk-id": 5,
        "chunk": "confidence (when available) and label coverage. Experiments on three tasks and\neleven LRLs (from three regions) demonstrate that SSP strongly outperforms\nexisting SOTA fine-tuned and prompting-based baselines in 0-CLT setup.",
        "authors": [
            "Vipul Rathore",
            "Aniruddha Deb",
            "Ankish Chandresh",
            "Parag Singla",
            "Mausam"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T04:21:59+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18880v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18880v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000006,
        "doi": null,
        "title": "ELCoRec: Enhance Language Understanding with Co-Propagation of Numerical and Categorical Features for Recommendation",
        "abstract": "Large language models have been flourishing in the natural language\nprocessing (NLP) domain, and their potential for recommendation has been paid\nmuch attention to. Despite the intelligence shown by the\nrecommendation-oriented finetuned models, LLMs struggle to fully understand the\nuser behavior patterns due to their innate weakness in interpreting numerical\nfeatures and the overhead for long context, where the temporal relations among\nuser behaviors, subtle quantitative signals among different ratings, and\nvarious side features of items are not well explored. Existing works only\nfine-tune a sole LLM on given text data without introducing that important\ninformation to it, leaving these problems unsolved. In this paper, we propose\nELCoRec to Enhance Language understanding with CoPropagation of numerical and\ncategorical features for Recommendation. Concretely, we propose to inject the\npreference understanding capability into LLM via a GAT expert model where the\nuser preference is better encoded by parallelly propagating the temporal\nrelations, and rating signals as well as various side information of historical\nitems. The parallel propagation mechanism could stabilize heterogeneous\nfeatures and offer an informative user preference encoding, which is then\ninjected into the language models via soft prompting at the cost of a single\ntoken embedding. To further obtain the user's recent interests, we proposed a\nnovel Recent interaction Augmented Prompt (RAP) template. Experiment results\nover three datasets against strong baselines validate the effectiveness of\nELCoRec. The code is available at\nhttps://anonymous.4open.science/r/CIKM_Code_Repo-E6F5/README.md.",
        "chunk-id": 1,
        "chunk": "Large language models have been flourishing in the natural language\nprocessing (NLP) domain, and their potential for recommendation has been paid\nmuch attention to. Despite the intelligence shown by the\nrecommendation-oriented finetuned models, LLMs struggle to fully understand the\nuser behavior patterns due to their innate weakness in interpreting numerical",
        "authors": [
            "Jizheng Chen",
            "Kounianhua Du",
            "Jianghao Lin",
            "Bo Chen",
            "Ruiming Tang",
            "Weinan Zhang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T01:37:57+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18825v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18825v1",
        "categories": [
            "Information Retrieval"
        ]
    },
    {
        "id": 20000006,
        "doi": null,
        "title": "ELCoRec: Enhance Language Understanding with Co-Propagation of Numerical and Categorical Features for Recommendation",
        "abstract": "Large language models have been flourishing in the natural language\nprocessing (NLP) domain, and their potential for recommendation has been paid\nmuch attention to. Despite the intelligence shown by the\nrecommendation-oriented finetuned models, LLMs struggle to fully understand the\nuser behavior patterns due to their innate weakness in interpreting numerical\nfeatures and the overhead for long context, where the temporal relations among\nuser behaviors, subtle quantitative signals among different ratings, and\nvarious side features of items are not well explored. Existing works only\nfine-tune a sole LLM on given text data without introducing that important\ninformation to it, leaving these problems unsolved. In this paper, we propose\nELCoRec to Enhance Language understanding with CoPropagation of numerical and\ncategorical features for Recommendation. Concretely, we propose to inject the\npreference understanding capability into LLM via a GAT expert model where the\nuser preference is better encoded by parallelly propagating the temporal\nrelations, and rating signals as well as various side information of historical\nitems. The parallel propagation mechanism could stabilize heterogeneous\nfeatures and offer an informative user preference encoding, which is then\ninjected into the language models via soft prompting at the cost of a single\ntoken embedding. To further obtain the user's recent interests, we proposed a\nnovel Recent interaction Augmented Prompt (RAP) template. Experiment results\nover three datasets against strong baselines validate the effectiveness of\nELCoRec. The code is available at\nhttps://anonymous.4open.science/r/CIKM_Code_Repo-E6F5/README.md.",
        "chunk-id": 2,
        "chunk": "features and the overhead for long context, where the temporal relations among\nuser behaviors, subtle quantitative signals among different ratings, and\nvarious side features of items are not well explored. Existing works only\nfine-tune a sole LLM on given text data without introducing that important\ninformation to it, leaving these problems unsolved. In this paper, we propose",
        "authors": [
            "Jizheng Chen",
            "Kounianhua Du",
            "Jianghao Lin",
            "Bo Chen",
            "Ruiming Tang",
            "Weinan Zhang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T01:37:57+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18825v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18825v1",
        "categories": [
            "Information Retrieval"
        ]
    },
    {
        "id": 20000006,
        "doi": null,
        "title": "ELCoRec: Enhance Language Understanding with Co-Propagation of Numerical and Categorical Features for Recommendation",
        "abstract": "Large language models have been flourishing in the natural language\nprocessing (NLP) domain, and their potential for recommendation has been paid\nmuch attention to. Despite the intelligence shown by the\nrecommendation-oriented finetuned models, LLMs struggle to fully understand the\nuser behavior patterns due to their innate weakness in interpreting numerical\nfeatures and the overhead for long context, where the temporal relations among\nuser behaviors, subtle quantitative signals among different ratings, and\nvarious side features of items are not well explored. Existing works only\nfine-tune a sole LLM on given text data without introducing that important\ninformation to it, leaving these problems unsolved. In this paper, we propose\nELCoRec to Enhance Language understanding with CoPropagation of numerical and\ncategorical features for Recommendation. Concretely, we propose to inject the\npreference understanding capability into LLM via a GAT expert model where the\nuser preference is better encoded by parallelly propagating the temporal\nrelations, and rating signals as well as various side information of historical\nitems. The parallel propagation mechanism could stabilize heterogeneous\nfeatures and offer an informative user preference encoding, which is then\ninjected into the language models via soft prompting at the cost of a single\ntoken embedding. To further obtain the user's recent interests, we proposed a\nnovel Recent interaction Augmented Prompt (RAP) template. Experiment results\nover three datasets against strong baselines validate the effectiveness of\nELCoRec. The code is available at\nhttps://anonymous.4open.science/r/CIKM_Code_Repo-E6F5/README.md.",
        "chunk-id": 3,
        "chunk": "ELCoRec to Enhance Language understanding with CoPropagation of numerical and\ncategorical features for Recommendation. Concretely, we propose to inject the\npreference understanding capability into LLM via a GAT expert model where the\nuser preference is better encoded by parallelly propagating the temporal",
        "authors": [
            "Jizheng Chen",
            "Kounianhua Du",
            "Jianghao Lin",
            "Bo Chen",
            "Ruiming Tang",
            "Weinan Zhang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T01:37:57+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18825v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18825v1",
        "categories": [
            "Information Retrieval"
        ]
    },
    {
        "id": 20000006,
        "doi": null,
        "title": "ELCoRec: Enhance Language Understanding with Co-Propagation of Numerical and Categorical Features for Recommendation",
        "abstract": "Large language models have been flourishing in the natural language\nprocessing (NLP) domain, and their potential for recommendation has been paid\nmuch attention to. Despite the intelligence shown by the\nrecommendation-oriented finetuned models, LLMs struggle to fully understand the\nuser behavior patterns due to their innate weakness in interpreting numerical\nfeatures and the overhead for long context, where the temporal relations among\nuser behaviors, subtle quantitative signals among different ratings, and\nvarious side features of items are not well explored. Existing works only\nfine-tune a sole LLM on given text data without introducing that important\ninformation to it, leaving these problems unsolved. In this paper, we propose\nELCoRec to Enhance Language understanding with CoPropagation of numerical and\ncategorical features for Recommendation. Concretely, we propose to inject the\npreference understanding capability into LLM via a GAT expert model where the\nuser preference is better encoded by parallelly propagating the temporal\nrelations, and rating signals as well as various side information of historical\nitems. The parallel propagation mechanism could stabilize heterogeneous\nfeatures and offer an informative user preference encoding, which is then\ninjected into the language models via soft prompting at the cost of a single\ntoken embedding. To further obtain the user's recent interests, we proposed a\nnovel Recent interaction Augmented Prompt (RAP) template. Experiment results\nover three datasets against strong baselines validate the effectiveness of\nELCoRec. The code is available at\nhttps://anonymous.4open.science/r/CIKM_Code_Repo-E6F5/README.md.",
        "chunk-id": 4,
        "chunk": "relations, and rating signals as well as various side information of historical\nitems. The parallel propagation mechanism could stabilize heterogeneous\nfeatures and offer an informative user preference encoding, which is then\ninjected into the language models via soft prompting at the cost of a single\ntoken embedding. To further obtain the user's recent interests, we proposed a",
        "authors": [
            "Jizheng Chen",
            "Kounianhua Du",
            "Jianghao Lin",
            "Bo Chen",
            "Ruiming Tang",
            "Weinan Zhang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T01:37:57+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18825v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18825v1",
        "categories": [
            "Information Retrieval"
        ]
    },
    {
        "id": 20000006,
        "doi": null,
        "title": "ELCoRec: Enhance Language Understanding with Co-Propagation of Numerical and Categorical Features for Recommendation",
        "abstract": "Large language models have been flourishing in the natural language\nprocessing (NLP) domain, and their potential for recommendation has been paid\nmuch attention to. Despite the intelligence shown by the\nrecommendation-oriented finetuned models, LLMs struggle to fully understand the\nuser behavior patterns due to their innate weakness in interpreting numerical\nfeatures and the overhead for long context, where the temporal relations among\nuser behaviors, subtle quantitative signals among different ratings, and\nvarious side features of items are not well explored. Existing works only\nfine-tune a sole LLM on given text data without introducing that important\ninformation to it, leaving these problems unsolved. In this paper, we propose\nELCoRec to Enhance Language understanding with CoPropagation of numerical and\ncategorical features for Recommendation. Concretely, we propose to inject the\npreference understanding capability into LLM via a GAT expert model where the\nuser preference is better encoded by parallelly propagating the temporal\nrelations, and rating signals as well as various side information of historical\nitems. The parallel propagation mechanism could stabilize heterogeneous\nfeatures and offer an informative user preference encoding, which is then\ninjected into the language models via soft prompting at the cost of a single\ntoken embedding. To further obtain the user's recent interests, we proposed a\nnovel Recent interaction Augmented Prompt (RAP) template. Experiment results\nover three datasets against strong baselines validate the effectiveness of\nELCoRec. The code is available at\nhttps://anonymous.4open.science/r/CIKM_Code_Repo-E6F5/README.md.",
        "chunk-id": 5,
        "chunk": "novel Recent interaction Augmented Prompt (RAP) template. Experiment results\nover three datasets against strong baselines validate the effectiveness of\nELCoRec. The code is available at\nhttps://anonymous.4open.science/r/CIKM_Code_Repo-E6F5/README.md.",
        "authors": [
            "Jizheng Chen",
            "Kounianhua Du",
            "Jianghao Lin",
            "Bo Chen",
            "Ruiming Tang",
            "Weinan Zhang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T01:37:57+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18825v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18825v1",
        "categories": [
            "Information Retrieval"
        ]
    },
    {
        "id": 20000007,
        "doi": null,
        "title": "Implicit Discourse Relation Classification For Nigerian Pidgin",
        "abstract": "Despite attempts to make Large Language Models multi-lingual, many of the\nworld's languages are still severely under-resourced. This widens the\nperformance gap between NLP and AI applications aimed at well-financed, and\nthose aimed at less-resourced languages. In this paper, we focus on Nigerian\nPidgin (NP), which is spoken by nearly 100 million people, but has\ncomparatively very few NLP resources and corpora. We address the task of\nImplicit Discourse Relation Classification (IDRC) and systematically compare an\napproach translating NP data to English and then using a well-resourced IDRC\ntool and back-projecting the labels versus creating a synthetic discourse\ncorpus for NP, in which we translate PDTB and project PDTB labels, and then\ntrain an NP IDR classifier. The latter approach of learning a \"native\" NP\nclassifier outperforms our baseline by 13.27\\% and 33.98\\% in f$_{1}$ score for\n4-way and 11-way classification, respectively.",
        "chunk-id": 1,
        "chunk": "Despite attempts to make Large Language Models multi-lingual, many of the\nworld's languages are still severely under-resourced. This widens the\nperformance gap between NLP and AI applications aimed at well-financed, and\nthose aimed at less-resourced languages. In this paper, we focus on Nigerian\nPidgin (NP), which is spoken by nearly 100 million people, but has",
        "authors": [
            "Muhammed Saeed",
            "Peter Bourgonje",
            "Vera Demberg"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-26T22:10:15+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18776v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18776v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000007,
        "doi": null,
        "title": "Implicit Discourse Relation Classification For Nigerian Pidgin",
        "abstract": "Despite attempts to make Large Language Models multi-lingual, many of the\nworld's languages are still severely under-resourced. This widens the\nperformance gap between NLP and AI applications aimed at well-financed, and\nthose aimed at less-resourced languages. In this paper, we focus on Nigerian\nPidgin (NP), which is spoken by nearly 100 million people, but has\ncomparatively very few NLP resources and corpora. We address the task of\nImplicit Discourse Relation Classification (IDRC) and systematically compare an\napproach translating NP data to English and then using a well-resourced IDRC\ntool and back-projecting the labels versus creating a synthetic discourse\ncorpus for NP, in which we translate PDTB and project PDTB labels, and then\ntrain an NP IDR classifier. The latter approach of learning a \"native\" NP\nclassifier outperforms our baseline by 13.27\\% and 33.98\\% in f$_{1}$ score for\n4-way and 11-way classification, respectively.",
        "chunk-id": 2,
        "chunk": "comparatively very few NLP resources and corpora. We address the task of\nImplicit Discourse Relation Classification (IDRC) and systematically compare an\napproach translating NP data to English and then using a well-resourced IDRC\ntool and back-projecting the labels versus creating a synthetic discourse\ncorpus for NP, in which we translate PDTB and project PDTB labels, and then",
        "authors": [
            "Muhammed Saeed",
            "Peter Bourgonje",
            "Vera Demberg"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-26T22:10:15+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18776v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18776v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000007,
        "doi": null,
        "title": "Implicit Discourse Relation Classification For Nigerian Pidgin",
        "abstract": "Despite attempts to make Large Language Models multi-lingual, many of the\nworld's languages are still severely under-resourced. This widens the\nperformance gap between NLP and AI applications aimed at well-financed, and\nthose aimed at less-resourced languages. In this paper, we focus on Nigerian\nPidgin (NP), which is spoken by nearly 100 million people, but has\ncomparatively very few NLP resources and corpora. We address the task of\nImplicit Discourse Relation Classification (IDRC) and systematically compare an\napproach translating NP data to English and then using a well-resourced IDRC\ntool and back-projecting the labels versus creating a synthetic discourse\ncorpus for NP, in which we translate PDTB and project PDTB labels, and then\ntrain an NP IDR classifier. The latter approach of learning a \"native\" NP\nclassifier outperforms our baseline by 13.27\\% and 33.98\\% in f$_{1}$ score for\n4-way and 11-way classification, respectively.",
        "chunk-id": 3,
        "chunk": "train an NP IDR classifier. The latter approach of learning a \"native\" NP\nclassifier outperforms our baseline by 13.27\\% and 33.98\\% in f$_{1}$ score for\n4-way and 11-way classification, respectively.",
        "authors": [
            "Muhammed Saeed",
            "Peter Bourgonje",
            "Vera Demberg"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-26T22:10:15+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18776v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18776v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000008,
        "doi": null,
        "title": "PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation",
        "abstract": "Large language models (LLMs) have revolutionized the field of NLP. Notably,\ntheir in-context learning capabilities also enable their use as evaluation\nmetrics for natural language generation, making them particularly advantageous\nin low-resource scenarios and time-restricted applications. In this work, we\nintroduce PrExMe, a large-scale prompt exploration for metrics, where we\nevaluate more than 720 prompt templates for open-source LLM-based metrics on\nmachine translation (MT) and summarization datasets, totalling over 6.6M\nevaluations. This extensive comparison (1) serves as a benchmark of the\nperformance of recent open-source LLMs as metrics and (2) explores the\nstability and variability of different prompting strategies. We discover that,\non the one hand, there are scenarios for which prompts are stable. For\ninstance, some LLMs show idiosyncratic preferences and favor to grade generated\ntexts with textual labels while others prefer to return numeric scores. On the\nother hand, the stability of prompts and model rankings can be susceptible to\nseemingly innocuous changes. For example, changing the requested output format\nfrom \"0 to 100\" to \"-1 to +1\" can strongly affect the rankings in our\nevaluation. Our study contributes to understanding the impact of different\nprompting approaches on LLM-based metrics for MT and summarization evaluation,\nhighlighting the most stable prompting patterns and potential limitations.",
        "chunk-id": 1,
        "chunk": "Large language models (LLMs) have revolutionized the field of NLP. Notably,\ntheir in-context learning capabilities also enable their use as evaluation\nmetrics for natural language generation, making them particularly advantageous\nin low-resource scenarios and time-restricted applications. In this work, we\nintroduce PrExMe, a large-scale prompt exploration for metrics, where we",
        "authors": [
            "Christoph Leiter",
            "Steffen Eger"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-26T17:56:29+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18528v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18528v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000008,
        "doi": null,
        "title": "PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation",
        "abstract": "Large language models (LLMs) have revolutionized the field of NLP. Notably,\ntheir in-context learning capabilities also enable their use as evaluation\nmetrics for natural language generation, making them particularly advantageous\nin low-resource scenarios and time-restricted applications. In this work, we\nintroduce PrExMe, a large-scale prompt exploration for metrics, where we\nevaluate more than 720 prompt templates for open-source LLM-based metrics on\nmachine translation (MT) and summarization datasets, totalling over 6.6M\nevaluations. This extensive comparison (1) serves as a benchmark of the\nperformance of recent open-source LLMs as metrics and (2) explores the\nstability and variability of different prompting strategies. We discover that,\non the one hand, there are scenarios for which prompts are stable. For\ninstance, some LLMs show idiosyncratic preferences and favor to grade generated\ntexts with textual labels while others prefer to return numeric scores. On the\nother hand, the stability of prompts and model rankings can be susceptible to\nseemingly innocuous changes. For example, changing the requested output format\nfrom \"0 to 100\" to \"-1 to +1\" can strongly affect the rankings in our\nevaluation. Our study contributes to understanding the impact of different\nprompting approaches on LLM-based metrics for MT and summarization evaluation,\nhighlighting the most stable prompting patterns and potential limitations.",
        "chunk-id": 2,
        "chunk": "evaluate more than 720 prompt templates for open-source LLM-based metrics on\nmachine translation (MT) and summarization datasets, totalling over 6.6M\nevaluations. This extensive comparison (1) serves as a benchmark of the\nperformance of recent open-source LLMs as metrics and (2) explores the\nstability and variability of different prompting strategies. We discover that,",
        "authors": [
            "Christoph Leiter",
            "Steffen Eger"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-26T17:56:29+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18528v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18528v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000008,
        "doi": null,
        "title": "PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation",
        "abstract": "Large language models (LLMs) have revolutionized the field of NLP. Notably,\ntheir in-context learning capabilities also enable their use as evaluation\nmetrics for natural language generation, making them particularly advantageous\nin low-resource scenarios and time-restricted applications. In this work, we\nintroduce PrExMe, a large-scale prompt exploration for metrics, where we\nevaluate more than 720 prompt templates for open-source LLM-based metrics on\nmachine translation (MT) and summarization datasets, totalling over 6.6M\nevaluations. This extensive comparison (1) serves as a benchmark of the\nperformance of recent open-source LLMs as metrics and (2) explores the\nstability and variability of different prompting strategies. We discover that,\non the one hand, there are scenarios for which prompts are stable. For\ninstance, some LLMs show idiosyncratic preferences and favor to grade generated\ntexts with textual labels while others prefer to return numeric scores. On the\nother hand, the stability of prompts and model rankings can be susceptible to\nseemingly innocuous changes. For example, changing the requested output format\nfrom \"0 to 100\" to \"-1 to +1\" can strongly affect the rankings in our\nevaluation. Our study contributes to understanding the impact of different\nprompting approaches on LLM-based metrics for MT and summarization evaluation,\nhighlighting the most stable prompting patterns and potential limitations.",
        "chunk-id": 3,
        "chunk": "on the one hand, there are scenarios for which prompts are stable. For\ninstance, some LLMs show idiosyncratic preferences and favor to grade generated\ntexts with textual labels while others prefer to return numeric scores. On the\nother hand, the stability of prompts and model rankings can be susceptible to",
        "authors": [
            "Christoph Leiter",
            "Steffen Eger"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-26T17:56:29+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18528v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18528v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000008,
        "doi": null,
        "title": "PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation",
        "abstract": "Large language models (LLMs) have revolutionized the field of NLP. Notably,\ntheir in-context learning capabilities also enable their use as evaluation\nmetrics for natural language generation, making them particularly advantageous\nin low-resource scenarios and time-restricted applications. In this work, we\nintroduce PrExMe, a large-scale prompt exploration for metrics, where we\nevaluate more than 720 prompt templates for open-source LLM-based metrics on\nmachine translation (MT) and summarization datasets, totalling over 6.6M\nevaluations. This extensive comparison (1) serves as a benchmark of the\nperformance of recent open-source LLMs as metrics and (2) explores the\nstability and variability of different prompting strategies. We discover that,\non the one hand, there are scenarios for which prompts are stable. For\ninstance, some LLMs show idiosyncratic preferences and favor to grade generated\ntexts with textual labels while others prefer to return numeric scores. On the\nother hand, the stability of prompts and model rankings can be susceptible to\nseemingly innocuous changes. For example, changing the requested output format\nfrom \"0 to 100\" to \"-1 to +1\" can strongly affect the rankings in our\nevaluation. Our study contributes to understanding the impact of different\nprompting approaches on LLM-based metrics for MT and summarization evaluation,\nhighlighting the most stable prompting patterns and potential limitations.",
        "chunk-id": 4,
        "chunk": "seemingly innocuous changes. For example, changing the requested output format\nfrom \"0 to 100\" to \"-1 to +1\" can strongly affect the rankings in our\nevaluation. Our study contributes to understanding the impact of different\nprompting approaches on LLM-based metrics for MT and summarization evaluation,\nhighlighting the most stable prompting patterns and potential limitations.",
        "authors": [
            "Christoph Leiter",
            "Steffen Eger"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-26T17:56:29+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18528v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18528v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000009,
        "doi": null,
        "title": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks",
        "abstract": "There is an increasing trend towards evaluating NLP models with LLM-generated\njudgments instead of human judgments. In the absence of a comparison against\nhuman data, this raises concerns about the validity of these evaluations; in\ncase they are conducted with proprietary models, this also raises concerns over\nreproducibility. We provide JUDGE-BENCH, a collection of 20 NLP datasets with\nhuman annotations, and comprehensively evaluate 11 current LLMs, covering both\nopen-weight and proprietary models, for their ability to replicate the\nannotations. Our evaluations show that each LLM exhibits a large variance\nacross datasets in its correlation to human judgments. We conclude that LLMs\nare not yet ready to systematically replace human judges in NLP.",
        "chunk-id": 1,
        "chunk": "There is an increasing trend towards evaluating NLP models with LLM-generated\njudgments instead of human judgments. In the absence of a comparison against\nhuman data, this raises concerns about the validity of these evaluations; in\ncase they are conducted with proprietary models, this also raises concerns over",
        "authors": [
            "Anna Bavaresco",
            "Raffaella Bernardi",
            "Leonardo Bertolazzi",
            "Desmond Elliott",
            "Raquel Fern\u00e1ndez",
            "Albert Gatt",
            "Esam Ghaleb",
            "Mario Giulianelli",
            "Michael Hanna",
            "Alexander Koller",
            "Andr\u00e9 F. T. Martins",
            "Philipp Mondorf",
            "Vera Neplenbroek",
            "Sandro Pezzelle",
            "Barbara Plank",
            "David Schlangen",
            "Alessandro Suglia",
            "Aditya K Surikuchi",
            "Ece Takmaz",
            "Alberto Testoni"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-26T14:56:13+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18403v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18403v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000009,
        "doi": null,
        "title": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks",
        "abstract": "There is an increasing trend towards evaluating NLP models with LLM-generated\njudgments instead of human judgments. In the absence of a comparison against\nhuman data, this raises concerns about the validity of these evaluations; in\ncase they are conducted with proprietary models, this also raises concerns over\nreproducibility. We provide JUDGE-BENCH, a collection of 20 NLP datasets with\nhuman annotations, and comprehensively evaluate 11 current LLMs, covering both\nopen-weight and proprietary models, for their ability to replicate the\nannotations. Our evaluations show that each LLM exhibits a large variance\nacross datasets in its correlation to human judgments. We conclude that LLMs\nare not yet ready to systematically replace human judges in NLP.",
        "chunk-id": 2,
        "chunk": "reproducibility. We provide JUDGE-BENCH, a collection of 20 NLP datasets with\nhuman annotations, and comprehensively evaluate 11 current LLMs, covering both\nopen-weight and proprietary models, for their ability to replicate the\nannotations. Our evaluations show that each LLM exhibits a large variance\nacross datasets in its correlation to human judgments. We conclude that LLMs",
        "authors": [
            "Anna Bavaresco",
            "Raffaella Bernardi",
            "Leonardo Bertolazzi",
            "Desmond Elliott",
            "Raquel Fern\u00e1ndez",
            "Albert Gatt",
            "Esam Ghaleb",
            "Mario Giulianelli",
            "Michael Hanna",
            "Alexander Koller",
            "Andr\u00e9 F. T. Martins",
            "Philipp Mondorf",
            "Vera Neplenbroek",
            "Sandro Pezzelle",
            "Barbara Plank",
            "David Schlangen",
            "Alessandro Suglia",
            "Aditya K Surikuchi",
            "Ece Takmaz",
            "Alberto Testoni"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-26T14:56:13+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18403v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18403v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000009,
        "doi": null,
        "title": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks",
        "abstract": "There is an increasing trend towards evaluating NLP models with LLM-generated\njudgments instead of human judgments. In the absence of a comparison against\nhuman data, this raises concerns about the validity of these evaluations; in\ncase they are conducted with proprietary models, this also raises concerns over\nreproducibility. We provide JUDGE-BENCH, a collection of 20 NLP datasets with\nhuman annotations, and comprehensively evaluate 11 current LLMs, covering both\nopen-weight and proprietary models, for their ability to replicate the\nannotations. Our evaluations show that each LLM exhibits a large variance\nacross datasets in its correlation to human judgments. We conclude that LLMs\nare not yet ready to systematically replace human judges in NLP.",
        "chunk-id": 3,
        "chunk": "are not yet ready to systematically replace human judges in NLP.",
        "authors": [
            "Anna Bavaresco",
            "Raffaella Bernardi",
            "Leonardo Bertolazzi",
            "Desmond Elliott",
            "Raquel Fern\u00e1ndez",
            "Albert Gatt",
            "Esam Ghaleb",
            "Mario Giulianelli",
            "Michael Hanna",
            "Alexander Koller",
            "Andr\u00e9 F. T. Martins",
            "Philipp Mondorf",
            "Vera Neplenbroek",
            "Sandro Pezzelle",
            "Barbara Plank",
            "David Schlangen",
            "Alessandro Suglia",
            "Aditya K Surikuchi",
            "Ece Takmaz",
            "Alberto Testoni"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-26T14:56:13+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18403v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18403v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000010,
        "doi": null,
        "title": "Zero-shot prompt-based classification: topic labeling in times of foundation models in German Tweets",
        "abstract": "Filtering and annotating textual data are routine tasks in many areas, like\nsocial media or news analytics. Automating these tasks allows to scale the\nanalyses wrt. speed and breadth of content covered and decreases the manual\neffort required. Due to technical advancements in Natural Language Processing,\nspecifically the success of large foundation models, a new tool for automating\nsuch annotation processes by using a text-to-text interface given written\nguidelines without providing training samples has become available.\n  In this work, we assess these advancements in-the-wild by empirically testing\nthem in an annotation task on German Twitter data about social and political\nEuropean crises. We compare the prompt-based results with our human annotation\nand preceding classification approaches, including Naive Bayes and a BERT-based\nfine-tuning/domain adaptation pipeline. Our results show that the prompt-based\napproach - despite being limited by local computation resources during the\nmodel selection - is comparable with the fine-tuned BERT but without any\nannotated training data. Our findings emphasize the ongoing paradigm shift in\nthe NLP landscape, i.e., the unification of downstream tasks and elimination of\nthe need for pre-labeled training data.",
        "chunk-id": 1,
        "chunk": "Filtering and annotating textual data are routine tasks in many areas, like\nsocial media or news analytics. Automating these tasks allows to scale the\nanalyses wrt. speed and breadth of content covered and decreases the manual\neffort required. Due to technical advancements in Natural Language Processing,\nspecifically the success of large foundation models, a new tool for automating",
        "authors": [
            "Simon M\u00fcnker",
            "Kai Kugler",
            "Achim Rettinger"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-26T10:44:02+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18239v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18239v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000010,
        "doi": null,
        "title": "Zero-shot prompt-based classification: topic labeling in times of foundation models in German Tweets",
        "abstract": "Filtering and annotating textual data are routine tasks in many areas, like\nsocial media or news analytics. Automating these tasks allows to scale the\nanalyses wrt. speed and breadth of content covered and decreases the manual\neffort required. Due to technical advancements in Natural Language Processing,\nspecifically the success of large foundation models, a new tool for automating\nsuch annotation processes by using a text-to-text interface given written\nguidelines without providing training samples has become available.\n  In this work, we assess these advancements in-the-wild by empirically testing\nthem in an annotation task on German Twitter data about social and political\nEuropean crises. We compare the prompt-based results with our human annotation\nand preceding classification approaches, including Naive Bayes and a BERT-based\nfine-tuning/domain adaptation pipeline. Our results show that the prompt-based\napproach - despite being limited by local computation resources during the\nmodel selection - is comparable with the fine-tuned BERT but without any\nannotated training data. Our findings emphasize the ongoing paradigm shift in\nthe NLP landscape, i.e., the unification of downstream tasks and elimination of\nthe need for pre-labeled training data.",
        "chunk-id": 2,
        "chunk": "such annotation processes by using a text-to-text interface given written\nguidelines without providing training samples has become available.\n  In this work, we assess these advancements in-the-wild by empirically testing\nthem in an annotation task on German Twitter data about social and political\nEuropean crises. We compare the prompt-based results with our human annotation",
        "authors": [
            "Simon M\u00fcnker",
            "Kai Kugler",
            "Achim Rettinger"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-26T10:44:02+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18239v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18239v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000010,
        "doi": null,
        "title": "Zero-shot prompt-based classification: topic labeling in times of foundation models in German Tweets",
        "abstract": "Filtering and annotating textual data are routine tasks in many areas, like\nsocial media or news analytics. Automating these tasks allows to scale the\nanalyses wrt. speed and breadth of content covered and decreases the manual\neffort required. Due to technical advancements in Natural Language Processing,\nspecifically the success of large foundation models, a new tool for automating\nsuch annotation processes by using a text-to-text interface given written\nguidelines without providing training samples has become available.\n  In this work, we assess these advancements in-the-wild by empirically testing\nthem in an annotation task on German Twitter data about social and political\nEuropean crises. We compare the prompt-based results with our human annotation\nand preceding classification approaches, including Naive Bayes and a BERT-based\nfine-tuning/domain adaptation pipeline. Our results show that the prompt-based\napproach - despite being limited by local computation resources during the\nmodel selection - is comparable with the fine-tuned BERT but without any\nannotated training data. Our findings emphasize the ongoing paradigm shift in\nthe NLP landscape, i.e., the unification of downstream tasks and elimination of\nthe need for pre-labeled training data.",
        "chunk-id": 3,
        "chunk": "and preceding classification approaches, including Naive Bayes and a BERT-based\nfine-tuning/domain adaptation pipeline. Our results show that the prompt-based\napproach - despite being limited by local computation resources during the\nmodel selection - is comparable with the fine-tuned BERT but without any\nannotated training data. Our findings emphasize the ongoing paradigm shift in",
        "authors": [
            "Simon M\u00fcnker",
            "Kai Kugler",
            "Achim Rettinger"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-26T10:44:02+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18239v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18239v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000010,
        "doi": null,
        "title": "Zero-shot prompt-based classification: topic labeling in times of foundation models in German Tweets",
        "abstract": "Filtering and annotating textual data are routine tasks in many areas, like\nsocial media or news analytics. Automating these tasks allows to scale the\nanalyses wrt. speed and breadth of content covered and decreases the manual\neffort required. Due to technical advancements in Natural Language Processing,\nspecifically the success of large foundation models, a new tool for automating\nsuch annotation processes by using a text-to-text interface given written\nguidelines without providing training samples has become available.\n  In this work, we assess these advancements in-the-wild by empirically testing\nthem in an annotation task on German Twitter data about social and political\nEuropean crises. We compare the prompt-based results with our human annotation\nand preceding classification approaches, including Naive Bayes and a BERT-based\nfine-tuning/domain adaptation pipeline. Our results show that the prompt-based\napproach - despite being limited by local computation resources during the\nmodel selection - is comparable with the fine-tuned BERT but without any\nannotated training data. Our findings emphasize the ongoing paradigm shift in\nthe NLP landscape, i.e., the unification of downstream tasks and elimination of\nthe need for pre-labeled training data.",
        "chunk-id": 4,
        "chunk": "the NLP landscape, i.e., the unification of downstream tasks and elimination of\nthe need for pre-labeled training data.",
        "authors": [
            "Simon M\u00fcnker",
            "Kai Kugler",
            "Achim Rettinger"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-26T10:44:02+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18239v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18239v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000011,
        "doi": null,
        "title": "Poisoned LangChain: Jailbreak LLMs by LangChain",
        "abstract": "With the development of natural language processing (NLP), large language\nmodels (LLMs) are becoming increasingly popular. LLMs are integrating more into\neveryday life, raising public concerns about their security vulnerabilities.\nConsequently, the security of large language models is becoming critically\nimportant. Currently, the techniques for attacking and defending against LLMs\nare continuously evolving. One significant method type of attack is the\njailbreak attack, which designed to evade model safety mechanisms and induce\nthe generation of inappropriate content. Existing jailbreak attacks primarily\nrely on crafting inducement prompts for direct jailbreaks, which are less\neffective against large models with robust filtering and high comprehension\nabilities. Given the increasing demand for real-time capabilities in large\nlanguage models, real-time updates and iterations of new knowledge have become\nessential. Retrieval-Augmented Generation (RAG), an advanced technique to\ncompensate for the model's lack of new knowledge, is gradually becoming\nmainstream. As RAG enables the model to utilize external knowledge bases, it\nprovides a new avenue for jailbreak attacks.\n  In this paper, we conduct the first work to propose the concept of indirect\njailbreak and achieve Retrieval-Augmented Generation via LangChain. Building on\nthis, we further design a novel method of indirect jailbreak attack, termed\nPoisoned-LangChain (PLC), which leverages a poisoned external knowledge base to\ninteract with large language models, thereby causing the large models to\ngenerate malicious non-compliant dialogues.We tested this method on six\ndifferent large language models across three major categories of jailbreak\nissues. The experiments demonstrate that PLC successfully implemented indirect\njailbreak attacks under three different scenarios, achieving success rates of\n88.56%, 79.04%, and 82.69% respectively.",
        "chunk-id": 1,
        "chunk": "With the development of natural language processing (NLP), large language\nmodels (LLMs) are becoming increasingly popular. LLMs are integrating more into\neveryday life, raising public concerns about their security vulnerabilities.\nConsequently, the security of large language models is becoming critically\nimportant. Currently, the techniques for attacking and defending against LLMs",
        "authors": [
            "Ziqiu Wang",
            "Jun Liu",
            "Shengkai Zhang",
            "Yang Yang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-26T07:21:02+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18122v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18122v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000011,
        "doi": null,
        "title": "Poisoned LangChain: Jailbreak LLMs by LangChain",
        "abstract": "With the development of natural language processing (NLP), large language\nmodels (LLMs) are becoming increasingly popular. LLMs are integrating more into\neveryday life, raising public concerns about their security vulnerabilities.\nConsequently, the security of large language models is becoming critically\nimportant. Currently, the techniques for attacking and defending against LLMs\nare continuously evolving. One significant method type of attack is the\njailbreak attack, which designed to evade model safety mechanisms and induce\nthe generation of inappropriate content. Existing jailbreak attacks primarily\nrely on crafting inducement prompts for direct jailbreaks, which are less\neffective against large models with robust filtering and high comprehension\nabilities. Given the increasing demand for real-time capabilities in large\nlanguage models, real-time updates and iterations of new knowledge have become\nessential. Retrieval-Augmented Generation (RAG), an advanced technique to\ncompensate for the model's lack of new knowledge, is gradually becoming\nmainstream. As RAG enables the model to utilize external knowledge bases, it\nprovides a new avenue for jailbreak attacks.\n  In this paper, we conduct the first work to propose the concept of indirect\njailbreak and achieve Retrieval-Augmented Generation via LangChain. Building on\nthis, we further design a novel method of indirect jailbreak attack, termed\nPoisoned-LangChain (PLC), which leverages a poisoned external knowledge base to\ninteract with large language models, thereby causing the large models to\ngenerate malicious non-compliant dialogues.We tested this method on six\ndifferent large language models across three major categories of jailbreak\nissues. The experiments demonstrate that PLC successfully implemented indirect\njailbreak attacks under three different scenarios, achieving success rates of\n88.56%, 79.04%, and 82.69% respectively.",
        "chunk-id": 2,
        "chunk": "are continuously evolving. One significant method type of attack is the\njailbreak attack, which designed to evade model safety mechanisms and induce\nthe generation of inappropriate content. Existing jailbreak attacks primarily\nrely on crafting inducement prompts for direct jailbreaks, which are less\neffective against large models with robust filtering and high comprehension",
        "authors": [
            "Ziqiu Wang",
            "Jun Liu",
            "Shengkai Zhang",
            "Yang Yang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-26T07:21:02+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18122v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18122v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000011,
        "doi": null,
        "title": "Poisoned LangChain: Jailbreak LLMs by LangChain",
        "abstract": "With the development of natural language processing (NLP), large language\nmodels (LLMs) are becoming increasingly popular. LLMs are integrating more into\neveryday life, raising public concerns about their security vulnerabilities.\nConsequently, the security of large language models is becoming critically\nimportant. Currently, the techniques for attacking and defending against LLMs\nare continuously evolving. One significant method type of attack is the\njailbreak attack, which designed to evade model safety mechanisms and induce\nthe generation of inappropriate content. Existing jailbreak attacks primarily\nrely on crafting inducement prompts for direct jailbreaks, which are less\neffective against large models with robust filtering and high comprehension\nabilities. Given the increasing demand for real-time capabilities in large\nlanguage models, real-time updates and iterations of new knowledge have become\nessential. Retrieval-Augmented Generation (RAG), an advanced technique to\ncompensate for the model's lack of new knowledge, is gradually becoming\nmainstream. As RAG enables the model to utilize external knowledge bases, it\nprovides a new avenue for jailbreak attacks.\n  In this paper, we conduct the first work to propose the concept of indirect\njailbreak and achieve Retrieval-Augmented Generation via LangChain. Building on\nthis, we further design a novel method of indirect jailbreak attack, termed\nPoisoned-LangChain (PLC), which leverages a poisoned external knowledge base to\ninteract with large language models, thereby causing the large models to\ngenerate malicious non-compliant dialogues.We tested this method on six\ndifferent large language models across three major categories of jailbreak\nissues. The experiments demonstrate that PLC successfully implemented indirect\njailbreak attacks under three different scenarios, achieving success rates of\n88.56%, 79.04%, and 82.69% respectively.",
        "chunk-id": 3,
        "chunk": "abilities. Given the increasing demand for real-time capabilities in large\nlanguage models, real-time updates and iterations of new knowledge have become\nessential. Retrieval-Augmented Generation (RAG), an advanced technique to\ncompensate for the model's lack of new knowledge, is gradually becoming\nmainstream. As RAG enables the model to utilize external knowledge bases, it",
        "authors": [
            "Ziqiu Wang",
            "Jun Liu",
            "Shengkai Zhang",
            "Yang Yang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-26T07:21:02+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18122v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18122v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000011,
        "doi": null,
        "title": "Poisoned LangChain: Jailbreak LLMs by LangChain",
        "abstract": "With the development of natural language processing (NLP), large language\nmodels (LLMs) are becoming increasingly popular. LLMs are integrating more into\neveryday life, raising public concerns about their security vulnerabilities.\nConsequently, the security of large language models is becoming critically\nimportant. Currently, the techniques for attacking and defending against LLMs\nare continuously evolving. One significant method type of attack is the\njailbreak attack, which designed to evade model safety mechanisms and induce\nthe generation of inappropriate content. Existing jailbreak attacks primarily\nrely on crafting inducement prompts for direct jailbreaks, which are less\neffective against large models with robust filtering and high comprehension\nabilities. Given the increasing demand for real-time capabilities in large\nlanguage models, real-time updates and iterations of new knowledge have become\nessential. Retrieval-Augmented Generation (RAG), an advanced technique to\ncompensate for the model's lack of new knowledge, is gradually becoming\nmainstream. As RAG enables the model to utilize external knowledge bases, it\nprovides a new avenue for jailbreak attacks.\n  In this paper, we conduct the first work to propose the concept of indirect\njailbreak and achieve Retrieval-Augmented Generation via LangChain. Building on\nthis, we further design a novel method of indirect jailbreak attack, termed\nPoisoned-LangChain (PLC), which leverages a poisoned external knowledge base to\ninteract with large language models, thereby causing the large models to\ngenerate malicious non-compliant dialogues.We tested this method on six\ndifferent large language models across three major categories of jailbreak\nissues. The experiments demonstrate that PLC successfully implemented indirect\njailbreak attacks under three different scenarios, achieving success rates of\n88.56%, 79.04%, and 82.69% respectively.",
        "chunk-id": 4,
        "chunk": "provides a new avenue for jailbreak attacks.\n  In this paper, we conduct the first work to propose the concept of indirect\njailbreak and achieve Retrieval-Augmented Generation via LangChain. Building on\nthis, we further design a novel method of indirect jailbreak attack, termed\nPoisoned-LangChain (PLC), which leverages a poisoned external knowledge base to",
        "authors": [
            "Ziqiu Wang",
            "Jun Liu",
            "Shengkai Zhang",
            "Yang Yang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-26T07:21:02+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18122v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18122v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000011,
        "doi": null,
        "title": "Poisoned LangChain: Jailbreak LLMs by LangChain",
        "abstract": "With the development of natural language processing (NLP), large language\nmodels (LLMs) are becoming increasingly popular. LLMs are integrating more into\neveryday life, raising public concerns about their security vulnerabilities.\nConsequently, the security of large language models is becoming critically\nimportant. Currently, the techniques for attacking and defending against LLMs\nare continuously evolving. One significant method type of attack is the\njailbreak attack, which designed to evade model safety mechanisms and induce\nthe generation of inappropriate content. Existing jailbreak attacks primarily\nrely on crafting inducement prompts for direct jailbreaks, which are less\neffective against large models with robust filtering and high comprehension\nabilities. Given the increasing demand for real-time capabilities in large\nlanguage models, real-time updates and iterations of new knowledge have become\nessential. Retrieval-Augmented Generation (RAG), an advanced technique to\ncompensate for the model's lack of new knowledge, is gradually becoming\nmainstream. As RAG enables the model to utilize external knowledge bases, it\nprovides a new avenue for jailbreak attacks.\n  In this paper, we conduct the first work to propose the concept of indirect\njailbreak and achieve Retrieval-Augmented Generation via LangChain. Building on\nthis, we further design a novel method of indirect jailbreak attack, termed\nPoisoned-LangChain (PLC), which leverages a poisoned external knowledge base to\ninteract with large language models, thereby causing the large models to\ngenerate malicious non-compliant dialogues.We tested this method on six\ndifferent large language models across three major categories of jailbreak\nissues. The experiments demonstrate that PLC successfully implemented indirect\njailbreak attacks under three different scenarios, achieving success rates of\n88.56%, 79.04%, and 82.69% respectively.",
        "chunk-id": 5,
        "chunk": "interact with large language models, thereby causing the large models to\ngenerate malicious non-compliant dialogues.We tested this method on six\ndifferent large language models across three major categories of jailbreak\nissues. The experiments demonstrate that PLC successfully implemented indirect\njailbreak attacks under three different scenarios, achieving success rates of",
        "authors": [
            "Ziqiu Wang",
            "Jun Liu",
            "Shengkai Zhang",
            "Yang Yang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-26T07:21:02+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18122v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18122v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000011,
        "doi": null,
        "title": "Poisoned LangChain: Jailbreak LLMs by LangChain",
        "abstract": "With the development of natural language processing (NLP), large language\nmodels (LLMs) are becoming increasingly popular. LLMs are integrating more into\neveryday life, raising public concerns about their security vulnerabilities.\nConsequently, the security of large language models is becoming critically\nimportant. Currently, the techniques for attacking and defending against LLMs\nare continuously evolving. One significant method type of attack is the\njailbreak attack, which designed to evade model safety mechanisms and induce\nthe generation of inappropriate content. Existing jailbreak attacks primarily\nrely on crafting inducement prompts for direct jailbreaks, which are less\neffective against large models with robust filtering and high comprehension\nabilities. Given the increasing demand for real-time capabilities in large\nlanguage models, real-time updates and iterations of new knowledge have become\nessential. Retrieval-Augmented Generation (RAG), an advanced technique to\ncompensate for the model's lack of new knowledge, is gradually becoming\nmainstream. As RAG enables the model to utilize external knowledge bases, it\nprovides a new avenue for jailbreak attacks.\n  In this paper, we conduct the first work to propose the concept of indirect\njailbreak and achieve Retrieval-Augmented Generation via LangChain. Building on\nthis, we further design a novel method of indirect jailbreak attack, termed\nPoisoned-LangChain (PLC), which leverages a poisoned external knowledge base to\ninteract with large language models, thereby causing the large models to\ngenerate malicious non-compliant dialogues.We tested this method on six\ndifferent large language models across three major categories of jailbreak\nissues. The experiments demonstrate that PLC successfully implemented indirect\njailbreak attacks under three different scenarios, achieving success rates of\n88.56%, 79.04%, and 82.69% respectively.",
        "chunk-id": 6,
        "chunk": "88.56%, 79.04%, and 82.69% respectively.",
        "authors": [
            "Ziqiu Wang",
            "Jun Liu",
            "Shengkai Zhang",
            "Yang Yang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-26T07:21:02+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18122v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18122v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000012,
        "doi": null,
        "title": "LLM-Driven Multimodal Opinion Expression Identification",
        "abstract": "Opinion Expression Identification (OEI) is essential in NLP for applications\nranging from voice assistants to depression diagnosis. This study extends OEI\nto encompass multimodal inputs, underlining the significance of auditory cues\nin delivering emotional subtleties beyond the capabilities of text. We\nintroduce a novel multimodal OEI (MOEI) task, integrating text and speech to\nmirror real-world scenarios. Utilizing CMU MOSEI and IEMOCAP datasets, we\nconstruct the CI-MOEI dataset. Additionally, Text-to-Speech (TTS) technology is\napplied to the MPQA dataset to obtain the CIM-OEI dataset. We design a template\nfor the OEI task to take full advantage of the generative power of large\nlanguage models (LLMs). Advancing further, we propose an LLM-driven method\nSTOEI, which combines speech and text modal to identify opinion expressions.\nOur experiments demonstrate that MOEI significantly improves the performance\nwhile our method outperforms existing methods by 9.20\\% and obtains SOTA\nresults.",
        "chunk-id": 1,
        "chunk": "Opinion Expression Identification (OEI) is essential in NLP for applications\nranging from voice assistants to depression diagnosis. This study extends OEI\nto encompass multimodal inputs, underlining the significance of auditory cues\nin delivering emotional subtleties beyond the capabilities of text. We\nintroduce a novel multimodal OEI (MOEI) task, integrating text and speech to",
        "authors": [
            "Bonian Jia",
            "Huiyao Chen",
            "Yueheng Sun",
            "Meishan Zhang",
            "Min Zhang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-26T05:52:47+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18088v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18088v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Sound",
            "Audio and Speech Processing"
        ]
    },
    {
        "id": 20000012,
        "doi": null,
        "title": "LLM-Driven Multimodal Opinion Expression Identification",
        "abstract": "Opinion Expression Identification (OEI) is essential in NLP for applications\nranging from voice assistants to depression diagnosis. This study extends OEI\nto encompass multimodal inputs, underlining the significance of auditory cues\nin delivering emotional subtleties beyond the capabilities of text. We\nintroduce a novel multimodal OEI (MOEI) task, integrating text and speech to\nmirror real-world scenarios. Utilizing CMU MOSEI and IEMOCAP datasets, we\nconstruct the CI-MOEI dataset. Additionally, Text-to-Speech (TTS) technology is\napplied to the MPQA dataset to obtain the CIM-OEI dataset. We design a template\nfor the OEI task to take full advantage of the generative power of large\nlanguage models (LLMs). Advancing further, we propose an LLM-driven method\nSTOEI, which combines speech and text modal to identify opinion expressions.\nOur experiments demonstrate that MOEI significantly improves the performance\nwhile our method outperforms existing methods by 9.20\\% and obtains SOTA\nresults.",
        "chunk-id": 2,
        "chunk": "mirror real-world scenarios. Utilizing CMU MOSEI and IEMOCAP datasets, we\nconstruct the CI-MOEI dataset. Additionally, Text-to-Speech (TTS) technology is\napplied to the MPQA dataset to obtain the CIM-OEI dataset. We design a template\nfor the OEI task to take full advantage of the generative power of large\nlanguage models (LLMs). Advancing further, we propose an LLM-driven method",
        "authors": [
            "Bonian Jia",
            "Huiyao Chen",
            "Yueheng Sun",
            "Meishan Zhang",
            "Min Zhang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-26T05:52:47+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18088v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18088v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Sound",
            "Audio and Speech Processing"
        ]
    },
    {
        "id": 20000012,
        "doi": null,
        "title": "LLM-Driven Multimodal Opinion Expression Identification",
        "abstract": "Opinion Expression Identification (OEI) is essential in NLP for applications\nranging from voice assistants to depression diagnosis. This study extends OEI\nto encompass multimodal inputs, underlining the significance of auditory cues\nin delivering emotional subtleties beyond the capabilities of text. We\nintroduce a novel multimodal OEI (MOEI) task, integrating text and speech to\nmirror real-world scenarios. Utilizing CMU MOSEI and IEMOCAP datasets, we\nconstruct the CI-MOEI dataset. Additionally, Text-to-Speech (TTS) technology is\napplied to the MPQA dataset to obtain the CIM-OEI dataset. We design a template\nfor the OEI task to take full advantage of the generative power of large\nlanguage models (LLMs). Advancing further, we propose an LLM-driven method\nSTOEI, which combines speech and text modal to identify opinion expressions.\nOur experiments demonstrate that MOEI significantly improves the performance\nwhile our method outperforms existing methods by 9.20\\% and obtains SOTA\nresults.",
        "chunk-id": 3,
        "chunk": "STOEI, which combines speech and text modal to identify opinion expressions.\nOur experiments demonstrate that MOEI significantly improves the performance\nwhile our method outperforms existing methods by 9.20\\% and obtains SOTA\nresults.",
        "authors": [
            "Bonian Jia",
            "Huiyao Chen",
            "Yueheng Sun",
            "Meishan Zhang",
            "Min Zhang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-26T05:52:47+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18088v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18088v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Sound",
            "Audio and Speech Processing"
        ]
    },
    {
        "id": 20000013,
        "doi": null,
        "title": "PharmGPT: Domain-Specific Large Language Models for Bio-Pharmaceutical and Chemistry",
        "abstract": "Large language models (LLMs) have revolutionized Natural Language Processing\n(NLP) by by minimizing the need for complex feature engineering. However, the\napplication of LLMs in specialized domains like biopharmaceuticals and\nchemistry remains largely unexplored. These fields are characterized by\nintricate terminologies, specialized knowledge, and a high demand for precision\nareas where general purpose LLMs often fall short. In this study, we introduce\nPharmGPT, a suite of multilingual LLMs with 13 billion and 70 billion\nparameters, specifically trained on a comprehensive corpus of hundreds of\nbillions of tokens tailored to the Bio-Pharmaceutical and Chemical sectors. Our\nevaluation shows that PharmGPT matches or surpasses existing general models on\nkey benchmarks, such as NAPLEX, demonstrating its exceptional capability in\ndomain-specific tasks. This advancement establishes a new benchmark for LLMs in\nthe Bio-Pharmaceutical and Chemical fields, addressing the existing gap in\nspecialized language modeling. Furthermore, this suggests a promising path for\nenhanced research and development in these specialized areas, paving the way\nfor more precise and effective applications of NLP in specialized domains.",
        "chunk-id": 1,
        "chunk": "Large language models (LLMs) have revolutionized Natural Language Processing\n(NLP) by by minimizing the need for complex feature engineering. However, the\napplication of LLMs in specialized domains like biopharmaceuticals and\nchemistry remains largely unexplored. These fields are characterized by\nintricate terminologies, specialized knowledge, and a high demand for precision",
        "authors": [
            "Linqing Chen",
            "Weilei Wang",
            "Zilong Bai",
            "Peng Xu",
            "Yan Fang",
            "Jie Fang",
            "Wentao Wu",
            "Lizhi Zhou",
            "Ruiji Zhang",
            "Yubin Xia",
            "Chaobo Xu",
            "Ran Hu",
            "Licong Xu",
            "Qijun Cai",
            "Haoran Hua",
            "Jing Sun",
            "Jin Liu",
            "Tian Qiu",
            "Haowen Liu",
            "Meng Hu",
            "Xiuwen Li",
            "Fei Gao",
            "Yufu Wang",
            "Lin Tie",
            "Chaochao Wang",
            "Jianping Lu",
            "Cheng Sun",
            "Yixin Wang",
            "Shengjie Yang",
            "Yuancheng Li",
            "Lu Jin",
            "Lisha Zhang",
            "Fu Bian",
            "Changyang Tu"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-26T03:43:09+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18045v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18045v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000013,
        "doi": null,
        "title": "PharmGPT: Domain-Specific Large Language Models for Bio-Pharmaceutical and Chemistry",
        "abstract": "Large language models (LLMs) have revolutionized Natural Language Processing\n(NLP) by by minimizing the need for complex feature engineering. However, the\napplication of LLMs in specialized domains like biopharmaceuticals and\nchemistry remains largely unexplored. These fields are characterized by\nintricate terminologies, specialized knowledge, and a high demand for precision\nareas where general purpose LLMs often fall short. In this study, we introduce\nPharmGPT, a suite of multilingual LLMs with 13 billion and 70 billion\nparameters, specifically trained on a comprehensive corpus of hundreds of\nbillions of tokens tailored to the Bio-Pharmaceutical and Chemical sectors. Our\nevaluation shows that PharmGPT matches or surpasses existing general models on\nkey benchmarks, such as NAPLEX, demonstrating its exceptional capability in\ndomain-specific tasks. This advancement establishes a new benchmark for LLMs in\nthe Bio-Pharmaceutical and Chemical fields, addressing the existing gap in\nspecialized language modeling. Furthermore, this suggests a promising path for\nenhanced research and development in these specialized areas, paving the way\nfor more precise and effective applications of NLP in specialized domains.",
        "chunk-id": 2,
        "chunk": "areas where general purpose LLMs often fall short. In this study, we introduce\nPharmGPT, a suite of multilingual LLMs with 13 billion and 70 billion\nparameters, specifically trained on a comprehensive corpus of hundreds of\nbillions of tokens tailored to the Bio-Pharmaceutical and Chemical sectors. Our\nevaluation shows that PharmGPT matches or surpasses existing general models on",
        "authors": [
            "Linqing Chen",
            "Weilei Wang",
            "Zilong Bai",
            "Peng Xu",
            "Yan Fang",
            "Jie Fang",
            "Wentao Wu",
            "Lizhi Zhou",
            "Ruiji Zhang",
            "Yubin Xia",
            "Chaobo Xu",
            "Ran Hu",
            "Licong Xu",
            "Qijun Cai",
            "Haoran Hua",
            "Jing Sun",
            "Jin Liu",
            "Tian Qiu",
            "Haowen Liu",
            "Meng Hu",
            "Xiuwen Li",
            "Fei Gao",
            "Yufu Wang",
            "Lin Tie",
            "Chaochao Wang",
            "Jianping Lu",
            "Cheng Sun",
            "Yixin Wang",
            "Shengjie Yang",
            "Yuancheng Li",
            "Lu Jin",
            "Lisha Zhang",
            "Fu Bian",
            "Changyang Tu"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-26T03:43:09+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18045v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18045v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000013,
        "doi": null,
        "title": "PharmGPT: Domain-Specific Large Language Models for Bio-Pharmaceutical and Chemistry",
        "abstract": "Large language models (LLMs) have revolutionized Natural Language Processing\n(NLP) by by minimizing the need for complex feature engineering. However, the\napplication of LLMs in specialized domains like biopharmaceuticals and\nchemistry remains largely unexplored. These fields are characterized by\nintricate terminologies, specialized knowledge, and a high demand for precision\nareas where general purpose LLMs often fall short. In this study, we introduce\nPharmGPT, a suite of multilingual LLMs with 13 billion and 70 billion\nparameters, specifically trained on a comprehensive corpus of hundreds of\nbillions of tokens tailored to the Bio-Pharmaceutical and Chemical sectors. Our\nevaluation shows that PharmGPT matches or surpasses existing general models on\nkey benchmarks, such as NAPLEX, demonstrating its exceptional capability in\ndomain-specific tasks. This advancement establishes a new benchmark for LLMs in\nthe Bio-Pharmaceutical and Chemical fields, addressing the existing gap in\nspecialized language modeling. Furthermore, this suggests a promising path for\nenhanced research and development in these specialized areas, paving the way\nfor more precise and effective applications of NLP in specialized domains.",
        "chunk-id": 3,
        "chunk": "key benchmarks, such as NAPLEX, demonstrating its exceptional capability in\ndomain-specific tasks. This advancement establishes a new benchmark for LLMs in\nthe Bio-Pharmaceutical and Chemical fields, addressing the existing gap in\nspecialized language modeling. Furthermore, this suggests a promising path for",
        "authors": [
            "Linqing Chen",
            "Weilei Wang",
            "Zilong Bai",
            "Peng Xu",
            "Yan Fang",
            "Jie Fang",
            "Wentao Wu",
            "Lizhi Zhou",
            "Ruiji Zhang",
            "Yubin Xia",
            "Chaobo Xu",
            "Ran Hu",
            "Licong Xu",
            "Qijun Cai",
            "Haoran Hua",
            "Jing Sun",
            "Jin Liu",
            "Tian Qiu",
            "Haowen Liu",
            "Meng Hu",
            "Xiuwen Li",
            "Fei Gao",
            "Yufu Wang",
            "Lin Tie",
            "Chaochao Wang",
            "Jianping Lu",
            "Cheng Sun",
            "Yixin Wang",
            "Shengjie Yang",
            "Yuancheng Li",
            "Lu Jin",
            "Lisha Zhang",
            "Fu Bian",
            "Changyang Tu"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-26T03:43:09+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18045v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18045v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000013,
        "doi": null,
        "title": "PharmGPT: Domain-Specific Large Language Models for Bio-Pharmaceutical and Chemistry",
        "abstract": "Large language models (LLMs) have revolutionized Natural Language Processing\n(NLP) by by minimizing the need for complex feature engineering. However, the\napplication of LLMs in specialized domains like biopharmaceuticals and\nchemistry remains largely unexplored. These fields are characterized by\nintricate terminologies, specialized knowledge, and a high demand for precision\nareas where general purpose LLMs often fall short. In this study, we introduce\nPharmGPT, a suite of multilingual LLMs with 13 billion and 70 billion\nparameters, specifically trained on a comprehensive corpus of hundreds of\nbillions of tokens tailored to the Bio-Pharmaceutical and Chemical sectors. Our\nevaluation shows that PharmGPT matches or surpasses existing general models on\nkey benchmarks, such as NAPLEX, demonstrating its exceptional capability in\ndomain-specific tasks. This advancement establishes a new benchmark for LLMs in\nthe Bio-Pharmaceutical and Chemical fields, addressing the existing gap in\nspecialized language modeling. Furthermore, this suggests a promising path for\nenhanced research and development in these specialized areas, paving the way\nfor more precise and effective applications of NLP in specialized domains.",
        "chunk-id": 4,
        "chunk": "enhanced research and development in these specialized areas, paving the way\nfor more precise and effective applications of NLP in specialized domains.",
        "authors": [
            "Linqing Chen",
            "Weilei Wang",
            "Zilong Bai",
            "Peng Xu",
            "Yan Fang",
            "Jie Fang",
            "Wentao Wu",
            "Lizhi Zhou",
            "Ruiji Zhang",
            "Yubin Xia",
            "Chaobo Xu",
            "Ran Hu",
            "Licong Xu",
            "Qijun Cai",
            "Haoran Hua",
            "Jing Sun",
            "Jin Liu",
            "Tian Qiu",
            "Haowen Liu",
            "Meng Hu",
            "Xiuwen Li",
            "Fei Gao",
            "Yufu Wang",
            "Lin Tie",
            "Chaochao Wang",
            "Jianping Lu",
            "Cheng Sun",
            "Yixin Wang",
            "Shengjie Yang",
            "Yuancheng Li",
            "Lu Jin",
            "Lisha Zhang",
            "Fu Bian",
            "Changyang Tu"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-26T03:43:09+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.18045v1",
        "arxiv_link": "http://arxiv.org/abs/2406.18045v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000014,
        "doi": null,
        "title": "PAFT: A Parallel Training Paradigm for Effective LLM Fine-Tuning",
        "abstract": "Large language models (LLMs) have shown remarkable abilities in diverse\nnatural language processing (NLP) tasks. The LLMs generally undergo supervised\nfine-tuning (SFT) followed by preference alignment to be usable in downstream\napplications. However, this sequential training pipeline leads to alignment tax\nthat degrades the LLM performance.\n  This paper introduces PAFT, a new PArallel training paradigm for effective\nLLM Fine-Tuning, which independently performs SFT and preference alignment\n(e.g., DPO and ORPO, etc.) with the same pre-trained model on respective\ndatasets. The model produced by SFT and the model from preference alignment are\nthen merged into a final model by parameter fusing for use in downstream\napplications. This work reveals important findings that preference alignment\nlike DPO naturally results in a sparse model while SFT leads to a natural dense\nmodel which needs to be sparsified for effective model merging. This paper\nintroduces an effective interference resolution which reduces the redundancy by\nsparsifying the delta parameters. The LLM resulted from the new training\nparadigm achieved Rank #1 on the HuggingFace Open LLM Leaderboard.\nComprehensive evaluation shows the effectiveness of the parallel training\nparadigm.",
        "chunk-id": 1,
        "chunk": "Large language models (LLMs) have shown remarkable abilities in diverse\nnatural language processing (NLP) tasks. The LLMs generally undergo supervised\nfine-tuning (SFT) followed by preference alignment to be usable in downstream\napplications. However, this sequential training pipeline leads to alignment tax\nthat degrades the LLM performance.",
        "authors": [
            "Shiva Kumar Pentyala",
            "Zhichao Wang",
            "Bin Bi",
            "Kiran Ramnath",
            "Xiang-Bo Mao",
            "Regunathan Radhakrishnan",
            "Sitaram Asur",
            "Na",
            "Cheng"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-25T20:11:37+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.17923v1",
        "arxiv_link": "http://arxiv.org/abs/2406.17923v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000014,
        "doi": null,
        "title": "PAFT: A Parallel Training Paradigm for Effective LLM Fine-Tuning",
        "abstract": "Large language models (LLMs) have shown remarkable abilities in diverse\nnatural language processing (NLP) tasks. The LLMs generally undergo supervised\nfine-tuning (SFT) followed by preference alignment to be usable in downstream\napplications. However, this sequential training pipeline leads to alignment tax\nthat degrades the LLM performance.\n  This paper introduces PAFT, a new PArallel training paradigm for effective\nLLM Fine-Tuning, which independently performs SFT and preference alignment\n(e.g., DPO and ORPO, etc.) with the same pre-trained model on respective\ndatasets. The model produced by SFT and the model from preference alignment are\nthen merged into a final model by parameter fusing for use in downstream\napplications. This work reveals important findings that preference alignment\nlike DPO naturally results in a sparse model while SFT leads to a natural dense\nmodel which needs to be sparsified for effective model merging. This paper\nintroduces an effective interference resolution which reduces the redundancy by\nsparsifying the delta parameters. The LLM resulted from the new training\nparadigm achieved Rank #1 on the HuggingFace Open LLM Leaderboard.\nComprehensive evaluation shows the effectiveness of the parallel training\nparadigm.",
        "chunk-id": 2,
        "chunk": "This paper introduces PAFT, a new PArallel training paradigm for effective\nLLM Fine-Tuning, which independently performs SFT and preference alignment\n(e.g., DPO and ORPO, etc.) with the same pre-trained model on respective\ndatasets. The model produced by SFT and the model from preference alignment are\nthen merged into a final model by parameter fusing for use in downstream",
        "authors": [
            "Shiva Kumar Pentyala",
            "Zhichao Wang",
            "Bin Bi",
            "Kiran Ramnath",
            "Xiang-Bo Mao",
            "Regunathan Radhakrishnan",
            "Sitaram Asur",
            "Na",
            "Cheng"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-25T20:11:37+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.17923v1",
        "arxiv_link": "http://arxiv.org/abs/2406.17923v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000014,
        "doi": null,
        "title": "PAFT: A Parallel Training Paradigm for Effective LLM Fine-Tuning",
        "abstract": "Large language models (LLMs) have shown remarkable abilities in diverse\nnatural language processing (NLP) tasks. The LLMs generally undergo supervised\nfine-tuning (SFT) followed by preference alignment to be usable in downstream\napplications. However, this sequential training pipeline leads to alignment tax\nthat degrades the LLM performance.\n  This paper introduces PAFT, a new PArallel training paradigm for effective\nLLM Fine-Tuning, which independently performs SFT and preference alignment\n(e.g., DPO and ORPO, etc.) with the same pre-trained model on respective\ndatasets. The model produced by SFT and the model from preference alignment are\nthen merged into a final model by parameter fusing for use in downstream\napplications. This work reveals important findings that preference alignment\nlike DPO naturally results in a sparse model while SFT leads to a natural dense\nmodel which needs to be sparsified for effective model merging. This paper\nintroduces an effective interference resolution which reduces the redundancy by\nsparsifying the delta parameters. The LLM resulted from the new training\nparadigm achieved Rank #1 on the HuggingFace Open LLM Leaderboard.\nComprehensive evaluation shows the effectiveness of the parallel training\nparadigm.",
        "chunk-id": 3,
        "chunk": "applications. This work reveals important findings that preference alignment\nlike DPO naturally results in a sparse model while SFT leads to a natural dense\nmodel which needs to be sparsified for effective model merging. This paper\nintroduces an effective interference resolution which reduces the redundancy by\nsparsifying the delta parameters. The LLM resulted from the new training",
        "authors": [
            "Shiva Kumar Pentyala",
            "Zhichao Wang",
            "Bin Bi",
            "Kiran Ramnath",
            "Xiang-Bo Mao",
            "Regunathan Radhakrishnan",
            "Sitaram Asur",
            "Na",
            "Cheng"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-25T20:11:37+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.17923v1",
        "arxiv_link": "http://arxiv.org/abs/2406.17923v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000014,
        "doi": null,
        "title": "PAFT: A Parallel Training Paradigm for Effective LLM Fine-Tuning",
        "abstract": "Large language models (LLMs) have shown remarkable abilities in diverse\nnatural language processing (NLP) tasks. The LLMs generally undergo supervised\nfine-tuning (SFT) followed by preference alignment to be usable in downstream\napplications. However, this sequential training pipeline leads to alignment tax\nthat degrades the LLM performance.\n  This paper introduces PAFT, a new PArallel training paradigm for effective\nLLM Fine-Tuning, which independently performs SFT and preference alignment\n(e.g., DPO and ORPO, etc.) with the same pre-trained model on respective\ndatasets. The model produced by SFT and the model from preference alignment are\nthen merged into a final model by parameter fusing for use in downstream\napplications. This work reveals important findings that preference alignment\nlike DPO naturally results in a sparse model while SFT leads to a natural dense\nmodel which needs to be sparsified for effective model merging. This paper\nintroduces an effective interference resolution which reduces the redundancy by\nsparsifying the delta parameters. The LLM resulted from the new training\nparadigm achieved Rank #1 on the HuggingFace Open LLM Leaderboard.\nComprehensive evaluation shows the effectiveness of the parallel training\nparadigm.",
        "chunk-id": 4,
        "chunk": "paradigm achieved Rank #1 on the HuggingFace Open LLM Leaderboard.\nComprehensive evaluation shows the effectiveness of the parallel training\nparadigm.",
        "authors": [
            "Shiva Kumar Pentyala",
            "Zhichao Wang",
            "Bin Bi",
            "Kiran Ramnath",
            "Xiang-Bo Mao",
            "Regunathan Radhakrishnan",
            "Sitaram Asur",
            "Na",
            "Cheng"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-25T20:11:37+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.17923v1",
        "arxiv_link": "http://arxiv.org/abs/2406.17923v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000015,
        "doi": null,
        "title": "Cloaked Classifiers: Pseudonymization Strategies on Sensitive Classification Tasks",
        "abstract": "Protecting privacy is essential when sharing data, particularly in the case\nof an online radicalization dataset that may contain personal information. In\nthis paper, we explore the balance between preserving data usefulness and\nensuring robust privacy safeguards, since regulations like the European GDPR\nshape how personal information must be handled. We share our method for\nmanually pseudonymizing a multilingual radicalization dataset, ensuring\nperformance comparable to the original data. Furthermore, we highlight the\nimportance of establishing comprehensive guidelines for processing sensitive\nNLP data by sharing our complete pseudonymization process, our guidelines, the\nchallenges we encountered as well as the resulting dataset.",
        "chunk-id": 1,
        "chunk": "Protecting privacy is essential when sharing data, particularly in the case\nof an online radicalization dataset that may contain personal information. In\nthis paper, we explore the balance between preserving data usefulness and\nensuring robust privacy safeguards, since regulations like the European GDPR\nshape how personal information must be handled. We share our method for",
        "authors": [
            "Arij Riabi",
            "Menel Mahamdi",
            "Virginie Mouilleron",
            "Djam\u00e9 Seddah"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-25T18:30:25+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.17875v1",
        "arxiv_link": "http://arxiv.org/abs/2406.17875v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000015,
        "doi": null,
        "title": "Cloaked Classifiers: Pseudonymization Strategies on Sensitive Classification Tasks",
        "abstract": "Protecting privacy is essential when sharing data, particularly in the case\nof an online radicalization dataset that may contain personal information. In\nthis paper, we explore the balance between preserving data usefulness and\nensuring robust privacy safeguards, since regulations like the European GDPR\nshape how personal information must be handled. We share our method for\nmanually pseudonymizing a multilingual radicalization dataset, ensuring\nperformance comparable to the original data. Furthermore, we highlight the\nimportance of establishing comprehensive guidelines for processing sensitive\nNLP data by sharing our complete pseudonymization process, our guidelines, the\nchallenges we encountered as well as the resulting dataset.",
        "chunk-id": 2,
        "chunk": "manually pseudonymizing a multilingual radicalization dataset, ensuring\nperformance comparable to the original data. Furthermore, we highlight the\nimportance of establishing comprehensive guidelines for processing sensitive\nNLP data by sharing our complete pseudonymization process, our guidelines, the\nchallenges we encountered as well as the resulting dataset.",
        "authors": [
            "Arij Riabi",
            "Menel Mahamdi",
            "Virginie Mouilleron",
            "Djam\u00e9 Seddah"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-25T18:30:25+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.17875v1",
        "arxiv_link": "http://arxiv.org/abs/2406.17875v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000016,
        "doi": null,
        "title": "ViANLI: Adversarial Natural Language Inference for Vietnamese",
        "abstract": "The development of Natural Language Processing (NLI) datasets and models has\nbeen inspired by innovations in annotation design. With the rapid development\nof machine learning models today, the performance of existing machine learning\nmodels has quickly reached state-of-the-art results on a variety of tasks\nrelated to natural language processing, including natural language inference\ntasks. By using a pre-trained model during the annotation process, it is\npossible to challenge current NLI models by having humans produce\npremise-hypothesis combinations that the machine model cannot correctly\npredict. To remain attractive and challenging in the research of natural\nlanguage inference for Vietnamese, in this paper, we introduce the adversarial\nNLI dataset to the NLP research community with the name ViANLI. This data set\ncontains more than 10K premise-hypothesis pairs and is built by a continuously\nadjusting process to obtain the most out of the patterns generated by the\nannotators. ViANLI dataset has brought many difficulties to many current SOTA\nmodels when the accuracy of the most powerful model on the test set only\nreached 48.4%. Additionally, the experimental results show that the models\ntrained on our dataset have significantly improved the results on other\nVietnamese NLI datasets.",
        "chunk-id": 1,
        "chunk": "The development of Natural Language Processing (NLI) datasets and models has\nbeen inspired by innovations in annotation design. With the rapid development\nof machine learning models today, the performance of existing machine learning\nmodels has quickly reached state-of-the-art results on a variety of tasks\nrelated to natural language processing, including natural language inference",
        "authors": [
            "Tin Van Huynh",
            "Kiet Van Nguyen",
            "Ngan Luu-Thuy Nguyen"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-25T16:58:19+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.17716v1",
        "arxiv_link": "http://arxiv.org/abs/2406.17716v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000016,
        "doi": null,
        "title": "ViANLI: Adversarial Natural Language Inference for Vietnamese",
        "abstract": "The development of Natural Language Processing (NLI) datasets and models has\nbeen inspired by innovations in annotation design. With the rapid development\nof machine learning models today, the performance of existing machine learning\nmodels has quickly reached state-of-the-art results on a variety of tasks\nrelated to natural language processing, including natural language inference\ntasks. By using a pre-trained model during the annotation process, it is\npossible to challenge current NLI models by having humans produce\npremise-hypothesis combinations that the machine model cannot correctly\npredict. To remain attractive and challenging in the research of natural\nlanguage inference for Vietnamese, in this paper, we introduce the adversarial\nNLI dataset to the NLP research community with the name ViANLI. This data set\ncontains more than 10K premise-hypothesis pairs and is built by a continuously\nadjusting process to obtain the most out of the patterns generated by the\nannotators. ViANLI dataset has brought many difficulties to many current SOTA\nmodels when the accuracy of the most powerful model on the test set only\nreached 48.4%. Additionally, the experimental results show that the models\ntrained on our dataset have significantly improved the results on other\nVietnamese NLI datasets.",
        "chunk-id": 2,
        "chunk": "tasks. By using a pre-trained model during the annotation process, it is\npossible to challenge current NLI models by having humans produce\npremise-hypothesis combinations that the machine model cannot correctly\npredict. To remain attractive and challenging in the research of natural\nlanguage inference for Vietnamese, in this paper, we introduce the adversarial",
        "authors": [
            "Tin Van Huynh",
            "Kiet Van Nguyen",
            "Ngan Luu-Thuy Nguyen"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-25T16:58:19+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.17716v1",
        "arxiv_link": "http://arxiv.org/abs/2406.17716v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000016,
        "doi": null,
        "title": "ViANLI: Adversarial Natural Language Inference for Vietnamese",
        "abstract": "The development of Natural Language Processing (NLI) datasets and models has\nbeen inspired by innovations in annotation design. With the rapid development\nof machine learning models today, the performance of existing machine learning\nmodels has quickly reached state-of-the-art results on a variety of tasks\nrelated to natural language processing, including natural language inference\ntasks. By using a pre-trained model during the annotation process, it is\npossible to challenge current NLI models by having humans produce\npremise-hypothesis combinations that the machine model cannot correctly\npredict. To remain attractive and challenging in the research of natural\nlanguage inference for Vietnamese, in this paper, we introduce the adversarial\nNLI dataset to the NLP research community with the name ViANLI. This data set\ncontains more than 10K premise-hypothesis pairs and is built by a continuously\nadjusting process to obtain the most out of the patterns generated by the\nannotators. ViANLI dataset has brought many difficulties to many current SOTA\nmodels when the accuracy of the most powerful model on the test set only\nreached 48.4%. Additionally, the experimental results show that the models\ntrained on our dataset have significantly improved the results on other\nVietnamese NLI datasets.",
        "chunk-id": 3,
        "chunk": "NLI dataset to the NLP research community with the name ViANLI. This data set\ncontains more than 10K premise-hypothesis pairs and is built by a continuously\nadjusting process to obtain the most out of the patterns generated by the\nannotators. ViANLI dataset has brought many difficulties to many current SOTA\nmodels when the accuracy of the most powerful model on the test set only",
        "authors": [
            "Tin Van Huynh",
            "Kiet Van Nguyen",
            "Ngan Luu-Thuy Nguyen"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-25T16:58:19+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.17716v1",
        "arxiv_link": "http://arxiv.org/abs/2406.17716v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000016,
        "doi": null,
        "title": "ViANLI: Adversarial Natural Language Inference for Vietnamese",
        "abstract": "The development of Natural Language Processing (NLI) datasets and models has\nbeen inspired by innovations in annotation design. With the rapid development\nof machine learning models today, the performance of existing machine learning\nmodels has quickly reached state-of-the-art results on a variety of tasks\nrelated to natural language processing, including natural language inference\ntasks. By using a pre-trained model during the annotation process, it is\npossible to challenge current NLI models by having humans produce\npremise-hypothesis combinations that the machine model cannot correctly\npredict. To remain attractive and challenging in the research of natural\nlanguage inference for Vietnamese, in this paper, we introduce the adversarial\nNLI dataset to the NLP research community with the name ViANLI. This data set\ncontains more than 10K premise-hypothesis pairs and is built by a continuously\nadjusting process to obtain the most out of the patterns generated by the\nannotators. ViANLI dataset has brought many difficulties to many current SOTA\nmodels when the accuracy of the most powerful model on the test set only\nreached 48.4%. Additionally, the experimental results show that the models\ntrained on our dataset have significantly improved the results on other\nVietnamese NLI datasets.",
        "chunk-id": 4,
        "chunk": "reached 48.4%. Additionally, the experimental results show that the models\ntrained on our dataset have significantly improved the results on other\nVietnamese NLI datasets.",
        "authors": [
            "Tin Van Huynh",
            "Kiet Van Nguyen",
            "Ngan Luu-Thuy Nguyen"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-25T16:58:19+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.17716v1",
        "arxiv_link": "http://arxiv.org/abs/2406.17716v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000017,
        "doi": null,
        "title": "Variationist: Exploring Multifaceted Variation and Bias in Written Language Data",
        "abstract": "Exploring and understanding language data is a fundamental stage in all areas\ndealing with human language. It allows NLP practitioners to uncover quality\nconcerns and harmful biases in data before training, and helps linguists and\nsocial scientists to gain insight into language use and human behavior. Yet,\nthere is currently a lack of a unified, customizable tool to seamlessly inspect\nand visualize language variation and bias across multiple variables, language\nunits, and diverse metrics that go beyond descriptive statistics. In this\npaper, we introduce Variationist, a highly-modular, extensible, and\ntask-agnostic tool that fills this gap. Variationist handles at once a\npotentially unlimited combination of variable types and semantics across\ndiversity and association metrics with regards to the language unit of choice,\nand orchestrates the creation of up to five-dimensional interactive charts for\nover 30 variable type-semantics combinations. Through our case studies on\ncomputational dialectology, human label variation, and text generation, we show\nhow Variationist enables researchers from different disciplines to effortlessly\nanswer specific research questions or unveil undesired associations in language\ndata. A Python library, code, documentation, and tutorials are made publicly\navailable to the research community.",
        "chunk-id": 1,
        "chunk": "Exploring and understanding language data is a fundamental stage in all areas\ndealing with human language. It allows NLP practitioners to uncover quality\nconcerns and harmful biases in data before training, and helps linguists and\nsocial scientists to gain insight into language use and human behavior. Yet,",
        "authors": [
            "Alan Ramponi",
            "Camilla Casula",
            "Stefano Menini"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-25T15:41:07+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.17647v1",
        "arxiv_link": "http://arxiv.org/abs/2406.17647v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000017,
        "doi": null,
        "title": "Variationist: Exploring Multifaceted Variation and Bias in Written Language Data",
        "abstract": "Exploring and understanding language data is a fundamental stage in all areas\ndealing with human language. It allows NLP practitioners to uncover quality\nconcerns and harmful biases in data before training, and helps linguists and\nsocial scientists to gain insight into language use and human behavior. Yet,\nthere is currently a lack of a unified, customizable tool to seamlessly inspect\nand visualize language variation and bias across multiple variables, language\nunits, and diverse metrics that go beyond descriptive statistics. In this\npaper, we introduce Variationist, a highly-modular, extensible, and\ntask-agnostic tool that fills this gap. Variationist handles at once a\npotentially unlimited combination of variable types and semantics across\ndiversity and association metrics with regards to the language unit of choice,\nand orchestrates the creation of up to five-dimensional interactive charts for\nover 30 variable type-semantics combinations. Through our case studies on\ncomputational dialectology, human label variation, and text generation, we show\nhow Variationist enables researchers from different disciplines to effortlessly\nanswer specific research questions or unveil undesired associations in language\ndata. A Python library, code, documentation, and tutorials are made publicly\navailable to the research community.",
        "chunk-id": 2,
        "chunk": "there is currently a lack of a unified, customizable tool to seamlessly inspect\nand visualize language variation and bias across multiple variables, language\nunits, and diverse metrics that go beyond descriptive statistics. In this\npaper, we introduce Variationist, a highly-modular, extensible, and\ntask-agnostic tool that fills this gap. Variationist handles at once a",
        "authors": [
            "Alan Ramponi",
            "Camilla Casula",
            "Stefano Menini"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-25T15:41:07+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.17647v1",
        "arxiv_link": "http://arxiv.org/abs/2406.17647v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000017,
        "doi": null,
        "title": "Variationist: Exploring Multifaceted Variation and Bias in Written Language Data",
        "abstract": "Exploring and understanding language data is a fundamental stage in all areas\ndealing with human language. It allows NLP practitioners to uncover quality\nconcerns and harmful biases in data before training, and helps linguists and\nsocial scientists to gain insight into language use and human behavior. Yet,\nthere is currently a lack of a unified, customizable tool to seamlessly inspect\nand visualize language variation and bias across multiple variables, language\nunits, and diverse metrics that go beyond descriptive statistics. In this\npaper, we introduce Variationist, a highly-modular, extensible, and\ntask-agnostic tool that fills this gap. Variationist handles at once a\npotentially unlimited combination of variable types and semantics across\ndiversity and association metrics with regards to the language unit of choice,\nand orchestrates the creation of up to five-dimensional interactive charts for\nover 30 variable type-semantics combinations. Through our case studies on\ncomputational dialectology, human label variation, and text generation, we show\nhow Variationist enables researchers from different disciplines to effortlessly\nanswer specific research questions or unveil undesired associations in language\ndata. A Python library, code, documentation, and tutorials are made publicly\navailable to the research community.",
        "chunk-id": 3,
        "chunk": "potentially unlimited combination of variable types and semantics across\ndiversity and association metrics with regards to the language unit of choice,\nand orchestrates the creation of up to five-dimensional interactive charts for\nover 30 variable type-semantics combinations. Through our case studies on\ncomputational dialectology, human label variation, and text generation, we show",
        "authors": [
            "Alan Ramponi",
            "Camilla Casula",
            "Stefano Menini"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-25T15:41:07+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.17647v1",
        "arxiv_link": "http://arxiv.org/abs/2406.17647v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000017,
        "doi": null,
        "title": "Variationist: Exploring Multifaceted Variation and Bias in Written Language Data",
        "abstract": "Exploring and understanding language data is a fundamental stage in all areas\ndealing with human language. It allows NLP practitioners to uncover quality\nconcerns and harmful biases in data before training, and helps linguists and\nsocial scientists to gain insight into language use and human behavior. Yet,\nthere is currently a lack of a unified, customizable tool to seamlessly inspect\nand visualize language variation and bias across multiple variables, language\nunits, and diverse metrics that go beyond descriptive statistics. In this\npaper, we introduce Variationist, a highly-modular, extensible, and\ntask-agnostic tool that fills this gap. Variationist handles at once a\npotentially unlimited combination of variable types and semantics across\ndiversity and association metrics with regards to the language unit of choice,\nand orchestrates the creation of up to five-dimensional interactive charts for\nover 30 variable type-semantics combinations. Through our case studies on\ncomputational dialectology, human label variation, and text generation, we show\nhow Variationist enables researchers from different disciplines to effortlessly\nanswer specific research questions or unveil undesired associations in language\ndata. A Python library, code, documentation, and tutorials are made publicly\navailable to the research community.",
        "chunk-id": 4,
        "chunk": "how Variationist enables researchers from different disciplines to effortlessly\nanswer specific research questions or unveil undesired associations in language\ndata. A Python library, code, documentation, and tutorials are made publicly\navailable to the research community.",
        "authors": [
            "Alan Ramponi",
            "Camilla Casula",
            "Stefano Menini"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-25T15:41:07+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.17647v1",
        "arxiv_link": "http://arxiv.org/abs/2406.17647v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000018,
        "doi": null,
        "title": "LumberChunker: Long-Form Narrative Document Segmentation",
        "abstract": "Modern NLP tasks increasingly rely on dense retrieval methods to access\nup-to-date and relevant contextual information. We are motivated by the premise\nthat retrieval benefits from segments that can vary in size such that a\ncontent's semantic independence is better captured. We propose LumberChunker, a\nmethod leveraging an LLM to dynamically segment documents, which iteratively\nprompts the LLM to identify the point within a group of sequential passages\nwhere the content begins to shift. To evaluate our method, we introduce\nGutenQA, a benchmark with 3000 \"needle in a haystack\" type of question-answer\npairs derived from 100 public domain narrative books available on Project\nGutenberg. Our experiments show that LumberChunker not only outperforms the\nmost competitive baseline by 7.37% in retrieval performance (DCG@20) but also\nthat, when integrated into a RAG pipeline, LumberChunker proves to be more\neffective than other chunking methods and competitive baselines, such as the\nGemini 1.5M Pro. Our Code and Data are available at\nhttps://github.com/joaodsmarques/LumberChunker",
        "chunk-id": 1,
        "chunk": "Modern NLP tasks increasingly rely on dense retrieval methods to access\nup-to-date and relevant contextual information. We are motivated by the premise\nthat retrieval benefits from segments that can vary in size such that a\ncontent's semantic independence is better captured. We propose LumberChunker, a\nmethod leveraging an LLM to dynamically segment documents, which iteratively",
        "authors": [
            "Andr\u00e9 V. Duarte",
            "Jo\u00e3o Marques",
            "Miguel Gra\u00e7a",
            "Miguel Freire",
            "Lei Li",
            "Arlindo L. Oliveira"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-25T13:08:35+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.17526v1",
        "arxiv_link": "http://arxiv.org/abs/2406.17526v1",
        "categories": [
            "Computation and Language",
            "Information Retrieval",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000018,
        "doi": null,
        "title": "LumberChunker: Long-Form Narrative Document Segmentation",
        "abstract": "Modern NLP tasks increasingly rely on dense retrieval methods to access\nup-to-date and relevant contextual information. We are motivated by the premise\nthat retrieval benefits from segments that can vary in size such that a\ncontent's semantic independence is better captured. We propose LumberChunker, a\nmethod leveraging an LLM to dynamically segment documents, which iteratively\nprompts the LLM to identify the point within a group of sequential passages\nwhere the content begins to shift. To evaluate our method, we introduce\nGutenQA, a benchmark with 3000 \"needle in a haystack\" type of question-answer\npairs derived from 100 public domain narrative books available on Project\nGutenberg. Our experiments show that LumberChunker not only outperforms the\nmost competitive baseline by 7.37% in retrieval performance (DCG@20) but also\nthat, when integrated into a RAG pipeline, LumberChunker proves to be more\neffective than other chunking methods and competitive baselines, such as the\nGemini 1.5M Pro. Our Code and Data are available at\nhttps://github.com/joaodsmarques/LumberChunker",
        "chunk-id": 2,
        "chunk": "prompts the LLM to identify the point within a group of sequential passages\nwhere the content begins to shift. To evaluate our method, we introduce\nGutenQA, a benchmark with 3000 \"needle in a haystack\" type of question-answer\npairs derived from 100 public domain narrative books available on Project\nGutenberg. Our experiments show that LumberChunker not only outperforms the",
        "authors": [
            "Andr\u00e9 V. Duarte",
            "Jo\u00e3o Marques",
            "Miguel Gra\u00e7a",
            "Miguel Freire",
            "Lei Li",
            "Arlindo L. Oliveira"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-25T13:08:35+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.17526v1",
        "arxiv_link": "http://arxiv.org/abs/2406.17526v1",
        "categories": [
            "Computation and Language",
            "Information Retrieval",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000018,
        "doi": null,
        "title": "LumberChunker: Long-Form Narrative Document Segmentation",
        "abstract": "Modern NLP tasks increasingly rely on dense retrieval methods to access\nup-to-date and relevant contextual information. We are motivated by the premise\nthat retrieval benefits from segments that can vary in size such that a\ncontent's semantic independence is better captured. We propose LumberChunker, a\nmethod leveraging an LLM to dynamically segment documents, which iteratively\nprompts the LLM to identify the point within a group of sequential passages\nwhere the content begins to shift. To evaluate our method, we introduce\nGutenQA, a benchmark with 3000 \"needle in a haystack\" type of question-answer\npairs derived from 100 public domain narrative books available on Project\nGutenberg. Our experiments show that LumberChunker not only outperforms the\nmost competitive baseline by 7.37% in retrieval performance (DCG@20) but also\nthat, when integrated into a RAG pipeline, LumberChunker proves to be more\neffective than other chunking methods and competitive baselines, such as the\nGemini 1.5M Pro. Our Code and Data are available at\nhttps://github.com/joaodsmarques/LumberChunker",
        "chunk-id": 3,
        "chunk": "most competitive baseline by 7.37% in retrieval performance (DCG@20) but also\nthat, when integrated into a RAG pipeline, LumberChunker proves to be more\neffective than other chunking methods and competitive baselines, such as the\nGemini 1.5M Pro. Our Code and Data are available at\nhttps://github.com/joaodsmarques/LumberChunker",
        "authors": [
            "Andr\u00e9 V. Duarte",
            "Jo\u00e3o Marques",
            "Miguel Gra\u00e7a",
            "Miguel Freire",
            "Lei Li",
            "Arlindo L. Oliveira"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-25T13:08:35+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.17526v1",
        "arxiv_link": "http://arxiv.org/abs/2406.17526v1",
        "categories": [
            "Computation and Language",
            "Information Retrieval",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000019,
        "doi": null,
        "title": "Leveraging LLMs for Dialogue Quality Measurement",
        "abstract": "In task-oriented conversational AI evaluation, unsupervised methods poorly\ncorrelate with human judgments, and supervised approaches lack generalization.\nRecent advances in large language models (LLMs) show robust zeroshot and\nfew-shot capabilities across NLP tasks. This paper explores using LLMs for\nautomated dialogue quality evaluation, experimenting with various\nconfigurations on public and proprietary datasets. Manipulating factors such as\nmodel size, in-context examples, and selection techniques, we examine\n\"chain-of-thought\" (CoT) reasoning and label extraction procedures. Our results\nshow that (1) larger models yield more accurate dialogue labels; (2)\nalgorithmic selection of in-context examples outperforms random selection; (3)\nCoT reasoning where an LLM is asked to provide justifications before outputting\nfinal labels improves performance; and (4) fine-tuned LLMs outperform\nout-of-the-box ones. Our results indicate that LLMs that are suitably\nfine-tuned and have sufficient reasoning capabilities can be leveraged for\nautomated dialogue evaluation.",
        "chunk-id": 1,
        "chunk": "In task-oriented conversational AI evaluation, unsupervised methods poorly\ncorrelate with human judgments, and supervised approaches lack generalization.\nRecent advances in large language models (LLMs) show robust zeroshot and\nfew-shot capabilities across NLP tasks. This paper explores using LLMs for\nautomated dialogue quality evaluation, experimenting with various",
        "authors": [
            "Jinghan Jia",
            "Abi Komma",
            "Timothy Leffel",
            "Xujun Peng",
            "Ajay Nagesh",
            "Tamer Soliman",
            "Aram Galstyan",
            "Anoop Kumar"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-25T06:19:47+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.17304v1",
        "arxiv_link": "http://arxiv.org/abs/2406.17304v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000019,
        "doi": null,
        "title": "Leveraging LLMs for Dialogue Quality Measurement",
        "abstract": "In task-oriented conversational AI evaluation, unsupervised methods poorly\ncorrelate with human judgments, and supervised approaches lack generalization.\nRecent advances in large language models (LLMs) show robust zeroshot and\nfew-shot capabilities across NLP tasks. This paper explores using LLMs for\nautomated dialogue quality evaluation, experimenting with various\nconfigurations on public and proprietary datasets. Manipulating factors such as\nmodel size, in-context examples, and selection techniques, we examine\n\"chain-of-thought\" (CoT) reasoning and label extraction procedures. Our results\nshow that (1) larger models yield more accurate dialogue labels; (2)\nalgorithmic selection of in-context examples outperforms random selection; (3)\nCoT reasoning where an LLM is asked to provide justifications before outputting\nfinal labels improves performance; and (4) fine-tuned LLMs outperform\nout-of-the-box ones. Our results indicate that LLMs that are suitably\nfine-tuned and have sufficient reasoning capabilities can be leveraged for\nautomated dialogue evaluation.",
        "chunk-id": 2,
        "chunk": "configurations on public and proprietary datasets. Manipulating factors such as\nmodel size, in-context examples, and selection techniques, we examine\n\"chain-of-thought\" (CoT) reasoning and label extraction procedures. Our results\nshow that (1) larger models yield more accurate dialogue labels; (2)\nalgorithmic selection of in-context examples outperforms random selection; (3)",
        "authors": [
            "Jinghan Jia",
            "Abi Komma",
            "Timothy Leffel",
            "Xujun Peng",
            "Ajay Nagesh",
            "Tamer Soliman",
            "Aram Galstyan",
            "Anoop Kumar"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-25T06:19:47+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.17304v1",
        "arxiv_link": "http://arxiv.org/abs/2406.17304v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000019,
        "doi": null,
        "title": "Leveraging LLMs for Dialogue Quality Measurement",
        "abstract": "In task-oriented conversational AI evaluation, unsupervised methods poorly\ncorrelate with human judgments, and supervised approaches lack generalization.\nRecent advances in large language models (LLMs) show robust zeroshot and\nfew-shot capabilities across NLP tasks. This paper explores using LLMs for\nautomated dialogue quality evaluation, experimenting with various\nconfigurations on public and proprietary datasets. Manipulating factors such as\nmodel size, in-context examples, and selection techniques, we examine\n\"chain-of-thought\" (CoT) reasoning and label extraction procedures. Our results\nshow that (1) larger models yield more accurate dialogue labels; (2)\nalgorithmic selection of in-context examples outperforms random selection; (3)\nCoT reasoning where an LLM is asked to provide justifications before outputting\nfinal labels improves performance; and (4) fine-tuned LLMs outperform\nout-of-the-box ones. Our results indicate that LLMs that are suitably\nfine-tuned and have sufficient reasoning capabilities can be leveraged for\nautomated dialogue evaluation.",
        "chunk-id": 3,
        "chunk": "CoT reasoning where an LLM is asked to provide justifications before outputting\nfinal labels improves performance; and (4) fine-tuned LLMs outperform\nout-of-the-box ones. Our results indicate that LLMs that are suitably\nfine-tuned and have sufficient reasoning capabilities can be leveraged for\nautomated dialogue evaluation.",
        "authors": [
            "Jinghan Jia",
            "Abi Komma",
            "Timothy Leffel",
            "Xujun Peng",
            "Ajay Nagesh",
            "Tamer Soliman",
            "Aram Galstyan",
            "Anoop Kumar"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-25T06:19:47+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.17304v1",
        "arxiv_link": "http://arxiv.org/abs/2406.17304v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000020,
        "doi": null,
        "title": "DARG: Dynamic Evaluation of Large Language Models via Adaptive Reasoning Graph",
        "abstract": "The current paradigm of evaluating Large Language Models (LLMs) through\nstatic benchmarks comes with significant limitations, such as vulnerability to\ndata contamination and a lack of adaptability to the evolving capabilities of\nLLMs. Therefore, evaluation methods that can adapt and generate evaluation data\nwith controlled complexity are urgently needed. In this work, we introduce\nDynamic Evaluation of LLMs via Adaptive Reasoning Graph Evolvement (DARG) to\ndynamically extend current benchmarks with controlled complexity and diversity.\nSpecifically, we first extract the reasoning graphs of data points in current\nbenchmarks and then perturb the reasoning graphs to generate novel testing\ndata. Such newly generated test samples can have different levels of complexity\nwhile maintaining linguistic diversity similar to the original benchmarks. We\nfurther use a code-augmented LLM to ensure the label correctness of newly\ngenerated data. We apply our DARG framework to diverse reasoning tasks in four\ndomains with 15 state-of-the-art LLMs. Experimental results show that almost\nall LLMs experience a performance decrease with increased complexity and\ncertain LLMs exhibit significant drops. Additionally, we find that LLMs exhibit\nmore biases when being evaluated via the data generated by DARG with higher\ncomplexity levels. These observations provide useful insights into how to\ndynamically and adaptively evaluate LLMs. The code is available at\nhttps://github.com/SALT-NLP/DARG.",
        "chunk-id": 1,
        "chunk": "The current paradigm of evaluating Large Language Models (LLMs) through\nstatic benchmarks comes with significant limitations, such as vulnerability to\ndata contamination and a lack of adaptability to the evolving capabilities of\nLLMs. Therefore, evaluation methods that can adapt and generate evaluation data\nwith controlled complexity are urgently needed. In this work, we introduce",
        "authors": [
            "Zhehao Zhang",
            "Jiaao Chen",
            "Diyi Yang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-25T04:27:53+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.17271v1",
        "arxiv_link": "http://arxiv.org/abs/2406.17271v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000020,
        "doi": null,
        "title": "DARG: Dynamic Evaluation of Large Language Models via Adaptive Reasoning Graph",
        "abstract": "The current paradigm of evaluating Large Language Models (LLMs) through\nstatic benchmarks comes with significant limitations, such as vulnerability to\ndata contamination and a lack of adaptability to the evolving capabilities of\nLLMs. Therefore, evaluation methods that can adapt and generate evaluation data\nwith controlled complexity are urgently needed. In this work, we introduce\nDynamic Evaluation of LLMs via Adaptive Reasoning Graph Evolvement (DARG) to\ndynamically extend current benchmarks with controlled complexity and diversity.\nSpecifically, we first extract the reasoning graphs of data points in current\nbenchmarks and then perturb the reasoning graphs to generate novel testing\ndata. Such newly generated test samples can have different levels of complexity\nwhile maintaining linguistic diversity similar to the original benchmarks. We\nfurther use a code-augmented LLM to ensure the label correctness of newly\ngenerated data. We apply our DARG framework to diverse reasoning tasks in four\ndomains with 15 state-of-the-art LLMs. Experimental results show that almost\nall LLMs experience a performance decrease with increased complexity and\ncertain LLMs exhibit significant drops. Additionally, we find that LLMs exhibit\nmore biases when being evaluated via the data generated by DARG with higher\ncomplexity levels. These observations provide useful insights into how to\ndynamically and adaptively evaluate LLMs. The code is available at\nhttps://github.com/SALT-NLP/DARG.",
        "chunk-id": 2,
        "chunk": "Dynamic Evaluation of LLMs via Adaptive Reasoning Graph Evolvement (DARG) to\ndynamically extend current benchmarks with controlled complexity and diversity.\nSpecifically, we first extract the reasoning graphs of data points in current\nbenchmarks and then perturb the reasoning graphs to generate novel testing",
        "authors": [
            "Zhehao Zhang",
            "Jiaao Chen",
            "Diyi Yang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-25T04:27:53+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.17271v1",
        "arxiv_link": "http://arxiv.org/abs/2406.17271v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000020,
        "doi": null,
        "title": "DARG: Dynamic Evaluation of Large Language Models via Adaptive Reasoning Graph",
        "abstract": "The current paradigm of evaluating Large Language Models (LLMs) through\nstatic benchmarks comes with significant limitations, such as vulnerability to\ndata contamination and a lack of adaptability to the evolving capabilities of\nLLMs. Therefore, evaluation methods that can adapt and generate evaluation data\nwith controlled complexity are urgently needed. In this work, we introduce\nDynamic Evaluation of LLMs via Adaptive Reasoning Graph Evolvement (DARG) to\ndynamically extend current benchmarks with controlled complexity and diversity.\nSpecifically, we first extract the reasoning graphs of data points in current\nbenchmarks and then perturb the reasoning graphs to generate novel testing\ndata. Such newly generated test samples can have different levels of complexity\nwhile maintaining linguistic diversity similar to the original benchmarks. We\nfurther use a code-augmented LLM to ensure the label correctness of newly\ngenerated data. We apply our DARG framework to diverse reasoning tasks in four\ndomains with 15 state-of-the-art LLMs. Experimental results show that almost\nall LLMs experience a performance decrease with increased complexity and\ncertain LLMs exhibit significant drops. Additionally, we find that LLMs exhibit\nmore biases when being evaluated via the data generated by DARG with higher\ncomplexity levels. These observations provide useful insights into how to\ndynamically and adaptively evaluate LLMs. The code is available at\nhttps://github.com/SALT-NLP/DARG.",
        "chunk-id": 3,
        "chunk": "data. Such newly generated test samples can have different levels of complexity\nwhile maintaining linguistic diversity similar to the original benchmarks. We\nfurther use a code-augmented LLM to ensure the label correctness of newly\ngenerated data. We apply our DARG framework to diverse reasoning tasks in four",
        "authors": [
            "Zhehao Zhang",
            "Jiaao Chen",
            "Diyi Yang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-25T04:27:53+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.17271v1",
        "arxiv_link": "http://arxiv.org/abs/2406.17271v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000020,
        "doi": null,
        "title": "DARG: Dynamic Evaluation of Large Language Models via Adaptive Reasoning Graph",
        "abstract": "The current paradigm of evaluating Large Language Models (LLMs) through\nstatic benchmarks comes with significant limitations, such as vulnerability to\ndata contamination and a lack of adaptability to the evolving capabilities of\nLLMs. Therefore, evaluation methods that can adapt and generate evaluation data\nwith controlled complexity are urgently needed. In this work, we introduce\nDynamic Evaluation of LLMs via Adaptive Reasoning Graph Evolvement (DARG) to\ndynamically extend current benchmarks with controlled complexity and diversity.\nSpecifically, we first extract the reasoning graphs of data points in current\nbenchmarks and then perturb the reasoning graphs to generate novel testing\ndata. Such newly generated test samples can have different levels of complexity\nwhile maintaining linguistic diversity similar to the original benchmarks. We\nfurther use a code-augmented LLM to ensure the label correctness of newly\ngenerated data. We apply our DARG framework to diverse reasoning tasks in four\ndomains with 15 state-of-the-art LLMs. Experimental results show that almost\nall LLMs experience a performance decrease with increased complexity and\ncertain LLMs exhibit significant drops. Additionally, we find that LLMs exhibit\nmore biases when being evaluated via the data generated by DARG with higher\ncomplexity levels. These observations provide useful insights into how to\ndynamically and adaptively evaluate LLMs. The code is available at\nhttps://github.com/SALT-NLP/DARG.",
        "chunk-id": 4,
        "chunk": "domains with 15 state-of-the-art LLMs. Experimental results show that almost\nall LLMs experience a performance decrease with increased complexity and\ncertain LLMs exhibit significant drops. Additionally, we find that LLMs exhibit\nmore biases when being evaluated via the data generated by DARG with higher\ncomplexity levels. These observations provide useful insights into how to",
        "authors": [
            "Zhehao Zhang",
            "Jiaao Chen",
            "Diyi Yang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-25T04:27:53+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.17271v1",
        "arxiv_link": "http://arxiv.org/abs/2406.17271v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000020,
        "doi": null,
        "title": "DARG: Dynamic Evaluation of Large Language Models via Adaptive Reasoning Graph",
        "abstract": "The current paradigm of evaluating Large Language Models (LLMs) through\nstatic benchmarks comes with significant limitations, such as vulnerability to\ndata contamination and a lack of adaptability to the evolving capabilities of\nLLMs. Therefore, evaluation methods that can adapt and generate evaluation data\nwith controlled complexity are urgently needed. In this work, we introduce\nDynamic Evaluation of LLMs via Adaptive Reasoning Graph Evolvement (DARG) to\ndynamically extend current benchmarks with controlled complexity and diversity.\nSpecifically, we first extract the reasoning graphs of data points in current\nbenchmarks and then perturb the reasoning graphs to generate novel testing\ndata. Such newly generated test samples can have different levels of complexity\nwhile maintaining linguistic diversity similar to the original benchmarks. We\nfurther use a code-augmented LLM to ensure the label correctness of newly\ngenerated data. We apply our DARG framework to diverse reasoning tasks in four\ndomains with 15 state-of-the-art LLMs. Experimental results show that almost\nall LLMs experience a performance decrease with increased complexity and\ncertain LLMs exhibit significant drops. Additionally, we find that LLMs exhibit\nmore biases when being evaluated via the data generated by DARG with higher\ncomplexity levels. These observations provide useful insights into how to\ndynamically and adaptively evaluate LLMs. The code is available at\nhttps://github.com/SALT-NLP/DARG.",
        "chunk-id": 5,
        "chunk": "dynamically and adaptively evaluate LLMs. The code is available at\nhttps://github.com/SALT-NLP/DARG.",
        "authors": [
            "Zhehao Zhang",
            "Jiaao Chen",
            "Diyi Yang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-25T04:27:53+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.17271v1",
        "arxiv_link": "http://arxiv.org/abs/2406.17271v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000021,
        "doi": null,
        "title": "USDC: A Dataset of $\\underline{U}$ser $\\underline{S}$tance and $\\underline{D}$ogmatism in Long $\\underline{C}$onversations",
        "abstract": "Identifying user's opinions and stances in long conversation threads on\nvarious topics can be extremely critical for enhanced personalization, market\nresearch, political campaigns, customer service, conflict resolution, targeted\nadvertising, and content moderation. Hence, training language models to\nautomate this task is critical. However, to train such models, gathering manual\nannotations has multiple challenges: 1) It is time-consuming and costly; 2)\nConversation threads could be very long, increasing chances of noisy\nannotations; and 3) Interpreting instances where a user changes their opinion\nwithin a conversation is difficult because often such transitions are subtle\nand not expressed explicitly. Inspired by the recent success of large language\nmodels (LLMs) for complex natural language processing (NLP) tasks, we leverage\nMistral Large and GPT-4 to automate the human annotation process on the\nfollowing two tasks while also providing reasoning: i) User Stance\nclassification, which involves labeling a user's stance of a post in a\nconversation on a five-point scale; ii) User Dogmatism classification, which\ndeals with labeling a user's overall opinion in the conversation on a\nfour-point scale. The majority voting on zero-shot, one-shot, and few-shot\nannotations from these two LLMs on 764 multi-user Reddit conversations helps us\ncurate the USDC dataset. USDC is then used to finetune and instruction-tune\nmultiple deployable small language models for the 5-class stance and 4-class\ndogmatism classification tasks. We make the code and dataset publicly available\n[https://anonymous.4open.science/r/USDC-0F7F].",
        "chunk-id": 1,
        "chunk": "Identifying user's opinions and stances in long conversation threads on\nvarious topics can be extremely critical for enhanced personalization, market\nresearch, political campaigns, customer service, conflict resolution, targeted\nadvertising, and content moderation. Hence, training language models to\nautomate this task is critical. However, to train such models, gathering manual",
        "authors": [
            "Mounika Marreddy",
            "Subba Reddy Oota",
            "Venkata Charan Chinni",
            "Manish Gupta",
            "Lucie Flek"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T17:41:53+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16833v1",
        "arxiv_link": "http://arxiv.org/abs/2406.16833v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Machine Learning"
        ]
    },
    {
        "id": 20000021,
        "doi": null,
        "title": "USDC: A Dataset of $\\underline{U}$ser $\\underline{S}$tance and $\\underline{D}$ogmatism in Long $\\underline{C}$onversations",
        "abstract": "Identifying user's opinions and stances in long conversation threads on\nvarious topics can be extremely critical for enhanced personalization, market\nresearch, political campaigns, customer service, conflict resolution, targeted\nadvertising, and content moderation. Hence, training language models to\nautomate this task is critical. However, to train such models, gathering manual\nannotations has multiple challenges: 1) It is time-consuming and costly; 2)\nConversation threads could be very long, increasing chances of noisy\nannotations; and 3) Interpreting instances where a user changes their opinion\nwithin a conversation is difficult because often such transitions are subtle\nand not expressed explicitly. Inspired by the recent success of large language\nmodels (LLMs) for complex natural language processing (NLP) tasks, we leverage\nMistral Large and GPT-4 to automate the human annotation process on the\nfollowing two tasks while also providing reasoning: i) User Stance\nclassification, which involves labeling a user's stance of a post in a\nconversation on a five-point scale; ii) User Dogmatism classification, which\ndeals with labeling a user's overall opinion in the conversation on a\nfour-point scale. The majority voting on zero-shot, one-shot, and few-shot\nannotations from these two LLMs on 764 multi-user Reddit conversations helps us\ncurate the USDC dataset. USDC is then used to finetune and instruction-tune\nmultiple deployable small language models for the 5-class stance and 4-class\ndogmatism classification tasks. We make the code and dataset publicly available\n[https://anonymous.4open.science/r/USDC-0F7F].",
        "chunk-id": 2,
        "chunk": "annotations has multiple challenges: 1) It is time-consuming and costly; 2)\nConversation threads could be very long, increasing chances of noisy\nannotations; and 3) Interpreting instances where a user changes their opinion\nwithin a conversation is difficult because often such transitions are subtle\nand not expressed explicitly. Inspired by the recent success of large language",
        "authors": [
            "Mounika Marreddy",
            "Subba Reddy Oota",
            "Venkata Charan Chinni",
            "Manish Gupta",
            "Lucie Flek"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T17:41:53+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16833v1",
        "arxiv_link": "http://arxiv.org/abs/2406.16833v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Machine Learning"
        ]
    },
    {
        "id": 20000021,
        "doi": null,
        "title": "USDC: A Dataset of $\\underline{U}$ser $\\underline{S}$tance and $\\underline{D}$ogmatism in Long $\\underline{C}$onversations",
        "abstract": "Identifying user's opinions and stances in long conversation threads on\nvarious topics can be extremely critical for enhanced personalization, market\nresearch, political campaigns, customer service, conflict resolution, targeted\nadvertising, and content moderation. Hence, training language models to\nautomate this task is critical. However, to train such models, gathering manual\nannotations has multiple challenges: 1) It is time-consuming and costly; 2)\nConversation threads could be very long, increasing chances of noisy\nannotations; and 3) Interpreting instances where a user changes their opinion\nwithin a conversation is difficult because often such transitions are subtle\nand not expressed explicitly. Inspired by the recent success of large language\nmodels (LLMs) for complex natural language processing (NLP) tasks, we leverage\nMistral Large and GPT-4 to automate the human annotation process on the\nfollowing two tasks while also providing reasoning: i) User Stance\nclassification, which involves labeling a user's stance of a post in a\nconversation on a five-point scale; ii) User Dogmatism classification, which\ndeals with labeling a user's overall opinion in the conversation on a\nfour-point scale. The majority voting on zero-shot, one-shot, and few-shot\nannotations from these two LLMs on 764 multi-user Reddit conversations helps us\ncurate the USDC dataset. USDC is then used to finetune and instruction-tune\nmultiple deployable small language models for the 5-class stance and 4-class\ndogmatism classification tasks. We make the code and dataset publicly available\n[https://anonymous.4open.science/r/USDC-0F7F].",
        "chunk-id": 3,
        "chunk": "models (LLMs) for complex natural language processing (NLP) tasks, we leverage\nMistral Large and GPT-4 to automate the human annotation process on the\nfollowing two tasks while also providing reasoning: i) User Stance\nclassification, which involves labeling a user's stance of a post in a\nconversation on a five-point scale; ii) User Dogmatism classification, which",
        "authors": [
            "Mounika Marreddy",
            "Subba Reddy Oota",
            "Venkata Charan Chinni",
            "Manish Gupta",
            "Lucie Flek"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T17:41:53+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16833v1",
        "arxiv_link": "http://arxiv.org/abs/2406.16833v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Machine Learning"
        ]
    },
    {
        "id": 20000021,
        "doi": null,
        "title": "USDC: A Dataset of $\\underline{U}$ser $\\underline{S}$tance and $\\underline{D}$ogmatism in Long $\\underline{C}$onversations",
        "abstract": "Identifying user's opinions and stances in long conversation threads on\nvarious topics can be extremely critical for enhanced personalization, market\nresearch, political campaigns, customer service, conflict resolution, targeted\nadvertising, and content moderation. Hence, training language models to\nautomate this task is critical. However, to train such models, gathering manual\nannotations has multiple challenges: 1) It is time-consuming and costly; 2)\nConversation threads could be very long, increasing chances of noisy\nannotations; and 3) Interpreting instances where a user changes their opinion\nwithin a conversation is difficult because often such transitions are subtle\nand not expressed explicitly. Inspired by the recent success of large language\nmodels (LLMs) for complex natural language processing (NLP) tasks, we leverage\nMistral Large and GPT-4 to automate the human annotation process on the\nfollowing two tasks while also providing reasoning: i) User Stance\nclassification, which involves labeling a user's stance of a post in a\nconversation on a five-point scale; ii) User Dogmatism classification, which\ndeals with labeling a user's overall opinion in the conversation on a\nfour-point scale. The majority voting on zero-shot, one-shot, and few-shot\nannotations from these two LLMs on 764 multi-user Reddit conversations helps us\ncurate the USDC dataset. USDC is then used to finetune and instruction-tune\nmultiple deployable small language models for the 5-class stance and 4-class\ndogmatism classification tasks. We make the code and dataset publicly available\n[https://anonymous.4open.science/r/USDC-0F7F].",
        "chunk-id": 4,
        "chunk": "deals with labeling a user's overall opinion in the conversation on a\nfour-point scale. The majority voting on zero-shot, one-shot, and few-shot\nannotations from these two LLMs on 764 multi-user Reddit conversations helps us\ncurate the USDC dataset. USDC is then used to finetune and instruction-tune\nmultiple deployable small language models for the 5-class stance and 4-class",
        "authors": [
            "Mounika Marreddy",
            "Subba Reddy Oota",
            "Venkata Charan Chinni",
            "Manish Gupta",
            "Lucie Flek"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T17:41:53+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16833v1",
        "arxiv_link": "http://arxiv.org/abs/2406.16833v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Machine Learning"
        ]
    },
    {
        "id": 20000021,
        "doi": null,
        "title": "USDC: A Dataset of $\\underline{U}$ser $\\underline{S}$tance and $\\underline{D}$ogmatism in Long $\\underline{C}$onversations",
        "abstract": "Identifying user's opinions and stances in long conversation threads on\nvarious topics can be extremely critical for enhanced personalization, market\nresearch, political campaigns, customer service, conflict resolution, targeted\nadvertising, and content moderation. Hence, training language models to\nautomate this task is critical. However, to train such models, gathering manual\nannotations has multiple challenges: 1) It is time-consuming and costly; 2)\nConversation threads could be very long, increasing chances of noisy\nannotations; and 3) Interpreting instances where a user changes their opinion\nwithin a conversation is difficult because often such transitions are subtle\nand not expressed explicitly. Inspired by the recent success of large language\nmodels (LLMs) for complex natural language processing (NLP) tasks, we leverage\nMistral Large and GPT-4 to automate the human annotation process on the\nfollowing two tasks while also providing reasoning: i) User Stance\nclassification, which involves labeling a user's stance of a post in a\nconversation on a five-point scale; ii) User Dogmatism classification, which\ndeals with labeling a user's overall opinion in the conversation on a\nfour-point scale. The majority voting on zero-shot, one-shot, and few-shot\nannotations from these two LLMs on 764 multi-user Reddit conversations helps us\ncurate the USDC dataset. USDC is then used to finetune and instruction-tune\nmultiple deployable small language models for the 5-class stance and 4-class\ndogmatism classification tasks. We make the code and dataset publicly available\n[https://anonymous.4open.science/r/USDC-0F7F].",
        "chunk-id": 5,
        "chunk": "dogmatism classification tasks. We make the code and dataset publicly available\n[https://anonymous.4open.science/r/USDC-0F7F].",
        "authors": [
            "Mounika Marreddy",
            "Subba Reddy Oota",
            "Venkata Charan Chinni",
            "Manish Gupta",
            "Lucie Flek"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T17:41:53+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16833v1",
        "arxiv_link": "http://arxiv.org/abs/2406.16833v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Machine Learning"
        ]
    },
    {
        "id": 20000022,
        "doi": null,
        "title": "M2Lingual: Enhancing Multilingual, Multi-Turn Instruction Alignment in Large Language Models",
        "abstract": "Instruction finetuning (IFT) is critical for aligning Large Language Models\n(LLMs) to follow instructions. Numerous effective IFT datasets have been\nproposed in the recent past, but most focus on high resource languages such as\nEnglish. In this work, we propose a fully synthetic, novel taxonomy (Evol)\nguided Multilingual, Multi-turn instruction finetuning dataset, called\nM2Lingual, to better align LLMs on a diverse set of languages and tasks.\nM2Lingual contains a total of 182K IFT pairs that are built upon diverse seeds,\ncovering 70 languages, 17 NLP tasks and general instruction-response pairs.\nLLMs finetuned with M2Lingual substantially outperform the majority of existing\nmultilingual IFT datasets. Importantly, LLMs trained with M2Lingual\nconsistently achieve competitive results across a wide variety of evaluation\nbenchmarks compared to existing multilingual IFT datasets. Specifically, LLMs\nfinetuned with M2Lingual achieve strong performance on our translated\nmultilingual, multi-turn evaluation benchmark as well as a wide variety of\nmultilingual tasks. Thus we contribute, and the 2 step Evol taxonomy used for\nits creation. M2Lingual repository -\nhttps://huggingface.co/datasets/ServiceNow-AI/M2Lingual",
        "chunk-id": 1,
        "chunk": "Instruction finetuning (IFT) is critical for aligning Large Language Models\n(LLMs) to follow instructions. Numerous effective IFT datasets have been\nproposed in the recent past, but most focus on high resource languages such as\nEnglish. In this work, we propose a fully synthetic, novel taxonomy (Evol)\nguided Multilingual, Multi-turn instruction finetuning dataset, called",
        "authors": [
            "Rishabh Maheshwary",
            "Vikas Yadav",
            "Hoang Nguyen",
            "Khyati Mahajan",
            "Sathwik Tejaswi Madhusudhan"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T16:45:13+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16783v1",
        "arxiv_link": "http://arxiv.org/abs/2406.16783v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Machine Learning"
        ]
    },
    {
        "id": 20000022,
        "doi": null,
        "title": "M2Lingual: Enhancing Multilingual, Multi-Turn Instruction Alignment in Large Language Models",
        "abstract": "Instruction finetuning (IFT) is critical for aligning Large Language Models\n(LLMs) to follow instructions. Numerous effective IFT datasets have been\nproposed in the recent past, but most focus on high resource languages such as\nEnglish. In this work, we propose a fully synthetic, novel taxonomy (Evol)\nguided Multilingual, Multi-turn instruction finetuning dataset, called\nM2Lingual, to better align LLMs on a diverse set of languages and tasks.\nM2Lingual contains a total of 182K IFT pairs that are built upon diverse seeds,\ncovering 70 languages, 17 NLP tasks and general instruction-response pairs.\nLLMs finetuned with M2Lingual substantially outperform the majority of existing\nmultilingual IFT datasets. Importantly, LLMs trained with M2Lingual\nconsistently achieve competitive results across a wide variety of evaluation\nbenchmarks compared to existing multilingual IFT datasets. Specifically, LLMs\nfinetuned with M2Lingual achieve strong performance on our translated\nmultilingual, multi-turn evaluation benchmark as well as a wide variety of\nmultilingual tasks. Thus we contribute, and the 2 step Evol taxonomy used for\nits creation. M2Lingual repository -\nhttps://huggingface.co/datasets/ServiceNow-AI/M2Lingual",
        "chunk-id": 2,
        "chunk": "M2Lingual, to better align LLMs on a diverse set of languages and tasks.\nM2Lingual contains a total of 182K IFT pairs that are built upon diverse seeds,\ncovering 70 languages, 17 NLP tasks and general instruction-response pairs.\nLLMs finetuned with M2Lingual substantially outperform the majority of existing\nmultilingual IFT datasets. Importantly, LLMs trained with M2Lingual",
        "authors": [
            "Rishabh Maheshwary",
            "Vikas Yadav",
            "Hoang Nguyen",
            "Khyati Mahajan",
            "Sathwik Tejaswi Madhusudhan"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T16:45:13+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16783v1",
        "arxiv_link": "http://arxiv.org/abs/2406.16783v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Machine Learning"
        ]
    },
    {
        "id": 20000022,
        "doi": null,
        "title": "M2Lingual: Enhancing Multilingual, Multi-Turn Instruction Alignment in Large Language Models",
        "abstract": "Instruction finetuning (IFT) is critical for aligning Large Language Models\n(LLMs) to follow instructions. Numerous effective IFT datasets have been\nproposed in the recent past, but most focus on high resource languages such as\nEnglish. In this work, we propose a fully synthetic, novel taxonomy (Evol)\nguided Multilingual, Multi-turn instruction finetuning dataset, called\nM2Lingual, to better align LLMs on a diverse set of languages and tasks.\nM2Lingual contains a total of 182K IFT pairs that are built upon diverse seeds,\ncovering 70 languages, 17 NLP tasks and general instruction-response pairs.\nLLMs finetuned with M2Lingual substantially outperform the majority of existing\nmultilingual IFT datasets. Importantly, LLMs trained with M2Lingual\nconsistently achieve competitive results across a wide variety of evaluation\nbenchmarks compared to existing multilingual IFT datasets. Specifically, LLMs\nfinetuned with M2Lingual achieve strong performance on our translated\nmultilingual, multi-turn evaluation benchmark as well as a wide variety of\nmultilingual tasks. Thus we contribute, and the 2 step Evol taxonomy used for\nits creation. M2Lingual repository -\nhttps://huggingface.co/datasets/ServiceNow-AI/M2Lingual",
        "chunk-id": 3,
        "chunk": "consistently achieve competitive results across a wide variety of evaluation\nbenchmarks compared to existing multilingual IFT datasets. Specifically, LLMs\nfinetuned with M2Lingual achieve strong performance on our translated\nmultilingual, multi-turn evaluation benchmark as well as a wide variety of\nmultilingual tasks. Thus we contribute, and the 2 step Evol taxonomy used for",
        "authors": [
            "Rishabh Maheshwary",
            "Vikas Yadav",
            "Hoang Nguyen",
            "Khyati Mahajan",
            "Sathwik Tejaswi Madhusudhan"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T16:45:13+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16783v1",
        "arxiv_link": "http://arxiv.org/abs/2406.16783v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Machine Learning"
        ]
    },
    {
        "id": 20000022,
        "doi": null,
        "title": "M2Lingual: Enhancing Multilingual, Multi-Turn Instruction Alignment in Large Language Models",
        "abstract": "Instruction finetuning (IFT) is critical for aligning Large Language Models\n(LLMs) to follow instructions. Numerous effective IFT datasets have been\nproposed in the recent past, but most focus on high resource languages such as\nEnglish. In this work, we propose a fully synthetic, novel taxonomy (Evol)\nguided Multilingual, Multi-turn instruction finetuning dataset, called\nM2Lingual, to better align LLMs on a diverse set of languages and tasks.\nM2Lingual contains a total of 182K IFT pairs that are built upon diverse seeds,\ncovering 70 languages, 17 NLP tasks and general instruction-response pairs.\nLLMs finetuned with M2Lingual substantially outperform the majority of existing\nmultilingual IFT datasets. Importantly, LLMs trained with M2Lingual\nconsistently achieve competitive results across a wide variety of evaluation\nbenchmarks compared to existing multilingual IFT datasets. Specifically, LLMs\nfinetuned with M2Lingual achieve strong performance on our translated\nmultilingual, multi-turn evaluation benchmark as well as a wide variety of\nmultilingual tasks. Thus we contribute, and the 2 step Evol taxonomy used for\nits creation. M2Lingual repository -\nhttps://huggingface.co/datasets/ServiceNow-AI/M2Lingual",
        "chunk-id": 4,
        "chunk": "its creation. M2Lingual repository -\nhttps://huggingface.co/datasets/ServiceNow-AI/M2Lingual",
        "authors": [
            "Rishabh Maheshwary",
            "Vikas Yadav",
            "Hoang Nguyen",
            "Khyati Mahajan",
            "Sathwik Tejaswi Madhusudhan"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T16:45:13+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16783v1",
        "arxiv_link": "http://arxiv.org/abs/2406.16783v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Machine Learning"
        ]
    },
    {
        "id": 20000023,
        "doi": null,
        "title": "Finding Transformer Circuits with Edge Pruning",
        "abstract": "The path to interpreting a language model often proceeds via analysis of\ncircuits -- sparse computational subgraphs of the model that capture specific\naspects of its behavior. Recent work has automated the task of discovering\ncircuits. Yet, these methods have practical limitations, as they rely either on\ninefficient search algorithms or inaccurate approximations. In this paper, we\nframe automated circuit discovery as an optimization problem and propose *Edge\nPruning* as an effective and scalable solution. Edge Pruning leverages\ngradient-based pruning techniques, but instead of removing neurons or\ncomponents, it prunes the \\emph{edges} between components. Our method finds\ncircuits in GPT-2 that use less than half the number of edges compared to\ncircuits found by previous methods while being equally faithful to the full\nmodel predictions on standard circuit-finding tasks. Edge Pruning is efficient\neven with as many as 100K examples, outperforming previous methods in speed and\nproducing substantially better circuits. It also perfectly recovers the\nground-truth circuits in two models compiled with Tracr. Thanks to its\nefficiency, we scale Edge Pruning to CodeLlama-13B, a model over 100x the scale\nthat prior methods operate on. We use this setting for a case study comparing\nthe mechanisms behind instruction prompting and in-context learning. We find\ntwo circuits with more than 99.96% sparsity that match the performance of the\nfull model and reveal that the mechanisms in the two settings overlap\nsubstantially. Our case study shows that Edge Pruning is a practical and\nscalable tool for interpretability and sheds light on behaviors that only\nemerge in large models.",
        "chunk-id": 1,
        "chunk": "The path to interpreting a language model often proceeds via analysis of\ncircuits -- sparse computational subgraphs of the model that capture specific\naspects of its behavior. Recent work has automated the task of discovering\ncircuits. Yet, these methods have practical limitations, as they rely either on\ninefficient search algorithms or inaccurate approximations. In this paper, we",
        "authors": [
            "Adithya Bhaskar",
            "Alexander Wettig",
            "Dan Friedman",
            "Danqi Chen"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T16:40:54+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16778v1",
        "arxiv_link": "http://arxiv.org/abs/2406.16778v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000023,
        "doi": null,
        "title": "Finding Transformer Circuits with Edge Pruning",
        "abstract": "The path to interpreting a language model often proceeds via analysis of\ncircuits -- sparse computational subgraphs of the model that capture specific\naspects of its behavior. Recent work has automated the task of discovering\ncircuits. Yet, these methods have practical limitations, as they rely either on\ninefficient search algorithms or inaccurate approximations. In this paper, we\nframe automated circuit discovery as an optimization problem and propose *Edge\nPruning* as an effective and scalable solution. Edge Pruning leverages\ngradient-based pruning techniques, but instead of removing neurons or\ncomponents, it prunes the \\emph{edges} between components. Our method finds\ncircuits in GPT-2 that use less than half the number of edges compared to\ncircuits found by previous methods while being equally faithful to the full\nmodel predictions on standard circuit-finding tasks. Edge Pruning is efficient\neven with as many as 100K examples, outperforming previous methods in speed and\nproducing substantially better circuits. It also perfectly recovers the\nground-truth circuits in two models compiled with Tracr. Thanks to its\nefficiency, we scale Edge Pruning to CodeLlama-13B, a model over 100x the scale\nthat prior methods operate on. We use this setting for a case study comparing\nthe mechanisms behind instruction prompting and in-context learning. We find\ntwo circuits with more than 99.96% sparsity that match the performance of the\nfull model and reveal that the mechanisms in the two settings overlap\nsubstantially. Our case study shows that Edge Pruning is a practical and\nscalable tool for interpretability and sheds light on behaviors that only\nemerge in large models.",
        "chunk-id": 2,
        "chunk": "frame automated circuit discovery as an optimization problem and propose *Edge\nPruning* as an effective and scalable solution. Edge Pruning leverages\ngradient-based pruning techniques, but instead of removing neurons or\ncomponents, it prunes the \\emph{edges} between components. Our method finds\ncircuits in GPT-2 that use less than half the number of edges compared to",
        "authors": [
            "Adithya Bhaskar",
            "Alexander Wettig",
            "Dan Friedman",
            "Danqi Chen"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T16:40:54+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16778v1",
        "arxiv_link": "http://arxiv.org/abs/2406.16778v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000023,
        "doi": null,
        "title": "Finding Transformer Circuits with Edge Pruning",
        "abstract": "The path to interpreting a language model often proceeds via analysis of\ncircuits -- sparse computational subgraphs of the model that capture specific\naspects of its behavior. Recent work has automated the task of discovering\ncircuits. Yet, these methods have practical limitations, as they rely either on\ninefficient search algorithms or inaccurate approximations. In this paper, we\nframe automated circuit discovery as an optimization problem and propose *Edge\nPruning* as an effective and scalable solution. Edge Pruning leverages\ngradient-based pruning techniques, but instead of removing neurons or\ncomponents, it prunes the \\emph{edges} between components. Our method finds\ncircuits in GPT-2 that use less than half the number of edges compared to\ncircuits found by previous methods while being equally faithful to the full\nmodel predictions on standard circuit-finding tasks. Edge Pruning is efficient\neven with as many as 100K examples, outperforming previous methods in speed and\nproducing substantially better circuits. It also perfectly recovers the\nground-truth circuits in two models compiled with Tracr. Thanks to its\nefficiency, we scale Edge Pruning to CodeLlama-13B, a model over 100x the scale\nthat prior methods operate on. We use this setting for a case study comparing\nthe mechanisms behind instruction prompting and in-context learning. We find\ntwo circuits with more than 99.96% sparsity that match the performance of the\nfull model and reveal that the mechanisms in the two settings overlap\nsubstantially. Our case study shows that Edge Pruning is a practical and\nscalable tool for interpretability and sheds light on behaviors that only\nemerge in large models.",
        "chunk-id": 3,
        "chunk": "circuits found by previous methods while being equally faithful to the full\nmodel predictions on standard circuit-finding tasks. Edge Pruning is efficient\neven with as many as 100K examples, outperforming previous methods in speed and\nproducing substantially better circuits. It also perfectly recovers the\nground-truth circuits in two models compiled with Tracr. Thanks to its",
        "authors": [
            "Adithya Bhaskar",
            "Alexander Wettig",
            "Dan Friedman",
            "Danqi Chen"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T16:40:54+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16778v1",
        "arxiv_link": "http://arxiv.org/abs/2406.16778v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000023,
        "doi": null,
        "title": "Finding Transformer Circuits with Edge Pruning",
        "abstract": "The path to interpreting a language model often proceeds via analysis of\ncircuits -- sparse computational subgraphs of the model that capture specific\naspects of its behavior. Recent work has automated the task of discovering\ncircuits. Yet, these methods have practical limitations, as they rely either on\ninefficient search algorithms or inaccurate approximations. In this paper, we\nframe automated circuit discovery as an optimization problem and propose *Edge\nPruning* as an effective and scalable solution. Edge Pruning leverages\ngradient-based pruning techniques, but instead of removing neurons or\ncomponents, it prunes the \\emph{edges} between components. Our method finds\ncircuits in GPT-2 that use less than half the number of edges compared to\ncircuits found by previous methods while being equally faithful to the full\nmodel predictions on standard circuit-finding tasks. Edge Pruning is efficient\neven with as many as 100K examples, outperforming previous methods in speed and\nproducing substantially better circuits. It also perfectly recovers the\nground-truth circuits in two models compiled with Tracr. Thanks to its\nefficiency, we scale Edge Pruning to CodeLlama-13B, a model over 100x the scale\nthat prior methods operate on. We use this setting for a case study comparing\nthe mechanisms behind instruction prompting and in-context learning. We find\ntwo circuits with more than 99.96% sparsity that match the performance of the\nfull model and reveal that the mechanisms in the two settings overlap\nsubstantially. Our case study shows that Edge Pruning is a practical and\nscalable tool for interpretability and sheds light on behaviors that only\nemerge in large models.",
        "chunk-id": 4,
        "chunk": "efficiency, we scale Edge Pruning to CodeLlama-13B, a model over 100x the scale\nthat prior methods operate on. We use this setting for a case study comparing\nthe mechanisms behind instruction prompting and in-context learning. We find\ntwo circuits with more than 99.96% sparsity that match the performance of the\nfull model and reveal that the mechanisms in the two settings overlap",
        "authors": [
            "Adithya Bhaskar",
            "Alexander Wettig",
            "Dan Friedman",
            "Danqi Chen"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T16:40:54+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16778v1",
        "arxiv_link": "http://arxiv.org/abs/2406.16778v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000023,
        "doi": null,
        "title": "Finding Transformer Circuits with Edge Pruning",
        "abstract": "The path to interpreting a language model often proceeds via analysis of\ncircuits -- sparse computational subgraphs of the model that capture specific\naspects of its behavior. Recent work has automated the task of discovering\ncircuits. Yet, these methods have practical limitations, as they rely either on\ninefficient search algorithms or inaccurate approximations. In this paper, we\nframe automated circuit discovery as an optimization problem and propose *Edge\nPruning* as an effective and scalable solution. Edge Pruning leverages\ngradient-based pruning techniques, but instead of removing neurons or\ncomponents, it prunes the \\emph{edges} between components. Our method finds\ncircuits in GPT-2 that use less than half the number of edges compared to\ncircuits found by previous methods while being equally faithful to the full\nmodel predictions on standard circuit-finding tasks. Edge Pruning is efficient\neven with as many as 100K examples, outperforming previous methods in speed and\nproducing substantially better circuits. It also perfectly recovers the\nground-truth circuits in two models compiled with Tracr. Thanks to its\nefficiency, we scale Edge Pruning to CodeLlama-13B, a model over 100x the scale\nthat prior methods operate on. We use this setting for a case study comparing\nthe mechanisms behind instruction prompting and in-context learning. We find\ntwo circuits with more than 99.96% sparsity that match the performance of the\nfull model and reveal that the mechanisms in the two settings overlap\nsubstantially. Our case study shows that Edge Pruning is a practical and\nscalable tool for interpretability and sheds light on behaviors that only\nemerge in large models.",
        "chunk-id": 5,
        "chunk": "substantially. Our case study shows that Edge Pruning is a practical and\nscalable tool for interpretability and sheds light on behaviors that only\nemerge in large models.",
        "authors": [
            "Adithya Bhaskar",
            "Alexander Wettig",
            "Dan Friedman",
            "Danqi Chen"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T16:40:54+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16778v1",
        "arxiv_link": "http://arxiv.org/abs/2406.16778v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000024,
        "doi": null,
        "title": "OlympicArena Medal Ranks: Who Is the Most Intelligent AI So Far?",
        "abstract": "In this report, we pose the following question: Who is the most intelligent\nAI model to date, as measured by the OlympicArena (an Olympic-level,\nmulti-discipline, multi-modal benchmark for superintelligent AI)? We\nspecifically focus on the most recently released models: Claude-3.5-Sonnet,\nGemini-1.5-Pro, and GPT-4o. For the first time, we propose using an Olympic\nmedal Table approach to rank AI models based on their comprehensive performance\nacross various disciplines. Empirical results reveal: (1) Claude-3.5-Sonnet\nshows highly competitive overall performance over GPT-4o, even surpassing\nGPT-4o on a few subjects (i.e., Physics, Chemistry, and Biology). (2)\nGemini-1.5-Pro and GPT-4V are ranked consecutively just behind GPT-4o and\nClaude-3.5-Sonnet, but with a clear performance gap between them. (3) The\nperformance of AI models from the open-source community significantly lags\nbehind these proprietary models. (4) The performance of these models on this\nbenchmark has been less than satisfactory, indicating that we still have a long\nway to go before achieving superintelligence. We remain committed to\ncontinuously tracking and evaluating the performance of the latest powerful\nmodels on this benchmark (available at\nhttps://github.com/GAIR-NLP/OlympicArena).",
        "chunk-id": 1,
        "chunk": "In this report, we pose the following question: Who is the most intelligent\nAI model to date, as measured by the OlympicArena (an Olympic-level,\nmulti-discipline, multi-modal benchmark for superintelligent AI)? We\nspecifically focus on the most recently released models: Claude-3.5-Sonnet,\nGemini-1.5-Pro, and GPT-4o. For the first time, we propose using an Olympic",
        "authors": [
            "Zhen Huang",
            "Zengzhi Wang",
            "Shijie Xia",
            "Pengfei Liu"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T16:31:12+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16772v2",
        "arxiv_link": "http://arxiv.org/abs/2406.16772v2",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000024,
        "doi": null,
        "title": "OlympicArena Medal Ranks: Who Is the Most Intelligent AI So Far?",
        "abstract": "In this report, we pose the following question: Who is the most intelligent\nAI model to date, as measured by the OlympicArena (an Olympic-level,\nmulti-discipline, multi-modal benchmark for superintelligent AI)? We\nspecifically focus on the most recently released models: Claude-3.5-Sonnet,\nGemini-1.5-Pro, and GPT-4o. For the first time, we propose using an Olympic\nmedal Table approach to rank AI models based on their comprehensive performance\nacross various disciplines. Empirical results reveal: (1) Claude-3.5-Sonnet\nshows highly competitive overall performance over GPT-4o, even surpassing\nGPT-4o on a few subjects (i.e., Physics, Chemistry, and Biology). (2)\nGemini-1.5-Pro and GPT-4V are ranked consecutively just behind GPT-4o and\nClaude-3.5-Sonnet, but with a clear performance gap between them. (3) The\nperformance of AI models from the open-source community significantly lags\nbehind these proprietary models. (4) The performance of these models on this\nbenchmark has been less than satisfactory, indicating that we still have a long\nway to go before achieving superintelligence. We remain committed to\ncontinuously tracking and evaluating the performance of the latest powerful\nmodels on this benchmark (available at\nhttps://github.com/GAIR-NLP/OlympicArena).",
        "chunk-id": 2,
        "chunk": "medal Table approach to rank AI models based on their comprehensive performance\nacross various disciplines. Empirical results reveal: (1) Claude-3.5-Sonnet\nshows highly competitive overall performance over GPT-4o, even surpassing\nGPT-4o on a few subjects (i.e., Physics, Chemistry, and Biology). (2)\nGemini-1.5-Pro and GPT-4V are ranked consecutively just behind GPT-4o and",
        "authors": [
            "Zhen Huang",
            "Zengzhi Wang",
            "Shijie Xia",
            "Pengfei Liu"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T16:31:12+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16772v2",
        "arxiv_link": "http://arxiv.org/abs/2406.16772v2",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000024,
        "doi": null,
        "title": "OlympicArena Medal Ranks: Who Is the Most Intelligent AI So Far?",
        "abstract": "In this report, we pose the following question: Who is the most intelligent\nAI model to date, as measured by the OlympicArena (an Olympic-level,\nmulti-discipline, multi-modal benchmark for superintelligent AI)? We\nspecifically focus on the most recently released models: Claude-3.5-Sonnet,\nGemini-1.5-Pro, and GPT-4o. For the first time, we propose using an Olympic\nmedal Table approach to rank AI models based on their comprehensive performance\nacross various disciplines. Empirical results reveal: (1) Claude-3.5-Sonnet\nshows highly competitive overall performance over GPT-4o, even surpassing\nGPT-4o on a few subjects (i.e., Physics, Chemistry, and Biology). (2)\nGemini-1.5-Pro and GPT-4V are ranked consecutively just behind GPT-4o and\nClaude-3.5-Sonnet, but with a clear performance gap between them. (3) The\nperformance of AI models from the open-source community significantly lags\nbehind these proprietary models. (4) The performance of these models on this\nbenchmark has been less than satisfactory, indicating that we still have a long\nway to go before achieving superintelligence. We remain committed to\ncontinuously tracking and evaluating the performance of the latest powerful\nmodels on this benchmark (available at\nhttps://github.com/GAIR-NLP/OlympicArena).",
        "chunk-id": 3,
        "chunk": "Claude-3.5-Sonnet, but with a clear performance gap between them. (3) The\nperformance of AI models from the open-source community significantly lags\nbehind these proprietary models. (4) The performance of these models on this\nbenchmark has been less than satisfactory, indicating that we still have a long\nway to go before achieving superintelligence. We remain committed to",
        "authors": [
            "Zhen Huang",
            "Zengzhi Wang",
            "Shijie Xia",
            "Pengfei Liu"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T16:31:12+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16772v2",
        "arxiv_link": "http://arxiv.org/abs/2406.16772v2",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000024,
        "doi": null,
        "title": "OlympicArena Medal Ranks: Who Is the Most Intelligent AI So Far?",
        "abstract": "In this report, we pose the following question: Who is the most intelligent\nAI model to date, as measured by the OlympicArena (an Olympic-level,\nmulti-discipline, multi-modal benchmark for superintelligent AI)? We\nspecifically focus on the most recently released models: Claude-3.5-Sonnet,\nGemini-1.5-Pro, and GPT-4o. For the first time, we propose using an Olympic\nmedal Table approach to rank AI models based on their comprehensive performance\nacross various disciplines. Empirical results reveal: (1) Claude-3.5-Sonnet\nshows highly competitive overall performance over GPT-4o, even surpassing\nGPT-4o on a few subjects (i.e., Physics, Chemistry, and Biology). (2)\nGemini-1.5-Pro and GPT-4V are ranked consecutively just behind GPT-4o and\nClaude-3.5-Sonnet, but with a clear performance gap between them. (3) The\nperformance of AI models from the open-source community significantly lags\nbehind these proprietary models. (4) The performance of these models on this\nbenchmark has been less than satisfactory, indicating that we still have a long\nway to go before achieving superintelligence. We remain committed to\ncontinuously tracking and evaluating the performance of the latest powerful\nmodels on this benchmark (available at\nhttps://github.com/GAIR-NLP/OlympicArena).",
        "chunk-id": 4,
        "chunk": "continuously tracking and evaluating the performance of the latest powerful\nmodels on this benchmark (available at\nhttps://github.com/GAIR-NLP/OlympicArena).",
        "authors": [
            "Zhen Huang",
            "Zengzhi Wang",
            "Shijie Xia",
            "Pengfei Liu"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T16:31:12+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16772v2",
        "arxiv_link": "http://arxiv.org/abs/2406.16772v2",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000025,
        "doi": null,
        "title": "CLIMATELI: Evaluating Entity Linking on Climate Change Data",
        "abstract": "Climate Change (CC) is a pressing topic of global importance, attracting\nincreasing attention across research fields, from social sciences to Natural\nLanguage Processing (NLP). CC is also discussed in various settings and\ncommunication platforms, from academic publications to social media forums.\nUnderstanding who and what is mentioned in such data is a first critical step\nto gaining new insights into CC. We present CLIMATELI (CLIMATe Entity LInking),\nthe first manually annotated CC dataset that links 3,087 entity spans to\nWikipedia. Using CLIMATELI (CLIMATe Entity LInking), we evaluate existing\nentity linking (EL) systems on the CC topic across various genres and propose\nautomated filtering methods for CC entities. We find that the performance of EL\nmodels notably lags behind humans at both token and entity levels. Testing\nwithin the scope of retaining or excluding non-nominal and/or non-CC entities\nparticularly impacts the models' performances.",
        "chunk-id": 1,
        "chunk": "Climate Change (CC) is a pressing topic of global importance, attracting\nincreasing attention across research fields, from social sciences to Natural\nLanguage Processing (NLP). CC is also discussed in various settings and\ncommunication platforms, from academic publications to social media forums.\nUnderstanding who and what is mentioned in such data is a first critical step",
        "authors": [
            "Shijia Zhou",
            "Siyao Peng",
            "Barbara Plank"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T15:36:00+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16732v2",
        "arxiv_link": "http://arxiv.org/abs/2406.16732v2",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000025,
        "doi": null,
        "title": "CLIMATELI: Evaluating Entity Linking on Climate Change Data",
        "abstract": "Climate Change (CC) is a pressing topic of global importance, attracting\nincreasing attention across research fields, from social sciences to Natural\nLanguage Processing (NLP). CC is also discussed in various settings and\ncommunication platforms, from academic publications to social media forums.\nUnderstanding who and what is mentioned in such data is a first critical step\nto gaining new insights into CC. We present CLIMATELI (CLIMATe Entity LInking),\nthe first manually annotated CC dataset that links 3,087 entity spans to\nWikipedia. Using CLIMATELI (CLIMATe Entity LInking), we evaluate existing\nentity linking (EL) systems on the CC topic across various genres and propose\nautomated filtering methods for CC entities. We find that the performance of EL\nmodels notably lags behind humans at both token and entity levels. Testing\nwithin the scope of retaining or excluding non-nominal and/or non-CC entities\nparticularly impacts the models' performances.",
        "chunk-id": 2,
        "chunk": "to gaining new insights into CC. We present CLIMATELI (CLIMATe Entity LInking),\nthe first manually annotated CC dataset that links 3,087 entity spans to\nWikipedia. Using CLIMATELI (CLIMATe Entity LInking), we evaluate existing\nentity linking (EL) systems on the CC topic across various genres and propose\nautomated filtering methods for CC entities. We find that the performance of EL",
        "authors": [
            "Shijia Zhou",
            "Siyao Peng",
            "Barbara Plank"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T15:36:00+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16732v2",
        "arxiv_link": "http://arxiv.org/abs/2406.16732v2",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000025,
        "doi": null,
        "title": "CLIMATELI: Evaluating Entity Linking on Climate Change Data",
        "abstract": "Climate Change (CC) is a pressing topic of global importance, attracting\nincreasing attention across research fields, from social sciences to Natural\nLanguage Processing (NLP). CC is also discussed in various settings and\ncommunication platforms, from academic publications to social media forums.\nUnderstanding who and what is mentioned in such data is a first critical step\nto gaining new insights into CC. We present CLIMATELI (CLIMATe Entity LInking),\nthe first manually annotated CC dataset that links 3,087 entity spans to\nWikipedia. Using CLIMATELI (CLIMATe Entity LInking), we evaluate existing\nentity linking (EL) systems on the CC topic across various genres and propose\nautomated filtering methods for CC entities. We find that the performance of EL\nmodels notably lags behind humans at both token and entity levels. Testing\nwithin the scope of retaining or excluding non-nominal and/or non-CC entities\nparticularly impacts the models' performances.",
        "chunk-id": 3,
        "chunk": "models notably lags behind humans at both token and entity levels. Testing\nwithin the scope of retaining or excluding non-nominal and/or non-CC entities\nparticularly impacts the models' performances.",
        "authors": [
            "Shijia Zhou",
            "Siyao Peng",
            "Barbara Plank"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T15:36:00+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16732v2",
        "arxiv_link": "http://arxiv.org/abs/2406.16732v2",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000026,
        "doi": null,
        "title": "Segment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation",
        "abstract": "Segmenting text into sentences plays an early and crucial role in many NLP\nsystems. This is commonly achieved by using rule-based or statistical methods\nrelying on lexical features such as punctuation. Although some recent works no\nlonger exclusively rely on punctuation, we find that no prior method achieves\nall of (i) robustness to missing punctuation, (ii) effective adaptability to\nnew domains, and (iii) high efficiency. We introduce a new model - Segment any\nText (SaT) - to solve this problem. To enhance robustness, we propose a new\npretraining scheme that ensures less reliance on punctuation. To address\nadaptability, we introduce an extra stage of parameter-efficient fine-tuning,\nestablishing state-of-the-art performance in distinct domains such as verses\nfrom lyrics and legal documents. Along the way, we introduce architectural\nmodifications that result in a threefold gain in speed over the previous state\nof the art and solve spurious reliance on context far in the future. Finally,\nwe introduce a variant of our model with fine-tuning on a diverse, multilingual\nmixture of sentence-segmented data, acting as a drop-in replacement and\nenhancement for existing segmentation tools. Overall, our contributions provide\na universal approach for segmenting any text. Our method outperforms all\nbaselines - including strong LLMs - across 8 corpora spanning diverse domains\nand languages, especially in practically relevant situations where text is\npoorly formatted. Our models and code, including documentation, are available\nat https://huggingface.co/segment-any-text under the MIT license.",
        "chunk-id": 1,
        "chunk": "Segmenting text into sentences plays an early and crucial role in many NLP\nsystems. This is commonly achieved by using rule-based or statistical methods\nrelying on lexical features such as punctuation. Although some recent works no\nlonger exclusively rely on punctuation, we find that no prior method achieves",
        "authors": [
            "Markus Frohmann",
            "Igor Sterner",
            "Ivan Vuli\u0107",
            "Benjamin Minixhofer",
            "Markus Schedl"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T14:36:11+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16678v1",
        "arxiv_link": "http://arxiv.org/abs/2406.16678v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Machine Learning"
        ]
    },
    {
        "id": 20000026,
        "doi": null,
        "title": "Segment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation",
        "abstract": "Segmenting text into sentences plays an early and crucial role in many NLP\nsystems. This is commonly achieved by using rule-based or statistical methods\nrelying on lexical features such as punctuation. Although some recent works no\nlonger exclusively rely on punctuation, we find that no prior method achieves\nall of (i) robustness to missing punctuation, (ii) effective adaptability to\nnew domains, and (iii) high efficiency. We introduce a new model - Segment any\nText (SaT) - to solve this problem. To enhance robustness, we propose a new\npretraining scheme that ensures less reliance on punctuation. To address\nadaptability, we introduce an extra stage of parameter-efficient fine-tuning,\nestablishing state-of-the-art performance in distinct domains such as verses\nfrom lyrics and legal documents. Along the way, we introduce architectural\nmodifications that result in a threefold gain in speed over the previous state\nof the art and solve spurious reliance on context far in the future. Finally,\nwe introduce a variant of our model with fine-tuning on a diverse, multilingual\nmixture of sentence-segmented data, acting as a drop-in replacement and\nenhancement for existing segmentation tools. Overall, our contributions provide\na universal approach for segmenting any text. Our method outperforms all\nbaselines - including strong LLMs - across 8 corpora spanning diverse domains\nand languages, especially in practically relevant situations where text is\npoorly formatted. Our models and code, including documentation, are available\nat https://huggingface.co/segment-any-text under the MIT license.",
        "chunk-id": 2,
        "chunk": "all of (i) robustness to missing punctuation, (ii) effective adaptability to\nnew domains, and (iii) high efficiency. We introduce a new model - Segment any\nText (SaT) - to solve this problem. To enhance robustness, we propose a new\npretraining scheme that ensures less reliance on punctuation. To address\nadaptability, we introduce an extra stage of parameter-efficient fine-tuning,",
        "authors": [
            "Markus Frohmann",
            "Igor Sterner",
            "Ivan Vuli\u0107",
            "Benjamin Minixhofer",
            "Markus Schedl"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T14:36:11+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16678v1",
        "arxiv_link": "http://arxiv.org/abs/2406.16678v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Machine Learning"
        ]
    },
    {
        "id": 20000026,
        "doi": null,
        "title": "Segment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation",
        "abstract": "Segmenting text into sentences plays an early and crucial role in many NLP\nsystems. This is commonly achieved by using rule-based or statistical methods\nrelying on lexical features such as punctuation. Although some recent works no\nlonger exclusively rely on punctuation, we find that no prior method achieves\nall of (i) robustness to missing punctuation, (ii) effective adaptability to\nnew domains, and (iii) high efficiency. We introduce a new model - Segment any\nText (SaT) - to solve this problem. To enhance robustness, we propose a new\npretraining scheme that ensures less reliance on punctuation. To address\nadaptability, we introduce an extra stage of parameter-efficient fine-tuning,\nestablishing state-of-the-art performance in distinct domains such as verses\nfrom lyrics and legal documents. Along the way, we introduce architectural\nmodifications that result in a threefold gain in speed over the previous state\nof the art and solve spurious reliance on context far in the future. Finally,\nwe introduce a variant of our model with fine-tuning on a diverse, multilingual\nmixture of sentence-segmented data, acting as a drop-in replacement and\nenhancement for existing segmentation tools. Overall, our contributions provide\na universal approach for segmenting any text. Our method outperforms all\nbaselines - including strong LLMs - across 8 corpora spanning diverse domains\nand languages, especially in practically relevant situations where text is\npoorly formatted. Our models and code, including documentation, are available\nat https://huggingface.co/segment-any-text under the MIT license.",
        "chunk-id": 3,
        "chunk": "establishing state-of-the-art performance in distinct domains such as verses\nfrom lyrics and legal documents. Along the way, we introduce architectural\nmodifications that result in a threefold gain in speed over the previous state\nof the art and solve spurious reliance on context far in the future. Finally,",
        "authors": [
            "Markus Frohmann",
            "Igor Sterner",
            "Ivan Vuli\u0107",
            "Benjamin Minixhofer",
            "Markus Schedl"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T14:36:11+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16678v1",
        "arxiv_link": "http://arxiv.org/abs/2406.16678v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Machine Learning"
        ]
    },
    {
        "id": 20000026,
        "doi": null,
        "title": "Segment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation",
        "abstract": "Segmenting text into sentences plays an early and crucial role in many NLP\nsystems. This is commonly achieved by using rule-based or statistical methods\nrelying on lexical features such as punctuation. Although some recent works no\nlonger exclusively rely on punctuation, we find that no prior method achieves\nall of (i) robustness to missing punctuation, (ii) effective adaptability to\nnew domains, and (iii) high efficiency. We introduce a new model - Segment any\nText (SaT) - to solve this problem. To enhance robustness, we propose a new\npretraining scheme that ensures less reliance on punctuation. To address\nadaptability, we introduce an extra stage of parameter-efficient fine-tuning,\nestablishing state-of-the-art performance in distinct domains such as verses\nfrom lyrics and legal documents. Along the way, we introduce architectural\nmodifications that result in a threefold gain in speed over the previous state\nof the art and solve spurious reliance on context far in the future. Finally,\nwe introduce a variant of our model with fine-tuning on a diverse, multilingual\nmixture of sentence-segmented data, acting as a drop-in replacement and\nenhancement for existing segmentation tools. Overall, our contributions provide\na universal approach for segmenting any text. Our method outperforms all\nbaselines - including strong LLMs - across 8 corpora spanning diverse domains\nand languages, especially in practically relevant situations where text is\npoorly formatted. Our models and code, including documentation, are available\nat https://huggingface.co/segment-any-text under the MIT license.",
        "chunk-id": 4,
        "chunk": "we introduce a variant of our model with fine-tuning on a diverse, multilingual\nmixture of sentence-segmented data, acting as a drop-in replacement and\nenhancement for existing segmentation tools. Overall, our contributions provide\na universal approach for segmenting any text. Our method outperforms all\nbaselines - including strong LLMs - across 8 corpora spanning diverse domains",
        "authors": [
            "Markus Frohmann",
            "Igor Sterner",
            "Ivan Vuli\u0107",
            "Benjamin Minixhofer",
            "Markus Schedl"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T14:36:11+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16678v1",
        "arxiv_link": "http://arxiv.org/abs/2406.16678v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Machine Learning"
        ]
    },
    {
        "id": 20000026,
        "doi": null,
        "title": "Segment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation",
        "abstract": "Segmenting text into sentences plays an early and crucial role in many NLP\nsystems. This is commonly achieved by using rule-based or statistical methods\nrelying on lexical features such as punctuation. Although some recent works no\nlonger exclusively rely on punctuation, we find that no prior method achieves\nall of (i) robustness to missing punctuation, (ii) effective adaptability to\nnew domains, and (iii) high efficiency. We introduce a new model - Segment any\nText (SaT) - to solve this problem. To enhance robustness, we propose a new\npretraining scheme that ensures less reliance on punctuation. To address\nadaptability, we introduce an extra stage of parameter-efficient fine-tuning,\nestablishing state-of-the-art performance in distinct domains such as verses\nfrom lyrics and legal documents. Along the way, we introduce architectural\nmodifications that result in a threefold gain in speed over the previous state\nof the art and solve spurious reliance on context far in the future. Finally,\nwe introduce a variant of our model with fine-tuning on a diverse, multilingual\nmixture of sentence-segmented data, acting as a drop-in replacement and\nenhancement for existing segmentation tools. Overall, our contributions provide\na universal approach for segmenting any text. Our method outperforms all\nbaselines - including strong LLMs - across 8 corpora spanning diverse domains\nand languages, especially in practically relevant situations where text is\npoorly formatted. Our models and code, including documentation, are available\nat https://huggingface.co/segment-any-text under the MIT license.",
        "chunk-id": 5,
        "chunk": "and languages, especially in practically relevant situations where text is\npoorly formatted. Our models and code, including documentation, are available\nat https://huggingface.co/segment-any-text under the MIT license.",
        "authors": [
            "Markus Frohmann",
            "Igor Sterner",
            "Ivan Vuli\u0107",
            "Benjamin Minixhofer",
            "Markus Schedl"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T14:36:11+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16678v1",
        "arxiv_link": "http://arxiv.org/abs/2406.16678v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Machine Learning"
        ]
    },
    {
        "id": 20000027,
        "doi": null,
        "title": "The Privileged Students: On the Value of Initialization in Multilingual Knowledge Distillation",
        "abstract": "Knowledge distillation (KD) has proven to be a successful strategy to improve\nthe performance of a smaller model in many NLP tasks. However, most of the work\nin KD only explores monolingual scenarios. In this paper, we investigate the\nvalue of KD in multilingual settings. We find the significance of KD and model\ninitialization by analyzing how well the student model acquires multilingual\nknowledge from the teacher model. Our proposed method emphasizes copying the\nteacher model's weights directly to the student model to enhance\ninitialization. Our finding shows that model initialization using copy-weight\nfrom the fine-tuned teacher contributes the most compared to the distillation\nprocess itself across various multilingual settings. Furthermore, we\ndemonstrate that efficient weight initialization preserves multilingual\ncapabilities even in low-resource scenarios.",
        "chunk-id": 1,
        "chunk": "Knowledge distillation (KD) has proven to be a successful strategy to improve\nthe performance of a smaller model in many NLP tasks. However, most of the work\nin KD only explores monolingual scenarios. In this paper, we investigate the\nvalue of KD in multilingual settings. We find the significance of KD and model",
        "authors": [
            "Haryo Akbarianto Wibowo",
            "Thamar Solorio",
            "Alham Fikri Aji"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T10:59:26+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16524v1",
        "arxiv_link": "http://arxiv.org/abs/2406.16524v1",
        "categories": [
            "Computation and Language",
            "Natural language processing"
        ]
    },
    {
        "id": 20000027,
        "doi": null,
        "title": "The Privileged Students: On the Value of Initialization in Multilingual Knowledge Distillation",
        "abstract": "Knowledge distillation (KD) has proven to be a successful strategy to improve\nthe performance of a smaller model in many NLP tasks. However, most of the work\nin KD only explores monolingual scenarios. In this paper, we investigate the\nvalue of KD in multilingual settings. We find the significance of KD and model\ninitialization by analyzing how well the student model acquires multilingual\nknowledge from the teacher model. Our proposed method emphasizes copying the\nteacher model's weights directly to the student model to enhance\ninitialization. Our finding shows that model initialization using copy-weight\nfrom the fine-tuned teacher contributes the most compared to the distillation\nprocess itself across various multilingual settings. Furthermore, we\ndemonstrate that efficient weight initialization preserves multilingual\ncapabilities even in low-resource scenarios.",
        "chunk-id": 2,
        "chunk": "initialization by analyzing how well the student model acquires multilingual\nknowledge from the teacher model. Our proposed method emphasizes copying the\nteacher model's weights directly to the student model to enhance\ninitialization. Our finding shows that model initialization using copy-weight\nfrom the fine-tuned teacher contributes the most compared to the distillation",
        "authors": [
            "Haryo Akbarianto Wibowo",
            "Thamar Solorio",
            "Alham Fikri Aji"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T10:59:26+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16524v1",
        "arxiv_link": "http://arxiv.org/abs/2406.16524v1",
        "categories": [
            "Computation and Language",
            "Natural language processing"
        ]
    },
    {
        "id": 20000027,
        "doi": null,
        "title": "The Privileged Students: On the Value of Initialization in Multilingual Knowledge Distillation",
        "abstract": "Knowledge distillation (KD) has proven to be a successful strategy to improve\nthe performance of a smaller model in many NLP tasks. However, most of the work\nin KD only explores monolingual scenarios. In this paper, we investigate the\nvalue of KD in multilingual settings. We find the significance of KD and model\ninitialization by analyzing how well the student model acquires multilingual\nknowledge from the teacher model. Our proposed method emphasizes copying the\nteacher model's weights directly to the student model to enhance\ninitialization. Our finding shows that model initialization using copy-weight\nfrom the fine-tuned teacher contributes the most compared to the distillation\nprocess itself across various multilingual settings. Furthermore, we\ndemonstrate that efficient weight initialization preserves multilingual\ncapabilities even in low-resource scenarios.",
        "chunk-id": 3,
        "chunk": "process itself across various multilingual settings. Furthermore, we\ndemonstrate that efficient weight initialization preserves multilingual\ncapabilities even in low-resource scenarios.",
        "authors": [
            "Haryo Akbarianto Wibowo",
            "Thamar Solorio",
            "Alham Fikri Aji"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T10:59:26+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16524v1",
        "arxiv_link": "http://arxiv.org/abs/2406.16524v1",
        "categories": [
            "Computation and Language",
            "Natural language processing"
        ]
    },
    {
        "id": 20000028,
        "doi": null,
        "title": "Deepfake tweets automatic detection",
        "abstract": "This study addresses the critical challenge of detecting DeepFake tweets by\nleveraging advanced natural language processing (NLP) techniques to distinguish\nbetween genuine and AI-generated texts. Given the increasing prevalence of\nmisinformation, our research utilizes the TweepFake dataset to train and\nevaluate various machine learning models. The objective is to identify\neffective strategies for recognizing DeepFake content, thereby enhancing the\nintegrity of digital communications. By developing reliable methods for\ndetecting AI-generated misinformation, this work contributes to a more\ntrustworthy online information environment.",
        "chunk-id": 1,
        "chunk": "This study addresses the critical challenge of detecting DeepFake tweets by\nleveraging advanced natural language processing (NLP) techniques to distinguish\nbetween genuine and AI-generated texts. Given the increasing prevalence of\nmisinformation, our research utilizes the TweepFake dataset to train and\nevaluate various machine learning models. The objective is to identify",
        "authors": [
            "Adam Frej",
            "Adrian Kaminski",
            "Piotr Marciniak",
            "Szymon Szmajdzinski",
            "Soveatin Kuntur",
            "Anna Wroblewska"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T09:55:31+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16489v1",
        "arxiv_link": "http://arxiv.org/abs/2406.16489v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000028,
        "doi": null,
        "title": "Deepfake tweets automatic detection",
        "abstract": "This study addresses the critical challenge of detecting DeepFake tweets by\nleveraging advanced natural language processing (NLP) techniques to distinguish\nbetween genuine and AI-generated texts. Given the increasing prevalence of\nmisinformation, our research utilizes the TweepFake dataset to train and\nevaluate various machine learning models. The objective is to identify\neffective strategies for recognizing DeepFake content, thereby enhancing the\nintegrity of digital communications. By developing reliable methods for\ndetecting AI-generated misinformation, this work contributes to a more\ntrustworthy online information environment.",
        "chunk-id": 2,
        "chunk": "effective strategies for recognizing DeepFake content, thereby enhancing the\nintegrity of digital communications. By developing reliable methods for\ndetecting AI-generated misinformation, this work contributes to a more\ntrustworthy online information environment.",
        "authors": [
            "Adam Frej",
            "Adrian Kaminski",
            "Piotr Marciniak",
            "Szymon Szmajdzinski",
            "Soveatin Kuntur",
            "Anna Wroblewska"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T09:55:31+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16489v1",
        "arxiv_link": "http://arxiv.org/abs/2406.16489v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000029,
        "doi": null,
        "title": "UniCoder: Scaling Code Large Language Model via Universal Code",
        "abstract": "Intermediate reasoning or acting steps have successfully improved large\nlanguage models (LLMs) for handling various downstream natural language\nprocessing (NLP) tasks. When applying LLMs for code generation, recent works\nmainly focus on directing the models to articulate intermediate\nnatural-language reasoning steps, as in chain-of-thought (CoT) prompting, and\nthen output code with the natural language or other structured intermediate\nsteps. However, such output is not suitable for code translation or generation\ntasks since the standard CoT has different logical structures and forms of\nexpression with the code. In this work, we introduce the universal code\n(UniCode) as the intermediate representation. It is a description of algorithm\nsteps using a mix of conventions of programming languages, such as assignment\noperator, conditional operator, and loop. Hence, we collect an instruction\ndataset UniCoder-Instruct to train our model UniCoder on multi-task learning\nobjectives. UniCoder-Instruct comprises natural-language questions, code\nsolutions, and the corresponding universal code. The alignment between the\nintermediate universal code representation and the final code solution\nsignificantly improves the quality of the generated code. The experimental\nresults demonstrate that UniCoder with the universal code significantly\noutperforms the previous prompting methods by a large margin, showcasing the\neffectiveness of the structural clues in pseudo-code.",
        "chunk-id": 1,
        "chunk": "Intermediate reasoning or acting steps have successfully improved large\nlanguage models (LLMs) for handling various downstream natural language\nprocessing (NLP) tasks. When applying LLMs for code generation, recent works\nmainly focus on directing the models to articulate intermediate\nnatural-language reasoning steps, as in chain-of-thought (CoT) prompting, and",
        "authors": [
            "Tao Sun",
            "Linzheng Chai",
            "Jian Yang",
            "Yuwei Yin",
            "Hongcheng Guo",
            "Jiaheng Liu",
            "Bing Wang",
            "Liqun Yang",
            "Zhoujun Li"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T08:32:48+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16441v1",
        "arxiv_link": "http://arxiv.org/abs/2406.16441v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000029,
        "doi": null,
        "title": "UniCoder: Scaling Code Large Language Model via Universal Code",
        "abstract": "Intermediate reasoning or acting steps have successfully improved large\nlanguage models (LLMs) for handling various downstream natural language\nprocessing (NLP) tasks. When applying LLMs for code generation, recent works\nmainly focus on directing the models to articulate intermediate\nnatural-language reasoning steps, as in chain-of-thought (CoT) prompting, and\nthen output code with the natural language or other structured intermediate\nsteps. However, such output is not suitable for code translation or generation\ntasks since the standard CoT has different logical structures and forms of\nexpression with the code. In this work, we introduce the universal code\n(UniCode) as the intermediate representation. It is a description of algorithm\nsteps using a mix of conventions of programming languages, such as assignment\noperator, conditional operator, and loop. Hence, we collect an instruction\ndataset UniCoder-Instruct to train our model UniCoder on multi-task learning\nobjectives. UniCoder-Instruct comprises natural-language questions, code\nsolutions, and the corresponding universal code. The alignment between the\nintermediate universal code representation and the final code solution\nsignificantly improves the quality of the generated code. The experimental\nresults demonstrate that UniCoder with the universal code significantly\noutperforms the previous prompting methods by a large margin, showcasing the\neffectiveness of the structural clues in pseudo-code.",
        "chunk-id": 2,
        "chunk": "then output code with the natural language or other structured intermediate\nsteps. However, such output is not suitable for code translation or generation\ntasks since the standard CoT has different logical structures and forms of\nexpression with the code. In this work, we introduce the universal code\n(UniCode) as the intermediate representation. It is a description of algorithm",
        "authors": [
            "Tao Sun",
            "Linzheng Chai",
            "Jian Yang",
            "Yuwei Yin",
            "Hongcheng Guo",
            "Jiaheng Liu",
            "Bing Wang",
            "Liqun Yang",
            "Zhoujun Li"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T08:32:48+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16441v1",
        "arxiv_link": "http://arxiv.org/abs/2406.16441v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000029,
        "doi": null,
        "title": "UniCoder: Scaling Code Large Language Model via Universal Code",
        "abstract": "Intermediate reasoning or acting steps have successfully improved large\nlanguage models (LLMs) for handling various downstream natural language\nprocessing (NLP) tasks. When applying LLMs for code generation, recent works\nmainly focus on directing the models to articulate intermediate\nnatural-language reasoning steps, as in chain-of-thought (CoT) prompting, and\nthen output code with the natural language or other structured intermediate\nsteps. However, such output is not suitable for code translation or generation\ntasks since the standard CoT has different logical structures and forms of\nexpression with the code. In this work, we introduce the universal code\n(UniCode) as the intermediate representation. It is a description of algorithm\nsteps using a mix of conventions of programming languages, such as assignment\noperator, conditional operator, and loop. Hence, we collect an instruction\ndataset UniCoder-Instruct to train our model UniCoder on multi-task learning\nobjectives. UniCoder-Instruct comprises natural-language questions, code\nsolutions, and the corresponding universal code. The alignment between the\nintermediate universal code representation and the final code solution\nsignificantly improves the quality of the generated code. The experimental\nresults demonstrate that UniCoder with the universal code significantly\noutperforms the previous prompting methods by a large margin, showcasing the\neffectiveness of the structural clues in pseudo-code.",
        "chunk-id": 3,
        "chunk": "steps using a mix of conventions of programming languages, such as assignment\noperator, conditional operator, and loop. Hence, we collect an instruction\ndataset UniCoder-Instruct to train our model UniCoder on multi-task learning\nobjectives. UniCoder-Instruct comprises natural-language questions, code\nsolutions, and the corresponding universal code. The alignment between the",
        "authors": [
            "Tao Sun",
            "Linzheng Chai",
            "Jian Yang",
            "Yuwei Yin",
            "Hongcheng Guo",
            "Jiaheng Liu",
            "Bing Wang",
            "Liqun Yang",
            "Zhoujun Li"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T08:32:48+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16441v1",
        "arxiv_link": "http://arxiv.org/abs/2406.16441v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000029,
        "doi": null,
        "title": "UniCoder: Scaling Code Large Language Model via Universal Code",
        "abstract": "Intermediate reasoning or acting steps have successfully improved large\nlanguage models (LLMs) for handling various downstream natural language\nprocessing (NLP) tasks. When applying LLMs for code generation, recent works\nmainly focus on directing the models to articulate intermediate\nnatural-language reasoning steps, as in chain-of-thought (CoT) prompting, and\nthen output code with the natural language or other structured intermediate\nsteps. However, such output is not suitable for code translation or generation\ntasks since the standard CoT has different logical structures and forms of\nexpression with the code. In this work, we introduce the universal code\n(UniCode) as the intermediate representation. It is a description of algorithm\nsteps using a mix of conventions of programming languages, such as assignment\noperator, conditional operator, and loop. Hence, we collect an instruction\ndataset UniCoder-Instruct to train our model UniCoder on multi-task learning\nobjectives. UniCoder-Instruct comprises natural-language questions, code\nsolutions, and the corresponding universal code. The alignment between the\nintermediate universal code representation and the final code solution\nsignificantly improves the quality of the generated code. The experimental\nresults demonstrate that UniCoder with the universal code significantly\noutperforms the previous prompting methods by a large margin, showcasing the\neffectiveness of the structural clues in pseudo-code.",
        "chunk-id": 4,
        "chunk": "intermediate universal code representation and the final code solution\nsignificantly improves the quality of the generated code. The experimental\nresults demonstrate that UniCoder with the universal code significantly\noutperforms the previous prompting methods by a large margin, showcasing the\neffectiveness of the structural clues in pseudo-code.",
        "authors": [
            "Tao Sun",
            "Linzheng Chai",
            "Jian Yang",
            "Yuwei Yin",
            "Hongcheng Guo",
            "Jiaheng Liu",
            "Bing Wang",
            "Liqun Yang",
            "Zhoujun Li"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T08:32:48+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16441v1",
        "arxiv_link": "http://arxiv.org/abs/2406.16441v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000030,
        "doi": null,
        "title": "Does Cross-Cultural Alignment Change the Commonsense Morality of Language Models?",
        "abstract": "Alignment of the language model with human preferences is a common approach\nto making a language model useful to end users. However, most alignment work is\ndone in English, and human preference datasets are dominated by English,\nreflecting only the preferences of English-speaking annotators. Nevertheless,\nit is common practice to use the English preference data, either directly or by\ntranslating it into the target language, when aligning a multilingual language\nmodel. The question is whether such an alignment strategy marginalizes the\npreference of non-English speaking users. To this end, we investigate the\neffect of aligning Japanese language models with (mostly) English resources. In\nparticular, we focus on evaluating whether the commonsense morality of the\nresulting fine-tuned models is aligned with Japanese culture using the\nJCommonsenseMorality (JCM) and ETHICS datasets. The experimental results show\nthat the fine-tuned model outperforms the SFT model. However, it does not\ndemonstrate the same level of improvement as a model fine-tuned using the JCM,\nsuggesting that while some aspects of commonsense morality are transferable,\nothers may not be.",
        "chunk-id": 1,
        "chunk": "Alignment of the language model with human preferences is a common approach\nto making a language model useful to end users. However, most alignment work is\ndone in English, and human preference datasets are dominated by English,\nreflecting only the preferences of English-speaking annotators. Nevertheless,",
        "authors": [
            "Yuu Jinnai"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T04:50:12+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16316v1",
        "arxiv_link": "http://arxiv.org/abs/2406.16316v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Computers and Society",
            "Machine Learning"
        ]
    },
    {
        "id": 20000030,
        "doi": null,
        "title": "Does Cross-Cultural Alignment Change the Commonsense Morality of Language Models?",
        "abstract": "Alignment of the language model with human preferences is a common approach\nto making a language model useful to end users. However, most alignment work is\ndone in English, and human preference datasets are dominated by English,\nreflecting only the preferences of English-speaking annotators. Nevertheless,\nit is common practice to use the English preference data, either directly or by\ntranslating it into the target language, when aligning a multilingual language\nmodel. The question is whether such an alignment strategy marginalizes the\npreference of non-English speaking users. To this end, we investigate the\neffect of aligning Japanese language models with (mostly) English resources. In\nparticular, we focus on evaluating whether the commonsense morality of the\nresulting fine-tuned models is aligned with Japanese culture using the\nJCommonsenseMorality (JCM) and ETHICS datasets. The experimental results show\nthat the fine-tuned model outperforms the SFT model. However, it does not\ndemonstrate the same level of improvement as a model fine-tuned using the JCM,\nsuggesting that while some aspects of commonsense morality are transferable,\nothers may not be.",
        "chunk-id": 2,
        "chunk": "it is common practice to use the English preference data, either directly or by\ntranslating it into the target language, when aligning a multilingual language\nmodel. The question is whether such an alignment strategy marginalizes the\npreference of non-English speaking users. To this end, we investigate the",
        "authors": [
            "Yuu Jinnai"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T04:50:12+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16316v1",
        "arxiv_link": "http://arxiv.org/abs/2406.16316v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Computers and Society",
            "Machine Learning"
        ]
    },
    {
        "id": 20000030,
        "doi": null,
        "title": "Does Cross-Cultural Alignment Change the Commonsense Morality of Language Models?",
        "abstract": "Alignment of the language model with human preferences is a common approach\nto making a language model useful to end users. However, most alignment work is\ndone in English, and human preference datasets are dominated by English,\nreflecting only the preferences of English-speaking annotators. Nevertheless,\nit is common practice to use the English preference data, either directly or by\ntranslating it into the target language, when aligning a multilingual language\nmodel. The question is whether such an alignment strategy marginalizes the\npreference of non-English speaking users. To this end, we investigate the\neffect of aligning Japanese language models with (mostly) English resources. In\nparticular, we focus on evaluating whether the commonsense morality of the\nresulting fine-tuned models is aligned with Japanese culture using the\nJCommonsenseMorality (JCM) and ETHICS datasets. The experimental results show\nthat the fine-tuned model outperforms the SFT model. However, it does not\ndemonstrate the same level of improvement as a model fine-tuned using the JCM,\nsuggesting that while some aspects of commonsense morality are transferable,\nothers may not be.",
        "chunk-id": 3,
        "chunk": "effect of aligning Japanese language models with (mostly) English resources. In\nparticular, we focus on evaluating whether the commonsense morality of the\nresulting fine-tuned models is aligned with Japanese culture using the\nJCommonsenseMorality (JCM) and ETHICS datasets. The experimental results show\nthat the fine-tuned model outperforms the SFT model. However, it does not",
        "authors": [
            "Yuu Jinnai"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T04:50:12+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16316v1",
        "arxiv_link": "http://arxiv.org/abs/2406.16316v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Computers and Society",
            "Machine Learning"
        ]
    },
    {
        "id": 20000030,
        "doi": null,
        "title": "Does Cross-Cultural Alignment Change the Commonsense Morality of Language Models?",
        "abstract": "Alignment of the language model with human preferences is a common approach\nto making a language model useful to end users. However, most alignment work is\ndone in English, and human preference datasets are dominated by English,\nreflecting only the preferences of English-speaking annotators. Nevertheless,\nit is common practice to use the English preference data, either directly or by\ntranslating it into the target language, when aligning a multilingual language\nmodel. The question is whether such an alignment strategy marginalizes the\npreference of non-English speaking users. To this end, we investigate the\neffect of aligning Japanese language models with (mostly) English resources. In\nparticular, we focus on evaluating whether the commonsense morality of the\nresulting fine-tuned models is aligned with Japanese culture using the\nJCommonsenseMorality (JCM) and ETHICS datasets. The experimental results show\nthat the fine-tuned model outperforms the SFT model. However, it does not\ndemonstrate the same level of improvement as a model fine-tuned using the JCM,\nsuggesting that while some aspects of commonsense morality are transferable,\nothers may not be.",
        "chunk-id": 4,
        "chunk": "demonstrate the same level of improvement as a model fine-tuned using the JCM,\nsuggesting that while some aspects of commonsense morality are transferable,\nothers may not be.",
        "authors": [
            "Yuu Jinnai"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T04:50:12+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16316v1",
        "arxiv_link": "http://arxiv.org/abs/2406.16316v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Computers and Society",
            "Machine Learning"
        ]
    },
    {
        "id": 20000031,
        "doi": null,
        "title": "LLMs Assist NLP Researchers: Critique Paper (Meta-)Reviewing",
        "abstract": "This work is motivated by two key trends. On one hand, large language models\n(LLMs) have shown remarkable versatility in various generative tasks such as\nwriting, drawing, and question answering, significantly reducing the time\nrequired for many routine tasks. On the other hand, researchers, whose work is\nnot only time-consuming but also highly expertise-demanding, face increasing\nchallenges as they have to spend more time reading, writing, and reviewing\npapers. This raises the question: how can LLMs potentially assist researchers\nin alleviating their heavy workload?\n  This study focuses on the topic of LLMs assist NLP Researchers, particularly\nexamining the effectiveness of LLM in assisting paper (meta-)reviewing and its\nrecognizability. To address this, we constructed the ReviewCritique dataset,\nwhich includes two types of information: (i) NLP papers (initial submissions\nrather than camera-ready) with both human-written and LLM-generated reviews,\nand (ii) each review comes with \"deficiency\" labels and corresponding\nexplanations for individual segments, annotated by experts. Using\nReviewCritique, this study explores two threads of research questions: (i)\n\"LLMs as Reviewers\", how do reviews generated by LLMs compare with those\nwritten by humans in terms of quality and distinguishability? (ii) \"LLMs as\nMetareviewers\", how effectively can LLMs identify potential issues, such as\nDeficient or unprofessional review segments, within individual paper reviews?\nTo our knowledge, this is the first work to provide such a comprehensive\nanalysis.",
        "chunk-id": 1,
        "chunk": "This work is motivated by two key trends. On one hand, large language models\n(LLMs) have shown remarkable versatility in various generative tasks such as\nwriting, drawing, and question answering, significantly reducing the time\nrequired for many routine tasks. On the other hand, researchers, whose work is\nnot only time-consuming but also highly expertise-demanding, face increasing",
        "authors": [
            "Jiangshu Du",
            "Yibo Wang",
            "Wenting Zhao",
            "Zhongfen Deng",
            "Shuaiqi Liu",
            "Renze Lou",
            "Henry Peng Zou",
            "Pranav Narayanan Venkit",
            "Nan Zhang",
            "Mukund Srinath",
            "Haoran Ranran Zhang",
            "Vipul Gupta",
            "Yinghui Li",
            "Tao Li",
            "Fei Wang",
            "Qin Liu",
            "Tianlin Liu",
            "Pengzhi Gao",
            "Congying Xia",
            "Chen Xing",
            "Jiayang Cheng",
            "Zhaowei Wang",
            "Ying Su",
            "Raj Sanjay Shah",
            "Ruohao Guo",
            "Jing Gu",
            "Haoran Li",
            "Kangda Wei",
            "Zihao Wang",
            "Lu Cheng",
            "Surangika Ranathunga",
            "Meng Fang",
            "Jie Fu",
            "Fei Liu",
            "Ruihong Huang",
            "Eduardo Blanco",
            "Yixin Cao",
            "Rui Zhang",
            "Philip S. Yu",
            "Wenpeng Yin"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T01:30:22+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16253v2",
        "arxiv_link": "http://arxiv.org/abs/2406.16253v2",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000031,
        "doi": null,
        "title": "LLMs Assist NLP Researchers: Critique Paper (Meta-)Reviewing",
        "abstract": "This work is motivated by two key trends. On one hand, large language models\n(LLMs) have shown remarkable versatility in various generative tasks such as\nwriting, drawing, and question answering, significantly reducing the time\nrequired for many routine tasks. On the other hand, researchers, whose work is\nnot only time-consuming but also highly expertise-demanding, face increasing\nchallenges as they have to spend more time reading, writing, and reviewing\npapers. This raises the question: how can LLMs potentially assist researchers\nin alleviating their heavy workload?\n  This study focuses on the topic of LLMs assist NLP Researchers, particularly\nexamining the effectiveness of LLM in assisting paper (meta-)reviewing and its\nrecognizability. To address this, we constructed the ReviewCritique dataset,\nwhich includes two types of information: (i) NLP papers (initial submissions\nrather than camera-ready) with both human-written and LLM-generated reviews,\nand (ii) each review comes with \"deficiency\" labels and corresponding\nexplanations for individual segments, annotated by experts. Using\nReviewCritique, this study explores two threads of research questions: (i)\n\"LLMs as Reviewers\", how do reviews generated by LLMs compare with those\nwritten by humans in terms of quality and distinguishability? (ii) \"LLMs as\nMetareviewers\", how effectively can LLMs identify potential issues, such as\nDeficient or unprofessional review segments, within individual paper reviews?\nTo our knowledge, this is the first work to provide such a comprehensive\nanalysis.",
        "chunk-id": 2,
        "chunk": "challenges as they have to spend more time reading, writing, and reviewing\npapers. This raises the question: how can LLMs potentially assist researchers\nin alleviating their heavy workload?\n  This study focuses on the topic of LLMs assist NLP Researchers, particularly\nexamining the effectiveness of LLM in assisting paper (meta-)reviewing and its",
        "authors": [
            "Jiangshu Du",
            "Yibo Wang",
            "Wenting Zhao",
            "Zhongfen Deng",
            "Shuaiqi Liu",
            "Renze Lou",
            "Henry Peng Zou",
            "Pranav Narayanan Venkit",
            "Nan Zhang",
            "Mukund Srinath",
            "Haoran Ranran Zhang",
            "Vipul Gupta",
            "Yinghui Li",
            "Tao Li",
            "Fei Wang",
            "Qin Liu",
            "Tianlin Liu",
            "Pengzhi Gao",
            "Congying Xia",
            "Chen Xing",
            "Jiayang Cheng",
            "Zhaowei Wang",
            "Ying Su",
            "Raj Sanjay Shah",
            "Ruohao Guo",
            "Jing Gu",
            "Haoran Li",
            "Kangda Wei",
            "Zihao Wang",
            "Lu Cheng",
            "Surangika Ranathunga",
            "Meng Fang",
            "Jie Fu",
            "Fei Liu",
            "Ruihong Huang",
            "Eduardo Blanco",
            "Yixin Cao",
            "Rui Zhang",
            "Philip S. Yu",
            "Wenpeng Yin"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T01:30:22+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16253v2",
        "arxiv_link": "http://arxiv.org/abs/2406.16253v2",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000031,
        "doi": null,
        "title": "LLMs Assist NLP Researchers: Critique Paper (Meta-)Reviewing",
        "abstract": "This work is motivated by two key trends. On one hand, large language models\n(LLMs) have shown remarkable versatility in various generative tasks such as\nwriting, drawing, and question answering, significantly reducing the time\nrequired for many routine tasks. On the other hand, researchers, whose work is\nnot only time-consuming but also highly expertise-demanding, face increasing\nchallenges as they have to spend more time reading, writing, and reviewing\npapers. This raises the question: how can LLMs potentially assist researchers\nin alleviating their heavy workload?\n  This study focuses on the topic of LLMs assist NLP Researchers, particularly\nexamining the effectiveness of LLM in assisting paper (meta-)reviewing and its\nrecognizability. To address this, we constructed the ReviewCritique dataset,\nwhich includes two types of information: (i) NLP papers (initial submissions\nrather than camera-ready) with both human-written and LLM-generated reviews,\nand (ii) each review comes with \"deficiency\" labels and corresponding\nexplanations for individual segments, annotated by experts. Using\nReviewCritique, this study explores two threads of research questions: (i)\n\"LLMs as Reviewers\", how do reviews generated by LLMs compare with those\nwritten by humans in terms of quality and distinguishability? (ii) \"LLMs as\nMetareviewers\", how effectively can LLMs identify potential issues, such as\nDeficient or unprofessional review segments, within individual paper reviews?\nTo our knowledge, this is the first work to provide such a comprehensive\nanalysis.",
        "chunk-id": 3,
        "chunk": "recognizability. To address this, we constructed the ReviewCritique dataset,\nwhich includes two types of information: (i) NLP papers (initial submissions\nrather than camera-ready) with both human-written and LLM-generated reviews,\nand (ii) each review comes with \"deficiency\" labels and corresponding\nexplanations for individual segments, annotated by experts. Using",
        "authors": [
            "Jiangshu Du",
            "Yibo Wang",
            "Wenting Zhao",
            "Zhongfen Deng",
            "Shuaiqi Liu",
            "Renze Lou",
            "Henry Peng Zou",
            "Pranav Narayanan Venkit",
            "Nan Zhang",
            "Mukund Srinath",
            "Haoran Ranran Zhang",
            "Vipul Gupta",
            "Yinghui Li",
            "Tao Li",
            "Fei Wang",
            "Qin Liu",
            "Tianlin Liu",
            "Pengzhi Gao",
            "Congying Xia",
            "Chen Xing",
            "Jiayang Cheng",
            "Zhaowei Wang",
            "Ying Su",
            "Raj Sanjay Shah",
            "Ruohao Guo",
            "Jing Gu",
            "Haoran Li",
            "Kangda Wei",
            "Zihao Wang",
            "Lu Cheng",
            "Surangika Ranathunga",
            "Meng Fang",
            "Jie Fu",
            "Fei Liu",
            "Ruihong Huang",
            "Eduardo Blanco",
            "Yixin Cao",
            "Rui Zhang",
            "Philip S. Yu",
            "Wenpeng Yin"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T01:30:22+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16253v2",
        "arxiv_link": "http://arxiv.org/abs/2406.16253v2",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000031,
        "doi": null,
        "title": "LLMs Assist NLP Researchers: Critique Paper (Meta-)Reviewing",
        "abstract": "This work is motivated by two key trends. On one hand, large language models\n(LLMs) have shown remarkable versatility in various generative tasks such as\nwriting, drawing, and question answering, significantly reducing the time\nrequired for many routine tasks. On the other hand, researchers, whose work is\nnot only time-consuming but also highly expertise-demanding, face increasing\nchallenges as they have to spend more time reading, writing, and reviewing\npapers. This raises the question: how can LLMs potentially assist researchers\nin alleviating their heavy workload?\n  This study focuses on the topic of LLMs assist NLP Researchers, particularly\nexamining the effectiveness of LLM in assisting paper (meta-)reviewing and its\nrecognizability. To address this, we constructed the ReviewCritique dataset,\nwhich includes two types of information: (i) NLP papers (initial submissions\nrather than camera-ready) with both human-written and LLM-generated reviews,\nand (ii) each review comes with \"deficiency\" labels and corresponding\nexplanations for individual segments, annotated by experts. Using\nReviewCritique, this study explores two threads of research questions: (i)\n\"LLMs as Reviewers\", how do reviews generated by LLMs compare with those\nwritten by humans in terms of quality and distinguishability? (ii) \"LLMs as\nMetareviewers\", how effectively can LLMs identify potential issues, such as\nDeficient or unprofessional review segments, within individual paper reviews?\nTo our knowledge, this is the first work to provide such a comprehensive\nanalysis.",
        "chunk-id": 4,
        "chunk": "ReviewCritique, this study explores two threads of research questions: (i)\n\"LLMs as Reviewers\", how do reviews generated by LLMs compare with those\nwritten by humans in terms of quality and distinguishability? (ii) \"LLMs as\nMetareviewers\", how effectively can LLMs identify potential issues, such as\nDeficient or unprofessional review segments, within individual paper reviews?",
        "authors": [
            "Jiangshu Du",
            "Yibo Wang",
            "Wenting Zhao",
            "Zhongfen Deng",
            "Shuaiqi Liu",
            "Renze Lou",
            "Henry Peng Zou",
            "Pranav Narayanan Venkit",
            "Nan Zhang",
            "Mukund Srinath",
            "Haoran Ranran Zhang",
            "Vipul Gupta",
            "Yinghui Li",
            "Tao Li",
            "Fei Wang",
            "Qin Liu",
            "Tianlin Liu",
            "Pengzhi Gao",
            "Congying Xia",
            "Chen Xing",
            "Jiayang Cheng",
            "Zhaowei Wang",
            "Ying Su",
            "Raj Sanjay Shah",
            "Ruohao Guo",
            "Jing Gu",
            "Haoran Li",
            "Kangda Wei",
            "Zihao Wang",
            "Lu Cheng",
            "Surangika Ranathunga",
            "Meng Fang",
            "Jie Fu",
            "Fei Liu",
            "Ruihong Huang",
            "Eduardo Blanco",
            "Yixin Cao",
            "Rui Zhang",
            "Philip S. Yu",
            "Wenpeng Yin"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T01:30:22+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16253v2",
        "arxiv_link": "http://arxiv.org/abs/2406.16253v2",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000031,
        "doi": null,
        "title": "LLMs Assist NLP Researchers: Critique Paper (Meta-)Reviewing",
        "abstract": "This work is motivated by two key trends. On one hand, large language models\n(LLMs) have shown remarkable versatility in various generative tasks such as\nwriting, drawing, and question answering, significantly reducing the time\nrequired for many routine tasks. On the other hand, researchers, whose work is\nnot only time-consuming but also highly expertise-demanding, face increasing\nchallenges as they have to spend more time reading, writing, and reviewing\npapers. This raises the question: how can LLMs potentially assist researchers\nin alleviating their heavy workload?\n  This study focuses on the topic of LLMs assist NLP Researchers, particularly\nexamining the effectiveness of LLM in assisting paper (meta-)reviewing and its\nrecognizability. To address this, we constructed the ReviewCritique dataset,\nwhich includes two types of information: (i) NLP papers (initial submissions\nrather than camera-ready) with both human-written and LLM-generated reviews,\nand (ii) each review comes with \"deficiency\" labels and corresponding\nexplanations for individual segments, annotated by experts. Using\nReviewCritique, this study explores two threads of research questions: (i)\n\"LLMs as Reviewers\", how do reviews generated by LLMs compare with those\nwritten by humans in terms of quality and distinguishability? (ii) \"LLMs as\nMetareviewers\", how effectively can LLMs identify potential issues, such as\nDeficient or unprofessional review segments, within individual paper reviews?\nTo our knowledge, this is the first work to provide such a comprehensive\nanalysis.",
        "chunk-id": 5,
        "chunk": "To our knowledge, this is the first work to provide such a comprehensive\nanalysis.",
        "authors": [
            "Jiangshu Du",
            "Yibo Wang",
            "Wenting Zhao",
            "Zhongfen Deng",
            "Shuaiqi Liu",
            "Renze Lou",
            "Henry Peng Zou",
            "Pranav Narayanan Venkit",
            "Nan Zhang",
            "Mukund Srinath",
            "Haoran Ranran Zhang",
            "Vipul Gupta",
            "Yinghui Li",
            "Tao Li",
            "Fei Wang",
            "Qin Liu",
            "Tianlin Liu",
            "Pengzhi Gao",
            "Congying Xia",
            "Chen Xing",
            "Jiayang Cheng",
            "Zhaowei Wang",
            "Ying Su",
            "Raj Sanjay Shah",
            "Ruohao Guo",
            "Jing Gu",
            "Haoran Li",
            "Kangda Wei",
            "Zihao Wang",
            "Lu Cheng",
            "Surangika Ranathunga",
            "Meng Fang",
            "Jie Fu",
            "Fei Liu",
            "Ruihong Huang",
            "Eduardo Blanco",
            "Yixin Cao",
            "Rui Zhang",
            "Philip S. Yu",
            "Wenpeng Yin"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-24T01:30:22+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16253v2",
        "arxiv_link": "http://arxiv.org/abs/2406.16253v2",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000032,
        "doi": null,
        "title": "GraphEval2000: Benchmarking and Improving Large Language Models on Graph Datasets",
        "abstract": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage processing (NLP), demonstrating significant capabilities in processing\nand understanding text data. However, recent studies have identified\nlimitations in LLMs' ability to reason about graph-structured data. To address\nthis gap, we introduce GraphEval2000, the first comprehensive graph dataset,\ncomprising 40 graph data structure problems along with 2000 test cases.\nAdditionally, we introduce an evaluation framework based on GraphEval2000,\ndesigned to assess the graph reasoning abilities of LLMs through coding\nchallenges. Our dataset categorizes test cases into four primary and four\nsub-categories, ensuring a comprehensive evaluation. We evaluate eight popular\nLLMs on GraphEval2000, revealing that LLMs exhibit a better understanding of\ndirected graphs compared to undirected ones. While private LLMs consistently\noutperform open-source models, the performance gap is narrowing. Furthermore,\nto improve the usability of our evaluation framework, we propose Structured\nSymbolic Decomposition (SSD), an instruction-based method designed to enhance\nLLM performance on GraphEval2000. Results show that SSD improves the\nperformance of GPT-3.5, GPT-4, and GPT-4o on complex graph problems, with an\nincrease of 11.11\\%, 33.37\\%, and 33.37\\%, respectively.",
        "chunk-id": 1,
        "chunk": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage processing (NLP), demonstrating significant capabilities in processing\nand understanding text data. However, recent studies have identified\nlimitations in LLMs' ability to reason about graph-structured data. To address\nthis gap, we introduce GraphEval2000, the first comprehensive graph dataset,",
        "authors": [
            "Qiming Wu",
            "Zichen Chen",
            "Will Corcoran",
            "Misha Sra",
            "Ambuj K. Singh"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-23T18:01:56+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16176v1",
        "arxiv_link": "http://arxiv.org/abs/2406.16176v1",
        "categories": [
            "Artificial Intelligence",
            "Computation and Language",
            "Machine Learning",
            "Database Applications, Natural Language Processing"
        ]
    },
    {
        "id": 20000032,
        "doi": null,
        "title": "GraphEval2000: Benchmarking and Improving Large Language Models on Graph Datasets",
        "abstract": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage processing (NLP), demonstrating significant capabilities in processing\nand understanding text data. However, recent studies have identified\nlimitations in LLMs' ability to reason about graph-structured data. To address\nthis gap, we introduce GraphEval2000, the first comprehensive graph dataset,\ncomprising 40 graph data structure problems along with 2000 test cases.\nAdditionally, we introduce an evaluation framework based on GraphEval2000,\ndesigned to assess the graph reasoning abilities of LLMs through coding\nchallenges. Our dataset categorizes test cases into four primary and four\nsub-categories, ensuring a comprehensive evaluation. We evaluate eight popular\nLLMs on GraphEval2000, revealing that LLMs exhibit a better understanding of\ndirected graphs compared to undirected ones. While private LLMs consistently\noutperform open-source models, the performance gap is narrowing. Furthermore,\nto improve the usability of our evaluation framework, we propose Structured\nSymbolic Decomposition (SSD), an instruction-based method designed to enhance\nLLM performance on GraphEval2000. Results show that SSD improves the\nperformance of GPT-3.5, GPT-4, and GPT-4o on complex graph problems, with an\nincrease of 11.11\\%, 33.37\\%, and 33.37\\%, respectively.",
        "chunk-id": 2,
        "chunk": "comprising 40 graph data structure problems along with 2000 test cases.\nAdditionally, we introduce an evaluation framework based on GraphEval2000,\ndesigned to assess the graph reasoning abilities of LLMs through coding\nchallenges. Our dataset categorizes test cases into four primary and four\nsub-categories, ensuring a comprehensive evaluation. We evaluate eight popular",
        "authors": [
            "Qiming Wu",
            "Zichen Chen",
            "Will Corcoran",
            "Misha Sra",
            "Ambuj K. Singh"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-23T18:01:56+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16176v1",
        "arxiv_link": "http://arxiv.org/abs/2406.16176v1",
        "categories": [
            "Artificial Intelligence",
            "Computation and Language",
            "Machine Learning",
            "Database Applications, Natural Language Processing"
        ]
    },
    {
        "id": 20000032,
        "doi": null,
        "title": "GraphEval2000: Benchmarking and Improving Large Language Models on Graph Datasets",
        "abstract": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage processing (NLP), demonstrating significant capabilities in processing\nand understanding text data. However, recent studies have identified\nlimitations in LLMs' ability to reason about graph-structured data. To address\nthis gap, we introduce GraphEval2000, the first comprehensive graph dataset,\ncomprising 40 graph data structure problems along with 2000 test cases.\nAdditionally, we introduce an evaluation framework based on GraphEval2000,\ndesigned to assess the graph reasoning abilities of LLMs through coding\nchallenges. Our dataset categorizes test cases into four primary and four\nsub-categories, ensuring a comprehensive evaluation. We evaluate eight popular\nLLMs on GraphEval2000, revealing that LLMs exhibit a better understanding of\ndirected graphs compared to undirected ones. While private LLMs consistently\noutperform open-source models, the performance gap is narrowing. Furthermore,\nto improve the usability of our evaluation framework, we propose Structured\nSymbolic Decomposition (SSD), an instruction-based method designed to enhance\nLLM performance on GraphEval2000. Results show that SSD improves the\nperformance of GPT-3.5, GPT-4, and GPT-4o on complex graph problems, with an\nincrease of 11.11\\%, 33.37\\%, and 33.37\\%, respectively.",
        "chunk-id": 3,
        "chunk": "LLMs on GraphEval2000, revealing that LLMs exhibit a better understanding of\ndirected graphs compared to undirected ones. While private LLMs consistently\noutperform open-source models, the performance gap is narrowing. Furthermore,\nto improve the usability of our evaluation framework, we propose Structured",
        "authors": [
            "Qiming Wu",
            "Zichen Chen",
            "Will Corcoran",
            "Misha Sra",
            "Ambuj K. Singh"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-23T18:01:56+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16176v1",
        "arxiv_link": "http://arxiv.org/abs/2406.16176v1",
        "categories": [
            "Artificial Intelligence",
            "Computation and Language",
            "Machine Learning",
            "Database Applications, Natural Language Processing"
        ]
    },
    {
        "id": 20000032,
        "doi": null,
        "title": "GraphEval2000: Benchmarking and Improving Large Language Models on Graph Datasets",
        "abstract": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage processing (NLP), demonstrating significant capabilities in processing\nand understanding text data. However, recent studies have identified\nlimitations in LLMs' ability to reason about graph-structured data. To address\nthis gap, we introduce GraphEval2000, the first comprehensive graph dataset,\ncomprising 40 graph data structure problems along with 2000 test cases.\nAdditionally, we introduce an evaluation framework based on GraphEval2000,\ndesigned to assess the graph reasoning abilities of LLMs through coding\nchallenges. Our dataset categorizes test cases into four primary and four\nsub-categories, ensuring a comprehensive evaluation. We evaluate eight popular\nLLMs on GraphEval2000, revealing that LLMs exhibit a better understanding of\ndirected graphs compared to undirected ones. While private LLMs consistently\noutperform open-source models, the performance gap is narrowing. Furthermore,\nto improve the usability of our evaluation framework, we propose Structured\nSymbolic Decomposition (SSD), an instruction-based method designed to enhance\nLLM performance on GraphEval2000. Results show that SSD improves the\nperformance of GPT-3.5, GPT-4, and GPT-4o on complex graph problems, with an\nincrease of 11.11\\%, 33.37\\%, and 33.37\\%, respectively.",
        "chunk-id": 4,
        "chunk": "Symbolic Decomposition (SSD), an instruction-based method designed to enhance\nLLM performance on GraphEval2000. Results show that SSD improves the\nperformance of GPT-3.5, GPT-4, and GPT-4o on complex graph problems, with an\nincrease of 11.11\\%, 33.37\\%, and 33.37\\%, respectively.",
        "authors": [
            "Qiming Wu",
            "Zichen Chen",
            "Will Corcoran",
            "Misha Sra",
            "Ambuj K. Singh"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-23T18:01:56+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.16176v1",
        "arxiv_link": "http://arxiv.org/abs/2406.16176v1",
        "categories": [
            "Artificial Intelligence",
            "Computation and Language",
            "Machine Learning",
            "Database Applications, Natural Language Processing"
        ]
    },
    {
        "id": 20000033,
        "doi": null,
        "title": "LaMSUM: A Novel Framework for Extractive Summarization of User Generated Content using LLMs",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of NLP tasks, including summarization. Inherently LLMs produce\nabstractive summaries, and the task of achieving extractive summaries through\nLLMs still remains largely unexplored. To bridge this gap, in this work, we\npropose a novel framework LaMSUM to generate extractive summaries through LLMs\nfor large user-generated text by leveraging voting algorithms. Our evaluation\non three popular open-source LLMs (Llama 3, Mixtral and Gemini) reveal that the\nLaMSUM outperforms state-of-the-art extractive summarization methods. We\nfurther attempt to provide the rationale behind the output summary produced by\nLLMs. Overall, this is one of the early attempts to achieve extractive\nsummarization for large user-generated text by utilizing LLMs, and likely to\ngenerate further interest in the community.",
        "chunk-id": 1,
        "chunk": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of NLP tasks, including summarization. Inherently LLMs produce\nabstractive summaries, and the task of achieving extractive summaries through\nLLMs still remains largely unexplored. To bridge this gap, in this work, we",
        "authors": [
            "Garima Chhikara",
            "Anurag Sharma",
            "V. Gurucharan",
            "Kripabandhu Ghosh",
            "Abhijnan Chakraborty"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-22T10:25:55+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.15809v1",
        "arxiv_link": "http://arxiv.org/abs/2406.15809v1",
        "categories": [
            "Computation and Language",
            "Machine Learning"
        ]
    },
    {
        "id": 20000033,
        "doi": null,
        "title": "LaMSUM: A Novel Framework for Extractive Summarization of User Generated Content using LLMs",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of NLP tasks, including summarization. Inherently LLMs produce\nabstractive summaries, and the task of achieving extractive summaries through\nLLMs still remains largely unexplored. To bridge this gap, in this work, we\npropose a novel framework LaMSUM to generate extractive summaries through LLMs\nfor large user-generated text by leveraging voting algorithms. Our evaluation\non three popular open-source LLMs (Llama 3, Mixtral and Gemini) reveal that the\nLaMSUM outperforms state-of-the-art extractive summarization methods. We\nfurther attempt to provide the rationale behind the output summary produced by\nLLMs. Overall, this is one of the early attempts to achieve extractive\nsummarization for large user-generated text by utilizing LLMs, and likely to\ngenerate further interest in the community.",
        "chunk-id": 2,
        "chunk": "propose a novel framework LaMSUM to generate extractive summaries through LLMs\nfor large user-generated text by leveraging voting algorithms. Our evaluation\non three popular open-source LLMs (Llama 3, Mixtral and Gemini) reveal that the\nLaMSUM outperforms state-of-the-art extractive summarization methods. We",
        "authors": [
            "Garima Chhikara",
            "Anurag Sharma",
            "V. Gurucharan",
            "Kripabandhu Ghosh",
            "Abhijnan Chakraborty"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-22T10:25:55+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.15809v1",
        "arxiv_link": "http://arxiv.org/abs/2406.15809v1",
        "categories": [
            "Computation and Language",
            "Machine Learning"
        ]
    },
    {
        "id": 20000033,
        "doi": null,
        "title": "LaMSUM: A Novel Framework for Extractive Summarization of User Generated Content using LLMs",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of NLP tasks, including summarization. Inherently LLMs produce\nabstractive summaries, and the task of achieving extractive summaries through\nLLMs still remains largely unexplored. To bridge this gap, in this work, we\npropose a novel framework LaMSUM to generate extractive summaries through LLMs\nfor large user-generated text by leveraging voting algorithms. Our evaluation\non three popular open-source LLMs (Llama 3, Mixtral and Gemini) reveal that the\nLaMSUM outperforms state-of-the-art extractive summarization methods. We\nfurther attempt to provide the rationale behind the output summary produced by\nLLMs. Overall, this is one of the early attempts to achieve extractive\nsummarization for large user-generated text by utilizing LLMs, and likely to\ngenerate further interest in the community.",
        "chunk-id": 3,
        "chunk": "further attempt to provide the rationale behind the output summary produced by\nLLMs. Overall, this is one of the early attempts to achieve extractive\nsummarization for large user-generated text by utilizing LLMs, and likely to\ngenerate further interest in the community.",
        "authors": [
            "Garima Chhikara",
            "Anurag Sharma",
            "V. Gurucharan",
            "Kripabandhu Ghosh",
            "Abhijnan Chakraborty"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-22T10:25:55+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.15809v1",
        "arxiv_link": "http://arxiv.org/abs/2406.15809v1",
        "categories": [
            "Computation and Language",
            "Machine Learning"
        ]
    },
    {
        "id": 20000034,
        "doi": null,
        "title": "NLP-KG: A System for Exploratory Search of Scientific Literature in Natural Language Processing",
        "abstract": "Scientific literature searches are often exploratory, whereby users are not\nyet familiar with a particular field or concept but are interested in learning\nmore about it. However, existing systems for scientific literature search are\ntypically tailored to keyword-based lookup searches, limiting the possibilities\nfor exploration. We propose NLP-KG, a feature-rich system designed to support\nthe exploration of research literature in unfamiliar natural language\nprocessing (NLP) fields. In addition to a semantic search, NLP-KG allows users\nto easily find survey papers that provide a quick introduction to a field of\ninterest. Further, a Fields of Study hierarchy graph enables users to\nfamiliarize themselves with a field and its related areas. Finally, a chat\ninterface allows users to ask questions about unfamiliar concepts or specific\narticles in NLP and obtain answers grounded in knowledge retrieved from\nscientific publications. Our system provides users with comprehensive\nexploration possibilities, supporting them in investigating the relationships\nbetween different fields, understanding unfamiliar concepts in NLP, and finding\nrelevant research literature. Demo, video, and code are available at:\nhttps://github.com/NLP-Knowledge-Graph/NLP-KG-WebApp.",
        "chunk-id": 1,
        "chunk": "Scientific literature searches are often exploratory, whereby users are not\nyet familiar with a particular field or concept but are interested in learning\nmore about it. However, existing systems for scientific literature search are\ntypically tailored to keyword-based lookup searches, limiting the possibilities",
        "authors": [
            "Tim Schopf",
            "Florian Matthes"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-21T16:38:22+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.15294v1",
        "arxiv_link": "http://arxiv.org/abs/2406.15294v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000034,
        "doi": null,
        "title": "NLP-KG: A System for Exploratory Search of Scientific Literature in Natural Language Processing",
        "abstract": "Scientific literature searches are often exploratory, whereby users are not\nyet familiar with a particular field or concept but are interested in learning\nmore about it. However, existing systems for scientific literature search are\ntypically tailored to keyword-based lookup searches, limiting the possibilities\nfor exploration. We propose NLP-KG, a feature-rich system designed to support\nthe exploration of research literature in unfamiliar natural language\nprocessing (NLP) fields. In addition to a semantic search, NLP-KG allows users\nto easily find survey papers that provide a quick introduction to a field of\ninterest. Further, a Fields of Study hierarchy graph enables users to\nfamiliarize themselves with a field and its related areas. Finally, a chat\ninterface allows users to ask questions about unfamiliar concepts or specific\narticles in NLP and obtain answers grounded in knowledge retrieved from\nscientific publications. Our system provides users with comprehensive\nexploration possibilities, supporting them in investigating the relationships\nbetween different fields, understanding unfamiliar concepts in NLP, and finding\nrelevant research literature. Demo, video, and code are available at:\nhttps://github.com/NLP-Knowledge-Graph/NLP-KG-WebApp.",
        "chunk-id": 2,
        "chunk": "for exploration. We propose NLP-KG, a feature-rich system designed to support\nthe exploration of research literature in unfamiliar natural language\nprocessing (NLP) fields. In addition to a semantic search, NLP-KG allows users\nto easily find survey papers that provide a quick introduction to a field of\ninterest. Further, a Fields of Study hierarchy graph enables users to",
        "authors": [
            "Tim Schopf",
            "Florian Matthes"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-21T16:38:22+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.15294v1",
        "arxiv_link": "http://arxiv.org/abs/2406.15294v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000034,
        "doi": null,
        "title": "NLP-KG: A System for Exploratory Search of Scientific Literature in Natural Language Processing",
        "abstract": "Scientific literature searches are often exploratory, whereby users are not\nyet familiar with a particular field or concept but are interested in learning\nmore about it. However, existing systems for scientific literature search are\ntypically tailored to keyword-based lookup searches, limiting the possibilities\nfor exploration. We propose NLP-KG, a feature-rich system designed to support\nthe exploration of research literature in unfamiliar natural language\nprocessing (NLP) fields. In addition to a semantic search, NLP-KG allows users\nto easily find survey papers that provide a quick introduction to a field of\ninterest. Further, a Fields of Study hierarchy graph enables users to\nfamiliarize themselves with a field and its related areas. Finally, a chat\ninterface allows users to ask questions about unfamiliar concepts or specific\narticles in NLP and obtain answers grounded in knowledge retrieved from\nscientific publications. Our system provides users with comprehensive\nexploration possibilities, supporting them in investigating the relationships\nbetween different fields, understanding unfamiliar concepts in NLP, and finding\nrelevant research literature. Demo, video, and code are available at:\nhttps://github.com/NLP-Knowledge-Graph/NLP-KG-WebApp.",
        "chunk-id": 3,
        "chunk": "familiarize themselves with a field and its related areas. Finally, a chat\ninterface allows users to ask questions about unfamiliar concepts or specific\narticles in NLP and obtain answers grounded in knowledge retrieved from\nscientific publications. Our system provides users with comprehensive\nexploration possibilities, supporting them in investigating the relationships",
        "authors": [
            "Tim Schopf",
            "Florian Matthes"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-21T16:38:22+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.15294v1",
        "arxiv_link": "http://arxiv.org/abs/2406.15294v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000034,
        "doi": null,
        "title": "NLP-KG: A System for Exploratory Search of Scientific Literature in Natural Language Processing",
        "abstract": "Scientific literature searches are often exploratory, whereby users are not\nyet familiar with a particular field or concept but are interested in learning\nmore about it. However, existing systems for scientific literature search are\ntypically tailored to keyword-based lookup searches, limiting the possibilities\nfor exploration. We propose NLP-KG, a feature-rich system designed to support\nthe exploration of research literature in unfamiliar natural language\nprocessing (NLP) fields. In addition to a semantic search, NLP-KG allows users\nto easily find survey papers that provide a quick introduction to a field of\ninterest. Further, a Fields of Study hierarchy graph enables users to\nfamiliarize themselves with a field and its related areas. Finally, a chat\ninterface allows users to ask questions about unfamiliar concepts or specific\narticles in NLP and obtain answers grounded in knowledge retrieved from\nscientific publications. Our system provides users with comprehensive\nexploration possibilities, supporting them in investigating the relationships\nbetween different fields, understanding unfamiliar concepts in NLP, and finding\nrelevant research literature. Demo, video, and code are available at:\nhttps://github.com/NLP-Knowledge-Graph/NLP-KG-WebApp.",
        "chunk-id": 4,
        "chunk": "between different fields, understanding unfamiliar concepts in NLP, and finding\nrelevant research literature. Demo, video, and code are available at:\nhttps://github.com/NLP-Knowledge-Graph/NLP-KG-WebApp.",
        "authors": [
            "Tim Schopf",
            "Florian Matthes"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-21T16:38:22+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.15294v1",
        "arxiv_link": "http://arxiv.org/abs/2406.15294v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000035,
        "doi": null,
        "title": "Enhancing Idiomatic Representation in Multiple Languages via an Adaptive Contrastive Triplet Loss",
        "abstract": "Accurately modeling idiomatic or non-compositional language has been a\nlongstanding challenge in Natural Language Processing (NLP). This is partly\nbecause these expressions do not derive their meanings solely from their\nconstituent words, but also due to the scarcity of relevant data resources, and\ntheir impact on the performance of downstream tasks such as machine translation\nand simplification. In this paper we propose an approach to model idiomaticity\neffectively using a triplet loss that incorporates the asymmetric contribution\nof components words to an idiomatic meaning for training language models by\nusing adaptive contrastive learning and resampling miners to build an\nidiomatic-aware learning objective. Our proposed method is evaluated on a\nSemEval challenge and outperforms previous alternatives significantly in many\nmetrics.",
        "chunk-id": 1,
        "chunk": "Accurately modeling idiomatic or non-compositional language has been a\nlongstanding challenge in Natural Language Processing (NLP). This is partly\nbecause these expressions do not derive their meanings solely from their\nconstituent words, but also due to the scarcity of relevant data resources, and\ntheir impact on the performance of downstream tasks such as machine translation",
        "authors": [
            "Wei He",
            "Marco Idiart",
            "Carolina Scarton",
            "Aline Villavicencio"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-21T14:21:41+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.15175v1",
        "arxiv_link": "http://arxiv.org/abs/2406.15175v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000035,
        "doi": null,
        "title": "Enhancing Idiomatic Representation in Multiple Languages via an Adaptive Contrastive Triplet Loss",
        "abstract": "Accurately modeling idiomatic or non-compositional language has been a\nlongstanding challenge in Natural Language Processing (NLP). This is partly\nbecause these expressions do not derive their meanings solely from their\nconstituent words, but also due to the scarcity of relevant data resources, and\ntheir impact on the performance of downstream tasks such as machine translation\nand simplification. In this paper we propose an approach to model idiomaticity\neffectively using a triplet loss that incorporates the asymmetric contribution\nof components words to an idiomatic meaning for training language models by\nusing adaptive contrastive learning and resampling miners to build an\nidiomatic-aware learning objective. Our proposed method is evaluated on a\nSemEval challenge and outperforms previous alternatives significantly in many\nmetrics.",
        "chunk-id": 2,
        "chunk": "and simplification. In this paper we propose an approach to model idiomaticity\neffectively using a triplet loss that incorporates the asymmetric contribution\nof components words to an idiomatic meaning for training language models by\nusing adaptive contrastive learning and resampling miners to build an\nidiomatic-aware learning objective. Our proposed method is evaluated on a",
        "authors": [
            "Wei He",
            "Marco Idiart",
            "Carolina Scarton",
            "Aline Villavicencio"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-21T14:21:41+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.15175v1",
        "arxiv_link": "http://arxiv.org/abs/2406.15175v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000035,
        "doi": null,
        "title": "Enhancing Idiomatic Representation in Multiple Languages via an Adaptive Contrastive Triplet Loss",
        "abstract": "Accurately modeling idiomatic or non-compositional language has been a\nlongstanding challenge in Natural Language Processing (NLP). This is partly\nbecause these expressions do not derive their meanings solely from their\nconstituent words, but also due to the scarcity of relevant data resources, and\ntheir impact on the performance of downstream tasks such as machine translation\nand simplification. In this paper we propose an approach to model idiomaticity\neffectively using a triplet loss that incorporates the asymmetric contribution\nof components words to an idiomatic meaning for training language models by\nusing adaptive contrastive learning and resampling miners to build an\nidiomatic-aware learning objective. Our proposed method is evaluated on a\nSemEval challenge and outperforms previous alternatives significantly in many\nmetrics.",
        "chunk-id": 3,
        "chunk": "SemEval challenge and outperforms previous alternatives significantly in many\nmetrics.",
        "authors": [
            "Wei He",
            "Marco Idiart",
            "Carolina Scarton",
            "Aline Villavicencio"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-21T14:21:41+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.15175v1",
        "arxiv_link": "http://arxiv.org/abs/2406.15175v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000036,
        "doi": null,
        "title": "A Syntax-Injected Approach for Faster and More Accurate Sentiment Analysis",
        "abstract": "Sentiment Analysis (SA) is a crucial aspect of Natural Language Processing\n(NLP), addressing subjective assessments in textual content. Syntactic parsing\nis useful in SA because explicit syntactic information can improve accuracy\nwhile providing explainability, but it tends to be a computational bottleneck\nin practice due to the slowness of parsing algorithms. This paper addresses\nsaid bottleneck by using a SEquence Labeling Syntactic Parser (SELSP) to inject\nsyntax into SA. By treating dependency parsing as a sequence labeling problem,\nwe greatly enhance the speed of syntax-based SA. SELSP is trained and evaluated\non a ternary polarity classification task, demonstrating its faster performance\nand better accuracy in polarity prediction tasks compared to conventional\nparsers like Stanza and to heuristic approaches that use shallow syntactic\nrules for SA like VADER. This increased speed and improved accuracy make SELSP\nparticularly appealing to SA practitioners in both research and industry. In\naddition, we test several sentiment dictionaries on our SELSP to see which one\nimproves the performance in polarity prediction tasks. Moreover, we compare the\nSELSP with Transformer-based models trained on a 5-label classification task.\nThe results show that dictionaries that capture polarity judgment variation\nprovide better results than dictionaries that ignore polarity judgment\nvariation. Moreover, we show that SELSP is considerably faster than\nTransformer-based models in polarity prediction tasks.",
        "chunk-id": 1,
        "chunk": "Sentiment Analysis (SA) is a crucial aspect of Natural Language Processing\n(NLP), addressing subjective assessments in textual content. Syntactic parsing\nis useful in SA because explicit syntactic information can improve accuracy\nwhile providing explainability, but it tends to be a computational bottleneck\nin practice due to the slowness of parsing algorithms. This paper addresses",
        "authors": [
            "Muhammad Imran",
            "Olga Kellert",
            "Carlos G\u00f3mez-Rodr\u00edguez"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-21T14:08:25+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.15163v1",
        "arxiv_link": "http://arxiv.org/abs/2406.15163v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000036,
        "doi": null,
        "title": "A Syntax-Injected Approach for Faster and More Accurate Sentiment Analysis",
        "abstract": "Sentiment Analysis (SA) is a crucial aspect of Natural Language Processing\n(NLP), addressing subjective assessments in textual content. Syntactic parsing\nis useful in SA because explicit syntactic information can improve accuracy\nwhile providing explainability, but it tends to be a computational bottleneck\nin practice due to the slowness of parsing algorithms. This paper addresses\nsaid bottleneck by using a SEquence Labeling Syntactic Parser (SELSP) to inject\nsyntax into SA. By treating dependency parsing as a sequence labeling problem,\nwe greatly enhance the speed of syntax-based SA. SELSP is trained and evaluated\non a ternary polarity classification task, demonstrating its faster performance\nand better accuracy in polarity prediction tasks compared to conventional\nparsers like Stanza and to heuristic approaches that use shallow syntactic\nrules for SA like VADER. This increased speed and improved accuracy make SELSP\nparticularly appealing to SA practitioners in both research and industry. In\naddition, we test several sentiment dictionaries on our SELSP to see which one\nimproves the performance in polarity prediction tasks. Moreover, we compare the\nSELSP with Transformer-based models trained on a 5-label classification task.\nThe results show that dictionaries that capture polarity judgment variation\nprovide better results than dictionaries that ignore polarity judgment\nvariation. Moreover, we show that SELSP is considerably faster than\nTransformer-based models in polarity prediction tasks.",
        "chunk-id": 2,
        "chunk": "said bottleneck by using a SEquence Labeling Syntactic Parser (SELSP) to inject\nsyntax into SA. By treating dependency parsing as a sequence labeling problem,\nwe greatly enhance the speed of syntax-based SA. SELSP is trained and evaluated\non a ternary polarity classification task, demonstrating its faster performance",
        "authors": [
            "Muhammad Imran",
            "Olga Kellert",
            "Carlos G\u00f3mez-Rodr\u00edguez"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-21T14:08:25+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.15163v1",
        "arxiv_link": "http://arxiv.org/abs/2406.15163v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000036,
        "doi": null,
        "title": "A Syntax-Injected Approach for Faster and More Accurate Sentiment Analysis",
        "abstract": "Sentiment Analysis (SA) is a crucial aspect of Natural Language Processing\n(NLP), addressing subjective assessments in textual content. Syntactic parsing\nis useful in SA because explicit syntactic information can improve accuracy\nwhile providing explainability, but it tends to be a computational bottleneck\nin practice due to the slowness of parsing algorithms. This paper addresses\nsaid bottleneck by using a SEquence Labeling Syntactic Parser (SELSP) to inject\nsyntax into SA. By treating dependency parsing as a sequence labeling problem,\nwe greatly enhance the speed of syntax-based SA. SELSP is trained and evaluated\non a ternary polarity classification task, demonstrating its faster performance\nand better accuracy in polarity prediction tasks compared to conventional\nparsers like Stanza and to heuristic approaches that use shallow syntactic\nrules for SA like VADER. This increased speed and improved accuracy make SELSP\nparticularly appealing to SA practitioners in both research and industry. In\naddition, we test several sentiment dictionaries on our SELSP to see which one\nimproves the performance in polarity prediction tasks. Moreover, we compare the\nSELSP with Transformer-based models trained on a 5-label classification task.\nThe results show that dictionaries that capture polarity judgment variation\nprovide better results than dictionaries that ignore polarity judgment\nvariation. Moreover, we show that SELSP is considerably faster than\nTransformer-based models in polarity prediction tasks.",
        "chunk-id": 3,
        "chunk": "and better accuracy in polarity prediction tasks compared to conventional\nparsers like Stanza and to heuristic approaches that use shallow syntactic\nrules for SA like VADER. This increased speed and improved accuracy make SELSP\nparticularly appealing to SA practitioners in both research and industry. In\naddition, we test several sentiment dictionaries on our SELSP to see which one",
        "authors": [
            "Muhammad Imran",
            "Olga Kellert",
            "Carlos G\u00f3mez-Rodr\u00edguez"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-21T14:08:25+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.15163v1",
        "arxiv_link": "http://arxiv.org/abs/2406.15163v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000036,
        "doi": null,
        "title": "A Syntax-Injected Approach for Faster and More Accurate Sentiment Analysis",
        "abstract": "Sentiment Analysis (SA) is a crucial aspect of Natural Language Processing\n(NLP), addressing subjective assessments in textual content. Syntactic parsing\nis useful in SA because explicit syntactic information can improve accuracy\nwhile providing explainability, but it tends to be a computational bottleneck\nin practice due to the slowness of parsing algorithms. This paper addresses\nsaid bottleneck by using a SEquence Labeling Syntactic Parser (SELSP) to inject\nsyntax into SA. By treating dependency parsing as a sequence labeling problem,\nwe greatly enhance the speed of syntax-based SA. SELSP is trained and evaluated\non a ternary polarity classification task, demonstrating its faster performance\nand better accuracy in polarity prediction tasks compared to conventional\nparsers like Stanza and to heuristic approaches that use shallow syntactic\nrules for SA like VADER. This increased speed and improved accuracy make SELSP\nparticularly appealing to SA practitioners in both research and industry. In\naddition, we test several sentiment dictionaries on our SELSP to see which one\nimproves the performance in polarity prediction tasks. Moreover, we compare the\nSELSP with Transformer-based models trained on a 5-label classification task.\nThe results show that dictionaries that capture polarity judgment variation\nprovide better results than dictionaries that ignore polarity judgment\nvariation. Moreover, we show that SELSP is considerably faster than\nTransformer-based models in polarity prediction tasks.",
        "chunk-id": 4,
        "chunk": "improves the performance in polarity prediction tasks. Moreover, we compare the\nSELSP with Transformer-based models trained on a 5-label classification task.\nThe results show that dictionaries that capture polarity judgment variation\nprovide better results than dictionaries that ignore polarity judgment\nvariation. Moreover, we show that SELSP is considerably faster than",
        "authors": [
            "Muhammad Imran",
            "Olga Kellert",
            "Carlos G\u00f3mez-Rodr\u00edguez"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-21T14:08:25+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.15163v1",
        "arxiv_link": "http://arxiv.org/abs/2406.15163v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000036,
        "doi": null,
        "title": "A Syntax-Injected Approach for Faster and More Accurate Sentiment Analysis",
        "abstract": "Sentiment Analysis (SA) is a crucial aspect of Natural Language Processing\n(NLP), addressing subjective assessments in textual content. Syntactic parsing\nis useful in SA because explicit syntactic information can improve accuracy\nwhile providing explainability, but it tends to be a computational bottleneck\nin practice due to the slowness of parsing algorithms. This paper addresses\nsaid bottleneck by using a SEquence Labeling Syntactic Parser (SELSP) to inject\nsyntax into SA. By treating dependency parsing as a sequence labeling problem,\nwe greatly enhance the speed of syntax-based SA. SELSP is trained and evaluated\non a ternary polarity classification task, demonstrating its faster performance\nand better accuracy in polarity prediction tasks compared to conventional\nparsers like Stanza and to heuristic approaches that use shallow syntactic\nrules for SA like VADER. This increased speed and improved accuracy make SELSP\nparticularly appealing to SA practitioners in both research and industry. In\naddition, we test several sentiment dictionaries on our SELSP to see which one\nimproves the performance in polarity prediction tasks. Moreover, we compare the\nSELSP with Transformer-based models trained on a 5-label classification task.\nThe results show that dictionaries that capture polarity judgment variation\nprovide better results than dictionaries that ignore polarity judgment\nvariation. Moreover, we show that SELSP is considerably faster than\nTransformer-based models in polarity prediction tasks.",
        "chunk-id": 5,
        "chunk": "Transformer-based models in polarity prediction tasks.",
        "authors": [
            "Muhammad Imran",
            "Olga Kellert",
            "Carlos G\u00f3mez-Rodr\u00edguez"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-21T14:08:25+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.15163v1",
        "arxiv_link": "http://arxiv.org/abs/2406.15163v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000037,
        "doi": null,
        "title": "Uni-Mol2: Exploring Molecular Pretraining Model at Scale",
        "abstract": "In recent years, pretraining models have made significant advancements in the\nfields of natural language processing (NLP), computer vision (CV), and life\nsciences. The significant advancements in NLP and CV are predominantly driven\nby the expansion of model parameters and data size, a phenomenon now recognized\nas the scaling laws. However, research exploring scaling law in molecular\npretraining models remains unexplored. In this work, we present Uni-Mol2 , an\ninnovative molecular pretraining model that leverages a two-track transformer\nto effectively integrate features at the atomic level, graph level, and\ngeometry structure level. Along with this, we systematically investigate the\nscaling law within molecular pretraining models, characterizing the power-law\ncorrelations between validation loss and model size, dataset size, and\ncomputational resources. Consequently, we successfully scale Uni-Mol2 to 1.1\nbillion parameters through pretraining on 800 million conformations, making it\nthe largest molecular pretraining model to date. Extensive experiments show\nconsistent improvement in the downstream tasks as the model size grows. The\nUni-Mol2 with 1.1B parameters also outperforms existing methods, achieving an\naverage 27% improvement on the QM9 and 14% on COMPAS-1D dataset.",
        "chunk-id": 1,
        "chunk": "In recent years, pretraining models have made significant advancements in the\nfields of natural language processing (NLP), computer vision (CV), and life\nsciences. The significant advancements in NLP and CV are predominantly driven\nby the expansion of model parameters and data size, a phenomenon now recognized",
        "authors": [
            "Xiaohong Ji",
            "Wang Zhen",
            "Zhifeng Gao",
            "Hang Zheng",
            "Linfeng Zhang",
            "Guolin Ke",
            "Weinan E"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-21T08:28:54+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14969v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14969v1",
        "categories": [
            "Machine Learning",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000037,
        "doi": null,
        "title": "Uni-Mol2: Exploring Molecular Pretraining Model at Scale",
        "abstract": "In recent years, pretraining models have made significant advancements in the\nfields of natural language processing (NLP), computer vision (CV), and life\nsciences. The significant advancements in NLP and CV are predominantly driven\nby the expansion of model parameters and data size, a phenomenon now recognized\nas the scaling laws. However, research exploring scaling law in molecular\npretraining models remains unexplored. In this work, we present Uni-Mol2 , an\ninnovative molecular pretraining model that leverages a two-track transformer\nto effectively integrate features at the atomic level, graph level, and\ngeometry structure level. Along with this, we systematically investigate the\nscaling law within molecular pretraining models, characterizing the power-law\ncorrelations between validation loss and model size, dataset size, and\ncomputational resources. Consequently, we successfully scale Uni-Mol2 to 1.1\nbillion parameters through pretraining on 800 million conformations, making it\nthe largest molecular pretraining model to date. Extensive experiments show\nconsistent improvement in the downstream tasks as the model size grows. The\nUni-Mol2 with 1.1B parameters also outperforms existing methods, achieving an\naverage 27% improvement on the QM9 and 14% on COMPAS-1D dataset.",
        "chunk-id": 2,
        "chunk": "as the scaling laws. However, research exploring scaling law in molecular\npretraining models remains unexplored. In this work, we present Uni-Mol2 , an\ninnovative molecular pretraining model that leverages a two-track transformer\nto effectively integrate features at the atomic level, graph level, and\ngeometry structure level. Along with this, we systematically investigate the",
        "authors": [
            "Xiaohong Ji",
            "Wang Zhen",
            "Zhifeng Gao",
            "Hang Zheng",
            "Linfeng Zhang",
            "Guolin Ke",
            "Weinan E"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-21T08:28:54+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14969v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14969v1",
        "categories": [
            "Machine Learning",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000037,
        "doi": null,
        "title": "Uni-Mol2: Exploring Molecular Pretraining Model at Scale",
        "abstract": "In recent years, pretraining models have made significant advancements in the\nfields of natural language processing (NLP), computer vision (CV), and life\nsciences. The significant advancements in NLP and CV are predominantly driven\nby the expansion of model parameters and data size, a phenomenon now recognized\nas the scaling laws. However, research exploring scaling law in molecular\npretraining models remains unexplored. In this work, we present Uni-Mol2 , an\ninnovative molecular pretraining model that leverages a two-track transformer\nto effectively integrate features at the atomic level, graph level, and\ngeometry structure level. Along with this, we systematically investigate the\nscaling law within molecular pretraining models, characterizing the power-law\ncorrelations between validation loss and model size, dataset size, and\ncomputational resources. Consequently, we successfully scale Uni-Mol2 to 1.1\nbillion parameters through pretraining on 800 million conformations, making it\nthe largest molecular pretraining model to date. Extensive experiments show\nconsistent improvement in the downstream tasks as the model size grows. The\nUni-Mol2 with 1.1B parameters also outperforms existing methods, achieving an\naverage 27% improvement on the QM9 and 14% on COMPAS-1D dataset.",
        "chunk-id": 3,
        "chunk": "scaling law within molecular pretraining models, characterizing the power-law\ncorrelations between validation loss and model size, dataset size, and\ncomputational resources. Consequently, we successfully scale Uni-Mol2 to 1.1\nbillion parameters through pretraining on 800 million conformations, making it\nthe largest molecular pretraining model to date. Extensive experiments show",
        "authors": [
            "Xiaohong Ji",
            "Wang Zhen",
            "Zhifeng Gao",
            "Hang Zheng",
            "Linfeng Zhang",
            "Guolin Ke",
            "Weinan E"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-21T08:28:54+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14969v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14969v1",
        "categories": [
            "Machine Learning",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000037,
        "doi": null,
        "title": "Uni-Mol2: Exploring Molecular Pretraining Model at Scale",
        "abstract": "In recent years, pretraining models have made significant advancements in the\nfields of natural language processing (NLP), computer vision (CV), and life\nsciences. The significant advancements in NLP and CV are predominantly driven\nby the expansion of model parameters and data size, a phenomenon now recognized\nas the scaling laws. However, research exploring scaling law in molecular\npretraining models remains unexplored. In this work, we present Uni-Mol2 , an\ninnovative molecular pretraining model that leverages a two-track transformer\nto effectively integrate features at the atomic level, graph level, and\ngeometry structure level. Along with this, we systematically investigate the\nscaling law within molecular pretraining models, characterizing the power-law\ncorrelations between validation loss and model size, dataset size, and\ncomputational resources. Consequently, we successfully scale Uni-Mol2 to 1.1\nbillion parameters through pretraining on 800 million conformations, making it\nthe largest molecular pretraining model to date. Extensive experiments show\nconsistent improvement in the downstream tasks as the model size grows. The\nUni-Mol2 with 1.1B parameters also outperforms existing methods, achieving an\naverage 27% improvement on the QM9 and 14% on COMPAS-1D dataset.",
        "chunk-id": 4,
        "chunk": "consistent improvement in the downstream tasks as the model size grows. The\nUni-Mol2 with 1.1B parameters also outperforms existing methods, achieving an\naverage 27% improvement on the QM9 and 14% on COMPAS-1D dataset.",
        "authors": [
            "Xiaohong Ji",
            "Wang Zhen",
            "Zhifeng Gao",
            "Hang Zheng",
            "Linfeng Zhang",
            "Guolin Ke",
            "Weinan E"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-21T08:28:54+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14969v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14969v1",
        "categories": [
            "Machine Learning",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000038,
        "doi": null,
        "title": "Data Efficient Evaluation of Large Language Models and Text-to-Image Models via Adaptive Sampling",
        "abstract": "Evaluating LLMs and text-to-image models is a computationally intensive task\noften overlooked. Efficient evaluation is crucial for understanding the diverse\ncapabilities of these models and enabling comparisons across a growing number\nof new models and benchmarks. To address this, we introduce SubLIME, a\ndata-efficient evaluation framework that employs adaptive sampling techniques,\nsuch as clustering and quality-based methods, to create representative subsets\nof benchmarks. Our approach ensures statistically aligned model rankings\ncompared to full datasets, evidenced by high Pearson correlation coefficients.\nEmpirical analysis across six NLP benchmarks reveals that: (1) quality-based\nsampling consistently achieves strong correlations (0.85 to 0.95) with full\ndatasets at a 10\\% sampling rate such as Quality SE and Quality CPD (2)\nclustering methods excel in specific benchmarks such as MMLU (3) no single\nmethod universally outperforms others across all metrics. Extending this\nframework, we leverage the HEIM leaderboard to cover 25 text-to-image models on\n17 different benchmarks. SubLIME dynamically selects the optimal technique for\neach benchmark, significantly reducing evaluation costs while preserving\nranking integrity and score distribution. Notably, a minimal sampling rate of\n1% proves effective for benchmarks like MMLU. Additionally, we demonstrate that\nemploying difficulty-based sampling to target more challenging benchmark\nsegments enhances model differentiation with broader score distributions. We\nalso combine semantic search, tool use, and GPT-4 review to identify redundancy\nacross benchmarks within specific LLM categories, such as coding benchmarks.\nThis allows us to further reduce the number of samples needed to maintain\ntargeted rank preservation. Overall, SubLIME offers a versatile and\ncost-effective solution for the robust evaluation of LLMs and text-to-image\nmodels.",
        "chunk-id": 1,
        "chunk": "Evaluating LLMs and text-to-image models is a computationally intensive task\noften overlooked. Efficient evaluation is crucial for understanding the diverse\ncapabilities of these models and enabling comparisons across a growing number\nof new models and benchmarks. To address this, we introduce SubLIME, a\ndata-efficient evaluation framework that employs adaptive sampling techniques,",
        "authors": [
            "Cong Xu",
            "Gayathri Saranathan",
            "Mahammad Parwez Alam",
            "Arpit Shah",
            "James Lim",
            "Soon Yee Wong",
            "Foltin Martin",
            "Suparna Bhattacharya"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-21T07:38:55+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.15527v1",
        "arxiv_link": "http://arxiv.org/abs/2406.15527v1",
        "categories": [
            "Machine Learning",
            "Computation and Language"
        ]
    },
    {
        "id": 20000038,
        "doi": null,
        "title": "Data Efficient Evaluation of Large Language Models and Text-to-Image Models via Adaptive Sampling",
        "abstract": "Evaluating LLMs and text-to-image models is a computationally intensive task\noften overlooked. Efficient evaluation is crucial for understanding the diverse\ncapabilities of these models and enabling comparisons across a growing number\nof new models and benchmarks. To address this, we introduce SubLIME, a\ndata-efficient evaluation framework that employs adaptive sampling techniques,\nsuch as clustering and quality-based methods, to create representative subsets\nof benchmarks. Our approach ensures statistically aligned model rankings\ncompared to full datasets, evidenced by high Pearson correlation coefficients.\nEmpirical analysis across six NLP benchmarks reveals that: (1) quality-based\nsampling consistently achieves strong correlations (0.85 to 0.95) with full\ndatasets at a 10\\% sampling rate such as Quality SE and Quality CPD (2)\nclustering methods excel in specific benchmarks such as MMLU (3) no single\nmethod universally outperforms others across all metrics. Extending this\nframework, we leverage the HEIM leaderboard to cover 25 text-to-image models on\n17 different benchmarks. SubLIME dynamically selects the optimal technique for\neach benchmark, significantly reducing evaluation costs while preserving\nranking integrity and score distribution. Notably, a minimal sampling rate of\n1% proves effective for benchmarks like MMLU. Additionally, we demonstrate that\nemploying difficulty-based sampling to target more challenging benchmark\nsegments enhances model differentiation with broader score distributions. We\nalso combine semantic search, tool use, and GPT-4 review to identify redundancy\nacross benchmarks within specific LLM categories, such as coding benchmarks.\nThis allows us to further reduce the number of samples needed to maintain\ntargeted rank preservation. Overall, SubLIME offers a versatile and\ncost-effective solution for the robust evaluation of LLMs and text-to-image\nmodels.",
        "chunk-id": 2,
        "chunk": "such as clustering and quality-based methods, to create representative subsets\nof benchmarks. Our approach ensures statistically aligned model rankings\ncompared to full datasets, evidenced by high Pearson correlation coefficients.\nEmpirical analysis across six NLP benchmarks reveals that: (1) quality-based\nsampling consistently achieves strong correlations (0.85 to 0.95) with full",
        "authors": [
            "Cong Xu",
            "Gayathri Saranathan",
            "Mahammad Parwez Alam",
            "Arpit Shah",
            "James Lim",
            "Soon Yee Wong",
            "Foltin Martin",
            "Suparna Bhattacharya"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-21T07:38:55+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.15527v1",
        "arxiv_link": "http://arxiv.org/abs/2406.15527v1",
        "categories": [
            "Machine Learning",
            "Computation and Language"
        ]
    },
    {
        "id": 20000038,
        "doi": null,
        "title": "Data Efficient Evaluation of Large Language Models and Text-to-Image Models via Adaptive Sampling",
        "abstract": "Evaluating LLMs and text-to-image models is a computationally intensive task\noften overlooked. Efficient evaluation is crucial for understanding the diverse\ncapabilities of these models and enabling comparisons across a growing number\nof new models and benchmarks. To address this, we introduce SubLIME, a\ndata-efficient evaluation framework that employs adaptive sampling techniques,\nsuch as clustering and quality-based methods, to create representative subsets\nof benchmarks. Our approach ensures statistically aligned model rankings\ncompared to full datasets, evidenced by high Pearson correlation coefficients.\nEmpirical analysis across six NLP benchmarks reveals that: (1) quality-based\nsampling consistently achieves strong correlations (0.85 to 0.95) with full\ndatasets at a 10\\% sampling rate such as Quality SE and Quality CPD (2)\nclustering methods excel in specific benchmarks such as MMLU (3) no single\nmethod universally outperforms others across all metrics. Extending this\nframework, we leverage the HEIM leaderboard to cover 25 text-to-image models on\n17 different benchmarks. SubLIME dynamically selects the optimal technique for\neach benchmark, significantly reducing evaluation costs while preserving\nranking integrity and score distribution. Notably, a minimal sampling rate of\n1% proves effective for benchmarks like MMLU. Additionally, we demonstrate that\nemploying difficulty-based sampling to target more challenging benchmark\nsegments enhances model differentiation with broader score distributions. We\nalso combine semantic search, tool use, and GPT-4 review to identify redundancy\nacross benchmarks within specific LLM categories, such as coding benchmarks.\nThis allows us to further reduce the number of samples needed to maintain\ntargeted rank preservation. Overall, SubLIME offers a versatile and\ncost-effective solution for the robust evaluation of LLMs and text-to-image\nmodels.",
        "chunk-id": 3,
        "chunk": "datasets at a 10\\% sampling rate such as Quality SE and Quality CPD (2)\nclustering methods excel in specific benchmarks such as MMLU (3) no single\nmethod universally outperforms others across all metrics. Extending this\nframework, we leverage the HEIM leaderboard to cover 25 text-to-image models on\n17 different benchmarks. SubLIME dynamically selects the optimal technique for",
        "authors": [
            "Cong Xu",
            "Gayathri Saranathan",
            "Mahammad Parwez Alam",
            "Arpit Shah",
            "James Lim",
            "Soon Yee Wong",
            "Foltin Martin",
            "Suparna Bhattacharya"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-21T07:38:55+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.15527v1",
        "arxiv_link": "http://arxiv.org/abs/2406.15527v1",
        "categories": [
            "Machine Learning",
            "Computation and Language"
        ]
    },
    {
        "id": 20000038,
        "doi": null,
        "title": "Data Efficient Evaluation of Large Language Models and Text-to-Image Models via Adaptive Sampling",
        "abstract": "Evaluating LLMs and text-to-image models is a computationally intensive task\noften overlooked. Efficient evaluation is crucial for understanding the diverse\ncapabilities of these models and enabling comparisons across a growing number\nof new models and benchmarks. To address this, we introduce SubLIME, a\ndata-efficient evaluation framework that employs adaptive sampling techniques,\nsuch as clustering and quality-based methods, to create representative subsets\nof benchmarks. Our approach ensures statistically aligned model rankings\ncompared to full datasets, evidenced by high Pearson correlation coefficients.\nEmpirical analysis across six NLP benchmarks reveals that: (1) quality-based\nsampling consistently achieves strong correlations (0.85 to 0.95) with full\ndatasets at a 10\\% sampling rate such as Quality SE and Quality CPD (2)\nclustering methods excel in specific benchmarks such as MMLU (3) no single\nmethod universally outperforms others across all metrics. Extending this\nframework, we leverage the HEIM leaderboard to cover 25 text-to-image models on\n17 different benchmarks. SubLIME dynamically selects the optimal technique for\neach benchmark, significantly reducing evaluation costs while preserving\nranking integrity and score distribution. Notably, a minimal sampling rate of\n1% proves effective for benchmarks like MMLU. Additionally, we demonstrate that\nemploying difficulty-based sampling to target more challenging benchmark\nsegments enhances model differentiation with broader score distributions. We\nalso combine semantic search, tool use, and GPT-4 review to identify redundancy\nacross benchmarks within specific LLM categories, such as coding benchmarks.\nThis allows us to further reduce the number of samples needed to maintain\ntargeted rank preservation. Overall, SubLIME offers a versatile and\ncost-effective solution for the robust evaluation of LLMs and text-to-image\nmodels.",
        "chunk-id": 4,
        "chunk": "each benchmark, significantly reducing evaluation costs while preserving\nranking integrity and score distribution. Notably, a minimal sampling rate of\n1% proves effective for benchmarks like MMLU. Additionally, we demonstrate that\nemploying difficulty-based sampling to target more challenging benchmark\nsegments enhances model differentiation with broader score distributions. We",
        "authors": [
            "Cong Xu",
            "Gayathri Saranathan",
            "Mahammad Parwez Alam",
            "Arpit Shah",
            "James Lim",
            "Soon Yee Wong",
            "Foltin Martin",
            "Suparna Bhattacharya"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-21T07:38:55+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.15527v1",
        "arxiv_link": "http://arxiv.org/abs/2406.15527v1",
        "categories": [
            "Machine Learning",
            "Computation and Language"
        ]
    },
    {
        "id": 20000038,
        "doi": null,
        "title": "Data Efficient Evaluation of Large Language Models and Text-to-Image Models via Adaptive Sampling",
        "abstract": "Evaluating LLMs and text-to-image models is a computationally intensive task\noften overlooked. Efficient evaluation is crucial for understanding the diverse\ncapabilities of these models and enabling comparisons across a growing number\nof new models and benchmarks. To address this, we introduce SubLIME, a\ndata-efficient evaluation framework that employs adaptive sampling techniques,\nsuch as clustering and quality-based methods, to create representative subsets\nof benchmarks. Our approach ensures statistically aligned model rankings\ncompared to full datasets, evidenced by high Pearson correlation coefficients.\nEmpirical analysis across six NLP benchmarks reveals that: (1) quality-based\nsampling consistently achieves strong correlations (0.85 to 0.95) with full\ndatasets at a 10\\% sampling rate such as Quality SE and Quality CPD (2)\nclustering methods excel in specific benchmarks such as MMLU (3) no single\nmethod universally outperforms others across all metrics. Extending this\nframework, we leverage the HEIM leaderboard to cover 25 text-to-image models on\n17 different benchmarks. SubLIME dynamically selects the optimal technique for\neach benchmark, significantly reducing evaluation costs while preserving\nranking integrity and score distribution. Notably, a minimal sampling rate of\n1% proves effective for benchmarks like MMLU. Additionally, we demonstrate that\nemploying difficulty-based sampling to target more challenging benchmark\nsegments enhances model differentiation with broader score distributions. We\nalso combine semantic search, tool use, and GPT-4 review to identify redundancy\nacross benchmarks within specific LLM categories, such as coding benchmarks.\nThis allows us to further reduce the number of samples needed to maintain\ntargeted rank preservation. Overall, SubLIME offers a versatile and\ncost-effective solution for the robust evaluation of LLMs and text-to-image\nmodels.",
        "chunk-id": 5,
        "chunk": "also combine semantic search, tool use, and GPT-4 review to identify redundancy\nacross benchmarks within specific LLM categories, such as coding benchmarks.\nThis allows us to further reduce the number of samples needed to maintain\ntargeted rank preservation. Overall, SubLIME offers a versatile and\ncost-effective solution for the robust evaluation of LLMs and text-to-image\nmodels.",
        "authors": [
            "Cong Xu",
            "Gayathri Saranathan",
            "Mahammad Parwez Alam",
            "Arpit Shah",
            "James Lim",
            "Soon Yee Wong",
            "Foltin Martin",
            "Suparna Bhattacharya"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-21T07:38:55+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.15527v1",
        "arxiv_link": "http://arxiv.org/abs/2406.15527v1",
        "categories": [
            "Machine Learning",
            "Computation and Language"
        ]
    },
    {
        "id": 20000039,
        "doi": null,
        "title": "Sports Intelligence: Assessing the Sports Understanding Capabilities of Language Models through Question Answering from Text to Video",
        "abstract": "Understanding sports is crucial for the advancement of Natural Language\nProcessing (NLP) due to its intricate and dynamic nature. Reasoning over\ncomplex sports scenarios has posed significant challenges to current NLP\ntechnologies which require advanced cognitive capabilities. Toward addressing\nthe limitations of existing benchmarks on sports understanding in the NLP\nfield, we extensively evaluated mainstream large language models for various\nsports tasks. Our evaluation spans from simple queries on basic rules and\nhistorical facts to complex, context-specific reasoning, leveraging strategies\nfrom zero-shot to few-shot learning, and chain-of-thought techniques. In\naddition to unimodal analysis, we further assessed the sports reasoning\ncapabilities of mainstream video language models to bridge the gap in\nmultimodal sports understanding benchmarking. Our findings highlighted the\ncritical challenges of sports understanding for NLP. We proposed a new\nbenchmark based on a comprehensive overview of existing sports datasets and\nprovided extensive error analysis which we hope can help identify future\nresearch priorities in this field.",
        "chunk-id": 1,
        "chunk": "Understanding sports is crucial for the advancement of Natural Language\nProcessing (NLP) due to its intricate and dynamic nature. Reasoning over\ncomplex sports scenarios has posed significant challenges to current NLP\ntechnologies which require advanced cognitive capabilities. Toward addressing\nthe limitations of existing benchmarks on sports understanding in the NLP",
        "authors": [
            "Zhengbang Yang",
            "Haotian Xia",
            "Jingxi Li",
            "Zezhi Chen",
            "Zhuangdi Zhu",
            "Weining Shen"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-21T05:57:50+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14877v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14877v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000039,
        "doi": null,
        "title": "Sports Intelligence: Assessing the Sports Understanding Capabilities of Language Models through Question Answering from Text to Video",
        "abstract": "Understanding sports is crucial for the advancement of Natural Language\nProcessing (NLP) due to its intricate and dynamic nature. Reasoning over\ncomplex sports scenarios has posed significant challenges to current NLP\ntechnologies which require advanced cognitive capabilities. Toward addressing\nthe limitations of existing benchmarks on sports understanding in the NLP\nfield, we extensively evaluated mainstream large language models for various\nsports tasks. Our evaluation spans from simple queries on basic rules and\nhistorical facts to complex, context-specific reasoning, leveraging strategies\nfrom zero-shot to few-shot learning, and chain-of-thought techniques. In\naddition to unimodal analysis, we further assessed the sports reasoning\ncapabilities of mainstream video language models to bridge the gap in\nmultimodal sports understanding benchmarking. Our findings highlighted the\ncritical challenges of sports understanding for NLP. We proposed a new\nbenchmark based on a comprehensive overview of existing sports datasets and\nprovided extensive error analysis which we hope can help identify future\nresearch priorities in this field.",
        "chunk-id": 2,
        "chunk": "field, we extensively evaluated mainstream large language models for various\nsports tasks. Our evaluation spans from simple queries on basic rules and\nhistorical facts to complex, context-specific reasoning, leveraging strategies\nfrom zero-shot to few-shot learning, and chain-of-thought techniques. In\naddition to unimodal analysis, we further assessed the sports reasoning",
        "authors": [
            "Zhengbang Yang",
            "Haotian Xia",
            "Jingxi Li",
            "Zezhi Chen",
            "Zhuangdi Zhu",
            "Weining Shen"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-21T05:57:50+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14877v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14877v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000039,
        "doi": null,
        "title": "Sports Intelligence: Assessing the Sports Understanding Capabilities of Language Models through Question Answering from Text to Video",
        "abstract": "Understanding sports is crucial for the advancement of Natural Language\nProcessing (NLP) due to its intricate and dynamic nature. Reasoning over\ncomplex sports scenarios has posed significant challenges to current NLP\ntechnologies which require advanced cognitive capabilities. Toward addressing\nthe limitations of existing benchmarks on sports understanding in the NLP\nfield, we extensively evaluated mainstream large language models for various\nsports tasks. Our evaluation spans from simple queries on basic rules and\nhistorical facts to complex, context-specific reasoning, leveraging strategies\nfrom zero-shot to few-shot learning, and chain-of-thought techniques. In\naddition to unimodal analysis, we further assessed the sports reasoning\ncapabilities of mainstream video language models to bridge the gap in\nmultimodal sports understanding benchmarking. Our findings highlighted the\ncritical challenges of sports understanding for NLP. We proposed a new\nbenchmark based on a comprehensive overview of existing sports datasets and\nprovided extensive error analysis which we hope can help identify future\nresearch priorities in this field.",
        "chunk-id": 3,
        "chunk": "capabilities of mainstream video language models to bridge the gap in\nmultimodal sports understanding benchmarking. Our findings highlighted the\ncritical challenges of sports understanding for NLP. We proposed a new\nbenchmark based on a comprehensive overview of existing sports datasets and\nprovided extensive error analysis which we hope can help identify future",
        "authors": [
            "Zhengbang Yang",
            "Haotian Xia",
            "Jingxi Li",
            "Zezhi Chen",
            "Zhuangdi Zhu",
            "Weining Shen"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-21T05:57:50+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14877v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14877v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000039,
        "doi": null,
        "title": "Sports Intelligence: Assessing the Sports Understanding Capabilities of Language Models through Question Answering from Text to Video",
        "abstract": "Understanding sports is crucial for the advancement of Natural Language\nProcessing (NLP) due to its intricate and dynamic nature. Reasoning over\ncomplex sports scenarios has posed significant challenges to current NLP\ntechnologies which require advanced cognitive capabilities. Toward addressing\nthe limitations of existing benchmarks on sports understanding in the NLP\nfield, we extensively evaluated mainstream large language models for various\nsports tasks. Our evaluation spans from simple queries on basic rules and\nhistorical facts to complex, context-specific reasoning, leveraging strategies\nfrom zero-shot to few-shot learning, and chain-of-thought techniques. In\naddition to unimodal analysis, we further assessed the sports reasoning\ncapabilities of mainstream video language models to bridge the gap in\nmultimodal sports understanding benchmarking. Our findings highlighted the\ncritical challenges of sports understanding for NLP. We proposed a new\nbenchmark based on a comprehensive overview of existing sports datasets and\nprovided extensive error analysis which we hope can help identify future\nresearch priorities in this field.",
        "chunk-id": 4,
        "chunk": "research priorities in this field.",
        "authors": [
            "Zhengbang Yang",
            "Haotian Xia",
            "Jingxi Li",
            "Zezhi Chen",
            "Zhuangdi Zhu",
            "Weining Shen"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-21T05:57:50+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14877v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14877v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000040,
        "doi": null,
        "title": "A review of feature selection strategies utilizing graph data structures and knowledge graphs",
        "abstract": "Feature selection in Knowledge Graphs (KGs) are increasingly utilized in\ndiverse domains, including biomedical research, Natural Language Processing\n(NLP), and personalized recommendation systems. This paper delves into the\nmethodologies for feature selection within KGs, emphasizing their roles in\nenhancing machine learning (ML) model efficacy, hypothesis generation, and\ninterpretability. Through this comprehensive review, we aim to catalyze further\ninnovation in feature selection for KGs, paving the way for more insightful,\nefficient, and interpretable analytical models across various domains. Our\nexploration reveals the critical importance of scalability, accuracy, and\ninterpretability in feature selection techniques, advocating for the\nintegration of domain knowledge to refine the selection process. We highlight\nthe burgeoning potential of multi-objective optimization and interdisciplinary\ncollaboration in advancing KG feature selection, underscoring the\ntransformative impact of such methodologies on precision medicine, among other\nfields. The paper concludes by charting future directions, including the\ndevelopment of scalable, dynamic feature selection algorithms and the\nintegration of explainable AI principles to foster transparency and trust in\nKG-driven models.",
        "chunk-id": 1,
        "chunk": "Feature selection in Knowledge Graphs (KGs) are increasingly utilized in\ndiverse domains, including biomedical research, Natural Language Processing\n(NLP), and personalized recommendation systems. This paper delves into the\nmethodologies for feature selection within KGs, emphasizing their roles in\nenhancing machine learning (ML) model efficacy, hypothesis generation, and",
        "authors": [
            "Sisi Shao",
            "Pedro Henrique Ribeiro",
            "Christina Ramirez",
            "Jason H. Moore"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-21T04:50:02+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14864v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14864v1",
        "categories": [
            "Machine Learning",
            "Applications",
            "Machine Learning"
        ]
    },
    {
        "id": 20000040,
        "doi": null,
        "title": "A review of feature selection strategies utilizing graph data structures and knowledge graphs",
        "abstract": "Feature selection in Knowledge Graphs (KGs) are increasingly utilized in\ndiverse domains, including biomedical research, Natural Language Processing\n(NLP), and personalized recommendation systems. This paper delves into the\nmethodologies for feature selection within KGs, emphasizing their roles in\nenhancing machine learning (ML) model efficacy, hypothesis generation, and\ninterpretability. Through this comprehensive review, we aim to catalyze further\ninnovation in feature selection for KGs, paving the way for more insightful,\nefficient, and interpretable analytical models across various domains. Our\nexploration reveals the critical importance of scalability, accuracy, and\ninterpretability in feature selection techniques, advocating for the\nintegration of domain knowledge to refine the selection process. We highlight\nthe burgeoning potential of multi-objective optimization and interdisciplinary\ncollaboration in advancing KG feature selection, underscoring the\ntransformative impact of such methodologies on precision medicine, among other\nfields. The paper concludes by charting future directions, including the\ndevelopment of scalable, dynamic feature selection algorithms and the\nintegration of explainable AI principles to foster transparency and trust in\nKG-driven models.",
        "chunk-id": 2,
        "chunk": "interpretability. Through this comprehensive review, we aim to catalyze further\ninnovation in feature selection for KGs, paving the way for more insightful,\nefficient, and interpretable analytical models across various domains. Our\nexploration reveals the critical importance of scalability, accuracy, and\ninterpretability in feature selection techniques, advocating for the",
        "authors": [
            "Sisi Shao",
            "Pedro Henrique Ribeiro",
            "Christina Ramirez",
            "Jason H. Moore"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-21T04:50:02+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14864v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14864v1",
        "categories": [
            "Machine Learning",
            "Applications",
            "Machine Learning"
        ]
    },
    {
        "id": 20000040,
        "doi": null,
        "title": "A review of feature selection strategies utilizing graph data structures and knowledge graphs",
        "abstract": "Feature selection in Knowledge Graphs (KGs) are increasingly utilized in\ndiverse domains, including biomedical research, Natural Language Processing\n(NLP), and personalized recommendation systems. This paper delves into the\nmethodologies for feature selection within KGs, emphasizing their roles in\nenhancing machine learning (ML) model efficacy, hypothesis generation, and\ninterpretability. Through this comprehensive review, we aim to catalyze further\ninnovation in feature selection for KGs, paving the way for more insightful,\nefficient, and interpretable analytical models across various domains. Our\nexploration reveals the critical importance of scalability, accuracy, and\ninterpretability in feature selection techniques, advocating for the\nintegration of domain knowledge to refine the selection process. We highlight\nthe burgeoning potential of multi-objective optimization and interdisciplinary\ncollaboration in advancing KG feature selection, underscoring the\ntransformative impact of such methodologies on precision medicine, among other\nfields. The paper concludes by charting future directions, including the\ndevelopment of scalable, dynamic feature selection algorithms and the\nintegration of explainable AI principles to foster transparency and trust in\nKG-driven models.",
        "chunk-id": 3,
        "chunk": "integration of domain knowledge to refine the selection process. We highlight\nthe burgeoning potential of multi-objective optimization and interdisciplinary\ncollaboration in advancing KG feature selection, underscoring the\ntransformative impact of such methodologies on precision medicine, among other\nfields. The paper concludes by charting future directions, including the",
        "authors": [
            "Sisi Shao",
            "Pedro Henrique Ribeiro",
            "Christina Ramirez",
            "Jason H. Moore"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-21T04:50:02+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14864v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14864v1",
        "categories": [
            "Machine Learning",
            "Applications",
            "Machine Learning"
        ]
    },
    {
        "id": 20000040,
        "doi": null,
        "title": "A review of feature selection strategies utilizing graph data structures and knowledge graphs",
        "abstract": "Feature selection in Knowledge Graphs (KGs) are increasingly utilized in\ndiverse domains, including biomedical research, Natural Language Processing\n(NLP), and personalized recommendation systems. This paper delves into the\nmethodologies for feature selection within KGs, emphasizing their roles in\nenhancing machine learning (ML) model efficacy, hypothesis generation, and\ninterpretability. Through this comprehensive review, we aim to catalyze further\ninnovation in feature selection for KGs, paving the way for more insightful,\nefficient, and interpretable analytical models across various domains. Our\nexploration reveals the critical importance of scalability, accuracy, and\ninterpretability in feature selection techniques, advocating for the\nintegration of domain knowledge to refine the selection process. We highlight\nthe burgeoning potential of multi-objective optimization and interdisciplinary\ncollaboration in advancing KG feature selection, underscoring the\ntransformative impact of such methodologies on precision medicine, among other\nfields. The paper concludes by charting future directions, including the\ndevelopment of scalable, dynamic feature selection algorithms and the\nintegration of explainable AI principles to foster transparency and trust in\nKG-driven models.",
        "chunk-id": 4,
        "chunk": "development of scalable, dynamic feature selection algorithms and the\nintegration of explainable AI principles to foster transparency and trust in\nKG-driven models.",
        "authors": [
            "Sisi Shao",
            "Pedro Henrique Ribeiro",
            "Christina Ramirez",
            "Jason H. Moore"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-21T04:50:02+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14864v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14864v1",
        "categories": [
            "Machine Learning",
            "Applications",
            "Machine Learning"
        ]
    },
    {
        "id": 20000041,
        "doi": null,
        "title": "A Large Language Model Outperforms Other Computational Approaches to the High-Throughput Phenotyping of Physician Notes",
        "abstract": "High-throughput phenotyping, the automated mapping of patient signs and\nsymptoms to standardized ontology concepts, is essential to gaining value from\nelectronic health records (EHR) in the support of precision medicine. Despite\ntechnological advances, high-throughput phenotyping remains a challenge. This\nstudy compares three computational approaches to high-throughput phenotyping: a\nLarge Language Model (LLM) incorporating generative AI, a Natural Language\nProcessing (NLP) approach utilizing deep learning for span categorization, and\na hybrid approach combining word vectors with machine learning. The approach\nthat implemented GPT-4 (a Large Language Model) demonstrated superior\nperformance, suggesting that Large Language Models are poised to be the\npreferred method for high-throughput phenotyping of physician notes.",
        "chunk-id": 1,
        "chunk": "High-throughput phenotyping, the automated mapping of patient signs and\nsymptoms to standardized ontology concepts, is essential to gaining value from\nelectronic health records (EHR) in the support of precision medicine. Despite\ntechnological advances, high-throughput phenotyping remains a challenge. This",
        "authors": [
            "Syed I. Munzir",
            "Daniel B. Hier",
            "Chelsea Oommen",
            "Michael D. Carrithers"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T22:05:34+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14757v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14757v1",
        "categories": [
            "Artificial Intelligence",
            "Experimental work for problems pertaining to biology",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000041,
        "doi": null,
        "title": "A Large Language Model Outperforms Other Computational Approaches to the High-Throughput Phenotyping of Physician Notes",
        "abstract": "High-throughput phenotyping, the automated mapping of patient signs and\nsymptoms to standardized ontology concepts, is essential to gaining value from\nelectronic health records (EHR) in the support of precision medicine. Despite\ntechnological advances, high-throughput phenotyping remains a challenge. This\nstudy compares three computational approaches to high-throughput phenotyping: a\nLarge Language Model (LLM) incorporating generative AI, a Natural Language\nProcessing (NLP) approach utilizing deep learning for span categorization, and\na hybrid approach combining word vectors with machine learning. The approach\nthat implemented GPT-4 (a Large Language Model) demonstrated superior\nperformance, suggesting that Large Language Models are poised to be the\npreferred method for high-throughput phenotyping of physician notes.",
        "chunk-id": 2,
        "chunk": "study compares three computational approaches to high-throughput phenotyping: a\nLarge Language Model (LLM) incorporating generative AI, a Natural Language\nProcessing (NLP) approach utilizing deep learning for span categorization, and\na hybrid approach combining word vectors with machine learning. The approach\nthat implemented GPT-4 (a Large Language Model) demonstrated superior",
        "authors": [
            "Syed I. Munzir",
            "Daniel B. Hier",
            "Chelsea Oommen",
            "Michael D. Carrithers"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T22:05:34+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14757v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14757v1",
        "categories": [
            "Artificial Intelligence",
            "Experimental work for problems pertaining to biology",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000041,
        "doi": null,
        "title": "A Large Language Model Outperforms Other Computational Approaches to the High-Throughput Phenotyping of Physician Notes",
        "abstract": "High-throughput phenotyping, the automated mapping of patient signs and\nsymptoms to standardized ontology concepts, is essential to gaining value from\nelectronic health records (EHR) in the support of precision medicine. Despite\ntechnological advances, high-throughput phenotyping remains a challenge. This\nstudy compares three computational approaches to high-throughput phenotyping: a\nLarge Language Model (LLM) incorporating generative AI, a Natural Language\nProcessing (NLP) approach utilizing deep learning for span categorization, and\na hybrid approach combining word vectors with machine learning. The approach\nthat implemented GPT-4 (a Large Language Model) demonstrated superior\nperformance, suggesting that Large Language Models are poised to be the\npreferred method for high-throughput phenotyping of physician notes.",
        "chunk-id": 3,
        "chunk": "performance, suggesting that Large Language Models are poised to be the\npreferred method for high-throughput phenotyping of physician notes.",
        "authors": [
            "Syed I. Munzir",
            "Daniel B. Hier",
            "Chelsea Oommen",
            "Michael D. Carrithers"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T22:05:34+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14757v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14757v1",
        "categories": [
            "Artificial Intelligence",
            "Experimental work for problems pertaining to biology",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000042,
        "doi": null,
        "title": "Dravidian language family through Universal Dependencies lens",
        "abstract": "The Universal Dependencies (UD) project aims to create a cross-linguistically\nconsistent dependency annotation for multiple languages, to facilitate\nmultilingual NLP. It currently supports 114 languages. Dravidian languages are\nspoken by over 200 million people across the word, and yet there are only two\nlanguages from this family in UD. This paper examines some of the morphological\nand syntactic features of Dravidian languages and explores how they can be\nannotated in the UD framework.",
        "chunk-id": 1,
        "chunk": "The Universal Dependencies (UD) project aims to create a cross-linguistically\nconsistent dependency annotation for multiple languages, to facilitate\nmultilingual NLP. It currently supports 114 languages. Dravidian languages are\nspoken by over 200 million people across the word, and yet there are only two",
        "authors": [
            "Taraka Rama",
            "Sowmya Vajjala"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T18:59:46+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14680v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14680v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000042,
        "doi": null,
        "title": "Dravidian language family through Universal Dependencies lens",
        "abstract": "The Universal Dependencies (UD) project aims to create a cross-linguistically\nconsistent dependency annotation for multiple languages, to facilitate\nmultilingual NLP. It currently supports 114 languages. Dravidian languages are\nspoken by over 200 million people across the word, and yet there are only two\nlanguages from this family in UD. This paper examines some of the morphological\nand syntactic features of Dravidian languages and explores how they can be\nannotated in the UD framework.",
        "chunk-id": 2,
        "chunk": "languages from this family in UD. This paper examines some of the morphological\nand syntactic features of Dravidian languages and explores how they can be\nannotated in the UD framework.",
        "authors": [
            "Taraka Rama",
            "Sowmya Vajjala"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T18:59:46+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14680v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14680v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000043,
        "doi": null,
        "title": "DeciMamba: Exploring the Length Extrapolation Potential of Mamba",
        "abstract": "Long-range sequence processing poses a significant challenge for Transformers\ndue to their quadratic complexity in input length. A promising alternative is\nMamba, which demonstrates high performance and achieves Transformer-level\ncapabilities while requiring substantially fewer computational resources. In\nthis paper we explore the length-generalization capabilities of Mamba, which we\nfind to be relatively limited. Through a series of visualizations and analyses\nwe identify that the limitations arise from a restricted effective receptive\nfield, dictated by the sequence length used during training. To address this\nconstraint, we introduce DeciMamba, a context-extension method specifically\ndesigned for Mamba. This mechanism, built on top of a hidden filtering\nmechanism embedded within the S6 layer, enables the trained model to\nextrapolate well even without additional training. Empirical experiments over\nreal-world long-range NLP tasks show that DeciMamba can extrapolate to context\nlengths that are 25x times longer than the ones seen during training, and does\nso without utilizing additional computational resources. We will release our\ncode and models.",
        "chunk-id": 1,
        "chunk": "Long-range sequence processing poses a significant challenge for Transformers\ndue to their quadratic complexity in input length. A promising alternative is\nMamba, which demonstrates high performance and achieves Transformer-level\ncapabilities while requiring substantially fewer computational resources. In",
        "authors": [
            "Assaf Ben-Kish",
            "Itamar Zimerman",
            "Shady Abu-Hussein",
            "Nadav Cohen",
            "Amir Globerson",
            "Lior Wolf",
            "Raja Giryes"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T17:40:18+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14528v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14528v1",
        "categories": [
            "Machine Learning",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000043,
        "doi": null,
        "title": "DeciMamba: Exploring the Length Extrapolation Potential of Mamba",
        "abstract": "Long-range sequence processing poses a significant challenge for Transformers\ndue to their quadratic complexity in input length. A promising alternative is\nMamba, which demonstrates high performance and achieves Transformer-level\ncapabilities while requiring substantially fewer computational resources. In\nthis paper we explore the length-generalization capabilities of Mamba, which we\nfind to be relatively limited. Through a series of visualizations and analyses\nwe identify that the limitations arise from a restricted effective receptive\nfield, dictated by the sequence length used during training. To address this\nconstraint, we introduce DeciMamba, a context-extension method specifically\ndesigned for Mamba. This mechanism, built on top of a hidden filtering\nmechanism embedded within the S6 layer, enables the trained model to\nextrapolate well even without additional training. Empirical experiments over\nreal-world long-range NLP tasks show that DeciMamba can extrapolate to context\nlengths that are 25x times longer than the ones seen during training, and does\nso without utilizing additional computational resources. We will release our\ncode and models.",
        "chunk-id": 2,
        "chunk": "this paper we explore the length-generalization capabilities of Mamba, which we\nfind to be relatively limited. Through a series of visualizations and analyses\nwe identify that the limitations arise from a restricted effective receptive\nfield, dictated by the sequence length used during training. To address this",
        "authors": [
            "Assaf Ben-Kish",
            "Itamar Zimerman",
            "Shady Abu-Hussein",
            "Nadav Cohen",
            "Amir Globerson",
            "Lior Wolf",
            "Raja Giryes"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T17:40:18+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14528v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14528v1",
        "categories": [
            "Machine Learning",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000043,
        "doi": null,
        "title": "DeciMamba: Exploring the Length Extrapolation Potential of Mamba",
        "abstract": "Long-range sequence processing poses a significant challenge for Transformers\ndue to their quadratic complexity in input length. A promising alternative is\nMamba, which demonstrates high performance and achieves Transformer-level\ncapabilities while requiring substantially fewer computational resources. In\nthis paper we explore the length-generalization capabilities of Mamba, which we\nfind to be relatively limited. Through a series of visualizations and analyses\nwe identify that the limitations arise from a restricted effective receptive\nfield, dictated by the sequence length used during training. To address this\nconstraint, we introduce DeciMamba, a context-extension method specifically\ndesigned for Mamba. This mechanism, built on top of a hidden filtering\nmechanism embedded within the S6 layer, enables the trained model to\nextrapolate well even without additional training. Empirical experiments over\nreal-world long-range NLP tasks show that DeciMamba can extrapolate to context\nlengths that are 25x times longer than the ones seen during training, and does\nso without utilizing additional computational resources. We will release our\ncode and models.",
        "chunk-id": 3,
        "chunk": "constraint, we introduce DeciMamba, a context-extension method specifically\ndesigned for Mamba. This mechanism, built on top of a hidden filtering\nmechanism embedded within the S6 layer, enables the trained model to\nextrapolate well even without additional training. Empirical experiments over\nreal-world long-range NLP tasks show that DeciMamba can extrapolate to context",
        "authors": [
            "Assaf Ben-Kish",
            "Itamar Zimerman",
            "Shady Abu-Hussein",
            "Nadav Cohen",
            "Amir Globerson",
            "Lior Wolf",
            "Raja Giryes"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T17:40:18+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14528v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14528v1",
        "categories": [
            "Machine Learning",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000043,
        "doi": null,
        "title": "DeciMamba: Exploring the Length Extrapolation Potential of Mamba",
        "abstract": "Long-range sequence processing poses a significant challenge for Transformers\ndue to their quadratic complexity in input length. A promising alternative is\nMamba, which demonstrates high performance and achieves Transformer-level\ncapabilities while requiring substantially fewer computational resources. In\nthis paper we explore the length-generalization capabilities of Mamba, which we\nfind to be relatively limited. Through a series of visualizations and analyses\nwe identify that the limitations arise from a restricted effective receptive\nfield, dictated by the sequence length used during training. To address this\nconstraint, we introduce DeciMamba, a context-extension method specifically\ndesigned for Mamba. This mechanism, built on top of a hidden filtering\nmechanism embedded within the S6 layer, enables the trained model to\nextrapolate well even without additional training. Empirical experiments over\nreal-world long-range NLP tasks show that DeciMamba can extrapolate to context\nlengths that are 25x times longer than the ones seen during training, and does\nso without utilizing additional computational resources. We will release our\ncode and models.",
        "chunk-id": 4,
        "chunk": "lengths that are 25x times longer than the ones seen during training, and does\nso without utilizing additional computational resources. We will release our\ncode and models.",
        "authors": [
            "Assaf Ben-Kish",
            "Itamar Zimerman",
            "Shady Abu-Hussein",
            "Nadav Cohen",
            "Amir Globerson",
            "Lior Wolf",
            "Raja Giryes"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T17:40:18+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14528v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14528v1",
        "categories": [
            "Machine Learning",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000044,
        "doi": null,
        "title": "On Layer-wise Representation Similarity: Application for Multi-Exit Models with a Single Classifier",
        "abstract": "Analyzing the similarity of internal representations within and across\ndifferent models has been an important technique for understanding the behavior\nof deep neural networks. Most existing methods for analyzing the similarity\nbetween representations of high dimensions, such as those based on Canonical\nCorrelation Analysis (CCA) and widely used Centered Kernel Alignment (CKA),\nrely on statistical properties of the representations for a set of data points.\nIn this paper, we focus on transformer models and study the similarity of\nrepresentations between the hidden layers of individual transformers. In this\ncontext, we show that a simple sample-wise cosine similarity metric is capable\nof capturing the similarity and aligns with the complicated CKA. Our\nexperimental results on common transformers reveal that representations across\nlayers are positively correlated, albeit the similarity decreases when layers\nare far apart. We then propose an aligned training approach to enhance the\nsimilarity between internal representations, with trained models that enjoy the\nfollowing properties: (1) the last-layer classifier can be directly applied\nright after any hidden layers, yielding intermediate layer accuracies much\nhigher than those under standard training, (2) the layer-wise accuracies\nmonotonically increase and reveal the minimal depth needed for the given task,\n(3) when served as multi-exit models, they achieve on-par performance with\nstandard multi-exit architectures which consist of additional classifiers\ndesigned for early exiting in shallow layers. To our knowledge, our work is the\nfirst to show that one common classifier is sufficient for multi-exit models.\nWe conduct experiments on both vision and NLP tasks to demonstrate the\nperformance of the proposed aligned training.",
        "chunk-id": 1,
        "chunk": "Analyzing the similarity of internal representations within and across\ndifferent models has been an important technique for understanding the behavior\nof deep neural networks. Most existing methods for analyzing the similarity\nbetween representations of high dimensions, such as those based on Canonical\nCorrelation Analysis (CCA) and widely used Centered Kernel Alignment (CKA),",
        "authors": [
            "Jiachen Jiang",
            "Jinxin Zhou",
            "Zhihui Zhu"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T16:41:09+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14479v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14479v1",
        "categories": [
            "Artificial Intelligence",
            "Computation and Language",
            "Computer Vision and Pattern Recognition",
            "Machine Learning"
        ]
    },
    {
        "id": 20000044,
        "doi": null,
        "title": "On Layer-wise Representation Similarity: Application for Multi-Exit Models with a Single Classifier",
        "abstract": "Analyzing the similarity of internal representations within and across\ndifferent models has been an important technique for understanding the behavior\nof deep neural networks. Most existing methods for analyzing the similarity\nbetween representations of high dimensions, such as those based on Canonical\nCorrelation Analysis (CCA) and widely used Centered Kernel Alignment (CKA),\nrely on statistical properties of the representations for a set of data points.\nIn this paper, we focus on transformer models and study the similarity of\nrepresentations between the hidden layers of individual transformers. In this\ncontext, we show that a simple sample-wise cosine similarity metric is capable\nof capturing the similarity and aligns with the complicated CKA. Our\nexperimental results on common transformers reveal that representations across\nlayers are positively correlated, albeit the similarity decreases when layers\nare far apart. We then propose an aligned training approach to enhance the\nsimilarity between internal representations, with trained models that enjoy the\nfollowing properties: (1) the last-layer classifier can be directly applied\nright after any hidden layers, yielding intermediate layer accuracies much\nhigher than those under standard training, (2) the layer-wise accuracies\nmonotonically increase and reveal the minimal depth needed for the given task,\n(3) when served as multi-exit models, they achieve on-par performance with\nstandard multi-exit architectures which consist of additional classifiers\ndesigned for early exiting in shallow layers. To our knowledge, our work is the\nfirst to show that one common classifier is sufficient for multi-exit models.\nWe conduct experiments on both vision and NLP tasks to demonstrate the\nperformance of the proposed aligned training.",
        "chunk-id": 2,
        "chunk": "rely on statistical properties of the representations for a set of data points.\nIn this paper, we focus on transformer models and study the similarity of\nrepresentations between the hidden layers of individual transformers. In this\ncontext, we show that a simple sample-wise cosine similarity metric is capable\nof capturing the similarity and aligns with the complicated CKA. Our",
        "authors": [
            "Jiachen Jiang",
            "Jinxin Zhou",
            "Zhihui Zhu"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T16:41:09+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14479v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14479v1",
        "categories": [
            "Artificial Intelligence",
            "Computation and Language",
            "Computer Vision and Pattern Recognition",
            "Machine Learning"
        ]
    },
    {
        "id": 20000044,
        "doi": null,
        "title": "On Layer-wise Representation Similarity: Application for Multi-Exit Models with a Single Classifier",
        "abstract": "Analyzing the similarity of internal representations within and across\ndifferent models has been an important technique for understanding the behavior\nof deep neural networks. Most existing methods for analyzing the similarity\nbetween representations of high dimensions, such as those based on Canonical\nCorrelation Analysis (CCA) and widely used Centered Kernel Alignment (CKA),\nrely on statistical properties of the representations for a set of data points.\nIn this paper, we focus on transformer models and study the similarity of\nrepresentations between the hidden layers of individual transformers. In this\ncontext, we show that a simple sample-wise cosine similarity metric is capable\nof capturing the similarity and aligns with the complicated CKA. Our\nexperimental results on common transformers reveal that representations across\nlayers are positively correlated, albeit the similarity decreases when layers\nare far apart. We then propose an aligned training approach to enhance the\nsimilarity between internal representations, with trained models that enjoy the\nfollowing properties: (1) the last-layer classifier can be directly applied\nright after any hidden layers, yielding intermediate layer accuracies much\nhigher than those under standard training, (2) the layer-wise accuracies\nmonotonically increase and reveal the minimal depth needed for the given task,\n(3) when served as multi-exit models, they achieve on-par performance with\nstandard multi-exit architectures which consist of additional classifiers\ndesigned for early exiting in shallow layers. To our knowledge, our work is the\nfirst to show that one common classifier is sufficient for multi-exit models.\nWe conduct experiments on both vision and NLP tasks to demonstrate the\nperformance of the proposed aligned training.",
        "chunk-id": 3,
        "chunk": "experimental results on common transformers reveal that representations across\nlayers are positively correlated, albeit the similarity decreases when layers\nare far apart. We then propose an aligned training approach to enhance the\nsimilarity between internal representations, with trained models that enjoy the",
        "authors": [
            "Jiachen Jiang",
            "Jinxin Zhou",
            "Zhihui Zhu"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T16:41:09+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14479v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14479v1",
        "categories": [
            "Artificial Intelligence",
            "Computation and Language",
            "Computer Vision and Pattern Recognition",
            "Machine Learning"
        ]
    },
    {
        "id": 20000044,
        "doi": null,
        "title": "On Layer-wise Representation Similarity: Application for Multi-Exit Models with a Single Classifier",
        "abstract": "Analyzing the similarity of internal representations within and across\ndifferent models has been an important technique for understanding the behavior\nof deep neural networks. Most existing methods for analyzing the similarity\nbetween representations of high dimensions, such as those based on Canonical\nCorrelation Analysis (CCA) and widely used Centered Kernel Alignment (CKA),\nrely on statistical properties of the representations for a set of data points.\nIn this paper, we focus on transformer models and study the similarity of\nrepresentations between the hidden layers of individual transformers. In this\ncontext, we show that a simple sample-wise cosine similarity metric is capable\nof capturing the similarity and aligns with the complicated CKA. Our\nexperimental results on common transformers reveal that representations across\nlayers are positively correlated, albeit the similarity decreases when layers\nare far apart. We then propose an aligned training approach to enhance the\nsimilarity between internal representations, with trained models that enjoy the\nfollowing properties: (1) the last-layer classifier can be directly applied\nright after any hidden layers, yielding intermediate layer accuracies much\nhigher than those under standard training, (2) the layer-wise accuracies\nmonotonically increase and reveal the minimal depth needed for the given task,\n(3) when served as multi-exit models, they achieve on-par performance with\nstandard multi-exit architectures which consist of additional classifiers\ndesigned for early exiting in shallow layers. To our knowledge, our work is the\nfirst to show that one common classifier is sufficient for multi-exit models.\nWe conduct experiments on both vision and NLP tasks to demonstrate the\nperformance of the proposed aligned training.",
        "chunk-id": 4,
        "chunk": "following properties: (1) the last-layer classifier can be directly applied\nright after any hidden layers, yielding intermediate layer accuracies much\nhigher than those under standard training, (2) the layer-wise accuracies\nmonotonically increase and reveal the minimal depth needed for the given task,\n(3) when served as multi-exit models, they achieve on-par performance with",
        "authors": [
            "Jiachen Jiang",
            "Jinxin Zhou",
            "Zhihui Zhu"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T16:41:09+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14479v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14479v1",
        "categories": [
            "Artificial Intelligence",
            "Computation and Language",
            "Computer Vision and Pattern Recognition",
            "Machine Learning"
        ]
    },
    {
        "id": 20000044,
        "doi": null,
        "title": "On Layer-wise Representation Similarity: Application for Multi-Exit Models with a Single Classifier",
        "abstract": "Analyzing the similarity of internal representations within and across\ndifferent models has been an important technique for understanding the behavior\nof deep neural networks. Most existing methods for analyzing the similarity\nbetween representations of high dimensions, such as those based on Canonical\nCorrelation Analysis (CCA) and widely used Centered Kernel Alignment (CKA),\nrely on statistical properties of the representations for a set of data points.\nIn this paper, we focus on transformer models and study the similarity of\nrepresentations between the hidden layers of individual transformers. In this\ncontext, we show that a simple sample-wise cosine similarity metric is capable\nof capturing the similarity and aligns with the complicated CKA. Our\nexperimental results on common transformers reveal that representations across\nlayers are positively correlated, albeit the similarity decreases when layers\nare far apart. We then propose an aligned training approach to enhance the\nsimilarity between internal representations, with trained models that enjoy the\nfollowing properties: (1) the last-layer classifier can be directly applied\nright after any hidden layers, yielding intermediate layer accuracies much\nhigher than those under standard training, (2) the layer-wise accuracies\nmonotonically increase and reveal the minimal depth needed for the given task,\n(3) when served as multi-exit models, they achieve on-par performance with\nstandard multi-exit architectures which consist of additional classifiers\ndesigned for early exiting in shallow layers. To our knowledge, our work is the\nfirst to show that one common classifier is sufficient for multi-exit models.\nWe conduct experiments on both vision and NLP tasks to demonstrate the\nperformance of the proposed aligned training.",
        "chunk-id": 5,
        "chunk": "standard multi-exit architectures which consist of additional classifiers\ndesigned for early exiting in shallow layers. To our knowledge, our work is the\nfirst to show that one common classifier is sufficient for multi-exit models.\nWe conduct experiments on both vision and NLP tasks to demonstrate the\nperformance of the proposed aligned training.",
        "authors": [
            "Jiachen Jiang",
            "Jinxin Zhou",
            "Zhihui Zhu"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T16:41:09+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14479v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14479v1",
        "categories": [
            "Artificial Intelligence",
            "Computation and Language",
            "Computer Vision and Pattern Recognition",
            "Machine Learning"
        ]
    },
    {
        "id": 20000045,
        "doi": null,
        "title": "Healing Powers of BERT: How Task-Specific Fine-Tuning Recovers Corrupted Language Models",
        "abstract": "Language models like BERT excel at sentence classification tasks due to\nextensive pre-training on general data, but their robustness to parameter\ncorruption is unexplored. To understand this better, we look at what happens if\na language model is \"broken\", in the sense that some of its parameters are\ncorrupted and then recovered by fine-tuning. Strategically corrupting BERT\nvariants at different levels, we find corrupted models struggle to fully\nrecover their original performance, with higher corruption causing more severe\ndegradation. Notably, bottom-layer corruption affecting fundamental linguistic\nfeatures is more detrimental than top-layer corruption. Our insights contribute\nto understanding language model robustness and adaptability under adverse\nconditions, informing strategies for developing resilient NLP systems against\nparameter perturbations.",
        "chunk-id": 1,
        "chunk": "Language models like BERT excel at sentence classification tasks due to\nextensive pre-training on general data, but their robustness to parameter\ncorruption is unexplored. To understand this better, we look at what happens if\na language model is \"broken\", in the sense that some of its parameters are\ncorrupted and then recovered by fine-tuning. Strategically corrupting BERT",
        "authors": [
            "Shijie Han",
            "Zhenyu Zhang",
            "Andrei Arsene Simion"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T16:18:04+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14459v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14459v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000045,
        "doi": null,
        "title": "Healing Powers of BERT: How Task-Specific Fine-Tuning Recovers Corrupted Language Models",
        "abstract": "Language models like BERT excel at sentence classification tasks due to\nextensive pre-training on general data, but their robustness to parameter\ncorruption is unexplored. To understand this better, we look at what happens if\na language model is \"broken\", in the sense that some of its parameters are\ncorrupted and then recovered by fine-tuning. Strategically corrupting BERT\nvariants at different levels, we find corrupted models struggle to fully\nrecover their original performance, with higher corruption causing more severe\ndegradation. Notably, bottom-layer corruption affecting fundamental linguistic\nfeatures is more detrimental than top-layer corruption. Our insights contribute\nto understanding language model robustness and adaptability under adverse\nconditions, informing strategies for developing resilient NLP systems against\nparameter perturbations.",
        "chunk-id": 2,
        "chunk": "variants at different levels, we find corrupted models struggle to fully\nrecover their original performance, with higher corruption causing more severe\ndegradation. Notably, bottom-layer corruption affecting fundamental linguistic\nfeatures is more detrimental than top-layer corruption. Our insights contribute\nto understanding language model robustness and adaptability under adverse",
        "authors": [
            "Shijie Han",
            "Zhenyu Zhang",
            "Andrei Arsene Simion"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T16:18:04+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14459v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14459v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000045,
        "doi": null,
        "title": "Healing Powers of BERT: How Task-Specific Fine-Tuning Recovers Corrupted Language Models",
        "abstract": "Language models like BERT excel at sentence classification tasks due to\nextensive pre-training on general data, but their robustness to parameter\ncorruption is unexplored. To understand this better, we look at what happens if\na language model is \"broken\", in the sense that some of its parameters are\ncorrupted and then recovered by fine-tuning. Strategically corrupting BERT\nvariants at different levels, we find corrupted models struggle to fully\nrecover their original performance, with higher corruption causing more severe\ndegradation. Notably, bottom-layer corruption affecting fundamental linguistic\nfeatures is more detrimental than top-layer corruption. Our insights contribute\nto understanding language model robustness and adaptability under adverse\nconditions, informing strategies for developing resilient NLP systems against\nparameter perturbations.",
        "chunk-id": 3,
        "chunk": "conditions, informing strategies for developing resilient NLP systems against\nparameter perturbations.",
        "authors": [
            "Shijie Han",
            "Zhenyu Zhang",
            "Andrei Arsene Simion"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T16:18:04+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14459v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14459v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000046,
        "doi": null,
        "title": "On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?",
        "abstract": "While multilingual language models (MLMs) have been trained on 100+\nlanguages, they are typically only evaluated across a handful of them due to a\nlack of available test data in most languages. This is particularly problematic\nwhen assessing MLM's potential for low-resource and unseen languages. In this\npaper, we present an analysis of existing evaluation frameworks in multilingual\nNLP, discuss their limitations, and propose several directions for more robust\nand reliable evaluation practices. Furthermore, we empirically study to what\nextent machine translation offers a {reliable alternative to human translation}\nfor large-scale evaluation of MLMs across a wide set of languages. We use a\nSOTA translation model to translate test data from 4 tasks to 198 languages and\nuse them to evaluate three MLMs. We show that while the selected subsets of\nhigh-resource test languages are generally sufficiently representative of a\nwider range of high-resource languages, we tend to overestimate MLMs' ability\non low-resource languages. Finally, we show that simpler baselines can achieve\nrelatively strong performance without having benefited from large-scale\nmultilingual pretraining.",
        "chunk-id": 1,
        "chunk": "While multilingual language models (MLMs) have been trained on 100+\nlanguages, they are typically only evaluated across a handful of them due to a\nlack of available test data in most languages. This is particularly problematic\nwhen assessing MLM's potential for low-resource and unseen languages. In this\npaper, we present an analysis of existing evaluation frameworks in multilingual",
        "authors": [
            "Rochelle Choenni",
            "Sara Rajaee",
            "Christof Monz",
            "Ekaterina Shutova"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T12:46:12+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14267v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14267v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000046,
        "doi": null,
        "title": "On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?",
        "abstract": "While multilingual language models (MLMs) have been trained on 100+\nlanguages, they are typically only evaluated across a handful of them due to a\nlack of available test data in most languages. This is particularly problematic\nwhen assessing MLM's potential for low-resource and unseen languages. In this\npaper, we present an analysis of existing evaluation frameworks in multilingual\nNLP, discuss their limitations, and propose several directions for more robust\nand reliable evaluation practices. Furthermore, we empirically study to what\nextent machine translation offers a {reliable alternative to human translation}\nfor large-scale evaluation of MLMs across a wide set of languages. We use a\nSOTA translation model to translate test data from 4 tasks to 198 languages and\nuse them to evaluate three MLMs. We show that while the selected subsets of\nhigh-resource test languages are generally sufficiently representative of a\nwider range of high-resource languages, we tend to overestimate MLMs' ability\non low-resource languages. Finally, we show that simpler baselines can achieve\nrelatively strong performance without having benefited from large-scale\nmultilingual pretraining.",
        "chunk-id": 2,
        "chunk": "NLP, discuss their limitations, and propose several directions for more robust\nand reliable evaluation practices. Furthermore, we empirically study to what\nextent machine translation offers a {reliable alternative to human translation}\nfor large-scale evaluation of MLMs across a wide set of languages. We use a",
        "authors": [
            "Rochelle Choenni",
            "Sara Rajaee",
            "Christof Monz",
            "Ekaterina Shutova"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T12:46:12+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14267v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14267v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000046,
        "doi": null,
        "title": "On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?",
        "abstract": "While multilingual language models (MLMs) have been trained on 100+\nlanguages, they are typically only evaluated across a handful of them due to a\nlack of available test data in most languages. This is particularly problematic\nwhen assessing MLM's potential for low-resource and unseen languages. In this\npaper, we present an analysis of existing evaluation frameworks in multilingual\nNLP, discuss their limitations, and propose several directions for more robust\nand reliable evaluation practices. Furthermore, we empirically study to what\nextent machine translation offers a {reliable alternative to human translation}\nfor large-scale evaluation of MLMs across a wide set of languages. We use a\nSOTA translation model to translate test data from 4 tasks to 198 languages and\nuse them to evaluate three MLMs. We show that while the selected subsets of\nhigh-resource test languages are generally sufficiently representative of a\nwider range of high-resource languages, we tend to overestimate MLMs' ability\non low-resource languages. Finally, we show that simpler baselines can achieve\nrelatively strong performance without having benefited from large-scale\nmultilingual pretraining.",
        "chunk-id": 3,
        "chunk": "SOTA translation model to translate test data from 4 tasks to 198 languages and\nuse them to evaluate three MLMs. We show that while the selected subsets of\nhigh-resource test languages are generally sufficiently representative of a\nwider range of high-resource languages, we tend to overestimate MLMs' ability",
        "authors": [
            "Rochelle Choenni",
            "Sara Rajaee",
            "Christof Monz",
            "Ekaterina Shutova"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T12:46:12+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14267v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14267v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000046,
        "doi": null,
        "title": "On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?",
        "abstract": "While multilingual language models (MLMs) have been trained on 100+\nlanguages, they are typically only evaluated across a handful of them due to a\nlack of available test data in most languages. This is particularly problematic\nwhen assessing MLM's potential for low-resource and unseen languages. In this\npaper, we present an analysis of existing evaluation frameworks in multilingual\nNLP, discuss their limitations, and propose several directions for more robust\nand reliable evaluation practices. Furthermore, we empirically study to what\nextent machine translation offers a {reliable alternative to human translation}\nfor large-scale evaluation of MLMs across a wide set of languages. We use a\nSOTA translation model to translate test data from 4 tasks to 198 languages and\nuse them to evaluate three MLMs. We show that while the selected subsets of\nhigh-resource test languages are generally sufficiently representative of a\nwider range of high-resource languages, we tend to overestimate MLMs' ability\non low-resource languages. Finally, we show that simpler baselines can achieve\nrelatively strong performance without having benefited from large-scale\nmultilingual pretraining.",
        "chunk-id": 4,
        "chunk": "on low-resource languages. Finally, we show that simpler baselines can achieve\nrelatively strong performance without having benefited from large-scale\nmultilingual pretraining.",
        "authors": [
            "Rochelle Choenni",
            "Sara Rajaee",
            "Christof Monz",
            "Ekaterina Shutova"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T12:46:12+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14267v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14267v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000047,
        "doi": null,
        "title": "Defending Against Sophisticated Poisoning Attacks with RL-based Aggregation in Federated Learning",
        "abstract": "Federated learning is highly susceptible to model poisoning attacks,\nespecially those meticulously crafted for servers. Traditional defense methods\nmainly focus on updating assessments or robust aggregation against manually\ncrafted myopic attacks. When facing advanced attacks, their defense stability\nis notably insufficient. Therefore, it is imperative to develop adaptive\ndefenses against such advanced poisoning attacks. We find that benign clients\nexhibit significantly higher data distribution stability than malicious clients\nin federated learning in both CV and NLP tasks. Therefore, the malicious\nclients can be recognized by observing the stability of their data\ndistribution. In this paper, we propose AdaAggRL, an RL-based Adaptive\nAggregation method, to defend against sophisticated poisoning attacks.\nSpecifically, we first utilize distribution learning to simulate the clients'\ndata distributions. Then, we use the maximum mean discrepancy (MMD) to\ncalculate the pairwise similarity of the current local model data distribution,\nits historical data distribution, and global model data distribution. Finally,\nwe use policy learning to adaptively determine the aggregation weights based on\nthe above similarities. Experiments on four real-world datasets demonstrate\nthat the proposed defense model significantly outperforms widely adopted\ndefense models for sophisticated attacks.",
        "chunk-id": 1,
        "chunk": "Federated learning is highly susceptible to model poisoning attacks,\nespecially those meticulously crafted for servers. Traditional defense methods\nmainly focus on updating assessments or robust aggregation against manually\ncrafted myopic attacks. When facing advanced attacks, their defense stability\nis notably insufficient. Therefore, it is imperative to develop adaptive",
        "authors": [
            "Yujing Wang",
            "Hainan Zhang",
            "Sijia Wen",
            "Wangjie Qiu",
            "Binghui Guo"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T11:33:14+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14217v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14217v1",
        "categories": [
            "Machine Learning",
            "Cryptography and Security"
        ]
    },
    {
        "id": 20000047,
        "doi": null,
        "title": "Defending Against Sophisticated Poisoning Attacks with RL-based Aggregation in Federated Learning",
        "abstract": "Federated learning is highly susceptible to model poisoning attacks,\nespecially those meticulously crafted for servers. Traditional defense methods\nmainly focus on updating assessments or robust aggregation against manually\ncrafted myopic attacks. When facing advanced attacks, their defense stability\nis notably insufficient. Therefore, it is imperative to develop adaptive\ndefenses against such advanced poisoning attacks. We find that benign clients\nexhibit significantly higher data distribution stability than malicious clients\nin federated learning in both CV and NLP tasks. Therefore, the malicious\nclients can be recognized by observing the stability of their data\ndistribution. In this paper, we propose AdaAggRL, an RL-based Adaptive\nAggregation method, to defend against sophisticated poisoning attacks.\nSpecifically, we first utilize distribution learning to simulate the clients'\ndata distributions. Then, we use the maximum mean discrepancy (MMD) to\ncalculate the pairwise similarity of the current local model data distribution,\nits historical data distribution, and global model data distribution. Finally,\nwe use policy learning to adaptively determine the aggregation weights based on\nthe above similarities. Experiments on four real-world datasets demonstrate\nthat the proposed defense model significantly outperforms widely adopted\ndefense models for sophisticated attacks.",
        "chunk-id": 2,
        "chunk": "defenses against such advanced poisoning attacks. We find that benign clients\nexhibit significantly higher data distribution stability than malicious clients\nin federated learning in both CV and NLP tasks. Therefore, the malicious\nclients can be recognized by observing the stability of their data\ndistribution. In this paper, we propose AdaAggRL, an RL-based Adaptive",
        "authors": [
            "Yujing Wang",
            "Hainan Zhang",
            "Sijia Wen",
            "Wangjie Qiu",
            "Binghui Guo"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T11:33:14+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14217v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14217v1",
        "categories": [
            "Machine Learning",
            "Cryptography and Security"
        ]
    },
    {
        "id": 20000047,
        "doi": null,
        "title": "Defending Against Sophisticated Poisoning Attacks with RL-based Aggregation in Federated Learning",
        "abstract": "Federated learning is highly susceptible to model poisoning attacks,\nespecially those meticulously crafted for servers. Traditional defense methods\nmainly focus on updating assessments or robust aggregation against manually\ncrafted myopic attacks. When facing advanced attacks, their defense stability\nis notably insufficient. Therefore, it is imperative to develop adaptive\ndefenses against such advanced poisoning attacks. We find that benign clients\nexhibit significantly higher data distribution stability than malicious clients\nin federated learning in both CV and NLP tasks. Therefore, the malicious\nclients can be recognized by observing the stability of their data\ndistribution. In this paper, we propose AdaAggRL, an RL-based Adaptive\nAggregation method, to defend against sophisticated poisoning attacks.\nSpecifically, we first utilize distribution learning to simulate the clients'\ndata distributions. Then, we use the maximum mean discrepancy (MMD) to\ncalculate the pairwise similarity of the current local model data distribution,\nits historical data distribution, and global model data distribution. Finally,\nwe use policy learning to adaptively determine the aggregation weights based on\nthe above similarities. Experiments on four real-world datasets demonstrate\nthat the proposed defense model significantly outperforms widely adopted\ndefense models for sophisticated attacks.",
        "chunk-id": 3,
        "chunk": "Aggregation method, to defend against sophisticated poisoning attacks.\nSpecifically, we first utilize distribution learning to simulate the clients'\ndata distributions. Then, we use the maximum mean discrepancy (MMD) to\ncalculate the pairwise similarity of the current local model data distribution,\nits historical data distribution, and global model data distribution. Finally,",
        "authors": [
            "Yujing Wang",
            "Hainan Zhang",
            "Sijia Wen",
            "Wangjie Qiu",
            "Binghui Guo"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T11:33:14+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14217v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14217v1",
        "categories": [
            "Machine Learning",
            "Cryptography and Security"
        ]
    },
    {
        "id": 20000047,
        "doi": null,
        "title": "Defending Against Sophisticated Poisoning Attacks with RL-based Aggregation in Federated Learning",
        "abstract": "Federated learning is highly susceptible to model poisoning attacks,\nespecially those meticulously crafted for servers. Traditional defense methods\nmainly focus on updating assessments or robust aggregation against manually\ncrafted myopic attacks. When facing advanced attacks, their defense stability\nis notably insufficient. Therefore, it is imperative to develop adaptive\ndefenses against such advanced poisoning attacks. We find that benign clients\nexhibit significantly higher data distribution stability than malicious clients\nin federated learning in both CV and NLP tasks. Therefore, the malicious\nclients can be recognized by observing the stability of their data\ndistribution. In this paper, we propose AdaAggRL, an RL-based Adaptive\nAggregation method, to defend against sophisticated poisoning attacks.\nSpecifically, we first utilize distribution learning to simulate the clients'\ndata distributions. Then, we use the maximum mean discrepancy (MMD) to\ncalculate the pairwise similarity of the current local model data distribution,\nits historical data distribution, and global model data distribution. Finally,\nwe use policy learning to adaptively determine the aggregation weights based on\nthe above similarities. Experiments on four real-world datasets demonstrate\nthat the proposed defense model significantly outperforms widely adopted\ndefense models for sophisticated attacks.",
        "chunk-id": 4,
        "chunk": "we use policy learning to adaptively determine the aggregation weights based on\nthe above similarities. Experiments on four real-world datasets demonstrate\nthat the proposed defense model significantly outperforms widely adopted\ndefense models for sophisticated attacks.",
        "authors": [
            "Yujing Wang",
            "Hainan Zhang",
            "Sijia Wen",
            "Wangjie Qiu",
            "Binghui Guo"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T11:33:14+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14217v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14217v1",
        "categories": [
            "Machine Learning",
            "Cryptography and Security"
        ]
    },
    {
        "id": 20000048,
        "doi": null,
        "title": "A mapping-free NLP-based technique for sequence search in Nanopore long-reads",
        "abstract": "In unforeseen situations, such as nuclear power plant's or civilian radiation\naccidents, there is a need for effective and computationally inexpensive\nmethods to determine the expression level of a selected gene panel, allowing\nfor rough dose estimates in thousands of donors. The new generation in-situ\nmapper, fast and of low energy consumption, working at the level of single\nnanopore output, is in demand. We aim to create a sequence identification tool\nthat utilizes Natural Language Processing (NLP) techniques and ensures a high\nlevel of negative predictive value (NPV) compared to the classical approach.\nThe training dataset consisted of RNASeq data from 6 samples. Having tested\nmultiple NLP models, the best configuration analyses the entire sequence and\nuses a word length of 3 base pairs with one-word neighbor on each side. For the\nconsidered FDXR gene, the achieved mean balanced accuracy (BACC) was 98.29% and\nNPV 99.25%, compared to minimap2's performance in a cross-validation scenario.\nReducing the dictionary from 1024 to 145 changed BACC to 96.49% and the NPV to\n98.15%. Obtained NLP model, validated on an external independent genome\nsequencing dataset, gave NPV of 99.64% for complete and 95.87% for reduced\ndictionary. The salmon-estimated read counts differed from the classical\napproach on average by 3.48% for the complete dictionary and by 5.82% for the\nreduced one. We conclude that for long Oxford Nanopore reads, an NLP-based\napproach can successfully replace classical mapping in case of emergency. The\ndeveloped NLP model can be easily retrained to identify selected transcripts\nand/or work with various long-read sequencing techniques. Our results of the\nstudy clearly demonstrate the potential of applying techniques known from\nclassical text processing to nucleotide sequences and represent a significant\nadvancement in this field of science.",
        "chunk-id": 1,
        "chunk": "In unforeseen situations, such as nuclear power plant's or civilian radiation\naccidents, there is a need for effective and computationally inexpensive\nmethods to determine the expression level of a selected gene panel, allowing\nfor rough dose estimates in thousands of donors. The new generation in-situ\nmapper, fast and of low energy consumption, working at the level of single",
        "authors": [
            "Tomasz Strzoda",
            "Lourdes Cruz-Garcia",
            "Mustafa Najim",
            "Christophe Badie",
            "Joanna Polanska"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T10:48:19+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14187v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14187v1",
        "categories": [
            "Genomics"
        ]
    },
    {
        "id": 20000048,
        "doi": null,
        "title": "A mapping-free NLP-based technique for sequence search in Nanopore long-reads",
        "abstract": "In unforeseen situations, such as nuclear power plant's or civilian radiation\naccidents, there is a need for effective and computationally inexpensive\nmethods to determine the expression level of a selected gene panel, allowing\nfor rough dose estimates in thousands of donors. The new generation in-situ\nmapper, fast and of low energy consumption, working at the level of single\nnanopore output, is in demand. We aim to create a sequence identification tool\nthat utilizes Natural Language Processing (NLP) techniques and ensures a high\nlevel of negative predictive value (NPV) compared to the classical approach.\nThe training dataset consisted of RNASeq data from 6 samples. Having tested\nmultiple NLP models, the best configuration analyses the entire sequence and\nuses a word length of 3 base pairs with one-word neighbor on each side. For the\nconsidered FDXR gene, the achieved mean balanced accuracy (BACC) was 98.29% and\nNPV 99.25%, compared to minimap2's performance in a cross-validation scenario.\nReducing the dictionary from 1024 to 145 changed BACC to 96.49% and the NPV to\n98.15%. Obtained NLP model, validated on an external independent genome\nsequencing dataset, gave NPV of 99.64% for complete and 95.87% for reduced\ndictionary. The salmon-estimated read counts differed from the classical\napproach on average by 3.48% for the complete dictionary and by 5.82% for the\nreduced one. We conclude that for long Oxford Nanopore reads, an NLP-based\napproach can successfully replace classical mapping in case of emergency. The\ndeveloped NLP model can be easily retrained to identify selected transcripts\nand/or work with various long-read sequencing techniques. Our results of the\nstudy clearly demonstrate the potential of applying techniques known from\nclassical text processing to nucleotide sequences and represent a significant\nadvancement in this field of science.",
        "chunk-id": 2,
        "chunk": "nanopore output, is in demand. We aim to create a sequence identification tool\nthat utilizes Natural Language Processing (NLP) techniques and ensures a high\nlevel of negative predictive value (NPV) compared to the classical approach.\nThe training dataset consisted of RNASeq data from 6 samples. Having tested",
        "authors": [
            "Tomasz Strzoda",
            "Lourdes Cruz-Garcia",
            "Mustafa Najim",
            "Christophe Badie",
            "Joanna Polanska"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T10:48:19+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14187v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14187v1",
        "categories": [
            "Genomics"
        ]
    },
    {
        "id": 20000048,
        "doi": null,
        "title": "A mapping-free NLP-based technique for sequence search in Nanopore long-reads",
        "abstract": "In unforeseen situations, such as nuclear power plant's or civilian radiation\naccidents, there is a need for effective and computationally inexpensive\nmethods to determine the expression level of a selected gene panel, allowing\nfor rough dose estimates in thousands of donors. The new generation in-situ\nmapper, fast and of low energy consumption, working at the level of single\nnanopore output, is in demand. We aim to create a sequence identification tool\nthat utilizes Natural Language Processing (NLP) techniques and ensures a high\nlevel of negative predictive value (NPV) compared to the classical approach.\nThe training dataset consisted of RNASeq data from 6 samples. Having tested\nmultiple NLP models, the best configuration analyses the entire sequence and\nuses a word length of 3 base pairs with one-word neighbor on each side. For the\nconsidered FDXR gene, the achieved mean balanced accuracy (BACC) was 98.29% and\nNPV 99.25%, compared to minimap2's performance in a cross-validation scenario.\nReducing the dictionary from 1024 to 145 changed BACC to 96.49% and the NPV to\n98.15%. Obtained NLP model, validated on an external independent genome\nsequencing dataset, gave NPV of 99.64% for complete and 95.87% for reduced\ndictionary. The salmon-estimated read counts differed from the classical\napproach on average by 3.48% for the complete dictionary and by 5.82% for the\nreduced one. We conclude that for long Oxford Nanopore reads, an NLP-based\napproach can successfully replace classical mapping in case of emergency. The\ndeveloped NLP model can be easily retrained to identify selected transcripts\nand/or work with various long-read sequencing techniques. Our results of the\nstudy clearly demonstrate the potential of applying techniques known from\nclassical text processing to nucleotide sequences and represent a significant\nadvancement in this field of science.",
        "chunk-id": 3,
        "chunk": "multiple NLP models, the best configuration analyses the entire sequence and\nuses a word length of 3 base pairs with one-word neighbor on each side. For the\nconsidered FDXR gene, the achieved mean balanced accuracy (BACC) was 98.29% and\nNPV 99.25%, compared to minimap2's performance in a cross-validation scenario.",
        "authors": [
            "Tomasz Strzoda",
            "Lourdes Cruz-Garcia",
            "Mustafa Najim",
            "Christophe Badie",
            "Joanna Polanska"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T10:48:19+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14187v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14187v1",
        "categories": [
            "Genomics"
        ]
    },
    {
        "id": 20000048,
        "doi": null,
        "title": "A mapping-free NLP-based technique for sequence search in Nanopore long-reads",
        "abstract": "In unforeseen situations, such as nuclear power plant's or civilian radiation\naccidents, there is a need for effective and computationally inexpensive\nmethods to determine the expression level of a selected gene panel, allowing\nfor rough dose estimates in thousands of donors. The new generation in-situ\nmapper, fast and of low energy consumption, working at the level of single\nnanopore output, is in demand. We aim to create a sequence identification tool\nthat utilizes Natural Language Processing (NLP) techniques and ensures a high\nlevel of negative predictive value (NPV) compared to the classical approach.\nThe training dataset consisted of RNASeq data from 6 samples. Having tested\nmultiple NLP models, the best configuration analyses the entire sequence and\nuses a word length of 3 base pairs with one-word neighbor on each side. For the\nconsidered FDXR gene, the achieved mean balanced accuracy (BACC) was 98.29% and\nNPV 99.25%, compared to minimap2's performance in a cross-validation scenario.\nReducing the dictionary from 1024 to 145 changed BACC to 96.49% and the NPV to\n98.15%. Obtained NLP model, validated on an external independent genome\nsequencing dataset, gave NPV of 99.64% for complete and 95.87% for reduced\ndictionary. The salmon-estimated read counts differed from the classical\napproach on average by 3.48% for the complete dictionary and by 5.82% for the\nreduced one. We conclude that for long Oxford Nanopore reads, an NLP-based\napproach can successfully replace classical mapping in case of emergency. The\ndeveloped NLP model can be easily retrained to identify selected transcripts\nand/or work with various long-read sequencing techniques. Our results of the\nstudy clearly demonstrate the potential of applying techniques known from\nclassical text processing to nucleotide sequences and represent a significant\nadvancement in this field of science.",
        "chunk-id": 4,
        "chunk": "Reducing the dictionary from 1024 to 145 changed BACC to 96.49% and the NPV to\n98.15%. Obtained NLP model, validated on an external independent genome\nsequencing dataset, gave NPV of 99.64% for complete and 95.87% for reduced\ndictionary. The salmon-estimated read counts differed from the classical\napproach on average by 3.48% for the complete dictionary and by 5.82% for the",
        "authors": [
            "Tomasz Strzoda",
            "Lourdes Cruz-Garcia",
            "Mustafa Najim",
            "Christophe Badie",
            "Joanna Polanska"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T10:48:19+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14187v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14187v1",
        "categories": [
            "Genomics"
        ]
    },
    {
        "id": 20000048,
        "doi": null,
        "title": "A mapping-free NLP-based technique for sequence search in Nanopore long-reads",
        "abstract": "In unforeseen situations, such as nuclear power plant's or civilian radiation\naccidents, there is a need for effective and computationally inexpensive\nmethods to determine the expression level of a selected gene panel, allowing\nfor rough dose estimates in thousands of donors. The new generation in-situ\nmapper, fast and of low energy consumption, working at the level of single\nnanopore output, is in demand. We aim to create a sequence identification tool\nthat utilizes Natural Language Processing (NLP) techniques and ensures a high\nlevel of negative predictive value (NPV) compared to the classical approach.\nThe training dataset consisted of RNASeq data from 6 samples. Having tested\nmultiple NLP models, the best configuration analyses the entire sequence and\nuses a word length of 3 base pairs with one-word neighbor on each side. For the\nconsidered FDXR gene, the achieved mean balanced accuracy (BACC) was 98.29% and\nNPV 99.25%, compared to minimap2's performance in a cross-validation scenario.\nReducing the dictionary from 1024 to 145 changed BACC to 96.49% and the NPV to\n98.15%. Obtained NLP model, validated on an external independent genome\nsequencing dataset, gave NPV of 99.64% for complete and 95.87% for reduced\ndictionary. The salmon-estimated read counts differed from the classical\napproach on average by 3.48% for the complete dictionary and by 5.82% for the\nreduced one. We conclude that for long Oxford Nanopore reads, an NLP-based\napproach can successfully replace classical mapping in case of emergency. The\ndeveloped NLP model can be easily retrained to identify selected transcripts\nand/or work with various long-read sequencing techniques. Our results of the\nstudy clearly demonstrate the potential of applying techniques known from\nclassical text processing to nucleotide sequences and represent a significant\nadvancement in this field of science.",
        "chunk-id": 5,
        "chunk": "reduced one. We conclude that for long Oxford Nanopore reads, an NLP-based\napproach can successfully replace classical mapping in case of emergency. The\ndeveloped NLP model can be easily retrained to identify selected transcripts\nand/or work with various long-read sequencing techniques. Our results of the\nstudy clearly demonstrate the potential of applying techniques known from",
        "authors": [
            "Tomasz Strzoda",
            "Lourdes Cruz-Garcia",
            "Mustafa Najim",
            "Christophe Badie",
            "Joanna Polanska"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T10:48:19+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14187v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14187v1",
        "categories": [
            "Genomics"
        ]
    },
    {
        "id": 20000048,
        "doi": null,
        "title": "A mapping-free NLP-based technique for sequence search in Nanopore long-reads",
        "abstract": "In unforeseen situations, such as nuclear power plant's or civilian radiation\naccidents, there is a need for effective and computationally inexpensive\nmethods to determine the expression level of a selected gene panel, allowing\nfor rough dose estimates in thousands of donors. The new generation in-situ\nmapper, fast and of low energy consumption, working at the level of single\nnanopore output, is in demand. We aim to create a sequence identification tool\nthat utilizes Natural Language Processing (NLP) techniques and ensures a high\nlevel of negative predictive value (NPV) compared to the classical approach.\nThe training dataset consisted of RNASeq data from 6 samples. Having tested\nmultiple NLP models, the best configuration analyses the entire sequence and\nuses a word length of 3 base pairs with one-word neighbor on each side. For the\nconsidered FDXR gene, the achieved mean balanced accuracy (BACC) was 98.29% and\nNPV 99.25%, compared to minimap2's performance in a cross-validation scenario.\nReducing the dictionary from 1024 to 145 changed BACC to 96.49% and the NPV to\n98.15%. Obtained NLP model, validated on an external independent genome\nsequencing dataset, gave NPV of 99.64% for complete and 95.87% for reduced\ndictionary. The salmon-estimated read counts differed from the classical\napproach on average by 3.48% for the complete dictionary and by 5.82% for the\nreduced one. We conclude that for long Oxford Nanopore reads, an NLP-based\napproach can successfully replace classical mapping in case of emergency. The\ndeveloped NLP model can be easily retrained to identify selected transcripts\nand/or work with various long-read sequencing techniques. Our results of the\nstudy clearly demonstrate the potential of applying techniques known from\nclassical text processing to nucleotide sequences and represent a significant\nadvancement in this field of science.",
        "chunk-id": 6,
        "chunk": "classical text processing to nucleotide sequences and represent a significant\nadvancement in this field of science.",
        "authors": [
            "Tomasz Strzoda",
            "Lourdes Cruz-Garcia",
            "Mustafa Najim",
            "Christophe Badie",
            "Joanna Polanska"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T10:48:19+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14187v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14187v1",
        "categories": [
            "Genomics"
        ]
    },
    {
        "id": 20000049,
        "doi": null,
        "title": "Leveraging eBPF and AI for Ransomware Nose Out",
        "abstract": "In this work, we propose a two-phased approach for real-time detection and\ndeterrence of ransomware. To achieve this, we leverage the capabilities of eBPF\n(Extended Berkeley Packet Filter) and artificial intelligence to develop both\nproactive and reactive methods. In the first phase, we utilize signature based\ndetection, where we employ custom eBPF programs to trace the execution of new\nprocesses and perform hash-based analysis against a known ransomware dataset.\nIn the second, we employ a behavior-based technique that focuses on monitoring\nthe process activities using a custom eBPF program and the creation of ransom\nnotes, a prominent indicator of ransomware activity through the use of Natural\nLanguage Processing (NLP). By leveraging low-level tracing capabilities of eBPF\nand integrating NLP based machine learning algorithms, our solution achieves an\nimpressive 99.76% accuracy in identifying ransomware incidents within a few\nseconds on the onset of zero-day attacks.",
        "chunk-id": 1,
        "chunk": "In this work, we propose a two-phased approach for real-time detection and\ndeterrence of ransomware. To achieve this, we leverage the capabilities of eBPF\n(Extended Berkeley Packet Filter) and artificial intelligence to develop both\nproactive and reactive methods. In the first phase, we utilize signature based",
        "authors": [
            "Arjun Sekar",
            "Sameer G. Kulkarni",
            "Joy Kuri"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T06:35:15+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14020v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14020v1",
        "categories": [
            "Cryptography and Security",
            "Artificial Intelligence",
            "Emerging Technologies",
            "Networking and Internet Architecture"
        ]
    },
    {
        "id": 20000049,
        "doi": null,
        "title": "Leveraging eBPF and AI for Ransomware Nose Out",
        "abstract": "In this work, we propose a two-phased approach for real-time detection and\ndeterrence of ransomware. To achieve this, we leverage the capabilities of eBPF\n(Extended Berkeley Packet Filter) and artificial intelligence to develop both\nproactive and reactive methods. In the first phase, we utilize signature based\ndetection, where we employ custom eBPF programs to trace the execution of new\nprocesses and perform hash-based analysis against a known ransomware dataset.\nIn the second, we employ a behavior-based technique that focuses on monitoring\nthe process activities using a custom eBPF program and the creation of ransom\nnotes, a prominent indicator of ransomware activity through the use of Natural\nLanguage Processing (NLP). By leveraging low-level tracing capabilities of eBPF\nand integrating NLP based machine learning algorithms, our solution achieves an\nimpressive 99.76% accuracy in identifying ransomware incidents within a few\nseconds on the onset of zero-day attacks.",
        "chunk-id": 2,
        "chunk": "detection, where we employ custom eBPF programs to trace the execution of new\nprocesses and perform hash-based analysis against a known ransomware dataset.\nIn the second, we employ a behavior-based technique that focuses on monitoring\nthe process activities using a custom eBPF program and the creation of ransom",
        "authors": [
            "Arjun Sekar",
            "Sameer G. Kulkarni",
            "Joy Kuri"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T06:35:15+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14020v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14020v1",
        "categories": [
            "Cryptography and Security",
            "Artificial Intelligence",
            "Emerging Technologies",
            "Networking and Internet Architecture"
        ]
    },
    {
        "id": 20000049,
        "doi": null,
        "title": "Leveraging eBPF and AI for Ransomware Nose Out",
        "abstract": "In this work, we propose a two-phased approach for real-time detection and\ndeterrence of ransomware. To achieve this, we leverage the capabilities of eBPF\n(Extended Berkeley Packet Filter) and artificial intelligence to develop both\nproactive and reactive methods. In the first phase, we utilize signature based\ndetection, where we employ custom eBPF programs to trace the execution of new\nprocesses and perform hash-based analysis against a known ransomware dataset.\nIn the second, we employ a behavior-based technique that focuses on monitoring\nthe process activities using a custom eBPF program and the creation of ransom\nnotes, a prominent indicator of ransomware activity through the use of Natural\nLanguage Processing (NLP). By leveraging low-level tracing capabilities of eBPF\nand integrating NLP based machine learning algorithms, our solution achieves an\nimpressive 99.76% accuracy in identifying ransomware incidents within a few\nseconds on the onset of zero-day attacks.",
        "chunk-id": 3,
        "chunk": "notes, a prominent indicator of ransomware activity through the use of Natural\nLanguage Processing (NLP). By leveraging low-level tracing capabilities of eBPF\nand integrating NLP based machine learning algorithms, our solution achieves an\nimpressive 99.76% accuracy in identifying ransomware incidents within a few\nseconds on the onset of zero-day attacks.",
        "authors": [
            "Arjun Sekar",
            "Sameer G. Kulkarni",
            "Joy Kuri"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T06:35:15+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.14020v1",
        "arxiv_link": "http://arxiv.org/abs/2406.14020v1",
        "categories": [
            "Cryptography and Security",
            "Artificial Intelligence",
            "Emerging Technologies",
            "Networking and Internet Architecture"
        ]
    },
    {
        "id": 20000050,
        "doi": null,
        "title": "SPL: A Socratic Playground for Learning Powered by Large Language Model",
        "abstract": "Dialogue-based Intelligent Tutoring Systems (ITSs) have significantly\nadvanced adaptive and personalized learning by automating sophisticated human\ntutoring strategies within interactive dialogues. However, replicating the\nnuanced patterns of expert human communication remains a challenge in Natural\nLanguage Processing (NLP). Recent advancements in NLP, particularly Large\nLanguage Models (LLMs) such as OpenAI's GPT-4, offer promising solutions by\nproviding human-like and context-aware responses based on extensive pre-trained\nknowledge. Motivated by the effectiveness of LLMs in various educational tasks\n(e.g., content creation and summarization, problem-solving, and automated\nfeedback provision), our study introduces the Socratic Playground for Learning\n(SPL), a dialogue-based ITS powered by the GPT-4 model, which employs the\nSocratic teaching method to foster critical thinking among learners. Through\nextensive prompt engineering, SPL can generate specific learning scenarios and\nfacilitates efficient multi-turn tutoring dialogues. The SPL system aims to\nenhance personalized and adaptive learning experiences tailored to individual\nneeds, specifically focusing on improving critical thinking skills. Our pilot\nexperimental results from essay writing tasks demonstrate SPL has the potential\nto improve tutoring interactions and further enhance dialogue-based ITS\nfunctionalities. Our study, exemplified by SPL, demonstrates how LLMs enhance\ndialogue-based ITSs and expand the accessibility and efficacy of educational\ntechnologies.",
        "chunk-id": 1,
        "chunk": "Dialogue-based Intelligent Tutoring Systems (ITSs) have significantly\nadvanced adaptive and personalized learning by automating sophisticated human\ntutoring strategies within interactive dialogues. However, replicating the\nnuanced patterns of expert human communication remains a challenge in Natural\nLanguage Processing (NLP). Recent advancements in NLP, particularly Large",
        "authors": [
            "Liang Zhang",
            "Jionghao Lin",
            "Ziyi Kuang",
            "Sheng Xu",
            "Mohammed Yeasin",
            "Xiangen Hu"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T01:18:52+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13919v2",
        "arxiv_link": "http://arxiv.org/abs/2406.13919v2",
        "categories": [
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000050,
        "doi": null,
        "title": "SPL: A Socratic Playground for Learning Powered by Large Language Model",
        "abstract": "Dialogue-based Intelligent Tutoring Systems (ITSs) have significantly\nadvanced adaptive and personalized learning by automating sophisticated human\ntutoring strategies within interactive dialogues. However, replicating the\nnuanced patterns of expert human communication remains a challenge in Natural\nLanguage Processing (NLP). Recent advancements in NLP, particularly Large\nLanguage Models (LLMs) such as OpenAI's GPT-4, offer promising solutions by\nproviding human-like and context-aware responses based on extensive pre-trained\nknowledge. Motivated by the effectiveness of LLMs in various educational tasks\n(e.g., content creation and summarization, problem-solving, and automated\nfeedback provision), our study introduces the Socratic Playground for Learning\n(SPL), a dialogue-based ITS powered by the GPT-4 model, which employs the\nSocratic teaching method to foster critical thinking among learners. Through\nextensive prompt engineering, SPL can generate specific learning scenarios and\nfacilitates efficient multi-turn tutoring dialogues. The SPL system aims to\nenhance personalized and adaptive learning experiences tailored to individual\nneeds, specifically focusing on improving critical thinking skills. Our pilot\nexperimental results from essay writing tasks demonstrate SPL has the potential\nto improve tutoring interactions and further enhance dialogue-based ITS\nfunctionalities. Our study, exemplified by SPL, demonstrates how LLMs enhance\ndialogue-based ITSs and expand the accessibility and efficacy of educational\ntechnologies.",
        "chunk-id": 2,
        "chunk": "Language Models (LLMs) such as OpenAI's GPT-4, offer promising solutions by\nproviding human-like and context-aware responses based on extensive pre-trained\nknowledge. Motivated by the effectiveness of LLMs in various educational tasks\n(e.g., content creation and summarization, problem-solving, and automated",
        "authors": [
            "Liang Zhang",
            "Jionghao Lin",
            "Ziyi Kuang",
            "Sheng Xu",
            "Mohammed Yeasin",
            "Xiangen Hu"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T01:18:52+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13919v2",
        "arxiv_link": "http://arxiv.org/abs/2406.13919v2",
        "categories": [
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000050,
        "doi": null,
        "title": "SPL: A Socratic Playground for Learning Powered by Large Language Model",
        "abstract": "Dialogue-based Intelligent Tutoring Systems (ITSs) have significantly\nadvanced adaptive and personalized learning by automating sophisticated human\ntutoring strategies within interactive dialogues. However, replicating the\nnuanced patterns of expert human communication remains a challenge in Natural\nLanguage Processing (NLP). Recent advancements in NLP, particularly Large\nLanguage Models (LLMs) such as OpenAI's GPT-4, offer promising solutions by\nproviding human-like and context-aware responses based on extensive pre-trained\nknowledge. Motivated by the effectiveness of LLMs in various educational tasks\n(e.g., content creation and summarization, problem-solving, and automated\nfeedback provision), our study introduces the Socratic Playground for Learning\n(SPL), a dialogue-based ITS powered by the GPT-4 model, which employs the\nSocratic teaching method to foster critical thinking among learners. Through\nextensive prompt engineering, SPL can generate specific learning scenarios and\nfacilitates efficient multi-turn tutoring dialogues. The SPL system aims to\nenhance personalized and adaptive learning experiences tailored to individual\nneeds, specifically focusing on improving critical thinking skills. Our pilot\nexperimental results from essay writing tasks demonstrate SPL has the potential\nto improve tutoring interactions and further enhance dialogue-based ITS\nfunctionalities. Our study, exemplified by SPL, demonstrates how LLMs enhance\ndialogue-based ITSs and expand the accessibility and efficacy of educational\ntechnologies.",
        "chunk-id": 3,
        "chunk": "feedback provision), our study introduces the Socratic Playground for Learning\n(SPL), a dialogue-based ITS powered by the GPT-4 model, which employs the\nSocratic teaching method to foster critical thinking among learners. Through\nextensive prompt engineering, SPL can generate specific learning scenarios and\nfacilitates efficient multi-turn tutoring dialogues. The SPL system aims to",
        "authors": [
            "Liang Zhang",
            "Jionghao Lin",
            "Ziyi Kuang",
            "Sheng Xu",
            "Mohammed Yeasin",
            "Xiangen Hu"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T01:18:52+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13919v2",
        "arxiv_link": "http://arxiv.org/abs/2406.13919v2",
        "categories": [
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000050,
        "doi": null,
        "title": "SPL: A Socratic Playground for Learning Powered by Large Language Model",
        "abstract": "Dialogue-based Intelligent Tutoring Systems (ITSs) have significantly\nadvanced adaptive and personalized learning by automating sophisticated human\ntutoring strategies within interactive dialogues. However, replicating the\nnuanced patterns of expert human communication remains a challenge in Natural\nLanguage Processing (NLP). Recent advancements in NLP, particularly Large\nLanguage Models (LLMs) such as OpenAI's GPT-4, offer promising solutions by\nproviding human-like and context-aware responses based on extensive pre-trained\nknowledge. Motivated by the effectiveness of LLMs in various educational tasks\n(e.g., content creation and summarization, problem-solving, and automated\nfeedback provision), our study introduces the Socratic Playground for Learning\n(SPL), a dialogue-based ITS powered by the GPT-4 model, which employs the\nSocratic teaching method to foster critical thinking among learners. Through\nextensive prompt engineering, SPL can generate specific learning scenarios and\nfacilitates efficient multi-turn tutoring dialogues. The SPL system aims to\nenhance personalized and adaptive learning experiences tailored to individual\nneeds, specifically focusing on improving critical thinking skills. Our pilot\nexperimental results from essay writing tasks demonstrate SPL has the potential\nto improve tutoring interactions and further enhance dialogue-based ITS\nfunctionalities. Our study, exemplified by SPL, demonstrates how LLMs enhance\ndialogue-based ITSs and expand the accessibility and efficacy of educational\ntechnologies.",
        "chunk-id": 4,
        "chunk": "enhance personalized and adaptive learning experiences tailored to individual\nneeds, specifically focusing on improving critical thinking skills. Our pilot\nexperimental results from essay writing tasks demonstrate SPL has the potential\nto improve tutoring interactions and further enhance dialogue-based ITS",
        "authors": [
            "Liang Zhang",
            "Jionghao Lin",
            "Ziyi Kuang",
            "Sheng Xu",
            "Mohammed Yeasin",
            "Xiangen Hu"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T01:18:52+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13919v2",
        "arxiv_link": "http://arxiv.org/abs/2406.13919v2",
        "categories": [
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000050,
        "doi": null,
        "title": "SPL: A Socratic Playground for Learning Powered by Large Language Model",
        "abstract": "Dialogue-based Intelligent Tutoring Systems (ITSs) have significantly\nadvanced adaptive and personalized learning by automating sophisticated human\ntutoring strategies within interactive dialogues. However, replicating the\nnuanced patterns of expert human communication remains a challenge in Natural\nLanguage Processing (NLP). Recent advancements in NLP, particularly Large\nLanguage Models (LLMs) such as OpenAI's GPT-4, offer promising solutions by\nproviding human-like and context-aware responses based on extensive pre-trained\nknowledge. Motivated by the effectiveness of LLMs in various educational tasks\n(e.g., content creation and summarization, problem-solving, and automated\nfeedback provision), our study introduces the Socratic Playground for Learning\n(SPL), a dialogue-based ITS powered by the GPT-4 model, which employs the\nSocratic teaching method to foster critical thinking among learners. Through\nextensive prompt engineering, SPL can generate specific learning scenarios and\nfacilitates efficient multi-turn tutoring dialogues. The SPL system aims to\nenhance personalized and adaptive learning experiences tailored to individual\nneeds, specifically focusing on improving critical thinking skills. Our pilot\nexperimental results from essay writing tasks demonstrate SPL has the potential\nto improve tutoring interactions and further enhance dialogue-based ITS\nfunctionalities. Our study, exemplified by SPL, demonstrates how LLMs enhance\ndialogue-based ITSs and expand the accessibility and efficacy of educational\ntechnologies.",
        "chunk-id": 5,
        "chunk": "functionalities. Our study, exemplified by SPL, demonstrates how LLMs enhance\ndialogue-based ITSs and expand the accessibility and efficacy of educational\ntechnologies.",
        "authors": [
            "Liang Zhang",
            "Jionghao Lin",
            "Ziyi Kuang",
            "Sheng Xu",
            "Mohammed Yeasin",
            "Xiangen Hu"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T01:18:52+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13919v2",
        "arxiv_link": "http://arxiv.org/abs/2406.13919v2",
        "categories": [
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000051,
        "doi": null,
        "title": "Persuasiveness of Generated Free-Text Rationales in Subjective Decisions: A Case Study on Pairwise Argument Ranking",
        "abstract": "Generating free-text rationales is among the emergent capabilities of Large\nLanguage Models (LLMs). These rationales have been found to enhance LLM\nperformance across various NLP tasks. Recently, there has been growing interest\nin using these rationales to provide insights for various important downstream\ntasks. In this paper, we analyze generated free-text rationales in tasks with\nsubjective answers, emphasizing the importance of rationalization in such\nscenarios. We focus on pairwise argument ranking, a highly subjective task with\nsignificant potential for real-world applications, such as debate assistance.\nWe evaluate the persuasiveness of rationales generated by nine LLMs to support\ntheir subjective choices. Our findings suggest that open-source LLMs,\nparticularly Llama2-70B-chat, are capable of providing highly persuasive\nrationalizations, surpassing even GPT models. Additionally, our experiments\nshow that rationale persuasiveness can be improved by controlling its\nparameters through prompting or through self-refinement.",
        "chunk-id": 1,
        "chunk": "Generating free-text rationales is among the emergent capabilities of Large\nLanguage Models (LLMs). These rationales have been found to enhance LLM\nperformance across various NLP tasks. Recently, there has been growing interest\nin using these rationales to provide insights for various important downstream\ntasks. In this paper, we analyze generated free-text rationales in tasks with",
        "authors": [
            "Mohamed Elaraby",
            "Diane Litman",
            "Xiang Lorraine Li",
            "Ahmed Magooda"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T00:28:33+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13905v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13905v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000051,
        "doi": null,
        "title": "Persuasiveness of Generated Free-Text Rationales in Subjective Decisions: A Case Study on Pairwise Argument Ranking",
        "abstract": "Generating free-text rationales is among the emergent capabilities of Large\nLanguage Models (LLMs). These rationales have been found to enhance LLM\nperformance across various NLP tasks. Recently, there has been growing interest\nin using these rationales to provide insights for various important downstream\ntasks. In this paper, we analyze generated free-text rationales in tasks with\nsubjective answers, emphasizing the importance of rationalization in such\nscenarios. We focus on pairwise argument ranking, a highly subjective task with\nsignificant potential for real-world applications, such as debate assistance.\nWe evaluate the persuasiveness of rationales generated by nine LLMs to support\ntheir subjective choices. Our findings suggest that open-source LLMs,\nparticularly Llama2-70B-chat, are capable of providing highly persuasive\nrationalizations, surpassing even GPT models. Additionally, our experiments\nshow that rationale persuasiveness can be improved by controlling its\nparameters through prompting or through self-refinement.",
        "chunk-id": 2,
        "chunk": "subjective answers, emphasizing the importance of rationalization in such\nscenarios. We focus on pairwise argument ranking, a highly subjective task with\nsignificant potential for real-world applications, such as debate assistance.\nWe evaluate the persuasiveness of rationales generated by nine LLMs to support\ntheir subjective choices. Our findings suggest that open-source LLMs,",
        "authors": [
            "Mohamed Elaraby",
            "Diane Litman",
            "Xiang Lorraine Li",
            "Ahmed Magooda"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T00:28:33+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13905v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13905v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000051,
        "doi": null,
        "title": "Persuasiveness of Generated Free-Text Rationales in Subjective Decisions: A Case Study on Pairwise Argument Ranking",
        "abstract": "Generating free-text rationales is among the emergent capabilities of Large\nLanguage Models (LLMs). These rationales have been found to enhance LLM\nperformance across various NLP tasks. Recently, there has been growing interest\nin using these rationales to provide insights for various important downstream\ntasks. In this paper, we analyze generated free-text rationales in tasks with\nsubjective answers, emphasizing the importance of rationalization in such\nscenarios. We focus on pairwise argument ranking, a highly subjective task with\nsignificant potential for real-world applications, such as debate assistance.\nWe evaluate the persuasiveness of rationales generated by nine LLMs to support\ntheir subjective choices. Our findings suggest that open-source LLMs,\nparticularly Llama2-70B-chat, are capable of providing highly persuasive\nrationalizations, surpassing even GPT models. Additionally, our experiments\nshow that rationale persuasiveness can be improved by controlling its\nparameters through prompting or through self-refinement.",
        "chunk-id": 3,
        "chunk": "particularly Llama2-70B-chat, are capable of providing highly persuasive\nrationalizations, surpassing even GPT models. Additionally, our experiments\nshow that rationale persuasiveness can be improved by controlling its\nparameters through prompting or through self-refinement.",
        "authors": [
            "Mohamed Elaraby",
            "Diane Litman",
            "Xiang Lorraine Li",
            "Ahmed Magooda"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-20T00:28:33+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13905v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13905v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000052,
        "doi": null,
        "title": "Open Generative Large Language Models for Galician",
        "abstract": "Large language models (LLMs) have transformed natural language processing.\nYet, their predominantly English-centric training has led to biases and\nperformance disparities across languages. This imbalance marginalizes\nminoritized languages, making equitable access to NLP technologies more\ndifficult for languages with lower resources, such as Galician. We present the\nfirst two generative LLMs focused on Galician to bridge this gap. These models,\nfreely available as open-source resources, were trained using a GPT\narchitecture with 1.3B parameters on a corpus of 2.1B words. Leveraging\ncontinual pretraining, we adapt to Galician two existing LLMs trained on larger\ncorpora, thus mitigating the data constraints that would arise if the training\nwere performed from scratch. The models were evaluated using human judgments\nand task-based datasets from standardized benchmarks. These evaluations reveal\na promising performance, underscoring the importance of linguistic diversity in\ngenerative models.",
        "chunk-id": 1,
        "chunk": "Large language models (LLMs) have transformed natural language processing.\nYet, their predominantly English-centric training has led to biases and\nperformance disparities across languages. This imbalance marginalizes\nminoritized languages, making equitable access to NLP technologies more\ndifficult for languages with lower resources, such as Galician. We present the",
        "authors": [
            "Pablo Gamallo",
            "Pablo Rodr\u00edguez",
            "Iria de-Dios-Flores",
            "Susana Sotelo",
            "Silvia Paniagua",
            "Daniel Bardanca",
            "Jos\u00e9 Ramom Pichel",
            "Marcos Garcia"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T23:49:56+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13893v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13893v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000052,
        "doi": null,
        "title": "Open Generative Large Language Models for Galician",
        "abstract": "Large language models (LLMs) have transformed natural language processing.\nYet, their predominantly English-centric training has led to biases and\nperformance disparities across languages. This imbalance marginalizes\nminoritized languages, making equitable access to NLP technologies more\ndifficult for languages with lower resources, such as Galician. We present the\nfirst two generative LLMs focused on Galician to bridge this gap. These models,\nfreely available as open-source resources, were trained using a GPT\narchitecture with 1.3B parameters on a corpus of 2.1B words. Leveraging\ncontinual pretraining, we adapt to Galician two existing LLMs trained on larger\ncorpora, thus mitigating the data constraints that would arise if the training\nwere performed from scratch. The models were evaluated using human judgments\nand task-based datasets from standardized benchmarks. These evaluations reveal\na promising performance, underscoring the importance of linguistic diversity in\ngenerative models.",
        "chunk-id": 2,
        "chunk": "first two generative LLMs focused on Galician to bridge this gap. These models,\nfreely available as open-source resources, were trained using a GPT\narchitecture with 1.3B parameters on a corpus of 2.1B words. Leveraging\ncontinual pretraining, we adapt to Galician two existing LLMs trained on larger\ncorpora, thus mitigating the data constraints that would arise if the training",
        "authors": [
            "Pablo Gamallo",
            "Pablo Rodr\u00edguez",
            "Iria de-Dios-Flores",
            "Susana Sotelo",
            "Silvia Paniagua",
            "Daniel Bardanca",
            "Jos\u00e9 Ramom Pichel",
            "Marcos Garcia"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T23:49:56+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13893v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13893v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000052,
        "doi": null,
        "title": "Open Generative Large Language Models for Galician",
        "abstract": "Large language models (LLMs) have transformed natural language processing.\nYet, their predominantly English-centric training has led to biases and\nperformance disparities across languages. This imbalance marginalizes\nminoritized languages, making equitable access to NLP technologies more\ndifficult for languages with lower resources, such as Galician. We present the\nfirst two generative LLMs focused on Galician to bridge this gap. These models,\nfreely available as open-source resources, were trained using a GPT\narchitecture with 1.3B parameters on a corpus of 2.1B words. Leveraging\ncontinual pretraining, we adapt to Galician two existing LLMs trained on larger\ncorpora, thus mitigating the data constraints that would arise if the training\nwere performed from scratch. The models were evaluated using human judgments\nand task-based datasets from standardized benchmarks. These evaluations reveal\na promising performance, underscoring the importance of linguistic diversity in\ngenerative models.",
        "chunk-id": 3,
        "chunk": "were performed from scratch. The models were evaluated using human judgments\nand task-based datasets from standardized benchmarks. These evaluations reveal\na promising performance, underscoring the importance of linguistic diversity in\ngenerative models.",
        "authors": [
            "Pablo Gamallo",
            "Pablo Rodr\u00edguez",
            "Iria de-Dios-Flores",
            "Susana Sotelo",
            "Silvia Paniagua",
            "Daniel Bardanca",
            "Jos\u00e9 Ramom Pichel",
            "Marcos Garcia"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T23:49:56+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13893v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13893v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000053,
        "doi": null,
        "title": "ClinicalLab: Aligning Agents for Multi-Departmental Clinical Diagnostics in the Real World",
        "abstract": "LLMs have achieved significant performance progress in various NLP\napplications. However, LLMs still struggle to meet the strict requirements for\naccuracy and reliability in the medical field and face many challenges in\nclinical applications. Existing clinical diagnostic evaluation benchmarks for\nevaluating medical agents powered by LLMs have severe limitations. Firstly,\nmost existing medical evaluation benchmarks face the risk of data leakage or\ncontamination. Secondly, existing benchmarks often neglect the characteristics\nof multiple departments and specializations in modern medical practice.\nThirdly, existing evaluation methods are limited to multiple-choice questions,\nwhich do not align with the real-world diagnostic scenarios. Lastly, existing\nevaluation methods lack comprehensive evaluations of end-to-end real clinical\nscenarios. These limitations in benchmarks in turn obstruct advancements of\nLLMs and agents for medicine. To address these limitations, we introduce\nClinicalLab, a comprehensive clinical diagnosis agent alignment suite.\nClinicalLab includes ClinicalBench, an end-to-end multi-departmental clinical\ndiagnostic evaluation benchmark for evaluating medical agents and LLMs.\nClinicalBench is based on real cases that cover 24 departments and 150\ndiseases. ClinicalLab also includes four novel metrics (ClinicalMetrics) for\nevaluating the effectiveness of LLMs in clinical diagnostic tasks. We evaluate\n17 LLMs and find that their performance varies significantly across different\ndepartments. Based on these findings, in ClinicalLab, we propose ClinicalAgent,\nan end-to-end clinical agent that aligns with real-world clinical diagnostic\npractices. We systematically investigate the performance and applicable\nscenarios of variants of ClinicalAgent on ClinicalBench. Our findings\ndemonstrate the importance of aligning with modern medical practices in\ndesigning medical agents.",
        "chunk-id": 1,
        "chunk": "LLMs have achieved significant performance progress in various NLP\napplications. However, LLMs still struggle to meet the strict requirements for\naccuracy and reliability in the medical field and face many challenges in\nclinical applications. Existing clinical diagnostic evaluation benchmarks for\nevaluating medical agents powered by LLMs have severe limitations. Firstly,",
        "authors": [
            "Weixiang Yan",
            "Haitian Liu",
            "Tengxiao Wu",
            "Qian Chen",
            "Wen Wang",
            "Haoyuan Chai",
            "Jiayi Wang",
            "Weishan Zhao",
            "Yixin Zhang",
            "Renjun Zhang",
            "Li Zhu"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T23:44:25+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13890v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13890v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000053,
        "doi": null,
        "title": "ClinicalLab: Aligning Agents for Multi-Departmental Clinical Diagnostics in the Real World",
        "abstract": "LLMs have achieved significant performance progress in various NLP\napplications. However, LLMs still struggle to meet the strict requirements for\naccuracy and reliability in the medical field and face many challenges in\nclinical applications. Existing clinical diagnostic evaluation benchmarks for\nevaluating medical agents powered by LLMs have severe limitations. Firstly,\nmost existing medical evaluation benchmarks face the risk of data leakage or\ncontamination. Secondly, existing benchmarks often neglect the characteristics\nof multiple departments and specializations in modern medical practice.\nThirdly, existing evaluation methods are limited to multiple-choice questions,\nwhich do not align with the real-world diagnostic scenarios. Lastly, existing\nevaluation methods lack comprehensive evaluations of end-to-end real clinical\nscenarios. These limitations in benchmarks in turn obstruct advancements of\nLLMs and agents for medicine. To address these limitations, we introduce\nClinicalLab, a comprehensive clinical diagnosis agent alignment suite.\nClinicalLab includes ClinicalBench, an end-to-end multi-departmental clinical\ndiagnostic evaluation benchmark for evaluating medical agents and LLMs.\nClinicalBench is based on real cases that cover 24 departments and 150\ndiseases. ClinicalLab also includes four novel metrics (ClinicalMetrics) for\nevaluating the effectiveness of LLMs in clinical diagnostic tasks. We evaluate\n17 LLMs and find that their performance varies significantly across different\ndepartments. Based on these findings, in ClinicalLab, we propose ClinicalAgent,\nan end-to-end clinical agent that aligns with real-world clinical diagnostic\npractices. We systematically investigate the performance and applicable\nscenarios of variants of ClinicalAgent on ClinicalBench. Our findings\ndemonstrate the importance of aligning with modern medical practices in\ndesigning medical agents.",
        "chunk-id": 2,
        "chunk": "most existing medical evaluation benchmarks face the risk of data leakage or\ncontamination. Secondly, existing benchmarks often neglect the characteristics\nof multiple departments and specializations in modern medical practice.\nThirdly, existing evaluation methods are limited to multiple-choice questions,\nwhich do not align with the real-world diagnostic scenarios. Lastly, existing",
        "authors": [
            "Weixiang Yan",
            "Haitian Liu",
            "Tengxiao Wu",
            "Qian Chen",
            "Wen Wang",
            "Haoyuan Chai",
            "Jiayi Wang",
            "Weishan Zhao",
            "Yixin Zhang",
            "Renjun Zhang",
            "Li Zhu"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T23:44:25+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13890v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13890v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000053,
        "doi": null,
        "title": "ClinicalLab: Aligning Agents for Multi-Departmental Clinical Diagnostics in the Real World",
        "abstract": "LLMs have achieved significant performance progress in various NLP\napplications. However, LLMs still struggle to meet the strict requirements for\naccuracy and reliability in the medical field and face many challenges in\nclinical applications. Existing clinical diagnostic evaluation benchmarks for\nevaluating medical agents powered by LLMs have severe limitations. Firstly,\nmost existing medical evaluation benchmarks face the risk of data leakage or\ncontamination. Secondly, existing benchmarks often neglect the characteristics\nof multiple departments and specializations in modern medical practice.\nThirdly, existing evaluation methods are limited to multiple-choice questions,\nwhich do not align with the real-world diagnostic scenarios. Lastly, existing\nevaluation methods lack comprehensive evaluations of end-to-end real clinical\nscenarios. These limitations in benchmarks in turn obstruct advancements of\nLLMs and agents for medicine. To address these limitations, we introduce\nClinicalLab, a comprehensive clinical diagnosis agent alignment suite.\nClinicalLab includes ClinicalBench, an end-to-end multi-departmental clinical\ndiagnostic evaluation benchmark for evaluating medical agents and LLMs.\nClinicalBench is based on real cases that cover 24 departments and 150\ndiseases. ClinicalLab also includes four novel metrics (ClinicalMetrics) for\nevaluating the effectiveness of LLMs in clinical diagnostic tasks. We evaluate\n17 LLMs and find that their performance varies significantly across different\ndepartments. Based on these findings, in ClinicalLab, we propose ClinicalAgent,\nan end-to-end clinical agent that aligns with real-world clinical diagnostic\npractices. We systematically investigate the performance and applicable\nscenarios of variants of ClinicalAgent on ClinicalBench. Our findings\ndemonstrate the importance of aligning with modern medical practices in\ndesigning medical agents.",
        "chunk-id": 3,
        "chunk": "evaluation methods lack comprehensive evaluations of end-to-end real clinical\nscenarios. These limitations in benchmarks in turn obstruct advancements of\nLLMs and agents for medicine. To address these limitations, we introduce\nClinicalLab, a comprehensive clinical diagnosis agent alignment suite.\nClinicalLab includes ClinicalBench, an end-to-end multi-departmental clinical",
        "authors": [
            "Weixiang Yan",
            "Haitian Liu",
            "Tengxiao Wu",
            "Qian Chen",
            "Wen Wang",
            "Haoyuan Chai",
            "Jiayi Wang",
            "Weishan Zhao",
            "Yixin Zhang",
            "Renjun Zhang",
            "Li Zhu"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T23:44:25+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13890v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13890v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000053,
        "doi": null,
        "title": "ClinicalLab: Aligning Agents for Multi-Departmental Clinical Diagnostics in the Real World",
        "abstract": "LLMs have achieved significant performance progress in various NLP\napplications. However, LLMs still struggle to meet the strict requirements for\naccuracy and reliability in the medical field and face many challenges in\nclinical applications. Existing clinical diagnostic evaluation benchmarks for\nevaluating medical agents powered by LLMs have severe limitations. Firstly,\nmost existing medical evaluation benchmarks face the risk of data leakage or\ncontamination. Secondly, existing benchmarks often neglect the characteristics\nof multiple departments and specializations in modern medical practice.\nThirdly, existing evaluation methods are limited to multiple-choice questions,\nwhich do not align with the real-world diagnostic scenarios. Lastly, existing\nevaluation methods lack comprehensive evaluations of end-to-end real clinical\nscenarios. These limitations in benchmarks in turn obstruct advancements of\nLLMs and agents for medicine. To address these limitations, we introduce\nClinicalLab, a comprehensive clinical diagnosis agent alignment suite.\nClinicalLab includes ClinicalBench, an end-to-end multi-departmental clinical\ndiagnostic evaluation benchmark for evaluating medical agents and LLMs.\nClinicalBench is based on real cases that cover 24 departments and 150\ndiseases. ClinicalLab also includes four novel metrics (ClinicalMetrics) for\nevaluating the effectiveness of LLMs in clinical diagnostic tasks. We evaluate\n17 LLMs and find that their performance varies significantly across different\ndepartments. Based on these findings, in ClinicalLab, we propose ClinicalAgent,\nan end-to-end clinical agent that aligns with real-world clinical diagnostic\npractices. We systematically investigate the performance and applicable\nscenarios of variants of ClinicalAgent on ClinicalBench. Our findings\ndemonstrate the importance of aligning with modern medical practices in\ndesigning medical agents.",
        "chunk-id": 4,
        "chunk": "diagnostic evaluation benchmark for evaluating medical agents and LLMs.\nClinicalBench is based on real cases that cover 24 departments and 150\ndiseases. ClinicalLab also includes four novel metrics (ClinicalMetrics) for\nevaluating the effectiveness of LLMs in clinical diagnostic tasks. We evaluate\n17 LLMs and find that their performance varies significantly across different",
        "authors": [
            "Weixiang Yan",
            "Haitian Liu",
            "Tengxiao Wu",
            "Qian Chen",
            "Wen Wang",
            "Haoyuan Chai",
            "Jiayi Wang",
            "Weishan Zhao",
            "Yixin Zhang",
            "Renjun Zhang",
            "Li Zhu"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T23:44:25+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13890v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13890v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000053,
        "doi": null,
        "title": "ClinicalLab: Aligning Agents for Multi-Departmental Clinical Diagnostics in the Real World",
        "abstract": "LLMs have achieved significant performance progress in various NLP\napplications. However, LLMs still struggle to meet the strict requirements for\naccuracy and reliability in the medical field and face many challenges in\nclinical applications. Existing clinical diagnostic evaluation benchmarks for\nevaluating medical agents powered by LLMs have severe limitations. Firstly,\nmost existing medical evaluation benchmarks face the risk of data leakage or\ncontamination. Secondly, existing benchmarks often neglect the characteristics\nof multiple departments and specializations in modern medical practice.\nThirdly, existing evaluation methods are limited to multiple-choice questions,\nwhich do not align with the real-world diagnostic scenarios. Lastly, existing\nevaluation methods lack comprehensive evaluations of end-to-end real clinical\nscenarios. These limitations in benchmarks in turn obstruct advancements of\nLLMs and agents for medicine. To address these limitations, we introduce\nClinicalLab, a comprehensive clinical diagnosis agent alignment suite.\nClinicalLab includes ClinicalBench, an end-to-end multi-departmental clinical\ndiagnostic evaluation benchmark for evaluating medical agents and LLMs.\nClinicalBench is based on real cases that cover 24 departments and 150\ndiseases. ClinicalLab also includes four novel metrics (ClinicalMetrics) for\nevaluating the effectiveness of LLMs in clinical diagnostic tasks. We evaluate\n17 LLMs and find that their performance varies significantly across different\ndepartments. Based on these findings, in ClinicalLab, we propose ClinicalAgent,\nan end-to-end clinical agent that aligns with real-world clinical diagnostic\npractices. We systematically investigate the performance and applicable\nscenarios of variants of ClinicalAgent on ClinicalBench. Our findings\ndemonstrate the importance of aligning with modern medical practices in\ndesigning medical agents.",
        "chunk-id": 5,
        "chunk": "departments. Based on these findings, in ClinicalLab, we propose ClinicalAgent,\nan end-to-end clinical agent that aligns with real-world clinical diagnostic\npractices. We systematically investigate the performance and applicable\nscenarios of variants of ClinicalAgent on ClinicalBench. Our findings\ndemonstrate the importance of aligning with modern medical practices in",
        "authors": [
            "Weixiang Yan",
            "Haitian Liu",
            "Tengxiao Wu",
            "Qian Chen",
            "Wen Wang",
            "Haoyuan Chai",
            "Jiayi Wang",
            "Weishan Zhao",
            "Yixin Zhang",
            "Renjun Zhang",
            "Li Zhu"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T23:44:25+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13890v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13890v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000053,
        "doi": null,
        "title": "ClinicalLab: Aligning Agents for Multi-Departmental Clinical Diagnostics in the Real World",
        "abstract": "LLMs have achieved significant performance progress in various NLP\napplications. However, LLMs still struggle to meet the strict requirements for\naccuracy and reliability in the medical field and face many challenges in\nclinical applications. Existing clinical diagnostic evaluation benchmarks for\nevaluating medical agents powered by LLMs have severe limitations. Firstly,\nmost existing medical evaluation benchmarks face the risk of data leakage or\ncontamination. Secondly, existing benchmarks often neglect the characteristics\nof multiple departments and specializations in modern medical practice.\nThirdly, existing evaluation methods are limited to multiple-choice questions,\nwhich do not align with the real-world diagnostic scenarios. Lastly, existing\nevaluation methods lack comprehensive evaluations of end-to-end real clinical\nscenarios. These limitations in benchmarks in turn obstruct advancements of\nLLMs and agents for medicine. To address these limitations, we introduce\nClinicalLab, a comprehensive clinical diagnosis agent alignment suite.\nClinicalLab includes ClinicalBench, an end-to-end multi-departmental clinical\ndiagnostic evaluation benchmark for evaluating medical agents and LLMs.\nClinicalBench is based on real cases that cover 24 departments and 150\ndiseases. ClinicalLab also includes four novel metrics (ClinicalMetrics) for\nevaluating the effectiveness of LLMs in clinical diagnostic tasks. We evaluate\n17 LLMs and find that their performance varies significantly across different\ndepartments. Based on these findings, in ClinicalLab, we propose ClinicalAgent,\nan end-to-end clinical agent that aligns with real-world clinical diagnostic\npractices. We systematically investigate the performance and applicable\nscenarios of variants of ClinicalAgent on ClinicalBench. Our findings\ndemonstrate the importance of aligning with modern medical practices in\ndesigning medical agents.",
        "chunk-id": 6,
        "chunk": "designing medical agents.",
        "authors": [
            "Weixiang Yan",
            "Haitian Liu",
            "Tengxiao Wu",
            "Qian Chen",
            "Wen Wang",
            "Haoyuan Chai",
            "Jiayi Wang",
            "Weishan Zhao",
            "Yixin Zhang",
            "Renjun Zhang",
            "Li Zhu"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T23:44:25+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13890v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13890v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000054,
        "doi": null,
        "title": "A Pure Transformer Pretraining Framework on Text-attributed Graphs",
        "abstract": "Pretraining plays a pivotal role in acquiring generalized knowledge from\nlarge-scale data, achieving remarkable successes as evidenced by large models\nin CV and NLP. However, progress in the graph domain remains limited due to\nfundamental challenges such as feature heterogeneity and structural\nheterogeneity. Recently, increasing efforts have been made to enhance node\nfeature quality with Large Language Models (LLMs) on text-attributed graphs\n(TAGs), demonstrating superiority to traditional bag-of-words or word2vec\ntechniques. These high-quality node features reduce the previously critical\nrole of graph structure, resulting in a modest performance gap between Graph\nNeural Networks (GNNs) and structure-agnostic Multi-Layer Perceptrons (MLPs).\nMotivated by this, we introduce a feature-centric pretraining perspective by\ntreating graph structure as a prior and leveraging the rich, unified feature\nspace to learn refined interaction patterns that generalizes across graphs. Our\nframework, Graph Sequence Pretraining with Transformer (GSPT), samples node\ncontexts through random walks and employs masked feature reconstruction to\ncapture pairwise proximity in the LLM-unified feature space using a standard\nTransformer. By utilizing unified text representations rather than varying\nstructures, our framework achieves significantly better transferability among\ngraphs within the same domain. GSPT can be easily adapted to both node\nclassification and link prediction, demonstrating promising empirical success\non various datasets.",
        "chunk-id": 1,
        "chunk": "Pretraining plays a pivotal role in acquiring generalized knowledge from\nlarge-scale data, achieving remarkable successes as evidenced by large models\nin CV and NLP. However, progress in the graph domain remains limited due to\nfundamental challenges such as feature heterogeneity and structural\nheterogeneity. Recently, increasing efforts have been made to enhance node",
        "authors": [
            "Yu Song",
            "Haitao Mao",
            "Jiachen Xiao",
            "Jingzhe Liu",
            "Zhikai Chen",
            "Wei Jin",
            "Carl Yang",
            "Jiliang Tang",
            "Hui Liu"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T22:30:08+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13873v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13873v1",
        "categories": [
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000054,
        "doi": null,
        "title": "A Pure Transformer Pretraining Framework on Text-attributed Graphs",
        "abstract": "Pretraining plays a pivotal role in acquiring generalized knowledge from\nlarge-scale data, achieving remarkable successes as evidenced by large models\nin CV and NLP. However, progress in the graph domain remains limited due to\nfundamental challenges such as feature heterogeneity and structural\nheterogeneity. Recently, increasing efforts have been made to enhance node\nfeature quality with Large Language Models (LLMs) on text-attributed graphs\n(TAGs), demonstrating superiority to traditional bag-of-words or word2vec\ntechniques. These high-quality node features reduce the previously critical\nrole of graph structure, resulting in a modest performance gap between Graph\nNeural Networks (GNNs) and structure-agnostic Multi-Layer Perceptrons (MLPs).\nMotivated by this, we introduce a feature-centric pretraining perspective by\ntreating graph structure as a prior and leveraging the rich, unified feature\nspace to learn refined interaction patterns that generalizes across graphs. Our\nframework, Graph Sequence Pretraining with Transformer (GSPT), samples node\ncontexts through random walks and employs masked feature reconstruction to\ncapture pairwise proximity in the LLM-unified feature space using a standard\nTransformer. By utilizing unified text representations rather than varying\nstructures, our framework achieves significantly better transferability among\ngraphs within the same domain. GSPT can be easily adapted to both node\nclassification and link prediction, demonstrating promising empirical success\non various datasets.",
        "chunk-id": 2,
        "chunk": "feature quality with Large Language Models (LLMs) on text-attributed graphs\n(TAGs), demonstrating superiority to traditional bag-of-words or word2vec\ntechniques. These high-quality node features reduce the previously critical\nrole of graph structure, resulting in a modest performance gap between Graph\nNeural Networks (GNNs) and structure-agnostic Multi-Layer Perceptrons (MLPs).",
        "authors": [
            "Yu Song",
            "Haitao Mao",
            "Jiachen Xiao",
            "Jingzhe Liu",
            "Zhikai Chen",
            "Wei Jin",
            "Carl Yang",
            "Jiliang Tang",
            "Hui Liu"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T22:30:08+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13873v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13873v1",
        "categories": [
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000054,
        "doi": null,
        "title": "A Pure Transformer Pretraining Framework on Text-attributed Graphs",
        "abstract": "Pretraining plays a pivotal role in acquiring generalized knowledge from\nlarge-scale data, achieving remarkable successes as evidenced by large models\nin CV and NLP. However, progress in the graph domain remains limited due to\nfundamental challenges such as feature heterogeneity and structural\nheterogeneity. Recently, increasing efforts have been made to enhance node\nfeature quality with Large Language Models (LLMs) on text-attributed graphs\n(TAGs), demonstrating superiority to traditional bag-of-words or word2vec\ntechniques. These high-quality node features reduce the previously critical\nrole of graph structure, resulting in a modest performance gap between Graph\nNeural Networks (GNNs) and structure-agnostic Multi-Layer Perceptrons (MLPs).\nMotivated by this, we introduce a feature-centric pretraining perspective by\ntreating graph structure as a prior and leveraging the rich, unified feature\nspace to learn refined interaction patterns that generalizes across graphs. Our\nframework, Graph Sequence Pretraining with Transformer (GSPT), samples node\ncontexts through random walks and employs masked feature reconstruction to\ncapture pairwise proximity in the LLM-unified feature space using a standard\nTransformer. By utilizing unified text representations rather than varying\nstructures, our framework achieves significantly better transferability among\ngraphs within the same domain. GSPT can be easily adapted to both node\nclassification and link prediction, demonstrating promising empirical success\non various datasets.",
        "chunk-id": 3,
        "chunk": "Motivated by this, we introduce a feature-centric pretraining perspective by\ntreating graph structure as a prior and leveraging the rich, unified feature\nspace to learn refined interaction patterns that generalizes across graphs. Our\nframework, Graph Sequence Pretraining with Transformer (GSPT), samples node\ncontexts through random walks and employs masked feature reconstruction to",
        "authors": [
            "Yu Song",
            "Haitao Mao",
            "Jiachen Xiao",
            "Jingzhe Liu",
            "Zhikai Chen",
            "Wei Jin",
            "Carl Yang",
            "Jiliang Tang",
            "Hui Liu"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T22:30:08+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13873v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13873v1",
        "categories": [
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000054,
        "doi": null,
        "title": "A Pure Transformer Pretraining Framework on Text-attributed Graphs",
        "abstract": "Pretraining plays a pivotal role in acquiring generalized knowledge from\nlarge-scale data, achieving remarkable successes as evidenced by large models\nin CV and NLP. However, progress in the graph domain remains limited due to\nfundamental challenges such as feature heterogeneity and structural\nheterogeneity. Recently, increasing efforts have been made to enhance node\nfeature quality with Large Language Models (LLMs) on text-attributed graphs\n(TAGs), demonstrating superiority to traditional bag-of-words or word2vec\ntechniques. These high-quality node features reduce the previously critical\nrole of graph structure, resulting in a modest performance gap between Graph\nNeural Networks (GNNs) and structure-agnostic Multi-Layer Perceptrons (MLPs).\nMotivated by this, we introduce a feature-centric pretraining perspective by\ntreating graph structure as a prior and leveraging the rich, unified feature\nspace to learn refined interaction patterns that generalizes across graphs. Our\nframework, Graph Sequence Pretraining with Transformer (GSPT), samples node\ncontexts through random walks and employs masked feature reconstruction to\ncapture pairwise proximity in the LLM-unified feature space using a standard\nTransformer. By utilizing unified text representations rather than varying\nstructures, our framework achieves significantly better transferability among\ngraphs within the same domain. GSPT can be easily adapted to both node\nclassification and link prediction, demonstrating promising empirical success\non various datasets.",
        "chunk-id": 4,
        "chunk": "capture pairwise proximity in the LLM-unified feature space using a standard\nTransformer. By utilizing unified text representations rather than varying\nstructures, our framework achieves significantly better transferability among\ngraphs within the same domain. GSPT can be easily adapted to both node\nclassification and link prediction, demonstrating promising empirical success",
        "authors": [
            "Yu Song",
            "Haitao Mao",
            "Jiachen Xiao",
            "Jingzhe Liu",
            "Zhikai Chen",
            "Wei Jin",
            "Carl Yang",
            "Jiliang Tang",
            "Hui Liu"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T22:30:08+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13873v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13873v1",
        "categories": [
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000054,
        "doi": null,
        "title": "A Pure Transformer Pretraining Framework on Text-attributed Graphs",
        "abstract": "Pretraining plays a pivotal role in acquiring generalized knowledge from\nlarge-scale data, achieving remarkable successes as evidenced by large models\nin CV and NLP. However, progress in the graph domain remains limited due to\nfundamental challenges such as feature heterogeneity and structural\nheterogeneity. Recently, increasing efforts have been made to enhance node\nfeature quality with Large Language Models (LLMs) on text-attributed graphs\n(TAGs), demonstrating superiority to traditional bag-of-words or word2vec\ntechniques. These high-quality node features reduce the previously critical\nrole of graph structure, resulting in a modest performance gap between Graph\nNeural Networks (GNNs) and structure-agnostic Multi-Layer Perceptrons (MLPs).\nMotivated by this, we introduce a feature-centric pretraining perspective by\ntreating graph structure as a prior and leveraging the rich, unified feature\nspace to learn refined interaction patterns that generalizes across graphs. Our\nframework, Graph Sequence Pretraining with Transformer (GSPT), samples node\ncontexts through random walks and employs masked feature reconstruction to\ncapture pairwise proximity in the LLM-unified feature space using a standard\nTransformer. By utilizing unified text representations rather than varying\nstructures, our framework achieves significantly better transferability among\ngraphs within the same domain. GSPT can be easily adapted to both node\nclassification and link prediction, demonstrating promising empirical success\non various datasets.",
        "chunk-id": 5,
        "chunk": "on various datasets.",
        "authors": [
            "Yu Song",
            "Haitao Mao",
            "Jiachen Xiao",
            "Jingzhe Liu",
            "Zhikai Chen",
            "Wei Jin",
            "Carl Yang",
            "Jiliang Tang",
            "Hui Liu"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T22:30:08+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13873v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13873v1",
        "categories": [
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000055,
        "doi": null,
        "title": "Benchmarking Open-Source Language Models for Efficient Question Answering in Industrial Applications",
        "abstract": "In the rapidly evolving landscape of Natural Language Processing (NLP), Large\nLanguage Models (LLMs) have demonstrated remarkable capabilities in tasks such\nas question answering (QA). However, the accessibility and practicality of\nutilizing these models for industrial applications pose significant challenges,\nparticularly concerning cost-effectiveness, inference speed, and resource\nefficiency. This paper presents a comprehensive benchmarking study comparing\nopen-source LLMs with their non-open-source counterparts on the task of\nquestion answering. Our objective is to identify open-source alternatives\ncapable of delivering comparable performance to proprietary models while being\nlightweight in terms of resource requirements and suitable for Central\nProcessing Unit (CPU)-based inference. Through rigorous evaluation across\nvarious metrics including accuracy, inference speed, and resource consumption,\nwe aim to provide insights into selecting efficient LLMs for real-world\napplications. Our findings shed light on viable open-source alternatives that\noffer acceptable performance and efficiency, addressing the pressing need for\naccessible and efficient NLP solutions in industry settings.",
        "chunk-id": 1,
        "chunk": "In the rapidly evolving landscape of Natural Language Processing (NLP), Large\nLanguage Models (LLMs) have demonstrated remarkable capabilities in tasks such\nas question answering (QA). However, the accessibility and practicality of\nutilizing these models for industrial applications pose significant challenges,",
        "authors": [
            "Mahaman Sanoussi Yahaya Alassan",
            "Jessica L\u00f3pez Espejel",
            "Merieme Bouhandi",
            "Walid Dahhane",
            "El Hassane Ettifouri"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T17:11:51+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13713v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13713v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000055,
        "doi": null,
        "title": "Benchmarking Open-Source Language Models for Efficient Question Answering in Industrial Applications",
        "abstract": "In the rapidly evolving landscape of Natural Language Processing (NLP), Large\nLanguage Models (LLMs) have demonstrated remarkable capabilities in tasks such\nas question answering (QA). However, the accessibility and practicality of\nutilizing these models for industrial applications pose significant challenges,\nparticularly concerning cost-effectiveness, inference speed, and resource\nefficiency. This paper presents a comprehensive benchmarking study comparing\nopen-source LLMs with their non-open-source counterparts on the task of\nquestion answering. Our objective is to identify open-source alternatives\ncapable of delivering comparable performance to proprietary models while being\nlightweight in terms of resource requirements and suitable for Central\nProcessing Unit (CPU)-based inference. Through rigorous evaluation across\nvarious metrics including accuracy, inference speed, and resource consumption,\nwe aim to provide insights into selecting efficient LLMs for real-world\napplications. Our findings shed light on viable open-source alternatives that\noffer acceptable performance and efficiency, addressing the pressing need for\naccessible and efficient NLP solutions in industry settings.",
        "chunk-id": 2,
        "chunk": "particularly concerning cost-effectiveness, inference speed, and resource\nefficiency. This paper presents a comprehensive benchmarking study comparing\nopen-source LLMs with their non-open-source counterparts on the task of\nquestion answering. Our objective is to identify open-source alternatives\ncapable of delivering comparable performance to proprietary models while being",
        "authors": [
            "Mahaman Sanoussi Yahaya Alassan",
            "Jessica L\u00f3pez Espejel",
            "Merieme Bouhandi",
            "Walid Dahhane",
            "El Hassane Ettifouri"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T17:11:51+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13713v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13713v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000055,
        "doi": null,
        "title": "Benchmarking Open-Source Language Models for Efficient Question Answering in Industrial Applications",
        "abstract": "In the rapidly evolving landscape of Natural Language Processing (NLP), Large\nLanguage Models (LLMs) have demonstrated remarkable capabilities in tasks such\nas question answering (QA). However, the accessibility and practicality of\nutilizing these models for industrial applications pose significant challenges,\nparticularly concerning cost-effectiveness, inference speed, and resource\nefficiency. This paper presents a comprehensive benchmarking study comparing\nopen-source LLMs with their non-open-source counterparts on the task of\nquestion answering. Our objective is to identify open-source alternatives\ncapable of delivering comparable performance to proprietary models while being\nlightweight in terms of resource requirements and suitable for Central\nProcessing Unit (CPU)-based inference. Through rigorous evaluation across\nvarious metrics including accuracy, inference speed, and resource consumption,\nwe aim to provide insights into selecting efficient LLMs for real-world\napplications. Our findings shed light on viable open-source alternatives that\noffer acceptable performance and efficiency, addressing the pressing need for\naccessible and efficient NLP solutions in industry settings.",
        "chunk-id": 3,
        "chunk": "lightweight in terms of resource requirements and suitable for Central\nProcessing Unit (CPU)-based inference. Through rigorous evaluation across\nvarious metrics including accuracy, inference speed, and resource consumption,\nwe aim to provide insights into selecting efficient LLMs for real-world\napplications. Our findings shed light on viable open-source alternatives that",
        "authors": [
            "Mahaman Sanoussi Yahaya Alassan",
            "Jessica L\u00f3pez Espejel",
            "Merieme Bouhandi",
            "Walid Dahhane",
            "El Hassane Ettifouri"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T17:11:51+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13713v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13713v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000055,
        "doi": null,
        "title": "Benchmarking Open-Source Language Models for Efficient Question Answering in Industrial Applications",
        "abstract": "In the rapidly evolving landscape of Natural Language Processing (NLP), Large\nLanguage Models (LLMs) have demonstrated remarkable capabilities in tasks such\nas question answering (QA). However, the accessibility and practicality of\nutilizing these models for industrial applications pose significant challenges,\nparticularly concerning cost-effectiveness, inference speed, and resource\nefficiency. This paper presents a comprehensive benchmarking study comparing\nopen-source LLMs with their non-open-source counterparts on the task of\nquestion answering. Our objective is to identify open-source alternatives\ncapable of delivering comparable performance to proprietary models while being\nlightweight in terms of resource requirements and suitable for Central\nProcessing Unit (CPU)-based inference. Through rigorous evaluation across\nvarious metrics including accuracy, inference speed, and resource consumption,\nwe aim to provide insights into selecting efficient LLMs for real-world\napplications. Our findings shed light on viable open-source alternatives that\noffer acceptable performance and efficiency, addressing the pressing need for\naccessible and efficient NLP solutions in industry settings.",
        "chunk-id": 4,
        "chunk": "offer acceptable performance and efficiency, addressing the pressing need for\naccessible and efficient NLP solutions in industry settings.",
        "authors": [
            "Mahaman Sanoussi Yahaya Alassan",
            "Jessica L\u00f3pez Espejel",
            "Merieme Bouhandi",
            "Walid Dahhane",
            "El Hassane Ettifouri"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T17:11:51+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13713v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13713v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000056,
        "doi": null,
        "title": "Multilingual De-Duplication Strategies: Applying scalable similarity search with monolingual & multilingual embedding models",
        "abstract": "This paper addresses the deduplication of multilingual textual data using\nadvanced NLP tools. We compare a two-step method involving translation to\nEnglish followed by embedding with mpnet, and a multilingual embedding model\n(distiluse). The two-step approach achieved a higher F1 score (82% vs. 60%),\nparticularly with less widely used languages, which can be increased up to 89%\nby leveraging expert rules based on domain knowledge. We also highlight\nlimitations related to token length constraints and computational efficiency.\nOur methodology suggests improvements for future multilingual deduplication\ntasks.",
        "chunk-id": 1,
        "chunk": "This paper addresses the deduplication of multilingual textual data using\nadvanced NLP tools. We compare a two-step method involving translation to\nEnglish followed by embedding with mpnet, and a multilingual embedding model\n(distiluse). The two-step approach achieved a higher F1 score (82% vs. 60%),\nparticularly with less widely used languages, which can be increased up to 89%",
        "authors": [
            "Stefan Pasch",
            "Dimitirios Petridis",
            "Jannic Cutura"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T16:48:14+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13695v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13695v1",
        "categories": [
            "Artificial Intelligence",
            "Natural Language Processing"
        ]
    },
    {
        "id": 20000056,
        "doi": null,
        "title": "Multilingual De-Duplication Strategies: Applying scalable similarity search with monolingual & multilingual embedding models",
        "abstract": "This paper addresses the deduplication of multilingual textual data using\nadvanced NLP tools. We compare a two-step method involving translation to\nEnglish followed by embedding with mpnet, and a multilingual embedding model\n(distiluse). The two-step approach achieved a higher F1 score (82% vs. 60%),\nparticularly with less widely used languages, which can be increased up to 89%\nby leveraging expert rules based on domain knowledge. We also highlight\nlimitations related to token length constraints and computational efficiency.\nOur methodology suggests improvements for future multilingual deduplication\ntasks.",
        "chunk-id": 2,
        "chunk": "by leveraging expert rules based on domain knowledge. We also highlight\nlimitations related to token length constraints and computational efficiency.\nOur methodology suggests improvements for future multilingual deduplication\ntasks.",
        "authors": [
            "Stefan Pasch",
            "Dimitirios Petridis",
            "Jannic Cutura"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T16:48:14+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13695v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13695v1",
        "categories": [
            "Artificial Intelligence",
            "Natural Language Processing"
        ]
    },
    {
        "id": 20000057,
        "doi": null,
        "title": "Leveraging Large Language Models to Measure Gender Bias in Gendered Languages",
        "abstract": "Gender bias in text corpora used in various natural language processing (NLP)\ncontexts, such as for training large language models (LLMs), can lead to the\nperpetuation and amplification of societal inequalities. This is particularly\npronounced in gendered languages like Spanish or French, where grammatical\nstructures inherently encode gender, making the bias analysis more challenging.\nExisting methods designed for English are inadequate for this task due to the\nintrinsic linguistic differences between English and gendered languages. This\npaper introduces a novel methodology that leverages the contextual\nunderstanding capabilities of LLMs to quantitatively analyze gender\nrepresentation in Spanish corpora. By utilizing LLMs to identify and classify\ngendered nouns and pronouns in relation to their reference to human entities,\nour approach provides a nuanced analysis of gender biases. We empirically\nvalidate our method on four widely-used benchmark datasets, uncovering\nsignificant gender disparities with a male-to-female ratio ranging from 4:1 to\n6:1. These findings demonstrate the value of our methodology for bias\nquantification in gendered languages and suggest its application in NLP,\ncontributing to the development of more equitable language technologies.",
        "chunk-id": 1,
        "chunk": "Gender bias in text corpora used in various natural language processing (NLP)\ncontexts, such as for training large language models (LLMs), can lead to the\nperpetuation and amplification of societal inequalities. This is particularly\npronounced in gendered languages like Spanish or French, where grammatical",
        "authors": [
            "Erik Derner",
            "Sara Sansalvador de la Fuente",
            "Yoan Guti\u00e9rrez",
            "Paloma Moreda",
            "Nuria Oliver"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T16:30:58+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13677v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13677v1",
        "categories": [
            "Computation and Language",
            "Computers and Society"
        ]
    },
    {
        "id": 20000057,
        "doi": null,
        "title": "Leveraging Large Language Models to Measure Gender Bias in Gendered Languages",
        "abstract": "Gender bias in text corpora used in various natural language processing (NLP)\ncontexts, such as for training large language models (LLMs), can lead to the\nperpetuation and amplification of societal inequalities. This is particularly\npronounced in gendered languages like Spanish or French, where grammatical\nstructures inherently encode gender, making the bias analysis more challenging.\nExisting methods designed for English are inadequate for this task due to the\nintrinsic linguistic differences between English and gendered languages. This\npaper introduces a novel methodology that leverages the contextual\nunderstanding capabilities of LLMs to quantitatively analyze gender\nrepresentation in Spanish corpora. By utilizing LLMs to identify and classify\ngendered nouns and pronouns in relation to their reference to human entities,\nour approach provides a nuanced analysis of gender biases. We empirically\nvalidate our method on four widely-used benchmark datasets, uncovering\nsignificant gender disparities with a male-to-female ratio ranging from 4:1 to\n6:1. These findings demonstrate the value of our methodology for bias\nquantification in gendered languages and suggest its application in NLP,\ncontributing to the development of more equitable language technologies.",
        "chunk-id": 2,
        "chunk": "structures inherently encode gender, making the bias analysis more challenging.\nExisting methods designed for English are inadequate for this task due to the\nintrinsic linguistic differences between English and gendered languages. This\npaper introduces a novel methodology that leverages the contextual\nunderstanding capabilities of LLMs to quantitatively analyze gender",
        "authors": [
            "Erik Derner",
            "Sara Sansalvador de la Fuente",
            "Yoan Guti\u00e9rrez",
            "Paloma Moreda",
            "Nuria Oliver"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T16:30:58+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13677v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13677v1",
        "categories": [
            "Computation and Language",
            "Computers and Society"
        ]
    },
    {
        "id": 20000057,
        "doi": null,
        "title": "Leveraging Large Language Models to Measure Gender Bias in Gendered Languages",
        "abstract": "Gender bias in text corpora used in various natural language processing (NLP)\ncontexts, such as for training large language models (LLMs), can lead to the\nperpetuation and amplification of societal inequalities. This is particularly\npronounced in gendered languages like Spanish or French, where grammatical\nstructures inherently encode gender, making the bias analysis more challenging.\nExisting methods designed for English are inadequate for this task due to the\nintrinsic linguistic differences between English and gendered languages. This\npaper introduces a novel methodology that leverages the contextual\nunderstanding capabilities of LLMs to quantitatively analyze gender\nrepresentation in Spanish corpora. By utilizing LLMs to identify and classify\ngendered nouns and pronouns in relation to their reference to human entities,\nour approach provides a nuanced analysis of gender biases. We empirically\nvalidate our method on four widely-used benchmark datasets, uncovering\nsignificant gender disparities with a male-to-female ratio ranging from 4:1 to\n6:1. These findings demonstrate the value of our methodology for bias\nquantification in gendered languages and suggest its application in NLP,\ncontributing to the development of more equitable language technologies.",
        "chunk-id": 3,
        "chunk": "representation in Spanish corpora. By utilizing LLMs to identify and classify\ngendered nouns and pronouns in relation to their reference to human entities,\nour approach provides a nuanced analysis of gender biases. We empirically\nvalidate our method on four widely-used benchmark datasets, uncovering\nsignificant gender disparities with a male-to-female ratio ranging from 4:1 to",
        "authors": [
            "Erik Derner",
            "Sara Sansalvador de la Fuente",
            "Yoan Guti\u00e9rrez",
            "Paloma Moreda",
            "Nuria Oliver"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T16:30:58+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13677v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13677v1",
        "categories": [
            "Computation and Language",
            "Computers and Society"
        ]
    },
    {
        "id": 20000057,
        "doi": null,
        "title": "Leveraging Large Language Models to Measure Gender Bias in Gendered Languages",
        "abstract": "Gender bias in text corpora used in various natural language processing (NLP)\ncontexts, such as for training large language models (LLMs), can lead to the\nperpetuation and amplification of societal inequalities. This is particularly\npronounced in gendered languages like Spanish or French, where grammatical\nstructures inherently encode gender, making the bias analysis more challenging.\nExisting methods designed for English are inadequate for this task due to the\nintrinsic linguistic differences between English and gendered languages. This\npaper introduces a novel methodology that leverages the contextual\nunderstanding capabilities of LLMs to quantitatively analyze gender\nrepresentation in Spanish corpora. By utilizing LLMs to identify and classify\ngendered nouns and pronouns in relation to their reference to human entities,\nour approach provides a nuanced analysis of gender biases. We empirically\nvalidate our method on four widely-used benchmark datasets, uncovering\nsignificant gender disparities with a male-to-female ratio ranging from 4:1 to\n6:1. These findings demonstrate the value of our methodology for bias\nquantification in gendered languages and suggest its application in NLP,\ncontributing to the development of more equitable language technologies.",
        "chunk-id": 4,
        "chunk": "6:1. These findings demonstrate the value of our methodology for bias\nquantification in gendered languages and suggest its application in NLP,\ncontributing to the development of more equitable language technologies.",
        "authors": [
            "Erik Derner",
            "Sara Sansalvador de la Fuente",
            "Yoan Guti\u00e9rrez",
            "Paloma Moreda",
            "Nuria Oliver"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T16:30:58+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13677v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13677v1",
        "categories": [
            "Computation and Language",
            "Computers and Society"
        ]
    },
    {
        "id": 20000058,
        "doi": null,
        "title": "Fine-Tuning Gemma-7B for Enhanced Sentiment Analysis of Financial News Headlines",
        "abstract": "In this study, we explore the application of sentiment analysis on financial\nnews headlines to understand investor sentiment. By leveraging Natural Language\nProcessing (NLP) and Large Language Models (LLM), we analyze sentiment from the\nperspective of retail investors. The FinancialPhraseBank dataset, which\ncontains categorized sentiments of financial news headlines, serves as the\nbasis for our analysis. We fine-tuned several models, including\ndistilbert-base-uncased, Llama, and gemma-7b, to evaluate their effectiveness\nin sentiment classification. Our experiments demonstrate that the fine-tuned\ngemma-7b model outperforms others, achieving the highest precision, recall, and\nF1 score. Specifically, the gemma-7b model showed significant improvements in\naccuracy after fine-tuning, indicating its robustness in capturing the nuances\nof financial sentiment. This model can be instrumental in providing market\ninsights, risk management, and aiding investment decisions by accurately\npredicting the sentiment of financial news. The results highlight the potential\nof advanced LLMs in transforming how we analyze and interpret financial\ninformation, offering a powerful tool for stakeholders in the financial\nindustry.",
        "chunk-id": 1,
        "chunk": "In this study, we explore the application of sentiment analysis on financial\nnews headlines to understand investor sentiment. By leveraging Natural Language\nProcessing (NLP) and Large Language Models (LLM), we analyze sentiment from the\nperspective of retail investors. The FinancialPhraseBank dataset, which\ncontains categorized sentiments of financial news headlines, serves as the",
        "authors": [
            "Kangtong Mo",
            "Wenyan Liu",
            "Xuanzhen Xu",
            "Chang Yu",
            "Yuelin Zou",
            "Fangqing Xia"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T15:20:19+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13626v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13626v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000058,
        "doi": null,
        "title": "Fine-Tuning Gemma-7B for Enhanced Sentiment Analysis of Financial News Headlines",
        "abstract": "In this study, we explore the application of sentiment analysis on financial\nnews headlines to understand investor sentiment. By leveraging Natural Language\nProcessing (NLP) and Large Language Models (LLM), we analyze sentiment from the\nperspective of retail investors. The FinancialPhraseBank dataset, which\ncontains categorized sentiments of financial news headlines, serves as the\nbasis for our analysis. We fine-tuned several models, including\ndistilbert-base-uncased, Llama, and gemma-7b, to evaluate their effectiveness\nin sentiment classification. Our experiments demonstrate that the fine-tuned\ngemma-7b model outperforms others, achieving the highest precision, recall, and\nF1 score. Specifically, the gemma-7b model showed significant improvements in\naccuracy after fine-tuning, indicating its robustness in capturing the nuances\nof financial sentiment. This model can be instrumental in providing market\ninsights, risk management, and aiding investment decisions by accurately\npredicting the sentiment of financial news. The results highlight the potential\nof advanced LLMs in transforming how we analyze and interpret financial\ninformation, offering a powerful tool for stakeholders in the financial\nindustry.",
        "chunk-id": 2,
        "chunk": "basis for our analysis. We fine-tuned several models, including\ndistilbert-base-uncased, Llama, and gemma-7b, to evaluate their effectiveness\nin sentiment classification. Our experiments demonstrate that the fine-tuned\ngemma-7b model outperforms others, achieving the highest precision, recall, and\nF1 score. Specifically, the gemma-7b model showed significant improvements in",
        "authors": [
            "Kangtong Mo",
            "Wenyan Liu",
            "Xuanzhen Xu",
            "Chang Yu",
            "Yuelin Zou",
            "Fangqing Xia"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T15:20:19+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13626v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13626v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000058,
        "doi": null,
        "title": "Fine-Tuning Gemma-7B for Enhanced Sentiment Analysis of Financial News Headlines",
        "abstract": "In this study, we explore the application of sentiment analysis on financial\nnews headlines to understand investor sentiment. By leveraging Natural Language\nProcessing (NLP) and Large Language Models (LLM), we analyze sentiment from the\nperspective of retail investors. The FinancialPhraseBank dataset, which\ncontains categorized sentiments of financial news headlines, serves as the\nbasis for our analysis. We fine-tuned several models, including\ndistilbert-base-uncased, Llama, and gemma-7b, to evaluate their effectiveness\nin sentiment classification. Our experiments demonstrate that the fine-tuned\ngemma-7b model outperforms others, achieving the highest precision, recall, and\nF1 score. Specifically, the gemma-7b model showed significant improvements in\naccuracy after fine-tuning, indicating its robustness in capturing the nuances\nof financial sentiment. This model can be instrumental in providing market\ninsights, risk management, and aiding investment decisions by accurately\npredicting the sentiment of financial news. The results highlight the potential\nof advanced LLMs in transforming how we analyze and interpret financial\ninformation, offering a powerful tool for stakeholders in the financial\nindustry.",
        "chunk-id": 3,
        "chunk": "accuracy after fine-tuning, indicating its robustness in capturing the nuances\nof financial sentiment. This model can be instrumental in providing market\ninsights, risk management, and aiding investment decisions by accurately\npredicting the sentiment of financial news. The results highlight the potential\nof advanced LLMs in transforming how we analyze and interpret financial",
        "authors": [
            "Kangtong Mo",
            "Wenyan Liu",
            "Xuanzhen Xu",
            "Chang Yu",
            "Yuelin Zou",
            "Fangqing Xia"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T15:20:19+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13626v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13626v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000058,
        "doi": null,
        "title": "Fine-Tuning Gemma-7B for Enhanced Sentiment Analysis of Financial News Headlines",
        "abstract": "In this study, we explore the application of sentiment analysis on financial\nnews headlines to understand investor sentiment. By leveraging Natural Language\nProcessing (NLP) and Large Language Models (LLM), we analyze sentiment from the\nperspective of retail investors. The FinancialPhraseBank dataset, which\ncontains categorized sentiments of financial news headlines, serves as the\nbasis for our analysis. We fine-tuned several models, including\ndistilbert-base-uncased, Llama, and gemma-7b, to evaluate their effectiveness\nin sentiment classification. Our experiments demonstrate that the fine-tuned\ngemma-7b model outperforms others, achieving the highest precision, recall, and\nF1 score. Specifically, the gemma-7b model showed significant improvements in\naccuracy after fine-tuning, indicating its robustness in capturing the nuances\nof financial sentiment. This model can be instrumental in providing market\ninsights, risk management, and aiding investment decisions by accurately\npredicting the sentiment of financial news. The results highlight the potential\nof advanced LLMs in transforming how we analyze and interpret financial\ninformation, offering a powerful tool for stakeholders in the financial\nindustry.",
        "chunk-id": 4,
        "chunk": "information, offering a powerful tool for stakeholders in the financial\nindustry.",
        "authors": [
            "Kangtong Mo",
            "Wenyan Liu",
            "Xuanzhen Xu",
            "Chang Yu",
            "Yuelin Zou",
            "Fangqing Xia"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T15:20:19+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13626v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13626v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000059,
        "doi": null,
        "title": "Improving Visual Commonsense in Language Models via Multiple Image Generation",
        "abstract": "Commonsense reasoning is fundamentally based on multimodal knowledge.\nHowever, existing large language models (LLMs) are primarily trained using\ntextual data only, limiting their ability to incorporate essential visual\ninformation. In contrast, Visual Language Models, which excel at\nvisually-oriented tasks, often fail at non-visual tasks such as basic\ncommonsense reasoning. This divergence highlights a critical challenge - the\nintegration of robust visual understanding with foundational text-based\nlanguage reasoning. To this end, we introduce a method aimed at enhancing LLMs'\nvisual commonsense. Specifically, our method generates multiple images based on\nthe input text prompt and integrates these into the model's decision-making\nprocess by mixing their prediction probabilities. To facilitate multimodal\ngrounded language modeling, we employ a late-fusion layer that combines the\nprojected visual features with the output of a pre-trained LLM conditioned on\ntext only. This late-fusion layer enables predictions based on comprehensive\nimage-text knowledge as well as text only when this is required. We evaluate\nour approach using several visual commonsense reasoning tasks together with\ntraditional NLP tasks, including common sense reasoning and reading\ncomprehension. Our experimental results demonstrate significant superiority\nover existing baselines. When applied to recent state-of-the-art LLMs (e.g.,\nLlama3), we observe improvements not only in visual common sense but also in\ntraditional NLP benchmarks. Code and models are available under\nhttps://github.com/guyyariv/vLMIG.",
        "chunk-id": 1,
        "chunk": "Commonsense reasoning is fundamentally based on multimodal knowledge.\nHowever, existing large language models (LLMs) are primarily trained using\ntextual data only, limiting their ability to incorporate essential visual\ninformation. In contrast, Visual Language Models, which excel at\nvisually-oriented tasks, often fail at non-visual tasks such as basic",
        "authors": [
            "Guy Yariv",
            "Idan Schwartz",
            "Yossi Adi",
            "Sagie Benaim"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T15:17:10+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13621v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13621v1",
        "categories": [
            "Computation and Language",
            "Computer Vision and Pattern Recognition",
            "Machine Learning"
        ]
    },
    {
        "id": 20000059,
        "doi": null,
        "title": "Improving Visual Commonsense in Language Models via Multiple Image Generation",
        "abstract": "Commonsense reasoning is fundamentally based on multimodal knowledge.\nHowever, existing large language models (LLMs) are primarily trained using\ntextual data only, limiting their ability to incorporate essential visual\ninformation. In contrast, Visual Language Models, which excel at\nvisually-oriented tasks, often fail at non-visual tasks such as basic\ncommonsense reasoning. This divergence highlights a critical challenge - the\nintegration of robust visual understanding with foundational text-based\nlanguage reasoning. To this end, we introduce a method aimed at enhancing LLMs'\nvisual commonsense. Specifically, our method generates multiple images based on\nthe input text prompt and integrates these into the model's decision-making\nprocess by mixing their prediction probabilities. To facilitate multimodal\ngrounded language modeling, we employ a late-fusion layer that combines the\nprojected visual features with the output of a pre-trained LLM conditioned on\ntext only. This late-fusion layer enables predictions based on comprehensive\nimage-text knowledge as well as text only when this is required. We evaluate\nour approach using several visual commonsense reasoning tasks together with\ntraditional NLP tasks, including common sense reasoning and reading\ncomprehension. Our experimental results demonstrate significant superiority\nover existing baselines. When applied to recent state-of-the-art LLMs (e.g.,\nLlama3), we observe improvements not only in visual common sense but also in\ntraditional NLP benchmarks. Code and models are available under\nhttps://github.com/guyyariv/vLMIG.",
        "chunk-id": 2,
        "chunk": "commonsense reasoning. This divergence highlights a critical challenge - the\nintegration of robust visual understanding with foundational text-based\nlanguage reasoning. To this end, we introduce a method aimed at enhancing LLMs'\nvisual commonsense. Specifically, our method generates multiple images based on\nthe input text prompt and integrates these into the model's decision-making",
        "authors": [
            "Guy Yariv",
            "Idan Schwartz",
            "Yossi Adi",
            "Sagie Benaim"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T15:17:10+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13621v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13621v1",
        "categories": [
            "Computation and Language",
            "Computer Vision and Pattern Recognition",
            "Machine Learning"
        ]
    },
    {
        "id": 20000059,
        "doi": null,
        "title": "Improving Visual Commonsense in Language Models via Multiple Image Generation",
        "abstract": "Commonsense reasoning is fundamentally based on multimodal knowledge.\nHowever, existing large language models (LLMs) are primarily trained using\ntextual data only, limiting their ability to incorporate essential visual\ninformation. In contrast, Visual Language Models, which excel at\nvisually-oriented tasks, often fail at non-visual tasks such as basic\ncommonsense reasoning. This divergence highlights a critical challenge - the\nintegration of robust visual understanding with foundational text-based\nlanguage reasoning. To this end, we introduce a method aimed at enhancing LLMs'\nvisual commonsense. Specifically, our method generates multiple images based on\nthe input text prompt and integrates these into the model's decision-making\nprocess by mixing their prediction probabilities. To facilitate multimodal\ngrounded language modeling, we employ a late-fusion layer that combines the\nprojected visual features with the output of a pre-trained LLM conditioned on\ntext only. This late-fusion layer enables predictions based on comprehensive\nimage-text knowledge as well as text only when this is required. We evaluate\nour approach using several visual commonsense reasoning tasks together with\ntraditional NLP tasks, including common sense reasoning and reading\ncomprehension. Our experimental results demonstrate significant superiority\nover existing baselines. When applied to recent state-of-the-art LLMs (e.g.,\nLlama3), we observe improvements not only in visual common sense but also in\ntraditional NLP benchmarks. Code and models are available under\nhttps://github.com/guyyariv/vLMIG.",
        "chunk-id": 3,
        "chunk": "process by mixing their prediction probabilities. To facilitate multimodal\ngrounded language modeling, we employ a late-fusion layer that combines the\nprojected visual features with the output of a pre-trained LLM conditioned on\ntext only. This late-fusion layer enables predictions based on comprehensive\nimage-text knowledge as well as text only when this is required. We evaluate",
        "authors": [
            "Guy Yariv",
            "Idan Schwartz",
            "Yossi Adi",
            "Sagie Benaim"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T15:17:10+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13621v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13621v1",
        "categories": [
            "Computation and Language",
            "Computer Vision and Pattern Recognition",
            "Machine Learning"
        ]
    },
    {
        "id": 20000059,
        "doi": null,
        "title": "Improving Visual Commonsense in Language Models via Multiple Image Generation",
        "abstract": "Commonsense reasoning is fundamentally based on multimodal knowledge.\nHowever, existing large language models (LLMs) are primarily trained using\ntextual data only, limiting their ability to incorporate essential visual\ninformation. In contrast, Visual Language Models, which excel at\nvisually-oriented tasks, often fail at non-visual tasks such as basic\ncommonsense reasoning. This divergence highlights a critical challenge - the\nintegration of robust visual understanding with foundational text-based\nlanguage reasoning. To this end, we introduce a method aimed at enhancing LLMs'\nvisual commonsense. Specifically, our method generates multiple images based on\nthe input text prompt and integrates these into the model's decision-making\nprocess by mixing their prediction probabilities. To facilitate multimodal\ngrounded language modeling, we employ a late-fusion layer that combines the\nprojected visual features with the output of a pre-trained LLM conditioned on\ntext only. This late-fusion layer enables predictions based on comprehensive\nimage-text knowledge as well as text only when this is required. We evaluate\nour approach using several visual commonsense reasoning tasks together with\ntraditional NLP tasks, including common sense reasoning and reading\ncomprehension. Our experimental results demonstrate significant superiority\nover existing baselines. When applied to recent state-of-the-art LLMs (e.g.,\nLlama3), we observe improvements not only in visual common sense but also in\ntraditional NLP benchmarks. Code and models are available under\nhttps://github.com/guyyariv/vLMIG.",
        "chunk-id": 4,
        "chunk": "our approach using several visual commonsense reasoning tasks together with\ntraditional NLP tasks, including common sense reasoning and reading\ncomprehension. Our experimental results demonstrate significant superiority\nover existing baselines. When applied to recent state-of-the-art LLMs (e.g.,\nLlama3), we observe improvements not only in visual common sense but also in",
        "authors": [
            "Guy Yariv",
            "Idan Schwartz",
            "Yossi Adi",
            "Sagie Benaim"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T15:17:10+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13621v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13621v1",
        "categories": [
            "Computation and Language",
            "Computer Vision and Pattern Recognition",
            "Machine Learning"
        ]
    },
    {
        "id": 20000059,
        "doi": null,
        "title": "Improving Visual Commonsense in Language Models via Multiple Image Generation",
        "abstract": "Commonsense reasoning is fundamentally based on multimodal knowledge.\nHowever, existing large language models (LLMs) are primarily trained using\ntextual data only, limiting their ability to incorporate essential visual\ninformation. In contrast, Visual Language Models, which excel at\nvisually-oriented tasks, often fail at non-visual tasks such as basic\ncommonsense reasoning. This divergence highlights a critical challenge - the\nintegration of robust visual understanding with foundational text-based\nlanguage reasoning. To this end, we introduce a method aimed at enhancing LLMs'\nvisual commonsense. Specifically, our method generates multiple images based on\nthe input text prompt and integrates these into the model's decision-making\nprocess by mixing their prediction probabilities. To facilitate multimodal\ngrounded language modeling, we employ a late-fusion layer that combines the\nprojected visual features with the output of a pre-trained LLM conditioned on\ntext only. This late-fusion layer enables predictions based on comprehensive\nimage-text knowledge as well as text only when this is required. We evaluate\nour approach using several visual commonsense reasoning tasks together with\ntraditional NLP tasks, including common sense reasoning and reading\ncomprehension. Our experimental results demonstrate significant superiority\nover existing baselines. When applied to recent state-of-the-art LLMs (e.g.,\nLlama3), we observe improvements not only in visual common sense but also in\ntraditional NLP benchmarks. Code and models are available under\nhttps://github.com/guyyariv/vLMIG.",
        "chunk-id": 5,
        "chunk": "traditional NLP benchmarks. Code and models are available under\nhttps://github.com/guyyariv/vLMIG.",
        "authors": [
            "Guy Yariv",
            "Idan Schwartz",
            "Yossi Adi",
            "Sagie Benaim"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T15:17:10+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13621v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13621v1",
        "categories": [
            "Computation and Language",
            "Computer Vision and Pattern Recognition",
            "Machine Learning"
        ]
    },
    {
        "id": 20000060,
        "doi": null,
        "title": "BiLD: Bi-directional Logits Difference Loss for Large Language Model Distillation",
        "abstract": "In recent years, large language models (LLMs) have shown exceptional\ncapabilities across various natural language processing (NLP) tasks. However,\nsuch impressive performance often comes with the trade-off of an increased\nparameter size, posing significant challenges for widespread deployment.\nKnowledge distillation (KD) provides a solution by transferring knowledge from\na large teacher model to a smaller student model. In this paper, we explore the\ntask-specific distillation of LLMs at the logit level. Our investigation\nreveals that the logits of fine-tuned LLMs exhibit a more extreme long-tail\ndistribution than those from vision models, with hidden \"noise\" in the long\ntail affecting distillation performance. Furthermore, existing logits\ndistillation methods often struggle to effectively utilize the internal ranking\ninformation from the logits. To address these, we propose the Bi-directional\nLogits Difference (BiLD) loss. The BiLD loss filters out the long-tail noise by\nutilizing only top-$k$ teacher and student logits, and leverages the internal\nlogits ranking information by constructing logits differences. To evaluate BiLD\nloss, we conduct comprehensive experiments on 13 datasets using two types of\nLLMs. Our results show that the BiLD loss, with only the top-8 logits,\noutperforms supervised fine-tuning (SFT), vanilla KL loss, and five other\ndistillation methods from both NLP and CV fields.",
        "chunk-id": 1,
        "chunk": "In recent years, large language models (LLMs) have shown exceptional\ncapabilities across various natural language processing (NLP) tasks. However,\nsuch impressive performance often comes with the trade-off of an increased\nparameter size, posing significant challenges for widespread deployment.\nKnowledge distillation (KD) provides a solution by transferring knowledge from",
        "authors": [
            "Minchong Li",
            "Feng Zhou",
            "Xiaohui Song"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T13:44:56+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13555v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13555v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000060,
        "doi": null,
        "title": "BiLD: Bi-directional Logits Difference Loss for Large Language Model Distillation",
        "abstract": "In recent years, large language models (LLMs) have shown exceptional\ncapabilities across various natural language processing (NLP) tasks. However,\nsuch impressive performance often comes with the trade-off of an increased\nparameter size, posing significant challenges for widespread deployment.\nKnowledge distillation (KD) provides a solution by transferring knowledge from\na large teacher model to a smaller student model. In this paper, we explore the\ntask-specific distillation of LLMs at the logit level. Our investigation\nreveals that the logits of fine-tuned LLMs exhibit a more extreme long-tail\ndistribution than those from vision models, with hidden \"noise\" in the long\ntail affecting distillation performance. Furthermore, existing logits\ndistillation methods often struggle to effectively utilize the internal ranking\ninformation from the logits. To address these, we propose the Bi-directional\nLogits Difference (BiLD) loss. The BiLD loss filters out the long-tail noise by\nutilizing only top-$k$ teacher and student logits, and leverages the internal\nlogits ranking information by constructing logits differences. To evaluate BiLD\nloss, we conduct comprehensive experiments on 13 datasets using two types of\nLLMs. Our results show that the BiLD loss, with only the top-8 logits,\noutperforms supervised fine-tuning (SFT), vanilla KL loss, and five other\ndistillation methods from both NLP and CV fields.",
        "chunk-id": 2,
        "chunk": "a large teacher model to a smaller student model. In this paper, we explore the\ntask-specific distillation of LLMs at the logit level. Our investigation\nreveals that the logits of fine-tuned LLMs exhibit a more extreme long-tail\ndistribution than those from vision models, with hidden \"noise\" in the long\ntail affecting distillation performance. Furthermore, existing logits",
        "authors": [
            "Minchong Li",
            "Feng Zhou",
            "Xiaohui Song"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T13:44:56+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13555v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13555v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000060,
        "doi": null,
        "title": "BiLD: Bi-directional Logits Difference Loss for Large Language Model Distillation",
        "abstract": "In recent years, large language models (LLMs) have shown exceptional\ncapabilities across various natural language processing (NLP) tasks. However,\nsuch impressive performance often comes with the trade-off of an increased\nparameter size, posing significant challenges for widespread deployment.\nKnowledge distillation (KD) provides a solution by transferring knowledge from\na large teacher model to a smaller student model. In this paper, we explore the\ntask-specific distillation of LLMs at the logit level. Our investigation\nreveals that the logits of fine-tuned LLMs exhibit a more extreme long-tail\ndistribution than those from vision models, with hidden \"noise\" in the long\ntail affecting distillation performance. Furthermore, existing logits\ndistillation methods often struggle to effectively utilize the internal ranking\ninformation from the logits. To address these, we propose the Bi-directional\nLogits Difference (BiLD) loss. The BiLD loss filters out the long-tail noise by\nutilizing only top-$k$ teacher and student logits, and leverages the internal\nlogits ranking information by constructing logits differences. To evaluate BiLD\nloss, we conduct comprehensive experiments on 13 datasets using two types of\nLLMs. Our results show that the BiLD loss, with only the top-8 logits,\noutperforms supervised fine-tuning (SFT), vanilla KL loss, and five other\ndistillation methods from both NLP and CV fields.",
        "chunk-id": 3,
        "chunk": "distillation methods often struggle to effectively utilize the internal ranking\ninformation from the logits. To address these, we propose the Bi-directional\nLogits Difference (BiLD) loss. The BiLD loss filters out the long-tail noise by\nutilizing only top-$k$ teacher and student logits, and leverages the internal",
        "authors": [
            "Minchong Li",
            "Feng Zhou",
            "Xiaohui Song"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T13:44:56+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13555v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13555v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000060,
        "doi": null,
        "title": "BiLD: Bi-directional Logits Difference Loss for Large Language Model Distillation",
        "abstract": "In recent years, large language models (LLMs) have shown exceptional\ncapabilities across various natural language processing (NLP) tasks. However,\nsuch impressive performance often comes with the trade-off of an increased\nparameter size, posing significant challenges for widespread deployment.\nKnowledge distillation (KD) provides a solution by transferring knowledge from\na large teacher model to a smaller student model. In this paper, we explore the\ntask-specific distillation of LLMs at the logit level. Our investigation\nreveals that the logits of fine-tuned LLMs exhibit a more extreme long-tail\ndistribution than those from vision models, with hidden \"noise\" in the long\ntail affecting distillation performance. Furthermore, existing logits\ndistillation methods often struggle to effectively utilize the internal ranking\ninformation from the logits. To address these, we propose the Bi-directional\nLogits Difference (BiLD) loss. The BiLD loss filters out the long-tail noise by\nutilizing only top-$k$ teacher and student logits, and leverages the internal\nlogits ranking information by constructing logits differences. To evaluate BiLD\nloss, we conduct comprehensive experiments on 13 datasets using two types of\nLLMs. Our results show that the BiLD loss, with only the top-8 logits,\noutperforms supervised fine-tuning (SFT), vanilla KL loss, and five other\ndistillation methods from both NLP and CV fields.",
        "chunk-id": 4,
        "chunk": "logits ranking information by constructing logits differences. To evaluate BiLD\nloss, we conduct comprehensive experiments on 13 datasets using two types of\nLLMs. Our results show that the BiLD loss, with only the top-8 logits,\noutperforms supervised fine-tuning (SFT), vanilla KL loss, and five other\ndistillation methods from both NLP and CV fields.",
        "authors": [
            "Minchong Li",
            "Feng Zhou",
            "Xiaohui Song"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T13:44:56+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13555v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13555v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000061,
        "doi": null,
        "title": "Mining United Nations General Assembly Debates",
        "abstract": "This project explores the application of Natural Language Processing (NLP)\ntechniques to analyse United Nations General Assembly (UNGA) speeches. Using\nNLP allows for the efficient processing and analysis of large volumes of\ntextual data, enabling the extraction of semantic patterns, sentiment analysis,\nand topic modelling. Our goal is to deliver a comprehensive dataset and a tool\n(interface with descriptive statistics and automatically extracted topics) from\nwhich political scientists can derive insights into international relations and\nhave the opportunity to have a nuanced understanding of global diplomatic\ndiscourse.",
        "chunk-id": 1,
        "chunk": "This project explores the application of Natural Language Processing (NLP)\ntechniques to analyse United Nations General Assembly (UNGA) speeches. Using\nNLP allows for the efficient processing and analysis of large volumes of\ntextual data, enabling the extraction of semantic patterns, sentiment analysis,\nand topic modelling. Our goal is to deliver a comprehensive dataset and a tool",
        "authors": [
            "Mateusz Grzyb",
            "Mateusz Krzyzi\u0144ski",
            "Bart\u0142omiej Sobieski",
            "Miko\u0142aj Spytek",
            "Bartosz Pieli\u0144ski",
            "Daniel Dan",
            "Anna Wr\u00f3blewska"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T13:43:27+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13553v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13553v1",
        "categories": [
            "Computation and Language",
            "Information Retrieval"
        ]
    },
    {
        "id": 20000061,
        "doi": null,
        "title": "Mining United Nations General Assembly Debates",
        "abstract": "This project explores the application of Natural Language Processing (NLP)\ntechniques to analyse United Nations General Assembly (UNGA) speeches. Using\nNLP allows for the efficient processing and analysis of large volumes of\ntextual data, enabling the extraction of semantic patterns, sentiment analysis,\nand topic modelling. Our goal is to deliver a comprehensive dataset and a tool\n(interface with descriptive statistics and automatically extracted topics) from\nwhich political scientists can derive insights into international relations and\nhave the opportunity to have a nuanced understanding of global diplomatic\ndiscourse.",
        "chunk-id": 2,
        "chunk": "(interface with descriptive statistics and automatically extracted topics) from\nwhich political scientists can derive insights into international relations and\nhave the opportunity to have a nuanced understanding of global diplomatic\ndiscourse.",
        "authors": [
            "Mateusz Grzyb",
            "Mateusz Krzyzi\u0144ski",
            "Bart\u0142omiej Sobieski",
            "Miko\u0142aj Spytek",
            "Bartosz Pieli\u0144ski",
            "Daniel Dan",
            "Anna Wr\u00f3blewska"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T13:43:27+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13553v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13553v1",
        "categories": [
            "Computation and Language",
            "Information Retrieval"
        ]
    },
    {
        "id": 20000062,
        "doi": null,
        "title": "Factual Confidence of LLMs: on Reliability and Robustness of Current Estimators",
        "abstract": "Large Language Models (LLMs) tend to be unreliable in the factuality of their\nanswers. To address this problem, NLP researchers have proposed a range of\ntechniques to estimate LLM's confidence over facts. However, due to the lack of\na systematic comparison, it is not clear how the different methods compare to\none another. To fill this gap, we present a survey and empirical comparison of\nestimators of factual confidence. We define an experimental framework allowing\nfor fair comparison, covering both fact-verification and question answering.\nOur experiments across a series of LLMs indicate that trained hidden-state\nprobes provide the most reliable confidence estimates, albeit at the expense of\nrequiring access to weights and training data. We also conduct a deeper\nassessment of factual confidence by measuring the consistency of model behavior\nunder meaning-preserving variations in the input. We find that the confidence\nof LLMs is often unstable across semantically equivalent inputs, suggesting\nthat there is much room for improvement of the stability of models' parametric\nknowledge. Our code is available at\n(https://github.com/amazon-science/factual-confidence-of-llms).",
        "chunk-id": 1,
        "chunk": "Large Language Models (LLMs) tend to be unreliable in the factuality of their\nanswers. To address this problem, NLP researchers have proposed a range of\ntechniques to estimate LLM's confidence over facts. However, due to the lack of\na systematic comparison, it is not clear how the different methods compare to",
        "authors": [
            "Mat\u00e9o Mahaut",
            "Laura Aina",
            "Paula Czarnowska",
            "Momchil Hardalov",
            "Thomas M\u00fcller",
            "Llu\u00eds M\u00e0rquez"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T10:11:37+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13415v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13415v1",
        "categories": [
            "Computation and Language",
            "Machine Learning"
        ]
    },
    {
        "id": 20000062,
        "doi": null,
        "title": "Factual Confidence of LLMs: on Reliability and Robustness of Current Estimators",
        "abstract": "Large Language Models (LLMs) tend to be unreliable in the factuality of their\nanswers. To address this problem, NLP researchers have proposed a range of\ntechniques to estimate LLM's confidence over facts. However, due to the lack of\na systematic comparison, it is not clear how the different methods compare to\none another. To fill this gap, we present a survey and empirical comparison of\nestimators of factual confidence. We define an experimental framework allowing\nfor fair comparison, covering both fact-verification and question answering.\nOur experiments across a series of LLMs indicate that trained hidden-state\nprobes provide the most reliable confidence estimates, albeit at the expense of\nrequiring access to weights and training data. We also conduct a deeper\nassessment of factual confidence by measuring the consistency of model behavior\nunder meaning-preserving variations in the input. We find that the confidence\nof LLMs is often unstable across semantically equivalent inputs, suggesting\nthat there is much room for improvement of the stability of models' parametric\nknowledge. Our code is available at\n(https://github.com/amazon-science/factual-confidence-of-llms).",
        "chunk-id": 2,
        "chunk": "one another. To fill this gap, we present a survey and empirical comparison of\nestimators of factual confidence. We define an experimental framework allowing\nfor fair comparison, covering both fact-verification and question answering.\nOur experiments across a series of LLMs indicate that trained hidden-state",
        "authors": [
            "Mat\u00e9o Mahaut",
            "Laura Aina",
            "Paula Czarnowska",
            "Momchil Hardalov",
            "Thomas M\u00fcller",
            "Llu\u00eds M\u00e0rquez"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T10:11:37+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13415v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13415v1",
        "categories": [
            "Computation and Language",
            "Machine Learning"
        ]
    },
    {
        "id": 20000062,
        "doi": null,
        "title": "Factual Confidence of LLMs: on Reliability and Robustness of Current Estimators",
        "abstract": "Large Language Models (LLMs) tend to be unreliable in the factuality of their\nanswers. To address this problem, NLP researchers have proposed a range of\ntechniques to estimate LLM's confidence over facts. However, due to the lack of\na systematic comparison, it is not clear how the different methods compare to\none another. To fill this gap, we present a survey and empirical comparison of\nestimators of factual confidence. We define an experimental framework allowing\nfor fair comparison, covering both fact-verification and question answering.\nOur experiments across a series of LLMs indicate that trained hidden-state\nprobes provide the most reliable confidence estimates, albeit at the expense of\nrequiring access to weights and training data. We also conduct a deeper\nassessment of factual confidence by measuring the consistency of model behavior\nunder meaning-preserving variations in the input. We find that the confidence\nof LLMs is often unstable across semantically equivalent inputs, suggesting\nthat there is much room for improvement of the stability of models' parametric\nknowledge. Our code is available at\n(https://github.com/amazon-science/factual-confidence-of-llms).",
        "chunk-id": 3,
        "chunk": "probes provide the most reliable confidence estimates, albeit at the expense of\nrequiring access to weights and training data. We also conduct a deeper\nassessment of factual confidence by measuring the consistency of model behavior\nunder meaning-preserving variations in the input. We find that the confidence",
        "authors": [
            "Mat\u00e9o Mahaut",
            "Laura Aina",
            "Paula Czarnowska",
            "Momchil Hardalov",
            "Thomas M\u00fcller",
            "Llu\u00eds M\u00e0rquez"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T10:11:37+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13415v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13415v1",
        "categories": [
            "Computation and Language",
            "Machine Learning"
        ]
    },
    {
        "id": 20000062,
        "doi": null,
        "title": "Factual Confidence of LLMs: on Reliability and Robustness of Current Estimators",
        "abstract": "Large Language Models (LLMs) tend to be unreliable in the factuality of their\nanswers. To address this problem, NLP researchers have proposed a range of\ntechniques to estimate LLM's confidence over facts. However, due to the lack of\na systematic comparison, it is not clear how the different methods compare to\none another. To fill this gap, we present a survey and empirical comparison of\nestimators of factual confidence. We define an experimental framework allowing\nfor fair comparison, covering both fact-verification and question answering.\nOur experiments across a series of LLMs indicate that trained hidden-state\nprobes provide the most reliable confidence estimates, albeit at the expense of\nrequiring access to weights and training data. We also conduct a deeper\nassessment of factual confidence by measuring the consistency of model behavior\nunder meaning-preserving variations in the input. We find that the confidence\nof LLMs is often unstable across semantically equivalent inputs, suggesting\nthat there is much room for improvement of the stability of models' parametric\nknowledge. Our code is available at\n(https://github.com/amazon-science/factual-confidence-of-llms).",
        "chunk-id": 4,
        "chunk": "of LLMs is often unstable across semantically equivalent inputs, suggesting\nthat there is much room for improvement of the stability of models' parametric\nknowledge. Our code is available at\n(https://github.com/amazon-science/factual-confidence-of-llms).",
        "authors": [
            "Mat\u00e9o Mahaut",
            "Laura Aina",
            "Paula Czarnowska",
            "Momchil Hardalov",
            "Thomas M\u00fcller",
            "Llu\u00eds M\u00e0rquez"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T10:11:37+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13415v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13415v1",
        "categories": [
            "Computation and Language",
            "Machine Learning"
        ]
    },
    {
        "id": 20000063,
        "doi": null,
        "title": "CoAct: A Global-Local Hierarchy for Autonomous Agent Collaboration",
        "abstract": "Existing LLMs exhibit remarkable performance on various NLP tasks, but still\nstruggle with complex real-world tasks, even equipped with advanced strategies\nlike CoT and ReAct. In this work, we propose the CoAct framework, which\ntransfers the hierarchical planning and collaboration patterns in human society\nto LLM systems. Specifically, our CoAct framework involves two agents: (1) A\nglobal planning agent, to comprehend the problem scope, formulate macro-level\nplans and provide detailed sub-task descriptions to local execution agents,\nwhich serves as the initial rendition of a global plan. (2) A local execution\nagent, to operate within the multi-tier task execution structure, focusing on\ndetailed execution and implementation of specific tasks within the global plan.\nExperimental results on the WebArena benchmark show that CoAct can re-arrange\nthe process trajectory when facing failures, and achieves superior performance\nover baseline methods on long-horizon web tasks. Code is available at\nhttps://github.com/xmhou2002/CoAct.",
        "chunk-id": 1,
        "chunk": "Existing LLMs exhibit remarkable performance on various NLP tasks, but still\nstruggle with complex real-world tasks, even equipped with advanced strategies\nlike CoT and ReAct. In this work, we propose the CoAct framework, which\ntransfers the hierarchical planning and collaboration patterns in human society\nto LLM systems. Specifically, our CoAct framework involves two agents: (1) A",
        "authors": [
            "Xinming Hou",
            "Mingming Yang",
            "Wenxiang Jiao",
            "Xing Wang",
            "Zhaopeng Tu",
            "Wayne Xin Zhao"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T09:23:53+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13381v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13381v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000063,
        "doi": null,
        "title": "CoAct: A Global-Local Hierarchy for Autonomous Agent Collaboration",
        "abstract": "Existing LLMs exhibit remarkable performance on various NLP tasks, but still\nstruggle with complex real-world tasks, even equipped with advanced strategies\nlike CoT and ReAct. In this work, we propose the CoAct framework, which\ntransfers the hierarchical planning and collaboration patterns in human society\nto LLM systems. Specifically, our CoAct framework involves two agents: (1) A\nglobal planning agent, to comprehend the problem scope, formulate macro-level\nplans and provide detailed sub-task descriptions to local execution agents,\nwhich serves as the initial rendition of a global plan. (2) A local execution\nagent, to operate within the multi-tier task execution structure, focusing on\ndetailed execution and implementation of specific tasks within the global plan.\nExperimental results on the WebArena benchmark show that CoAct can re-arrange\nthe process trajectory when facing failures, and achieves superior performance\nover baseline methods on long-horizon web tasks. Code is available at\nhttps://github.com/xmhou2002/CoAct.",
        "chunk-id": 2,
        "chunk": "global planning agent, to comprehend the problem scope, formulate macro-level\nplans and provide detailed sub-task descriptions to local execution agents,\nwhich serves as the initial rendition of a global plan. (2) A local execution\nagent, to operate within the multi-tier task execution structure, focusing on",
        "authors": [
            "Xinming Hou",
            "Mingming Yang",
            "Wenxiang Jiao",
            "Xing Wang",
            "Zhaopeng Tu",
            "Wayne Xin Zhao"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T09:23:53+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13381v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13381v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000063,
        "doi": null,
        "title": "CoAct: A Global-Local Hierarchy for Autonomous Agent Collaboration",
        "abstract": "Existing LLMs exhibit remarkable performance on various NLP tasks, but still\nstruggle with complex real-world tasks, even equipped with advanced strategies\nlike CoT and ReAct. In this work, we propose the CoAct framework, which\ntransfers the hierarchical planning and collaboration patterns in human society\nto LLM systems. Specifically, our CoAct framework involves two agents: (1) A\nglobal planning agent, to comprehend the problem scope, formulate macro-level\nplans and provide detailed sub-task descriptions to local execution agents,\nwhich serves as the initial rendition of a global plan. (2) A local execution\nagent, to operate within the multi-tier task execution structure, focusing on\ndetailed execution and implementation of specific tasks within the global plan.\nExperimental results on the WebArena benchmark show that CoAct can re-arrange\nthe process trajectory when facing failures, and achieves superior performance\nover baseline methods on long-horizon web tasks. Code is available at\nhttps://github.com/xmhou2002/CoAct.",
        "chunk-id": 3,
        "chunk": "detailed execution and implementation of specific tasks within the global plan.\nExperimental results on the WebArena benchmark show that CoAct can re-arrange\nthe process trajectory when facing failures, and achieves superior performance\nover baseline methods on long-horizon web tasks. Code is available at\nhttps://github.com/xmhou2002/CoAct.",
        "authors": [
            "Xinming Hou",
            "Mingming Yang",
            "Wenxiang Jiao",
            "Xing Wang",
            "Zhaopeng Tu",
            "Wayne Xin Zhao"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T09:23:53+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13381v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13381v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000064,
        "doi": null,
        "title": "ZeroDL: Zero-shot Distribution Learning for Text Clustering via Large Language Models",
        "abstract": "The recent advancements in large language models (LLMs) have brought\nsignificant progress in solving NLP tasks. Notably, in-context learning (ICL)\nis the key enabling mechanism for LLMs to understand specific tasks and\ngrasping nuances. In this paper, we propose a simple yet effective method to\ncontextualize a task toward a specific LLM, by (1) observing how a given LLM\ndescribes (all or a part of) target datasets, i.e., open-ended zero-shot\ninference, and (2) aggregating the open-ended inference results by the LLM, and\n(3) finally incorporate the aggregated meta-information for the actual task. We\nshow the effectiveness of this approach in text clustering tasks, and also\nhighlight the importance of the contextualization through examples of the above\nprocedure.",
        "chunk-id": 1,
        "chunk": "The recent advancements in large language models (LLMs) have brought\nsignificant progress in solving NLP tasks. Notably, in-context learning (ICL)\nis the key enabling mechanism for LLMs to understand specific tasks and\ngrasping nuances. In this paper, we propose a simple yet effective method to\ncontextualize a task toward a specific LLM, by (1) observing how a given LLM",
        "authors": [
            "Hwiyeol Jo",
            "Hyunwoo Lee",
            "Taiwoo Park"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T08:48:05+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13342v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13342v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000064,
        "doi": null,
        "title": "ZeroDL: Zero-shot Distribution Learning for Text Clustering via Large Language Models",
        "abstract": "The recent advancements in large language models (LLMs) have brought\nsignificant progress in solving NLP tasks. Notably, in-context learning (ICL)\nis the key enabling mechanism for LLMs to understand specific tasks and\ngrasping nuances. In this paper, we propose a simple yet effective method to\ncontextualize a task toward a specific LLM, by (1) observing how a given LLM\ndescribes (all or a part of) target datasets, i.e., open-ended zero-shot\ninference, and (2) aggregating the open-ended inference results by the LLM, and\n(3) finally incorporate the aggregated meta-information for the actual task. We\nshow the effectiveness of this approach in text clustering tasks, and also\nhighlight the importance of the contextualization through examples of the above\nprocedure.",
        "chunk-id": 2,
        "chunk": "describes (all or a part of) target datasets, i.e., open-ended zero-shot\ninference, and (2) aggregating the open-ended inference results by the LLM, and\n(3) finally incorporate the aggregated meta-information for the actual task. We\nshow the effectiveness of this approach in text clustering tasks, and also",
        "authors": [
            "Hwiyeol Jo",
            "Hyunwoo Lee",
            "Taiwoo Park"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T08:48:05+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13342v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13342v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000064,
        "doi": null,
        "title": "ZeroDL: Zero-shot Distribution Learning for Text Clustering via Large Language Models",
        "abstract": "The recent advancements in large language models (LLMs) have brought\nsignificant progress in solving NLP tasks. Notably, in-context learning (ICL)\nis the key enabling mechanism for LLMs to understand specific tasks and\ngrasping nuances. In this paper, we propose a simple yet effective method to\ncontextualize a task toward a specific LLM, by (1) observing how a given LLM\ndescribes (all or a part of) target datasets, i.e., open-ended zero-shot\ninference, and (2) aggregating the open-ended inference results by the LLM, and\n(3) finally incorporate the aggregated meta-information for the actual task. We\nshow the effectiveness of this approach in text clustering tasks, and also\nhighlight the importance of the contextualization through examples of the above\nprocedure.",
        "chunk-id": 3,
        "chunk": "highlight the importance of the contextualization through examples of the above\nprocedure.",
        "authors": [
            "Hwiyeol Jo",
            "Hyunwoo Lee",
            "Taiwoo Park"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T08:48:05+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13342v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13342v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000065,
        "doi": null,
        "title": "BeHonest: Benchmarking Honesty of Large Language Models",
        "abstract": "Previous works on Large Language Models (LLMs) have mainly focused on\nevaluating their helpfulness or harmlessness. However, honesty, another crucial\nalignment criterion, has received relatively less attention. Dishonest\nbehaviors in LLMs, such as spreading misinformation and defrauding users,\neroding user trust, and causing real-world harm, present severe risks that\nintensify as these models approach superintelligence levels. Enhancing honesty\nin LLMs addresses critical deficiencies and helps uncover latent capabilities\nthat are not readily expressed. This underscores the urgent need for reliable\nmethods and benchmarks to effectively ensure and evaluate the honesty of LLMs.\n  In this paper, we introduce BeHonest, a pioneering benchmark specifically\ndesigned to assess honesty in LLMs comprehensively. BeHonest evaluates three\nessential aspects of honesty: awareness of knowledge boundaries, avoidance of\ndeceit, and consistency in responses. Building on this foundation, we designed\n10 scenarios to evaluate and analyze 9 popular LLMs on the market, including\nboth closed-source and open-source models from different model families with\nvaried model sizes. Our findings indicate that there is still significant room\nfor improvement in the honesty of LLMs. We also encourage the AI community to\nprioritize honesty alignment in LLMs. Our benchmark and code can be found at:\n\\url{https://github.com/GAIR-NLP/BeHonest}.",
        "chunk-id": 1,
        "chunk": "Previous works on Large Language Models (LLMs) have mainly focused on\nevaluating their helpfulness or harmlessness. However, honesty, another crucial\nalignment criterion, has received relatively less attention. Dishonest\nbehaviors in LLMs, such as spreading misinformation and defrauding users,\neroding user trust, and causing real-world harm, present severe risks that",
        "authors": [
            "Steffi Chern",
            "Zhulin Hu",
            "Yuqing Yang",
            "Ethan Chern",
            "Yuan Guo",
            "Jiahe Jin",
            "Binjie Wang",
            "Pengfei Liu"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T06:46:59+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13261v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13261v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000065,
        "doi": null,
        "title": "BeHonest: Benchmarking Honesty of Large Language Models",
        "abstract": "Previous works on Large Language Models (LLMs) have mainly focused on\nevaluating their helpfulness or harmlessness. However, honesty, another crucial\nalignment criterion, has received relatively less attention. Dishonest\nbehaviors in LLMs, such as spreading misinformation and defrauding users,\neroding user trust, and causing real-world harm, present severe risks that\nintensify as these models approach superintelligence levels. Enhancing honesty\nin LLMs addresses critical deficiencies and helps uncover latent capabilities\nthat are not readily expressed. This underscores the urgent need for reliable\nmethods and benchmarks to effectively ensure and evaluate the honesty of LLMs.\n  In this paper, we introduce BeHonest, a pioneering benchmark specifically\ndesigned to assess honesty in LLMs comprehensively. BeHonest evaluates three\nessential aspects of honesty: awareness of knowledge boundaries, avoidance of\ndeceit, and consistency in responses. Building on this foundation, we designed\n10 scenarios to evaluate and analyze 9 popular LLMs on the market, including\nboth closed-source and open-source models from different model families with\nvaried model sizes. Our findings indicate that there is still significant room\nfor improvement in the honesty of LLMs. We also encourage the AI community to\nprioritize honesty alignment in LLMs. Our benchmark and code can be found at:\n\\url{https://github.com/GAIR-NLP/BeHonest}.",
        "chunk-id": 2,
        "chunk": "intensify as these models approach superintelligence levels. Enhancing honesty\nin LLMs addresses critical deficiencies and helps uncover latent capabilities\nthat are not readily expressed. This underscores the urgent need for reliable\nmethods and benchmarks to effectively ensure and evaluate the honesty of LLMs.",
        "authors": [
            "Steffi Chern",
            "Zhulin Hu",
            "Yuqing Yang",
            "Ethan Chern",
            "Yuan Guo",
            "Jiahe Jin",
            "Binjie Wang",
            "Pengfei Liu"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T06:46:59+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13261v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13261v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000065,
        "doi": null,
        "title": "BeHonest: Benchmarking Honesty of Large Language Models",
        "abstract": "Previous works on Large Language Models (LLMs) have mainly focused on\nevaluating their helpfulness or harmlessness. However, honesty, another crucial\nalignment criterion, has received relatively less attention. Dishonest\nbehaviors in LLMs, such as spreading misinformation and defrauding users,\neroding user trust, and causing real-world harm, present severe risks that\nintensify as these models approach superintelligence levels. Enhancing honesty\nin LLMs addresses critical deficiencies and helps uncover latent capabilities\nthat are not readily expressed. This underscores the urgent need for reliable\nmethods and benchmarks to effectively ensure and evaluate the honesty of LLMs.\n  In this paper, we introduce BeHonest, a pioneering benchmark specifically\ndesigned to assess honesty in LLMs comprehensively. BeHonest evaluates three\nessential aspects of honesty: awareness of knowledge boundaries, avoidance of\ndeceit, and consistency in responses. Building on this foundation, we designed\n10 scenarios to evaluate and analyze 9 popular LLMs on the market, including\nboth closed-source and open-source models from different model families with\nvaried model sizes. Our findings indicate that there is still significant room\nfor improvement in the honesty of LLMs. We also encourage the AI community to\nprioritize honesty alignment in LLMs. Our benchmark and code can be found at:\n\\url{https://github.com/GAIR-NLP/BeHonest}.",
        "chunk-id": 3,
        "chunk": "In this paper, we introduce BeHonest, a pioneering benchmark specifically\ndesigned to assess honesty in LLMs comprehensively. BeHonest evaluates three\nessential aspects of honesty: awareness of knowledge boundaries, avoidance of\ndeceit, and consistency in responses. Building on this foundation, we designed",
        "authors": [
            "Steffi Chern",
            "Zhulin Hu",
            "Yuqing Yang",
            "Ethan Chern",
            "Yuan Guo",
            "Jiahe Jin",
            "Binjie Wang",
            "Pengfei Liu"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T06:46:59+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13261v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13261v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000065,
        "doi": null,
        "title": "BeHonest: Benchmarking Honesty of Large Language Models",
        "abstract": "Previous works on Large Language Models (LLMs) have mainly focused on\nevaluating their helpfulness or harmlessness. However, honesty, another crucial\nalignment criterion, has received relatively less attention. Dishonest\nbehaviors in LLMs, such as spreading misinformation and defrauding users,\neroding user trust, and causing real-world harm, present severe risks that\nintensify as these models approach superintelligence levels. Enhancing honesty\nin LLMs addresses critical deficiencies and helps uncover latent capabilities\nthat are not readily expressed. This underscores the urgent need for reliable\nmethods and benchmarks to effectively ensure and evaluate the honesty of LLMs.\n  In this paper, we introduce BeHonest, a pioneering benchmark specifically\ndesigned to assess honesty in LLMs comprehensively. BeHonest evaluates three\nessential aspects of honesty: awareness of knowledge boundaries, avoidance of\ndeceit, and consistency in responses. Building on this foundation, we designed\n10 scenarios to evaluate and analyze 9 popular LLMs on the market, including\nboth closed-source and open-source models from different model families with\nvaried model sizes. Our findings indicate that there is still significant room\nfor improvement in the honesty of LLMs. We also encourage the AI community to\nprioritize honesty alignment in LLMs. Our benchmark and code can be found at:\n\\url{https://github.com/GAIR-NLP/BeHonest}.",
        "chunk-id": 4,
        "chunk": "10 scenarios to evaluate and analyze 9 popular LLMs on the market, including\nboth closed-source and open-source models from different model families with\nvaried model sizes. Our findings indicate that there is still significant room\nfor improvement in the honesty of LLMs. We also encourage the AI community to",
        "authors": [
            "Steffi Chern",
            "Zhulin Hu",
            "Yuqing Yang",
            "Ethan Chern",
            "Yuan Guo",
            "Jiahe Jin",
            "Binjie Wang",
            "Pengfei Liu"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T06:46:59+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13261v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13261v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000065,
        "doi": null,
        "title": "BeHonest: Benchmarking Honesty of Large Language Models",
        "abstract": "Previous works on Large Language Models (LLMs) have mainly focused on\nevaluating their helpfulness or harmlessness. However, honesty, another crucial\nalignment criterion, has received relatively less attention. Dishonest\nbehaviors in LLMs, such as spreading misinformation and defrauding users,\neroding user trust, and causing real-world harm, present severe risks that\nintensify as these models approach superintelligence levels. Enhancing honesty\nin LLMs addresses critical deficiencies and helps uncover latent capabilities\nthat are not readily expressed. This underscores the urgent need for reliable\nmethods and benchmarks to effectively ensure and evaluate the honesty of LLMs.\n  In this paper, we introduce BeHonest, a pioneering benchmark specifically\ndesigned to assess honesty in LLMs comprehensively. BeHonest evaluates three\nessential aspects of honesty: awareness of knowledge boundaries, avoidance of\ndeceit, and consistency in responses. Building on this foundation, we designed\n10 scenarios to evaluate and analyze 9 popular LLMs on the market, including\nboth closed-source and open-source models from different model families with\nvaried model sizes. Our findings indicate that there is still significant room\nfor improvement in the honesty of LLMs. We also encourage the AI community to\nprioritize honesty alignment in LLMs. Our benchmark and code can be found at:\n\\url{https://github.com/GAIR-NLP/BeHonest}.",
        "chunk-id": 5,
        "chunk": "prioritize honesty alignment in LLMs. Our benchmark and code can be found at:\n\\url{https://github.com/GAIR-NLP/BeHonest}.",
        "authors": [
            "Steffi Chern",
            "Zhulin Hu",
            "Yuqing Yang",
            "Ethan Chern",
            "Yuan Guo",
            "Jiahe Jin",
            "Binjie Wang",
            "Pengfei Liu"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T06:46:59+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13261v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13261v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000066,
        "doi": null,
        "title": "Learnable In-Context Vector for Visual Question Answering",
        "abstract": "As language models continue to scale, Large Language Models (LLMs) have\nexhibited emerging capabilities in In-Context Learning (ICL), enabling them to\nsolve language tasks by prefixing a few in-context demonstrations (ICDs) as\ncontext. Inspired by these advancements, researchers have extended these\ntechniques to develop Large Multimodal Models (LMMs) with ICL capabilities.\nHowever, applying ICL usually faces two major challenges: 1) using more ICDs\nwill largely increase the inference time and 2) the performance is sensitive to\nthe selection of ICDs. These challenges are further exacerbated in LMMs due to\nthe integration of multiple data types and the combinational complexity of\nmultimodal ICDs. Recently, to address these challenges, some NLP studies\nintroduce non-learnable In-Context Vectors (ICVs) which extract useful task\ninformation from ICDs into a single vector and then insert it into the LLM to\nhelp solve the corresponding task. However, although useful in simple NLP\ntasks, these non-learnable methods fail to handle complex multimodal tasks like\nVisual Question Answering (VQA). In this study, we propose \\textbf{Learnable\nICV} (L-ICV) to distill essential task information from demonstrations,\nimproving ICL performance in LMMs. Experiments show that L-ICV can\nsignificantly reduce computational costs while enhancing accuracy in VQA tasks\ncompared to traditional ICL and other non-learnable ICV methods.",
        "chunk-id": 1,
        "chunk": "As language models continue to scale, Large Language Models (LLMs) have\nexhibited emerging capabilities in In-Context Learning (ICL), enabling them to\nsolve language tasks by prefixing a few in-context demonstrations (ICDs) as\ncontext. Inspired by these advancements, researchers have extended these\ntechniques to develop Large Multimodal Models (LMMs) with ICL capabilities.",
        "authors": [
            "Yingzhe Peng",
            "Chenduo Hao",
            "Xu Yang",
            "Jiawei Peng",
            "Xinting Hu",
            "Xin Geng"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T03:33:45+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13185v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13185v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000066,
        "doi": null,
        "title": "Learnable In-Context Vector for Visual Question Answering",
        "abstract": "As language models continue to scale, Large Language Models (LLMs) have\nexhibited emerging capabilities in In-Context Learning (ICL), enabling them to\nsolve language tasks by prefixing a few in-context demonstrations (ICDs) as\ncontext. Inspired by these advancements, researchers have extended these\ntechniques to develop Large Multimodal Models (LMMs) with ICL capabilities.\nHowever, applying ICL usually faces two major challenges: 1) using more ICDs\nwill largely increase the inference time and 2) the performance is sensitive to\nthe selection of ICDs. These challenges are further exacerbated in LMMs due to\nthe integration of multiple data types and the combinational complexity of\nmultimodal ICDs. Recently, to address these challenges, some NLP studies\nintroduce non-learnable In-Context Vectors (ICVs) which extract useful task\ninformation from ICDs into a single vector and then insert it into the LLM to\nhelp solve the corresponding task. However, although useful in simple NLP\ntasks, these non-learnable methods fail to handle complex multimodal tasks like\nVisual Question Answering (VQA). In this study, we propose \\textbf{Learnable\nICV} (L-ICV) to distill essential task information from demonstrations,\nimproving ICL performance in LMMs. Experiments show that L-ICV can\nsignificantly reduce computational costs while enhancing accuracy in VQA tasks\ncompared to traditional ICL and other non-learnable ICV methods.",
        "chunk-id": 2,
        "chunk": "However, applying ICL usually faces two major challenges: 1) using more ICDs\nwill largely increase the inference time and 2) the performance is sensitive to\nthe selection of ICDs. These challenges are further exacerbated in LMMs due to\nthe integration of multiple data types and the combinational complexity of\nmultimodal ICDs. Recently, to address these challenges, some NLP studies",
        "authors": [
            "Yingzhe Peng",
            "Chenduo Hao",
            "Xu Yang",
            "Jiawei Peng",
            "Xinting Hu",
            "Xin Geng"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T03:33:45+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13185v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13185v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000066,
        "doi": null,
        "title": "Learnable In-Context Vector for Visual Question Answering",
        "abstract": "As language models continue to scale, Large Language Models (LLMs) have\nexhibited emerging capabilities in In-Context Learning (ICL), enabling them to\nsolve language tasks by prefixing a few in-context demonstrations (ICDs) as\ncontext. Inspired by these advancements, researchers have extended these\ntechniques to develop Large Multimodal Models (LMMs) with ICL capabilities.\nHowever, applying ICL usually faces two major challenges: 1) using more ICDs\nwill largely increase the inference time and 2) the performance is sensitive to\nthe selection of ICDs. These challenges are further exacerbated in LMMs due to\nthe integration of multiple data types and the combinational complexity of\nmultimodal ICDs. Recently, to address these challenges, some NLP studies\nintroduce non-learnable In-Context Vectors (ICVs) which extract useful task\ninformation from ICDs into a single vector and then insert it into the LLM to\nhelp solve the corresponding task. However, although useful in simple NLP\ntasks, these non-learnable methods fail to handle complex multimodal tasks like\nVisual Question Answering (VQA). In this study, we propose \\textbf{Learnable\nICV} (L-ICV) to distill essential task information from demonstrations,\nimproving ICL performance in LMMs. Experiments show that L-ICV can\nsignificantly reduce computational costs while enhancing accuracy in VQA tasks\ncompared to traditional ICL and other non-learnable ICV methods.",
        "chunk-id": 3,
        "chunk": "introduce non-learnable In-Context Vectors (ICVs) which extract useful task\ninformation from ICDs into a single vector and then insert it into the LLM to\nhelp solve the corresponding task. However, although useful in simple NLP\ntasks, these non-learnable methods fail to handle complex multimodal tasks like\nVisual Question Answering (VQA). In this study, we propose \\textbf{Learnable",
        "authors": [
            "Yingzhe Peng",
            "Chenduo Hao",
            "Xu Yang",
            "Jiawei Peng",
            "Xinting Hu",
            "Xin Geng"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T03:33:45+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13185v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13185v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000066,
        "doi": null,
        "title": "Learnable In-Context Vector for Visual Question Answering",
        "abstract": "As language models continue to scale, Large Language Models (LLMs) have\nexhibited emerging capabilities in In-Context Learning (ICL), enabling them to\nsolve language tasks by prefixing a few in-context demonstrations (ICDs) as\ncontext. Inspired by these advancements, researchers have extended these\ntechniques to develop Large Multimodal Models (LMMs) with ICL capabilities.\nHowever, applying ICL usually faces two major challenges: 1) using more ICDs\nwill largely increase the inference time and 2) the performance is sensitive to\nthe selection of ICDs. These challenges are further exacerbated in LMMs due to\nthe integration of multiple data types and the combinational complexity of\nmultimodal ICDs. Recently, to address these challenges, some NLP studies\nintroduce non-learnable In-Context Vectors (ICVs) which extract useful task\ninformation from ICDs into a single vector and then insert it into the LLM to\nhelp solve the corresponding task. However, although useful in simple NLP\ntasks, these non-learnable methods fail to handle complex multimodal tasks like\nVisual Question Answering (VQA). In this study, we propose \\textbf{Learnable\nICV} (L-ICV) to distill essential task information from demonstrations,\nimproving ICL performance in LMMs. Experiments show that L-ICV can\nsignificantly reduce computational costs while enhancing accuracy in VQA tasks\ncompared to traditional ICL and other non-learnable ICV methods.",
        "chunk-id": 4,
        "chunk": "ICV} (L-ICV) to distill essential task information from demonstrations,\nimproving ICL performance in LMMs. Experiments show that L-ICV can\nsignificantly reduce computational costs while enhancing accuracy in VQA tasks\ncompared to traditional ICL and other non-learnable ICV methods.",
        "authors": [
            "Yingzhe Peng",
            "Chenduo Hao",
            "Xu Yang",
            "Jiawei Peng",
            "Xinting Hu",
            "Xin Geng"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-19T03:33:45+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13185v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13185v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000067,
        "doi": null,
        "title": "MaskPure: Improving Defense Against Text Adversaries with Stochastic Purification",
        "abstract": "The improvement of language model robustness, including successful defense\nagainst adversarial attacks, remains an open problem. In computer vision\nsettings, the stochastic noising and de-noising process provided by diffusion\nmodels has proven useful for purifying input images, thus improving model\nrobustness against adversarial attacks. Similarly, some initial work has\nexplored the use of random noising and de-noising to mitigate adversarial\nattacks in an NLP setting, but improving the quality and efficiency of these\nmethods is necessary for them to remain competitive. We extend upon methods of\ninput text purification that are inspired by diffusion processes, which\nrandomly mask and refill portions of the input text before classification. Our\nnovel method, MaskPure, exceeds or matches robustness compared to other\ncontemporary defenses, while also requiring no adversarial classifier training\nand without assuming knowledge of the attack type. In addition, we show that\nMaskPure is provably certifiably robust. To our knowledge, MaskPure is the\nfirst stochastic-purification method with demonstrated success against both\ncharacter-level and word-level attacks, indicating the generalizable and\npromising nature of stochastic denoising defenses. In summary: the MaskPure\nalgorithm bridges literature on the current strongest certifiable and empirical\nadversarial defense methods, showing that both theoretical and practical\nrobustness can be obtained together. Code is available on GitHub at\nhttps://github.com/hubarruby/MaskPure.",
        "chunk-id": 1,
        "chunk": "The improvement of language model robustness, including successful defense\nagainst adversarial attacks, remains an open problem. In computer vision\nsettings, the stochastic noising and de-noising process provided by diffusion\nmodels has proven useful for purifying input images, thus improving model\nrobustness against adversarial attacks. Similarly, some initial work has",
        "authors": [
            "Harrison Gietz",
            "Jugal Kalita"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-18T21:27:13+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13066v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13066v1",
        "categories": [
            "Machine Learning",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000067,
        "doi": null,
        "title": "MaskPure: Improving Defense Against Text Adversaries with Stochastic Purification",
        "abstract": "The improvement of language model robustness, including successful defense\nagainst adversarial attacks, remains an open problem. In computer vision\nsettings, the stochastic noising and de-noising process provided by diffusion\nmodels has proven useful for purifying input images, thus improving model\nrobustness against adversarial attacks. Similarly, some initial work has\nexplored the use of random noising and de-noising to mitigate adversarial\nattacks in an NLP setting, but improving the quality and efficiency of these\nmethods is necessary for them to remain competitive. We extend upon methods of\ninput text purification that are inspired by diffusion processes, which\nrandomly mask and refill portions of the input text before classification. Our\nnovel method, MaskPure, exceeds or matches robustness compared to other\ncontemporary defenses, while also requiring no adversarial classifier training\nand without assuming knowledge of the attack type. In addition, we show that\nMaskPure is provably certifiably robust. To our knowledge, MaskPure is the\nfirst stochastic-purification method with demonstrated success against both\ncharacter-level and word-level attacks, indicating the generalizable and\npromising nature of stochastic denoising defenses. In summary: the MaskPure\nalgorithm bridges literature on the current strongest certifiable and empirical\nadversarial defense methods, showing that both theoretical and practical\nrobustness can be obtained together. Code is available on GitHub at\nhttps://github.com/hubarruby/MaskPure.",
        "chunk-id": 2,
        "chunk": "explored the use of random noising and de-noising to mitigate adversarial\nattacks in an NLP setting, but improving the quality and efficiency of these\nmethods is necessary for them to remain competitive. We extend upon methods of\ninput text purification that are inspired by diffusion processes, which\nrandomly mask and refill portions of the input text before classification. Our",
        "authors": [
            "Harrison Gietz",
            "Jugal Kalita"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-18T21:27:13+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13066v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13066v1",
        "categories": [
            "Machine Learning",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000067,
        "doi": null,
        "title": "MaskPure: Improving Defense Against Text Adversaries with Stochastic Purification",
        "abstract": "The improvement of language model robustness, including successful defense\nagainst adversarial attacks, remains an open problem. In computer vision\nsettings, the stochastic noising and de-noising process provided by diffusion\nmodels has proven useful for purifying input images, thus improving model\nrobustness against adversarial attacks. Similarly, some initial work has\nexplored the use of random noising and de-noising to mitigate adversarial\nattacks in an NLP setting, but improving the quality and efficiency of these\nmethods is necessary for them to remain competitive. We extend upon methods of\ninput text purification that are inspired by diffusion processes, which\nrandomly mask and refill portions of the input text before classification. Our\nnovel method, MaskPure, exceeds or matches robustness compared to other\ncontemporary defenses, while also requiring no adversarial classifier training\nand without assuming knowledge of the attack type. In addition, we show that\nMaskPure is provably certifiably robust. To our knowledge, MaskPure is the\nfirst stochastic-purification method with demonstrated success against both\ncharacter-level and word-level attacks, indicating the generalizable and\npromising nature of stochastic denoising defenses. In summary: the MaskPure\nalgorithm bridges literature on the current strongest certifiable and empirical\nadversarial defense methods, showing that both theoretical and practical\nrobustness can be obtained together. Code is available on GitHub at\nhttps://github.com/hubarruby/MaskPure.",
        "chunk-id": 3,
        "chunk": "novel method, MaskPure, exceeds or matches robustness compared to other\ncontemporary defenses, while also requiring no adversarial classifier training\nand without assuming knowledge of the attack type. In addition, we show that\nMaskPure is provably certifiably robust. To our knowledge, MaskPure is the\nfirst stochastic-purification method with demonstrated success against both",
        "authors": [
            "Harrison Gietz",
            "Jugal Kalita"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-18T21:27:13+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13066v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13066v1",
        "categories": [
            "Machine Learning",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000067,
        "doi": null,
        "title": "MaskPure: Improving Defense Against Text Adversaries with Stochastic Purification",
        "abstract": "The improvement of language model robustness, including successful defense\nagainst adversarial attacks, remains an open problem. In computer vision\nsettings, the stochastic noising and de-noising process provided by diffusion\nmodels has proven useful for purifying input images, thus improving model\nrobustness against adversarial attacks. Similarly, some initial work has\nexplored the use of random noising and de-noising to mitigate adversarial\nattacks in an NLP setting, but improving the quality and efficiency of these\nmethods is necessary for them to remain competitive. We extend upon methods of\ninput text purification that are inspired by diffusion processes, which\nrandomly mask and refill portions of the input text before classification. Our\nnovel method, MaskPure, exceeds or matches robustness compared to other\ncontemporary defenses, while also requiring no adversarial classifier training\nand without assuming knowledge of the attack type. In addition, we show that\nMaskPure is provably certifiably robust. To our knowledge, MaskPure is the\nfirst stochastic-purification method with demonstrated success against both\ncharacter-level and word-level attacks, indicating the generalizable and\npromising nature of stochastic denoising defenses. In summary: the MaskPure\nalgorithm bridges literature on the current strongest certifiable and empirical\nadversarial defense methods, showing that both theoretical and practical\nrobustness can be obtained together. Code is available on GitHub at\nhttps://github.com/hubarruby/MaskPure.",
        "chunk-id": 4,
        "chunk": "character-level and word-level attacks, indicating the generalizable and\npromising nature of stochastic denoising defenses. In summary: the MaskPure\nalgorithm bridges literature on the current strongest certifiable and empirical\nadversarial defense methods, showing that both theoretical and practical\nrobustness can be obtained together. Code is available on GitHub at",
        "authors": [
            "Harrison Gietz",
            "Jugal Kalita"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-18T21:27:13+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13066v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13066v1",
        "categories": [
            "Machine Learning",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000067,
        "doi": null,
        "title": "MaskPure: Improving Defense Against Text Adversaries with Stochastic Purification",
        "abstract": "The improvement of language model robustness, including successful defense\nagainst adversarial attacks, remains an open problem. In computer vision\nsettings, the stochastic noising and de-noising process provided by diffusion\nmodels has proven useful for purifying input images, thus improving model\nrobustness against adversarial attacks. Similarly, some initial work has\nexplored the use of random noising and de-noising to mitigate adversarial\nattacks in an NLP setting, but improving the quality and efficiency of these\nmethods is necessary for them to remain competitive. We extend upon methods of\ninput text purification that are inspired by diffusion processes, which\nrandomly mask and refill portions of the input text before classification. Our\nnovel method, MaskPure, exceeds or matches robustness compared to other\ncontemporary defenses, while also requiring no adversarial classifier training\nand without assuming knowledge of the attack type. In addition, we show that\nMaskPure is provably certifiably robust. To our knowledge, MaskPure is the\nfirst stochastic-purification method with demonstrated success against both\ncharacter-level and word-level attacks, indicating the generalizable and\npromising nature of stochastic denoising defenses. In summary: the MaskPure\nalgorithm bridges literature on the current strongest certifiable and empirical\nadversarial defense methods, showing that both theoretical and practical\nrobustness can be obtained together. Code is available on GitHub at\nhttps://github.com/hubarruby/MaskPure.",
        "chunk-id": 5,
        "chunk": "https://github.com/hubarruby/MaskPure.",
        "authors": [
            "Harrison Gietz",
            "Jugal Kalita"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-18T21:27:13+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.13066v1",
        "arxiv_link": "http://arxiv.org/abs/2406.13066v1",
        "categories": [
            "Machine Learning",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000068,
        "doi": null,
        "title": "Using LLMs to Aid Annotation and Collection of Clinically-Enriched Data in Bipolar Disorder and Schizophrenia",
        "abstract": "NLP in mental health has been primarily social media focused. Real world\npractitioners also have high case loads and often domain specific variables, of\nwhich modern LLMs lack context. We take a dataset made by recruiting 644\nparticipants, including individuals diagnosed with Bipolar Disorder (BD),\nSchizophrenia (SZ), and Healthy Controls (HC). Participants undertook tasks\nderived from a standardized mental health instrument, and the resulting data\nwere transcribed and annotated by experts across five clinical variables. This\npaper demonstrates the application of contemporary language models in\nsequence-to-sequence tasks to enhance mental health research. Specifically, we\nillustrate how these models can facilitate the deployment of mental health\ninstruments, data collection, and data annotation with high accuracy and\nscalability. We show that small models are capable of annotation for\ndomain-specific clinical variables, data collection for mental-health\ninstruments, and perform better then commercial large models.",
        "chunk-id": 1,
        "chunk": "NLP in mental health has been primarily social media focused. Real world\npractitioners also have high case loads and often domain specific variables, of\nwhich modern LLMs lack context. We take a dataset made by recruiting 644\nparticipants, including individuals diagnosed with Bipolar Disorder (BD),\nSchizophrenia (SZ), and Healthy Controls (HC). Participants undertook tasks",
        "authors": [
            "Ankit Aich",
            "Avery Quynh",
            "Pamela Osseyi",
            "Amy Pinkham",
            "Philip Harvey",
            "Brenda Curtis",
            "Colin Depp",
            "Natalie Parde"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-18T15:00:24+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12687v1",
        "arxiv_link": "http://arxiv.org/abs/2406.12687v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000068,
        "doi": null,
        "title": "Using LLMs to Aid Annotation and Collection of Clinically-Enriched Data in Bipolar Disorder and Schizophrenia",
        "abstract": "NLP in mental health has been primarily social media focused. Real world\npractitioners also have high case loads and often domain specific variables, of\nwhich modern LLMs lack context. We take a dataset made by recruiting 644\nparticipants, including individuals diagnosed with Bipolar Disorder (BD),\nSchizophrenia (SZ), and Healthy Controls (HC). Participants undertook tasks\nderived from a standardized mental health instrument, and the resulting data\nwere transcribed and annotated by experts across five clinical variables. This\npaper demonstrates the application of contemporary language models in\nsequence-to-sequence tasks to enhance mental health research. Specifically, we\nillustrate how these models can facilitate the deployment of mental health\ninstruments, data collection, and data annotation with high accuracy and\nscalability. We show that small models are capable of annotation for\ndomain-specific clinical variables, data collection for mental-health\ninstruments, and perform better then commercial large models.",
        "chunk-id": 2,
        "chunk": "derived from a standardized mental health instrument, and the resulting data\nwere transcribed and annotated by experts across five clinical variables. This\npaper demonstrates the application of contemporary language models in\nsequence-to-sequence tasks to enhance mental health research. Specifically, we\nillustrate how these models can facilitate the deployment of mental health",
        "authors": [
            "Ankit Aich",
            "Avery Quynh",
            "Pamela Osseyi",
            "Amy Pinkham",
            "Philip Harvey",
            "Brenda Curtis",
            "Colin Depp",
            "Natalie Parde"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-18T15:00:24+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12687v1",
        "arxiv_link": "http://arxiv.org/abs/2406.12687v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000068,
        "doi": null,
        "title": "Using LLMs to Aid Annotation and Collection of Clinically-Enriched Data in Bipolar Disorder and Schizophrenia",
        "abstract": "NLP in mental health has been primarily social media focused. Real world\npractitioners also have high case loads and often domain specific variables, of\nwhich modern LLMs lack context. We take a dataset made by recruiting 644\nparticipants, including individuals diagnosed with Bipolar Disorder (BD),\nSchizophrenia (SZ), and Healthy Controls (HC). Participants undertook tasks\nderived from a standardized mental health instrument, and the resulting data\nwere transcribed and annotated by experts across five clinical variables. This\npaper demonstrates the application of contemporary language models in\nsequence-to-sequence tasks to enhance mental health research. Specifically, we\nillustrate how these models can facilitate the deployment of mental health\ninstruments, data collection, and data annotation with high accuracy and\nscalability. We show that small models are capable of annotation for\ndomain-specific clinical variables, data collection for mental-health\ninstruments, and perform better then commercial large models.",
        "chunk-id": 3,
        "chunk": "instruments, data collection, and data annotation with high accuracy and\nscalability. We show that small models are capable of annotation for\ndomain-specific clinical variables, data collection for mental-health\ninstruments, and perform better then commercial large models.",
        "authors": [
            "Ankit Aich",
            "Avery Quynh",
            "Pamela Osseyi",
            "Amy Pinkham",
            "Philip Harvey",
            "Brenda Curtis",
            "Colin Depp",
            "Natalie Parde"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-18T15:00:24+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12687v1",
        "arxiv_link": "http://arxiv.org/abs/2406.12687v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000069,
        "doi": null,
        "title": "From Insights to Actions: The Impact of Interpretability and Analysis Research on NLP",
        "abstract": "Interpretability and analysis (IA) research is a growing subfield within NLP\nwith the goal of developing a deeper understanding of the behavior or inner\nworkings of NLP systems and methods. Despite growing interest in the subfield,\na commonly voiced criticism is that it lacks actionable insights and therefore\nhas little impact on NLP. In this paper, we seek to quantify the impact of IA\nresearch on the broader field of NLP. We approach this with a mixed-methods\nanalysis of: (1) a citation graph of 185K+ papers built from all papers\npublished at ACL and EMNLP conferences from 2018 to 2023, and (2) a survey of\n138 members of the NLP community. Our quantitative results show that IA work is\nwell-cited outside of IA, and central in the NLP citation graph. Through\nqualitative analysis of survey responses and manual annotation of 556 papers,\nwe find that NLP researchers build on findings from IA work and perceive it is\nimportant for progress in NLP, multiple subfields, and rely on its findings and\nterminology for their own work. Many novel methods are proposed based on IA\nfindings and highly influenced by them, but highly influential non-IA work\ncites IA findings without being driven by them. We end by summarizing what is\nmissing in IA work today and provide a call to action, to pave the way for a\nmore impactful future of IA research.",
        "chunk-id": 1,
        "chunk": "Interpretability and analysis (IA) research is a growing subfield within NLP\nwith the goal of developing a deeper understanding of the behavior or inner\nworkings of NLP systems and methods. Despite growing interest in the subfield,\na commonly voiced criticism is that it lacks actionable insights and therefore",
        "authors": [
            "Marius Mosbach",
            "Vagrant Gautam",
            "Tom\u00e1s Vergara-Browne",
            "Dietrich Klakow",
            "Mor Geva"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-18T13:45:07+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12618v1",
        "arxiv_link": "http://arxiv.org/abs/2406.12618v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000069,
        "doi": null,
        "title": "From Insights to Actions: The Impact of Interpretability and Analysis Research on NLP",
        "abstract": "Interpretability and analysis (IA) research is a growing subfield within NLP\nwith the goal of developing a deeper understanding of the behavior or inner\nworkings of NLP systems and methods. Despite growing interest in the subfield,\na commonly voiced criticism is that it lacks actionable insights and therefore\nhas little impact on NLP. In this paper, we seek to quantify the impact of IA\nresearch on the broader field of NLP. We approach this with a mixed-methods\nanalysis of: (1) a citation graph of 185K+ papers built from all papers\npublished at ACL and EMNLP conferences from 2018 to 2023, and (2) a survey of\n138 members of the NLP community. Our quantitative results show that IA work is\nwell-cited outside of IA, and central in the NLP citation graph. Through\nqualitative analysis of survey responses and manual annotation of 556 papers,\nwe find that NLP researchers build on findings from IA work and perceive it is\nimportant for progress in NLP, multiple subfields, and rely on its findings and\nterminology for their own work. Many novel methods are proposed based on IA\nfindings and highly influenced by them, but highly influential non-IA work\ncites IA findings without being driven by them. We end by summarizing what is\nmissing in IA work today and provide a call to action, to pave the way for a\nmore impactful future of IA research.",
        "chunk-id": 2,
        "chunk": "has little impact on NLP. In this paper, we seek to quantify the impact of IA\nresearch on the broader field of NLP. We approach this with a mixed-methods\nanalysis of: (1) a citation graph of 185K+ papers built from all papers\npublished at ACL and EMNLP conferences from 2018 to 2023, and (2) a survey of\n138 members of the NLP community. Our quantitative results show that IA work is",
        "authors": [
            "Marius Mosbach",
            "Vagrant Gautam",
            "Tom\u00e1s Vergara-Browne",
            "Dietrich Klakow",
            "Mor Geva"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-18T13:45:07+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12618v1",
        "arxiv_link": "http://arxiv.org/abs/2406.12618v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000069,
        "doi": null,
        "title": "From Insights to Actions: The Impact of Interpretability and Analysis Research on NLP",
        "abstract": "Interpretability and analysis (IA) research is a growing subfield within NLP\nwith the goal of developing a deeper understanding of the behavior or inner\nworkings of NLP systems and methods. Despite growing interest in the subfield,\na commonly voiced criticism is that it lacks actionable insights and therefore\nhas little impact on NLP. In this paper, we seek to quantify the impact of IA\nresearch on the broader field of NLP. We approach this with a mixed-methods\nanalysis of: (1) a citation graph of 185K+ papers built from all papers\npublished at ACL and EMNLP conferences from 2018 to 2023, and (2) a survey of\n138 members of the NLP community. Our quantitative results show that IA work is\nwell-cited outside of IA, and central in the NLP citation graph. Through\nqualitative analysis of survey responses and manual annotation of 556 papers,\nwe find that NLP researchers build on findings from IA work and perceive it is\nimportant for progress in NLP, multiple subfields, and rely on its findings and\nterminology for their own work. Many novel methods are proposed based on IA\nfindings and highly influenced by them, but highly influential non-IA work\ncites IA findings without being driven by them. We end by summarizing what is\nmissing in IA work today and provide a call to action, to pave the way for a\nmore impactful future of IA research.",
        "chunk-id": 3,
        "chunk": "well-cited outside of IA, and central in the NLP citation graph. Through\nqualitative analysis of survey responses and manual annotation of 556 papers,\nwe find that NLP researchers build on findings from IA work and perceive it is\nimportant for progress in NLP, multiple subfields, and rely on its findings and",
        "authors": [
            "Marius Mosbach",
            "Vagrant Gautam",
            "Tom\u00e1s Vergara-Browne",
            "Dietrich Klakow",
            "Mor Geva"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-18T13:45:07+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12618v1",
        "arxiv_link": "http://arxiv.org/abs/2406.12618v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000069,
        "doi": null,
        "title": "From Insights to Actions: The Impact of Interpretability and Analysis Research on NLP",
        "abstract": "Interpretability and analysis (IA) research is a growing subfield within NLP\nwith the goal of developing a deeper understanding of the behavior or inner\nworkings of NLP systems and methods. Despite growing interest in the subfield,\na commonly voiced criticism is that it lacks actionable insights and therefore\nhas little impact on NLP. In this paper, we seek to quantify the impact of IA\nresearch on the broader field of NLP. We approach this with a mixed-methods\nanalysis of: (1) a citation graph of 185K+ papers built from all papers\npublished at ACL and EMNLP conferences from 2018 to 2023, and (2) a survey of\n138 members of the NLP community. Our quantitative results show that IA work is\nwell-cited outside of IA, and central in the NLP citation graph. Through\nqualitative analysis of survey responses and manual annotation of 556 papers,\nwe find that NLP researchers build on findings from IA work and perceive it is\nimportant for progress in NLP, multiple subfields, and rely on its findings and\nterminology for their own work. Many novel methods are proposed based on IA\nfindings and highly influenced by them, but highly influential non-IA work\ncites IA findings without being driven by them. We end by summarizing what is\nmissing in IA work today and provide a call to action, to pave the way for a\nmore impactful future of IA research.",
        "chunk-id": 4,
        "chunk": "terminology for their own work. Many novel methods are proposed based on IA\nfindings and highly influenced by them, but highly influential non-IA work\ncites IA findings without being driven by them. We end by summarizing what is\nmissing in IA work today and provide a call to action, to pave the way for a\nmore impactful future of IA research.",
        "authors": [
            "Marius Mosbach",
            "Vagrant Gautam",
            "Tom\u00e1s Vergara-Browne",
            "Dietrich Klakow",
            "Mor Geva"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-18T13:45:07+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12618v1",
        "arxiv_link": "http://arxiv.org/abs/2406.12618v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000070,
        "doi": null,
        "title": "QueerBench: Quantifying Discrimination in Language Models Toward Queer Identities",
        "abstract": "With the increasing role of Natural Language Processing (NLP) in various\napplications, challenges concerning bias and stereotype perpetuation are\naccentuated, which often leads to hate speech and harm. Despite existing\nstudies on sexism and misogyny, issues like homophobia and transphobia remain\nunderexplored and often adopt binary perspectives, putting the safety of\nLGBTQIA+ individuals at high risk in online spaces. In this paper, we assess\nthe potential harm caused by sentence completions generated by English large\nlanguage models (LLMs) concerning LGBTQIA+ individuals. This is achieved using\nQueerBench, our new assessment framework, which employs a template-based\napproach and a Masked Language Modeling (MLM) task. The analysis indicates that\nlarge language models tend to exhibit discriminatory behaviour more frequently\ntowards individuals within the LGBTQIA+ community, reaching a difference gap of\n7.2% in the QueerBench score of harmfulness.",
        "chunk-id": 1,
        "chunk": "With the increasing role of Natural Language Processing (NLP) in various\napplications, challenges concerning bias and stereotype perpetuation are\naccentuated, which often leads to hate speech and harm. Despite existing\nstudies on sexism and misogyny, issues like homophobia and transphobia remain\nunderexplored and often adopt binary perspectives, putting the safety of",
        "authors": [
            "Mae Sosto",
            "Alberto Barr\u00f3n-Cede\u00f1o"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-18T08:40:29+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12399v1",
        "arxiv_link": "http://arxiv.org/abs/2406.12399v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Computers and Society"
        ]
    },
    {
        "id": 20000070,
        "doi": null,
        "title": "QueerBench: Quantifying Discrimination in Language Models Toward Queer Identities",
        "abstract": "With the increasing role of Natural Language Processing (NLP) in various\napplications, challenges concerning bias and stereotype perpetuation are\naccentuated, which often leads to hate speech and harm. Despite existing\nstudies on sexism and misogyny, issues like homophobia and transphobia remain\nunderexplored and often adopt binary perspectives, putting the safety of\nLGBTQIA+ individuals at high risk in online spaces. In this paper, we assess\nthe potential harm caused by sentence completions generated by English large\nlanguage models (LLMs) concerning LGBTQIA+ individuals. This is achieved using\nQueerBench, our new assessment framework, which employs a template-based\napproach and a Masked Language Modeling (MLM) task. The analysis indicates that\nlarge language models tend to exhibit discriminatory behaviour more frequently\ntowards individuals within the LGBTQIA+ community, reaching a difference gap of\n7.2% in the QueerBench score of harmfulness.",
        "chunk-id": 2,
        "chunk": "LGBTQIA+ individuals at high risk in online spaces. In this paper, we assess\nthe potential harm caused by sentence completions generated by English large\nlanguage models (LLMs) concerning LGBTQIA+ individuals. This is achieved using\nQueerBench, our new assessment framework, which employs a template-based",
        "authors": [
            "Mae Sosto",
            "Alberto Barr\u00f3n-Cede\u00f1o"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-18T08:40:29+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12399v1",
        "arxiv_link": "http://arxiv.org/abs/2406.12399v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Computers and Society"
        ]
    },
    {
        "id": 20000070,
        "doi": null,
        "title": "QueerBench: Quantifying Discrimination in Language Models Toward Queer Identities",
        "abstract": "With the increasing role of Natural Language Processing (NLP) in various\napplications, challenges concerning bias and stereotype perpetuation are\naccentuated, which often leads to hate speech and harm. Despite existing\nstudies on sexism and misogyny, issues like homophobia and transphobia remain\nunderexplored and often adopt binary perspectives, putting the safety of\nLGBTQIA+ individuals at high risk in online spaces. In this paper, we assess\nthe potential harm caused by sentence completions generated by English large\nlanguage models (LLMs) concerning LGBTQIA+ individuals. This is achieved using\nQueerBench, our new assessment framework, which employs a template-based\napproach and a Masked Language Modeling (MLM) task. The analysis indicates that\nlarge language models tend to exhibit discriminatory behaviour more frequently\ntowards individuals within the LGBTQIA+ community, reaching a difference gap of\n7.2% in the QueerBench score of harmfulness.",
        "chunk-id": 3,
        "chunk": "approach and a Masked Language Modeling (MLM) task. The analysis indicates that\nlarge language models tend to exhibit discriminatory behaviour more frequently\ntowards individuals within the LGBTQIA+ community, reaching a difference gap of\n7.2% in the QueerBench score of harmfulness.",
        "authors": [
            "Mae Sosto",
            "Alberto Barr\u00f3n-Cede\u00f1o"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-18T08:40:29+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12399v1",
        "arxiv_link": "http://arxiv.org/abs/2406.12399v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Computers and Society"
        ]
    },
    {
        "id": 20000071,
        "doi": null,
        "title": "Interpreting Bias in Large Language Models: A Feature-Based Approach",
        "abstract": "Large Language Models (LLMs) such as Mistral and LLaMA have showcased\nremarkable performance across various natural language processing (NLP) tasks.\nDespite their success, these models inherit social biases from the diverse\ndatasets on which they are trained. This paper investigates the propagation of\nbiases within LLMs through a novel feature-based analytical approach. Drawing\ninspiration from causal mediation analysis, we hypothesize the evolution of\nbias-related features and validate them using interpretability techniques like\nactivation and attribution patching. Our contributions are threefold: (1) We\nintroduce and empirically validate a feature-based method for bias analysis in\nLLMs, applied to LLaMA-2-7B, LLaMA-3-8B, and Mistral-7B-v0.3 with templates\nfrom a professions dataset. (2) We extend our method to another form of gender\nbias, demonstrating its generalizability. (3) We differentiate the roles of\nMLPs and attention heads in bias propagation and implement targeted debiasing\nusing a counterfactual dataset. Our findings reveal the complex nature of bias\nin LLMs and emphasize the necessity for tailored debiasing strategies, offering\na deeper understanding of bias mechanisms and pathways for effective\nmitigation.",
        "chunk-id": 1,
        "chunk": "Large Language Models (LLMs) such as Mistral and LLaMA have showcased\nremarkable performance across various natural language processing (NLP) tasks.\nDespite their success, these models inherit social biases from the diverse\ndatasets on which they are trained. This paper investigates the propagation of\nbiases within LLMs through a novel feature-based analytical approach. Drawing",
        "authors": [
            "Nirmalendu Prakash",
            "Lee Ka Wei Roy"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-18T07:28:15+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12347v1",
        "arxiv_link": "http://arxiv.org/abs/2406.12347v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000071,
        "doi": null,
        "title": "Interpreting Bias in Large Language Models: A Feature-Based Approach",
        "abstract": "Large Language Models (LLMs) such as Mistral and LLaMA have showcased\nremarkable performance across various natural language processing (NLP) tasks.\nDespite their success, these models inherit social biases from the diverse\ndatasets on which they are trained. This paper investigates the propagation of\nbiases within LLMs through a novel feature-based analytical approach. Drawing\ninspiration from causal mediation analysis, we hypothesize the evolution of\nbias-related features and validate them using interpretability techniques like\nactivation and attribution patching. Our contributions are threefold: (1) We\nintroduce and empirically validate a feature-based method for bias analysis in\nLLMs, applied to LLaMA-2-7B, LLaMA-3-8B, and Mistral-7B-v0.3 with templates\nfrom a professions dataset. (2) We extend our method to another form of gender\nbias, demonstrating its generalizability. (3) We differentiate the roles of\nMLPs and attention heads in bias propagation and implement targeted debiasing\nusing a counterfactual dataset. Our findings reveal the complex nature of bias\nin LLMs and emphasize the necessity for tailored debiasing strategies, offering\na deeper understanding of bias mechanisms and pathways for effective\nmitigation.",
        "chunk-id": 2,
        "chunk": "inspiration from causal mediation analysis, we hypothesize the evolution of\nbias-related features and validate them using interpretability techniques like\nactivation and attribution patching. Our contributions are threefold: (1) We\nintroduce and empirically validate a feature-based method for bias analysis in",
        "authors": [
            "Nirmalendu Prakash",
            "Lee Ka Wei Roy"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-18T07:28:15+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12347v1",
        "arxiv_link": "http://arxiv.org/abs/2406.12347v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000071,
        "doi": null,
        "title": "Interpreting Bias in Large Language Models: A Feature-Based Approach",
        "abstract": "Large Language Models (LLMs) such as Mistral and LLaMA have showcased\nremarkable performance across various natural language processing (NLP) tasks.\nDespite their success, these models inherit social biases from the diverse\ndatasets on which they are trained. This paper investigates the propagation of\nbiases within LLMs through a novel feature-based analytical approach. Drawing\ninspiration from causal mediation analysis, we hypothesize the evolution of\nbias-related features and validate them using interpretability techniques like\nactivation and attribution patching. Our contributions are threefold: (1) We\nintroduce and empirically validate a feature-based method for bias analysis in\nLLMs, applied to LLaMA-2-7B, LLaMA-3-8B, and Mistral-7B-v0.3 with templates\nfrom a professions dataset. (2) We extend our method to another form of gender\nbias, demonstrating its generalizability. (3) We differentiate the roles of\nMLPs and attention heads in bias propagation and implement targeted debiasing\nusing a counterfactual dataset. Our findings reveal the complex nature of bias\nin LLMs and emphasize the necessity for tailored debiasing strategies, offering\na deeper understanding of bias mechanisms and pathways for effective\nmitigation.",
        "chunk-id": 3,
        "chunk": "LLMs, applied to LLaMA-2-7B, LLaMA-3-8B, and Mistral-7B-v0.3 with templates\nfrom a professions dataset. (2) We extend our method to another form of gender\nbias, demonstrating its generalizability. (3) We differentiate the roles of\nMLPs and attention heads in bias propagation and implement targeted debiasing",
        "authors": [
            "Nirmalendu Prakash",
            "Lee Ka Wei Roy"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-18T07:28:15+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12347v1",
        "arxiv_link": "http://arxiv.org/abs/2406.12347v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000071,
        "doi": null,
        "title": "Interpreting Bias in Large Language Models: A Feature-Based Approach",
        "abstract": "Large Language Models (LLMs) such as Mistral and LLaMA have showcased\nremarkable performance across various natural language processing (NLP) tasks.\nDespite their success, these models inherit social biases from the diverse\ndatasets on which they are trained. This paper investigates the propagation of\nbiases within LLMs through a novel feature-based analytical approach. Drawing\ninspiration from causal mediation analysis, we hypothesize the evolution of\nbias-related features and validate them using interpretability techniques like\nactivation and attribution patching. Our contributions are threefold: (1) We\nintroduce and empirically validate a feature-based method for bias analysis in\nLLMs, applied to LLaMA-2-7B, LLaMA-3-8B, and Mistral-7B-v0.3 with templates\nfrom a professions dataset. (2) We extend our method to another form of gender\nbias, demonstrating its generalizability. (3) We differentiate the roles of\nMLPs and attention heads in bias propagation and implement targeted debiasing\nusing a counterfactual dataset. Our findings reveal the complex nature of bias\nin LLMs and emphasize the necessity for tailored debiasing strategies, offering\na deeper understanding of bias mechanisms and pathways for effective\nmitigation.",
        "chunk-id": 4,
        "chunk": "using a counterfactual dataset. Our findings reveal the complex nature of bias\nin LLMs and emphasize the necessity for tailored debiasing strategies, offering\na deeper understanding of bias mechanisms and pathways for effective\nmitigation.",
        "authors": [
            "Nirmalendu Prakash",
            "Lee Ka Wei Roy"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-18T07:28:15+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12347v1",
        "arxiv_link": "http://arxiv.org/abs/2406.12347v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000072,
        "doi": null,
        "title": "SNAP: Unlearning Selective Knowledge in Large Language Models with Negative Instructions",
        "abstract": "Instruction-following large language models (LLMs), such as ChatGPT, have\nbecome increasingly popular with the general audience, many of whom are\nincorporating them into their daily routines. However, these LLMs inadvertently\ndisclose personal or copyrighted information, which calls for a machine\nunlearning method to remove selective knowledge. Previous attempts sought to\nforget the link between the target information and its associated entities, but\nit rather led to generating undesirable responses about the target,\ncompromising the end-user experience. In this work, we propose SNAP, an\ninnovative framework designed to selectively unlearn information by 1) training\nan LLM with negative instructions to generate obliterated responses, 2)\naugmenting hard positives to retain the original LLM performance, and 3)\napplying the novel Wasserstein regularization to ensure adequate deviation from\nthe initial weights of the LLM. We evaluate our framework on various NLP\nbenchmarks and demonstrate that our approach retains the original LLM\ncapabilities, while successfully unlearning the specified information.",
        "chunk-id": 1,
        "chunk": "Instruction-following large language models (LLMs), such as ChatGPT, have\nbecome increasingly popular with the general audience, many of whom are\nincorporating them into their daily routines. However, these LLMs inadvertently\ndisclose personal or copyrighted information, which calls for a machine\nunlearning method to remove selective knowledge. Previous attempts sought to",
        "authors": [
            "Minseok Choi",
            "Daniel Rim",
            "Dohyun Lee",
            "Jaegul Choo"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-18T06:54:05+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12329v1",
        "arxiv_link": "http://arxiv.org/abs/2406.12329v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000072,
        "doi": null,
        "title": "SNAP: Unlearning Selective Knowledge in Large Language Models with Negative Instructions",
        "abstract": "Instruction-following large language models (LLMs), such as ChatGPT, have\nbecome increasingly popular with the general audience, many of whom are\nincorporating them into their daily routines. However, these LLMs inadvertently\ndisclose personal or copyrighted information, which calls for a machine\nunlearning method to remove selective knowledge. Previous attempts sought to\nforget the link between the target information and its associated entities, but\nit rather led to generating undesirable responses about the target,\ncompromising the end-user experience. In this work, we propose SNAP, an\ninnovative framework designed to selectively unlearn information by 1) training\nan LLM with negative instructions to generate obliterated responses, 2)\naugmenting hard positives to retain the original LLM performance, and 3)\napplying the novel Wasserstein regularization to ensure adequate deviation from\nthe initial weights of the LLM. We evaluate our framework on various NLP\nbenchmarks and demonstrate that our approach retains the original LLM\ncapabilities, while successfully unlearning the specified information.",
        "chunk-id": 2,
        "chunk": "forget the link between the target information and its associated entities, but\nit rather led to generating undesirable responses about the target,\ncompromising the end-user experience. In this work, we propose SNAP, an\ninnovative framework designed to selectively unlearn information by 1) training\nan LLM with negative instructions to generate obliterated responses, 2)",
        "authors": [
            "Minseok Choi",
            "Daniel Rim",
            "Dohyun Lee",
            "Jaegul Choo"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-18T06:54:05+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12329v1",
        "arxiv_link": "http://arxiv.org/abs/2406.12329v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000072,
        "doi": null,
        "title": "SNAP: Unlearning Selective Knowledge in Large Language Models with Negative Instructions",
        "abstract": "Instruction-following large language models (LLMs), such as ChatGPT, have\nbecome increasingly popular with the general audience, many of whom are\nincorporating them into their daily routines. However, these LLMs inadvertently\ndisclose personal or copyrighted information, which calls for a machine\nunlearning method to remove selective knowledge. Previous attempts sought to\nforget the link between the target information and its associated entities, but\nit rather led to generating undesirable responses about the target,\ncompromising the end-user experience. In this work, we propose SNAP, an\ninnovative framework designed to selectively unlearn information by 1) training\nan LLM with negative instructions to generate obliterated responses, 2)\naugmenting hard positives to retain the original LLM performance, and 3)\napplying the novel Wasserstein regularization to ensure adequate deviation from\nthe initial weights of the LLM. We evaluate our framework on various NLP\nbenchmarks and demonstrate that our approach retains the original LLM\ncapabilities, while successfully unlearning the specified information.",
        "chunk-id": 3,
        "chunk": "augmenting hard positives to retain the original LLM performance, and 3)\napplying the novel Wasserstein regularization to ensure adequate deviation from\nthe initial weights of the LLM. We evaluate our framework on various NLP\nbenchmarks and demonstrate that our approach retains the original LLM\ncapabilities, while successfully unlearning the specified information.",
        "authors": [
            "Minseok Choi",
            "Daniel Rim",
            "Dohyun Lee",
            "Jaegul Choo"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-18T06:54:05+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12329v1",
        "arxiv_link": "http://arxiv.org/abs/2406.12329v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000073,
        "doi": null,
        "title": "Language and Multimodal Models in Sports: A Survey of Datasets and Applications",
        "abstract": "Recent integration of Natural Language Processing (NLP) and multimodal models\nhas advanced the field of sports analytics. This survey presents a\ncomprehensive review of the datasets and applications driving these innovations\npost-2020. We overviewed and categorized datasets into three primary types:\nlanguage-based, multimodal, and convertible datasets. Language-based and\nmultimodal datasets are for tasks involving text or multimodality (e.g., text,\nvideo, audio), respectively. Convertible datasets, initially single-modal\n(video), can be enriched with additional annotations, such as explanations of\nactions and video descriptions, to become multimodal, offering future potential\nfor richer and more diverse applications. Our study highlights the\ncontributions of these datasets to various applications, from improving fan\nexperiences to supporting tactical analysis and medical diagnostics. We also\ndiscuss the challenges and future directions in dataset development,\nemphasizing the need for diverse, high-quality data to support real-time\nprocessing and personalized user experiences. This survey provides a\nfoundational resource for researchers and practitioners aiming to leverage NLP\nand multimodal models in sports, offering insights into current trends and\nfuture opportunities in the field.",
        "chunk-id": 1,
        "chunk": "Recent integration of Natural Language Processing (NLP) and multimodal models\nhas advanced the field of sports analytics. This survey presents a\ncomprehensive review of the datasets and applications driving these innovations\npost-2020. We overviewed and categorized datasets into three primary types:\nlanguage-based, multimodal, and convertible datasets. Language-based and",
        "authors": [
            "Haotian Xia",
            "Zhengbang Yang",
            "Yun Zhao",
            "Yuqing Wang",
            "Jingxi Li",
            "Rhys Tracy",
            "Zhuangdi Zhu",
            "Yuan-fang Wang",
            "Hanjie Chen",
            "Weining Shen"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-18T03:59:26+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12252v1",
        "arxiv_link": "http://arxiv.org/abs/2406.12252v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000073,
        "doi": null,
        "title": "Language and Multimodal Models in Sports: A Survey of Datasets and Applications",
        "abstract": "Recent integration of Natural Language Processing (NLP) and multimodal models\nhas advanced the field of sports analytics. This survey presents a\ncomprehensive review of the datasets and applications driving these innovations\npost-2020. We overviewed and categorized datasets into three primary types:\nlanguage-based, multimodal, and convertible datasets. Language-based and\nmultimodal datasets are for tasks involving text or multimodality (e.g., text,\nvideo, audio), respectively. Convertible datasets, initially single-modal\n(video), can be enriched with additional annotations, such as explanations of\nactions and video descriptions, to become multimodal, offering future potential\nfor richer and more diverse applications. Our study highlights the\ncontributions of these datasets to various applications, from improving fan\nexperiences to supporting tactical analysis and medical diagnostics. We also\ndiscuss the challenges and future directions in dataset development,\nemphasizing the need for diverse, high-quality data to support real-time\nprocessing and personalized user experiences. This survey provides a\nfoundational resource for researchers and practitioners aiming to leverage NLP\nand multimodal models in sports, offering insights into current trends and\nfuture opportunities in the field.",
        "chunk-id": 2,
        "chunk": "multimodal datasets are for tasks involving text or multimodality (e.g., text,\nvideo, audio), respectively. Convertible datasets, initially single-modal\n(video), can be enriched with additional annotations, such as explanations of\nactions and video descriptions, to become multimodal, offering future potential\nfor richer and more diverse applications. Our study highlights the",
        "authors": [
            "Haotian Xia",
            "Zhengbang Yang",
            "Yun Zhao",
            "Yuqing Wang",
            "Jingxi Li",
            "Rhys Tracy",
            "Zhuangdi Zhu",
            "Yuan-fang Wang",
            "Hanjie Chen",
            "Weining Shen"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-18T03:59:26+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12252v1",
        "arxiv_link": "http://arxiv.org/abs/2406.12252v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000073,
        "doi": null,
        "title": "Language and Multimodal Models in Sports: A Survey of Datasets and Applications",
        "abstract": "Recent integration of Natural Language Processing (NLP) and multimodal models\nhas advanced the field of sports analytics. This survey presents a\ncomprehensive review of the datasets and applications driving these innovations\npost-2020. We overviewed and categorized datasets into three primary types:\nlanguage-based, multimodal, and convertible datasets. Language-based and\nmultimodal datasets are for tasks involving text or multimodality (e.g., text,\nvideo, audio), respectively. Convertible datasets, initially single-modal\n(video), can be enriched with additional annotations, such as explanations of\nactions and video descriptions, to become multimodal, offering future potential\nfor richer and more diverse applications. Our study highlights the\ncontributions of these datasets to various applications, from improving fan\nexperiences to supporting tactical analysis and medical diagnostics. We also\ndiscuss the challenges and future directions in dataset development,\nemphasizing the need for diverse, high-quality data to support real-time\nprocessing and personalized user experiences. This survey provides a\nfoundational resource for researchers and practitioners aiming to leverage NLP\nand multimodal models in sports, offering insights into current trends and\nfuture opportunities in the field.",
        "chunk-id": 3,
        "chunk": "contributions of these datasets to various applications, from improving fan\nexperiences to supporting tactical analysis and medical diagnostics. We also\ndiscuss the challenges and future directions in dataset development,\nemphasizing the need for diverse, high-quality data to support real-time\nprocessing and personalized user experiences. This survey provides a",
        "authors": [
            "Haotian Xia",
            "Zhengbang Yang",
            "Yun Zhao",
            "Yuqing Wang",
            "Jingxi Li",
            "Rhys Tracy",
            "Zhuangdi Zhu",
            "Yuan-fang Wang",
            "Hanjie Chen",
            "Weining Shen"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-18T03:59:26+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12252v1",
        "arxiv_link": "http://arxiv.org/abs/2406.12252v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000073,
        "doi": null,
        "title": "Language and Multimodal Models in Sports: A Survey of Datasets and Applications",
        "abstract": "Recent integration of Natural Language Processing (NLP) and multimodal models\nhas advanced the field of sports analytics. This survey presents a\ncomprehensive review of the datasets and applications driving these innovations\npost-2020. We overviewed and categorized datasets into three primary types:\nlanguage-based, multimodal, and convertible datasets. Language-based and\nmultimodal datasets are for tasks involving text or multimodality (e.g., text,\nvideo, audio), respectively. Convertible datasets, initially single-modal\n(video), can be enriched with additional annotations, such as explanations of\nactions and video descriptions, to become multimodal, offering future potential\nfor richer and more diverse applications. Our study highlights the\ncontributions of these datasets to various applications, from improving fan\nexperiences to supporting tactical analysis and medical diagnostics. We also\ndiscuss the challenges and future directions in dataset development,\nemphasizing the need for diverse, high-quality data to support real-time\nprocessing and personalized user experiences. This survey provides a\nfoundational resource for researchers and practitioners aiming to leverage NLP\nand multimodal models in sports, offering insights into current trends and\nfuture opportunities in the field.",
        "chunk-id": 4,
        "chunk": "foundational resource for researchers and practitioners aiming to leverage NLP\nand multimodal models in sports, offering insights into current trends and\nfuture opportunities in the field.",
        "authors": [
            "Haotian Xia",
            "Zhengbang Yang",
            "Yun Zhao",
            "Yuqing Wang",
            "Jingxi Li",
            "Rhys Tracy",
            "Zhuangdi Zhu",
            "Yuan-fang Wang",
            "Hanjie Chen",
            "Weining Shen"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-18T03:59:26+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12252v1",
        "arxiv_link": "http://arxiv.org/abs/2406.12252v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000074,
        "doi": null,
        "title": "MCSD: An Efficient Language Model with Diverse Fusion",
        "abstract": "Transformers excel in Natural Language Processing (NLP) due to their prowess\nin capturing long-term dependencies but suffer from exponential resource\nconsumption with increasing sequence lengths. To address these challenges, we\npropose MCSD model, an efficient language model with linear scaling and fast\ninference speed. MCSD model leverages diverse feature fusion, primarily through\nthe multi-channel slope and decay (MCSD) block, to robustly represent features.\nThis block comprises slope and decay sections that extract features across\ndiverse temporal receptive fields, facilitating capture of both local and\nglobal information. In addition, MCSD block conducts element-wise fusion of\ndiverse features to further enhance the delicate feature extraction capability.\nFor inference, we formulate the inference process into a recurrent\nrepresentation, slashing space complexity to $O(1)$ and time complexity to\n$O(N)$ respectively. Our experiments show that MCSD attains higher throughput\nand lower GPU memory consumption compared to Transformers, while maintaining\ncomparable performance to larger-scale language learning models on benchmark\ntests. These attributes position MCSD as a promising base for edge deployment\nand embodied intelligence.",
        "chunk-id": 1,
        "chunk": "Transformers excel in Natural Language Processing (NLP) due to their prowess\nin capturing long-term dependencies but suffer from exponential resource\nconsumption with increasing sequence lengths. To address these challenges, we\npropose MCSD model, an efficient language model with linear scaling and fast\ninference speed. MCSD model leverages diverse feature fusion, primarily through",
        "authors": [
            "Hua Yang",
            "Duohai Li",
            "Shiman Li"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-18T03:08:01+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12230v1",
        "arxiv_link": "http://arxiv.org/abs/2406.12230v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000074,
        "doi": null,
        "title": "MCSD: An Efficient Language Model with Diverse Fusion",
        "abstract": "Transformers excel in Natural Language Processing (NLP) due to their prowess\nin capturing long-term dependencies but suffer from exponential resource\nconsumption with increasing sequence lengths. To address these challenges, we\npropose MCSD model, an efficient language model with linear scaling and fast\ninference speed. MCSD model leverages diverse feature fusion, primarily through\nthe multi-channel slope and decay (MCSD) block, to robustly represent features.\nThis block comprises slope and decay sections that extract features across\ndiverse temporal receptive fields, facilitating capture of both local and\nglobal information. In addition, MCSD block conducts element-wise fusion of\ndiverse features to further enhance the delicate feature extraction capability.\nFor inference, we formulate the inference process into a recurrent\nrepresentation, slashing space complexity to $O(1)$ and time complexity to\n$O(N)$ respectively. Our experiments show that MCSD attains higher throughput\nand lower GPU memory consumption compared to Transformers, while maintaining\ncomparable performance to larger-scale language learning models on benchmark\ntests. These attributes position MCSD as a promising base for edge deployment\nand embodied intelligence.",
        "chunk-id": 2,
        "chunk": "the multi-channel slope and decay (MCSD) block, to robustly represent features.\nThis block comprises slope and decay sections that extract features across\ndiverse temporal receptive fields, facilitating capture of both local and\nglobal information. In addition, MCSD block conducts element-wise fusion of\ndiverse features to further enhance the delicate feature extraction capability.",
        "authors": [
            "Hua Yang",
            "Duohai Li",
            "Shiman Li"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-18T03:08:01+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12230v1",
        "arxiv_link": "http://arxiv.org/abs/2406.12230v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000074,
        "doi": null,
        "title": "MCSD: An Efficient Language Model with Diverse Fusion",
        "abstract": "Transformers excel in Natural Language Processing (NLP) due to their prowess\nin capturing long-term dependencies but suffer from exponential resource\nconsumption with increasing sequence lengths. To address these challenges, we\npropose MCSD model, an efficient language model with linear scaling and fast\ninference speed. MCSD model leverages diverse feature fusion, primarily through\nthe multi-channel slope and decay (MCSD) block, to robustly represent features.\nThis block comprises slope and decay sections that extract features across\ndiverse temporal receptive fields, facilitating capture of both local and\nglobal information. In addition, MCSD block conducts element-wise fusion of\ndiverse features to further enhance the delicate feature extraction capability.\nFor inference, we formulate the inference process into a recurrent\nrepresentation, slashing space complexity to $O(1)$ and time complexity to\n$O(N)$ respectively. Our experiments show that MCSD attains higher throughput\nand lower GPU memory consumption compared to Transformers, while maintaining\ncomparable performance to larger-scale language learning models on benchmark\ntests. These attributes position MCSD as a promising base for edge deployment\nand embodied intelligence.",
        "chunk-id": 3,
        "chunk": "For inference, we formulate the inference process into a recurrent\nrepresentation, slashing space complexity to $O(1)$ and time complexity to\n$O(N)$ respectively. Our experiments show that MCSD attains higher throughput\nand lower GPU memory consumption compared to Transformers, while maintaining\ncomparable performance to larger-scale language learning models on benchmark",
        "authors": [
            "Hua Yang",
            "Duohai Li",
            "Shiman Li"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-18T03:08:01+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12230v1",
        "arxiv_link": "http://arxiv.org/abs/2406.12230v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000074,
        "doi": null,
        "title": "MCSD: An Efficient Language Model with Diverse Fusion",
        "abstract": "Transformers excel in Natural Language Processing (NLP) due to their prowess\nin capturing long-term dependencies but suffer from exponential resource\nconsumption with increasing sequence lengths. To address these challenges, we\npropose MCSD model, an efficient language model with linear scaling and fast\ninference speed. MCSD model leverages diverse feature fusion, primarily through\nthe multi-channel slope and decay (MCSD) block, to robustly represent features.\nThis block comprises slope and decay sections that extract features across\ndiverse temporal receptive fields, facilitating capture of both local and\nglobal information. In addition, MCSD block conducts element-wise fusion of\ndiverse features to further enhance the delicate feature extraction capability.\nFor inference, we formulate the inference process into a recurrent\nrepresentation, slashing space complexity to $O(1)$ and time complexity to\n$O(N)$ respectively. Our experiments show that MCSD attains higher throughput\nand lower GPU memory consumption compared to Transformers, while maintaining\ncomparable performance to larger-scale language learning models on benchmark\ntests. These attributes position MCSD as a promising base for edge deployment\nand embodied intelligence.",
        "chunk-id": 4,
        "chunk": "tests. These attributes position MCSD as a promising base for edge deployment\nand embodied intelligence.",
        "authors": [
            "Hua Yang",
            "Duohai Li",
            "Shiman Li"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-18T03:08:01+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12230v1",
        "arxiv_link": "http://arxiv.org/abs/2406.12230v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000075,
        "doi": null,
        "title": "AI \"News\" Content Farms Are Easy to Make and Hard to Detect: A Case Study in Italian",
        "abstract": "Large Language Models (LLMs) are increasingly used as \"content farm\" models\n(CFMs), to generate synthetic text that could pass for real news articles. This\nis already happening even for languages that do not have high-quality\nmonolingual LLMs. We show that fine-tuning Llama (v1), mostly trained on\nEnglish, on as little as 40K Italian news articles, is sufficient for producing\nnews-like texts that native speakers of Italian struggle to identify as\nsynthetic.\n  We investigate three LLMs and three methods of detecting synthetic texts\n(log-likelihood, DetectGPT, and supervised classification), finding that they\nall perform better than human raters, but they are all impractical in the real\nworld (requiring either access to token likelihood information or a large\ndataset of CFM texts). We also explore the possibility of creating a proxy CFM:\nan LLM fine-tuned on a similar dataset to one used by the real \"content farm\".\nWe find that even a small amount of fine-tuning data suffices for creating a\nsuccessful detector, but we need to know which base LLM is used, which is a\nmajor challenge.\n  Our results suggest that there are currently no practical methods for\ndetecting synthetic news-like texts 'in the wild', while generating them is too\neasy. We highlight the urgency of more NLP research on this problem.",
        "chunk-id": 1,
        "chunk": "Large Language Models (LLMs) are increasingly used as \"content farm\" models\n(CFMs), to generate synthetic text that could pass for real news articles. This\nis already happening even for languages that do not have high-quality\nmonolingual LLMs. We show that fine-tuning Llama (v1), mostly trained on\nEnglish, on as little as 40K Italian news articles, is sufficient for producing",
        "authors": [
            "Giovanni Puccetti",
            "Anna Rogers",
            "Chiara Alzetta",
            "Felice Dell'Orletta",
            "Andrea Esuli"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T22:19:00+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12128v1",
        "arxiv_link": "http://arxiv.org/abs/2406.12128v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000075,
        "doi": null,
        "title": "AI \"News\" Content Farms Are Easy to Make and Hard to Detect: A Case Study in Italian",
        "abstract": "Large Language Models (LLMs) are increasingly used as \"content farm\" models\n(CFMs), to generate synthetic text that could pass for real news articles. This\nis already happening even for languages that do not have high-quality\nmonolingual LLMs. We show that fine-tuning Llama (v1), mostly trained on\nEnglish, on as little as 40K Italian news articles, is sufficient for producing\nnews-like texts that native speakers of Italian struggle to identify as\nsynthetic.\n  We investigate three LLMs and three methods of detecting synthetic texts\n(log-likelihood, DetectGPT, and supervised classification), finding that they\nall perform better than human raters, but they are all impractical in the real\nworld (requiring either access to token likelihood information or a large\ndataset of CFM texts). We also explore the possibility of creating a proxy CFM:\nan LLM fine-tuned on a similar dataset to one used by the real \"content farm\".\nWe find that even a small amount of fine-tuning data suffices for creating a\nsuccessful detector, but we need to know which base LLM is used, which is a\nmajor challenge.\n  Our results suggest that there are currently no practical methods for\ndetecting synthetic news-like texts 'in the wild', while generating them is too\neasy. We highlight the urgency of more NLP research on this problem.",
        "chunk-id": 2,
        "chunk": "news-like texts that native speakers of Italian struggle to identify as\nsynthetic.\n  We investigate three LLMs and three methods of detecting synthetic texts\n(log-likelihood, DetectGPT, and supervised classification), finding that they\nall perform better than human raters, but they are all impractical in the real",
        "authors": [
            "Giovanni Puccetti",
            "Anna Rogers",
            "Chiara Alzetta",
            "Felice Dell'Orletta",
            "Andrea Esuli"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T22:19:00+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12128v1",
        "arxiv_link": "http://arxiv.org/abs/2406.12128v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000075,
        "doi": null,
        "title": "AI \"News\" Content Farms Are Easy to Make and Hard to Detect: A Case Study in Italian",
        "abstract": "Large Language Models (LLMs) are increasingly used as \"content farm\" models\n(CFMs), to generate synthetic text that could pass for real news articles. This\nis already happening even for languages that do not have high-quality\nmonolingual LLMs. We show that fine-tuning Llama (v1), mostly trained on\nEnglish, on as little as 40K Italian news articles, is sufficient for producing\nnews-like texts that native speakers of Italian struggle to identify as\nsynthetic.\n  We investigate three LLMs and three methods of detecting synthetic texts\n(log-likelihood, DetectGPT, and supervised classification), finding that they\nall perform better than human raters, but they are all impractical in the real\nworld (requiring either access to token likelihood information or a large\ndataset of CFM texts). We also explore the possibility of creating a proxy CFM:\nan LLM fine-tuned on a similar dataset to one used by the real \"content farm\".\nWe find that even a small amount of fine-tuning data suffices for creating a\nsuccessful detector, but we need to know which base LLM is used, which is a\nmajor challenge.\n  Our results suggest that there are currently no practical methods for\ndetecting synthetic news-like texts 'in the wild', while generating them is too\neasy. We highlight the urgency of more NLP research on this problem.",
        "chunk-id": 3,
        "chunk": "world (requiring either access to token likelihood information or a large\ndataset of CFM texts). We also explore the possibility of creating a proxy CFM:\nan LLM fine-tuned on a similar dataset to one used by the real \"content farm\".\nWe find that even a small amount of fine-tuning data suffices for creating a",
        "authors": [
            "Giovanni Puccetti",
            "Anna Rogers",
            "Chiara Alzetta",
            "Felice Dell'Orletta",
            "Andrea Esuli"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T22:19:00+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12128v1",
        "arxiv_link": "http://arxiv.org/abs/2406.12128v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000075,
        "doi": null,
        "title": "AI \"News\" Content Farms Are Easy to Make and Hard to Detect: A Case Study in Italian",
        "abstract": "Large Language Models (LLMs) are increasingly used as \"content farm\" models\n(CFMs), to generate synthetic text that could pass for real news articles. This\nis already happening even for languages that do not have high-quality\nmonolingual LLMs. We show that fine-tuning Llama (v1), mostly trained on\nEnglish, on as little as 40K Italian news articles, is sufficient for producing\nnews-like texts that native speakers of Italian struggle to identify as\nsynthetic.\n  We investigate three LLMs and three methods of detecting synthetic texts\n(log-likelihood, DetectGPT, and supervised classification), finding that they\nall perform better than human raters, but they are all impractical in the real\nworld (requiring either access to token likelihood information or a large\ndataset of CFM texts). We also explore the possibility of creating a proxy CFM:\nan LLM fine-tuned on a similar dataset to one used by the real \"content farm\".\nWe find that even a small amount of fine-tuning data suffices for creating a\nsuccessful detector, but we need to know which base LLM is used, which is a\nmajor challenge.\n  Our results suggest that there are currently no practical methods for\ndetecting synthetic news-like texts 'in the wild', while generating them is too\neasy. We highlight the urgency of more NLP research on this problem.",
        "chunk-id": 4,
        "chunk": "successful detector, but we need to know which base LLM is used, which is a\nmajor challenge.\n  Our results suggest that there are currently no practical methods for\ndetecting synthetic news-like texts 'in the wild', while generating them is too\neasy. We highlight the urgency of more NLP research on this problem.",
        "authors": [
            "Giovanni Puccetti",
            "Anna Rogers",
            "Chiara Alzetta",
            "Felice Dell'Orletta",
            "Andrea Esuli"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T22:19:00+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12128v1",
        "arxiv_link": "http://arxiv.org/abs/2406.12128v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000076,
        "doi": null,
        "title": "Can LLMs Learn Macroeconomic Narratives from Social Media?",
        "abstract": "This study empirically tests the $\\textit{Narrative Economics}$ hypothesis,\nwhich posits that narratives (ideas that are spread virally and affect public\nbeliefs) can influence economic fluctuations. We introduce two curated datasets\ncontaining posts from X (formerly Twitter) which capture economy-related\nnarratives (Data will be shared upon paper acceptance). Employing Natural\nLanguage Processing (NLP) methods, we extract and summarize narratives from the\ntweets. We test their predictive power for $\\textit{macroeconomic}$ forecasting\nby incorporating the tweets' or the extracted narratives' representations in\ndownstream financial prediction tasks. Our work highlights the challenges in\nimproving macroeconomic models with narrative data, paving the way for the\nresearch community to realistically address this important challenge. From a\nscientific perspective, our investigation offers valuable insights and NLP\ntools for narrative extraction and summarization using Large Language Models\n(LLMs), contributing to future research on the role of narratives in economics.",
        "chunk-id": 1,
        "chunk": "This study empirically tests the $\\textit{Narrative Economics}$ hypothesis,\nwhich posits that narratives (ideas that are spread virally and affect public\nbeliefs) can influence economic fluctuations. We introduce two curated datasets\ncontaining posts from X (formerly Twitter) which capture economy-related\nnarratives (Data will be shared upon paper acceptance). Employing Natural",
        "authors": [
            "Almog Gueta",
            "Amir Feder",
            "Zorik Gekhman",
            "Ariel Goldstein",
            "Roi Reichart"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T21:37:09+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12109v1",
        "arxiv_link": "http://arxiv.org/abs/2406.12109v1",
        "categories": [
            "Computation and Language",
            "Computational Engineering, Finance, and Science"
        ]
    },
    {
        "id": 20000076,
        "doi": null,
        "title": "Can LLMs Learn Macroeconomic Narratives from Social Media?",
        "abstract": "This study empirically tests the $\\textit{Narrative Economics}$ hypothesis,\nwhich posits that narratives (ideas that are spread virally and affect public\nbeliefs) can influence economic fluctuations. We introduce two curated datasets\ncontaining posts from X (formerly Twitter) which capture economy-related\nnarratives (Data will be shared upon paper acceptance). Employing Natural\nLanguage Processing (NLP) methods, we extract and summarize narratives from the\ntweets. We test their predictive power for $\\textit{macroeconomic}$ forecasting\nby incorporating the tweets' or the extracted narratives' representations in\ndownstream financial prediction tasks. Our work highlights the challenges in\nimproving macroeconomic models with narrative data, paving the way for the\nresearch community to realistically address this important challenge. From a\nscientific perspective, our investigation offers valuable insights and NLP\ntools for narrative extraction and summarization using Large Language Models\n(LLMs), contributing to future research on the role of narratives in economics.",
        "chunk-id": 2,
        "chunk": "Language Processing (NLP) methods, we extract and summarize narratives from the\ntweets. We test their predictive power for $\\textit{macroeconomic}$ forecasting\nby incorporating the tweets' or the extracted narratives' representations in\ndownstream financial prediction tasks. Our work highlights the challenges in",
        "authors": [
            "Almog Gueta",
            "Amir Feder",
            "Zorik Gekhman",
            "Ariel Goldstein",
            "Roi Reichart"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T21:37:09+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12109v1",
        "arxiv_link": "http://arxiv.org/abs/2406.12109v1",
        "categories": [
            "Computation and Language",
            "Computational Engineering, Finance, and Science"
        ]
    },
    {
        "id": 20000076,
        "doi": null,
        "title": "Can LLMs Learn Macroeconomic Narratives from Social Media?",
        "abstract": "This study empirically tests the $\\textit{Narrative Economics}$ hypothesis,\nwhich posits that narratives (ideas that are spread virally and affect public\nbeliefs) can influence economic fluctuations. We introduce two curated datasets\ncontaining posts from X (formerly Twitter) which capture economy-related\nnarratives (Data will be shared upon paper acceptance). Employing Natural\nLanguage Processing (NLP) methods, we extract and summarize narratives from the\ntweets. We test their predictive power for $\\textit{macroeconomic}$ forecasting\nby incorporating the tweets' or the extracted narratives' representations in\ndownstream financial prediction tasks. Our work highlights the challenges in\nimproving macroeconomic models with narrative data, paving the way for the\nresearch community to realistically address this important challenge. From a\nscientific perspective, our investigation offers valuable insights and NLP\ntools for narrative extraction and summarization using Large Language Models\n(LLMs), contributing to future research on the role of narratives in economics.",
        "chunk-id": 3,
        "chunk": "improving macroeconomic models with narrative data, paving the way for the\nresearch community to realistically address this important challenge. From a\nscientific perspective, our investigation offers valuable insights and NLP\ntools for narrative extraction and summarization using Large Language Models\n(LLMs), contributing to future research on the role of narratives in economics.",
        "authors": [
            "Almog Gueta",
            "Amir Feder",
            "Zorik Gekhman",
            "Ariel Goldstein",
            "Roi Reichart"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T21:37:09+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12109v1",
        "arxiv_link": "http://arxiv.org/abs/2406.12109v1",
        "categories": [
            "Computation and Language",
            "Computational Engineering, Finance, and Science"
        ]
    },
    {
        "id": 20000077,
        "doi": null,
        "title": "MedCalc-Bench: Evaluating Large Language Models for Medical Calculations",
        "abstract": "As opposed to evaluating computation and logic-based reasoning, current\nbenchmarks for evaluating large language models (LLMs) in medicine are\nprimarily focused on question-answering involving domain knowledge and\ndescriptive reasoning. While such qualitative capabilities are vital to medical\ndiagnosis, in real-world scenarios, doctors frequently use clinical calculators\nthat follow quantitative equations and rule-based reasoning paradigms for\nevidence-based decision support. To this end, we propose MedCalc-Bench, a\nfirst-of-its-kind dataset focused on evaluating the medical calculation\ncapability of LLMs. MedCalc-Bench contains an evaluation set of over 1000\nmanually reviewed instances from 55 different medical calculation tasks. Each\ninstance in MedCalc-Bench consists of a patient note, a question requesting to\ncompute a specific medical value, a ground truth answer, and a step-by-step\nexplanation showing how the answer is obtained. While our evaluation results\nshow the potential of LLMs in this area, none of them are effective enough for\nclinical settings. Common issues include extracting the incorrect entities, not\nusing the correct equation or rules for a calculation task, or incorrectly\nperforming the arithmetic for the computation. We hope our study highlights the\nquantitative knowledge and reasoning gaps in LLMs within medical settings,\nencouraging future improvements of LLMs for various clinical calculation tasks.",
        "chunk-id": 1,
        "chunk": "As opposed to evaluating computation and logic-based reasoning, current\nbenchmarks for evaluating large language models (LLMs) in medicine are\nprimarily focused on question-answering involving domain knowledge and\ndescriptive reasoning. While such qualitative capabilities are vital to medical\ndiagnosis, in real-world scenarios, doctors frequently use clinical calculators",
        "authors": [
            "Nikhil Khandekar",
            "Qiao Jin",
            "Guangzhi Xiong",
            "Soren Dunn",
            "Serina S Applebaum",
            "Zain Anwar",
            "Maame Sarfo-Gyamfi",
            "Conrad W Safranek",
            "Abid A Anwar",
            "Andrew Zhang",
            "Aidan Gilson",
            "Maxwell B Singer",
            "Amisha Dave",
            "Andrew Taylor",
            "Aidong Zhang",
            "Qingyu Chen",
            "Zhiyong Lu"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T19:07:21+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12036v3",
        "arxiv_link": "http://arxiv.org/abs/2406.12036v3",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000077,
        "doi": null,
        "title": "MedCalc-Bench: Evaluating Large Language Models for Medical Calculations",
        "abstract": "As opposed to evaluating computation and logic-based reasoning, current\nbenchmarks for evaluating large language models (LLMs) in medicine are\nprimarily focused on question-answering involving domain knowledge and\ndescriptive reasoning. While such qualitative capabilities are vital to medical\ndiagnosis, in real-world scenarios, doctors frequently use clinical calculators\nthat follow quantitative equations and rule-based reasoning paradigms for\nevidence-based decision support. To this end, we propose MedCalc-Bench, a\nfirst-of-its-kind dataset focused on evaluating the medical calculation\ncapability of LLMs. MedCalc-Bench contains an evaluation set of over 1000\nmanually reviewed instances from 55 different medical calculation tasks. Each\ninstance in MedCalc-Bench consists of a patient note, a question requesting to\ncompute a specific medical value, a ground truth answer, and a step-by-step\nexplanation showing how the answer is obtained. While our evaluation results\nshow the potential of LLMs in this area, none of them are effective enough for\nclinical settings. Common issues include extracting the incorrect entities, not\nusing the correct equation or rules for a calculation task, or incorrectly\nperforming the arithmetic for the computation. We hope our study highlights the\nquantitative knowledge and reasoning gaps in LLMs within medical settings,\nencouraging future improvements of LLMs for various clinical calculation tasks.",
        "chunk-id": 2,
        "chunk": "that follow quantitative equations and rule-based reasoning paradigms for\nevidence-based decision support. To this end, we propose MedCalc-Bench, a\nfirst-of-its-kind dataset focused on evaluating the medical calculation\ncapability of LLMs. MedCalc-Bench contains an evaluation set of over 1000\nmanually reviewed instances from 55 different medical calculation tasks. Each",
        "authors": [
            "Nikhil Khandekar",
            "Qiao Jin",
            "Guangzhi Xiong",
            "Soren Dunn",
            "Serina S Applebaum",
            "Zain Anwar",
            "Maame Sarfo-Gyamfi",
            "Conrad W Safranek",
            "Abid A Anwar",
            "Andrew Zhang",
            "Aidan Gilson",
            "Maxwell B Singer",
            "Amisha Dave",
            "Andrew Taylor",
            "Aidong Zhang",
            "Qingyu Chen",
            "Zhiyong Lu"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T19:07:21+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12036v3",
        "arxiv_link": "http://arxiv.org/abs/2406.12036v3",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000077,
        "doi": null,
        "title": "MedCalc-Bench: Evaluating Large Language Models for Medical Calculations",
        "abstract": "As opposed to evaluating computation and logic-based reasoning, current\nbenchmarks for evaluating large language models (LLMs) in medicine are\nprimarily focused on question-answering involving domain knowledge and\ndescriptive reasoning. While such qualitative capabilities are vital to medical\ndiagnosis, in real-world scenarios, doctors frequently use clinical calculators\nthat follow quantitative equations and rule-based reasoning paradigms for\nevidence-based decision support. To this end, we propose MedCalc-Bench, a\nfirst-of-its-kind dataset focused on evaluating the medical calculation\ncapability of LLMs. MedCalc-Bench contains an evaluation set of over 1000\nmanually reviewed instances from 55 different medical calculation tasks. Each\ninstance in MedCalc-Bench consists of a patient note, a question requesting to\ncompute a specific medical value, a ground truth answer, and a step-by-step\nexplanation showing how the answer is obtained. While our evaluation results\nshow the potential of LLMs in this area, none of them are effective enough for\nclinical settings. Common issues include extracting the incorrect entities, not\nusing the correct equation or rules for a calculation task, or incorrectly\nperforming the arithmetic for the computation. We hope our study highlights the\nquantitative knowledge and reasoning gaps in LLMs within medical settings,\nencouraging future improvements of LLMs for various clinical calculation tasks.",
        "chunk-id": 3,
        "chunk": "instance in MedCalc-Bench consists of a patient note, a question requesting to\ncompute a specific medical value, a ground truth answer, and a step-by-step\nexplanation showing how the answer is obtained. While our evaluation results\nshow the potential of LLMs in this area, none of them are effective enough for",
        "authors": [
            "Nikhil Khandekar",
            "Qiao Jin",
            "Guangzhi Xiong",
            "Soren Dunn",
            "Serina S Applebaum",
            "Zain Anwar",
            "Maame Sarfo-Gyamfi",
            "Conrad W Safranek",
            "Abid A Anwar",
            "Andrew Zhang",
            "Aidan Gilson",
            "Maxwell B Singer",
            "Amisha Dave",
            "Andrew Taylor",
            "Aidong Zhang",
            "Qingyu Chen",
            "Zhiyong Lu"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T19:07:21+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12036v3",
        "arxiv_link": "http://arxiv.org/abs/2406.12036v3",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000077,
        "doi": null,
        "title": "MedCalc-Bench: Evaluating Large Language Models for Medical Calculations",
        "abstract": "As opposed to evaluating computation and logic-based reasoning, current\nbenchmarks for evaluating large language models (LLMs) in medicine are\nprimarily focused on question-answering involving domain knowledge and\ndescriptive reasoning. While such qualitative capabilities are vital to medical\ndiagnosis, in real-world scenarios, doctors frequently use clinical calculators\nthat follow quantitative equations and rule-based reasoning paradigms for\nevidence-based decision support. To this end, we propose MedCalc-Bench, a\nfirst-of-its-kind dataset focused on evaluating the medical calculation\ncapability of LLMs. MedCalc-Bench contains an evaluation set of over 1000\nmanually reviewed instances from 55 different medical calculation tasks. Each\ninstance in MedCalc-Bench consists of a patient note, a question requesting to\ncompute a specific medical value, a ground truth answer, and a step-by-step\nexplanation showing how the answer is obtained. While our evaluation results\nshow the potential of LLMs in this area, none of them are effective enough for\nclinical settings. Common issues include extracting the incorrect entities, not\nusing the correct equation or rules for a calculation task, or incorrectly\nperforming the arithmetic for the computation. We hope our study highlights the\nquantitative knowledge and reasoning gaps in LLMs within medical settings,\nencouraging future improvements of LLMs for various clinical calculation tasks.",
        "chunk-id": 4,
        "chunk": "clinical settings. Common issues include extracting the incorrect entities, not\nusing the correct equation or rules for a calculation task, or incorrectly\nperforming the arithmetic for the computation. We hope our study highlights the\nquantitative knowledge and reasoning gaps in LLMs within medical settings,",
        "authors": [
            "Nikhil Khandekar",
            "Qiao Jin",
            "Guangzhi Xiong",
            "Soren Dunn",
            "Serina S Applebaum",
            "Zain Anwar",
            "Maame Sarfo-Gyamfi",
            "Conrad W Safranek",
            "Abid A Anwar",
            "Andrew Zhang",
            "Aidan Gilson",
            "Maxwell B Singer",
            "Amisha Dave",
            "Andrew Taylor",
            "Aidong Zhang",
            "Qingyu Chen",
            "Zhiyong Lu"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T19:07:21+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12036v3",
        "arxiv_link": "http://arxiv.org/abs/2406.12036v3",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000077,
        "doi": null,
        "title": "MedCalc-Bench: Evaluating Large Language Models for Medical Calculations",
        "abstract": "As opposed to evaluating computation and logic-based reasoning, current\nbenchmarks for evaluating large language models (LLMs) in medicine are\nprimarily focused on question-answering involving domain knowledge and\ndescriptive reasoning. While such qualitative capabilities are vital to medical\ndiagnosis, in real-world scenarios, doctors frequently use clinical calculators\nthat follow quantitative equations and rule-based reasoning paradigms for\nevidence-based decision support. To this end, we propose MedCalc-Bench, a\nfirst-of-its-kind dataset focused on evaluating the medical calculation\ncapability of LLMs. MedCalc-Bench contains an evaluation set of over 1000\nmanually reviewed instances from 55 different medical calculation tasks. Each\ninstance in MedCalc-Bench consists of a patient note, a question requesting to\ncompute a specific medical value, a ground truth answer, and a step-by-step\nexplanation showing how the answer is obtained. While our evaluation results\nshow the potential of LLMs in this area, none of them are effective enough for\nclinical settings. Common issues include extracting the incorrect entities, not\nusing the correct equation or rules for a calculation task, or incorrectly\nperforming the arithmetic for the computation. We hope our study highlights the\nquantitative knowledge and reasoning gaps in LLMs within medical settings,\nencouraging future improvements of LLMs for various clinical calculation tasks.",
        "chunk-id": 5,
        "chunk": "encouraging future improvements of LLMs for various clinical calculation tasks.",
        "authors": [
            "Nikhil Khandekar",
            "Qiao Jin",
            "Guangzhi Xiong",
            "Soren Dunn",
            "Serina S Applebaum",
            "Zain Anwar",
            "Maame Sarfo-Gyamfi",
            "Conrad W Safranek",
            "Abid A Anwar",
            "Andrew Zhang",
            "Aidan Gilson",
            "Maxwell B Singer",
            "Amisha Dave",
            "Andrew Taylor",
            "Aidong Zhang",
            "Qingyu Chen",
            "Zhiyong Lu"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T19:07:21+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12036v3",
        "arxiv_link": "http://arxiv.org/abs/2406.12036v3",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000078,
        "doi": null,
        "title": "FinTruthQA: A Benchmark Dataset for Evaluating the Quality of Financial Information Disclosure",
        "abstract": "Accurate and transparent financial information disclosure is crucial in the\nfields of accounting and finance, ensuring market efficiency and investor\nconfidence. Among many information disclosure platforms, the Chinese stock\nexchanges' investor interactive platform provides a novel and interactive way\nfor listed firms to disclose information of interest to investors through an\nonline question-and-answer (Q&A) format. However, it is common for listed firms\nto respond to questions with limited or no substantive information, and\nautomatically evaluating the quality of financial information disclosure on\nlarge amounts of Q&A pairs is challenging. This paper builds a benchmark\nFinTruthQA, that can evaluate advanced natural language processing (NLP)\ntechniques for the automatic quality assessment of information disclosure in\nfinancial Q&A data. FinTruthQA comprises 6,000 real-world financial Q&A entries\nand each Q&A was manually annotated based on four conceptual dimensions of\naccounting. We benchmarked various NLP techniques on FinTruthQA, including\nstatistical machine learning models, pre-trained language model and their\nfine-tuned versions, as well as the large language model GPT-4. Experiments\nshowed that existing NLP models have strong predictive ability for real\nquestion identification and question relevance tasks, but are suboptimal for\nanswer relevance and answer readability tasks. By establishing this benchmark,\nwe provide a robust foundation for the automatic evaluation of information\ndisclosure, significantly enhancing the transparency and quality of financial\nreporting. FinTruthQA can be used by auditors, regulators, and financial\nanalysts for real-time monitoring and data-driven decision-making, as well as\nby researchers for advanced studies in accounting and finance, ultimately\nfostering greater trust and efficiency in the financial markets.",
        "chunk-id": 1,
        "chunk": "Accurate and transparent financial information disclosure is crucial in the\nfields of accounting and finance, ensuring market efficiency and investor\nconfidence. Among many information disclosure platforms, the Chinese stock\nexchanges' investor interactive platform provides a novel and interactive way\nfor listed firms to disclose information of interest to investors through an",
        "authors": [
            "Ziyue Xu",
            "Peilin Zhou",
            "Xinyu Shi",
            "Jiageng Wu",
            "Yikang Jiang",
            "Bin Ke",
            "Jie Yang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T18:25:02+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12009v1",
        "arxiv_link": "http://arxiv.org/abs/2406.12009v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000078,
        "doi": null,
        "title": "FinTruthQA: A Benchmark Dataset for Evaluating the Quality of Financial Information Disclosure",
        "abstract": "Accurate and transparent financial information disclosure is crucial in the\nfields of accounting and finance, ensuring market efficiency and investor\nconfidence. Among many information disclosure platforms, the Chinese stock\nexchanges' investor interactive platform provides a novel and interactive way\nfor listed firms to disclose information of interest to investors through an\nonline question-and-answer (Q&A) format. However, it is common for listed firms\nto respond to questions with limited or no substantive information, and\nautomatically evaluating the quality of financial information disclosure on\nlarge amounts of Q&A pairs is challenging. This paper builds a benchmark\nFinTruthQA, that can evaluate advanced natural language processing (NLP)\ntechniques for the automatic quality assessment of information disclosure in\nfinancial Q&A data. FinTruthQA comprises 6,000 real-world financial Q&A entries\nand each Q&A was manually annotated based on four conceptual dimensions of\naccounting. We benchmarked various NLP techniques on FinTruthQA, including\nstatistical machine learning models, pre-trained language model and their\nfine-tuned versions, as well as the large language model GPT-4. Experiments\nshowed that existing NLP models have strong predictive ability for real\nquestion identification and question relevance tasks, but are suboptimal for\nanswer relevance and answer readability tasks. By establishing this benchmark,\nwe provide a robust foundation for the automatic evaluation of information\ndisclosure, significantly enhancing the transparency and quality of financial\nreporting. FinTruthQA can be used by auditors, regulators, and financial\nanalysts for real-time monitoring and data-driven decision-making, as well as\nby researchers for advanced studies in accounting and finance, ultimately\nfostering greater trust and efficiency in the financial markets.",
        "chunk-id": 2,
        "chunk": "online question-and-answer (Q&A) format. However, it is common for listed firms\nto respond to questions with limited or no substantive information, and\nautomatically evaluating the quality of financial information disclosure on\nlarge amounts of Q&A pairs is challenging. This paper builds a benchmark\nFinTruthQA, that can evaluate advanced natural language processing (NLP)",
        "authors": [
            "Ziyue Xu",
            "Peilin Zhou",
            "Xinyu Shi",
            "Jiageng Wu",
            "Yikang Jiang",
            "Bin Ke",
            "Jie Yang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T18:25:02+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12009v1",
        "arxiv_link": "http://arxiv.org/abs/2406.12009v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000078,
        "doi": null,
        "title": "FinTruthQA: A Benchmark Dataset for Evaluating the Quality of Financial Information Disclosure",
        "abstract": "Accurate and transparent financial information disclosure is crucial in the\nfields of accounting and finance, ensuring market efficiency and investor\nconfidence. Among many information disclosure platforms, the Chinese stock\nexchanges' investor interactive platform provides a novel and interactive way\nfor listed firms to disclose information of interest to investors through an\nonline question-and-answer (Q&A) format. However, it is common for listed firms\nto respond to questions with limited or no substantive information, and\nautomatically evaluating the quality of financial information disclosure on\nlarge amounts of Q&A pairs is challenging. This paper builds a benchmark\nFinTruthQA, that can evaluate advanced natural language processing (NLP)\ntechniques for the automatic quality assessment of information disclosure in\nfinancial Q&A data. FinTruthQA comprises 6,000 real-world financial Q&A entries\nand each Q&A was manually annotated based on four conceptual dimensions of\naccounting. We benchmarked various NLP techniques on FinTruthQA, including\nstatistical machine learning models, pre-trained language model and their\nfine-tuned versions, as well as the large language model GPT-4. Experiments\nshowed that existing NLP models have strong predictive ability for real\nquestion identification and question relevance tasks, but are suboptimal for\nanswer relevance and answer readability tasks. By establishing this benchmark,\nwe provide a robust foundation for the automatic evaluation of information\ndisclosure, significantly enhancing the transparency and quality of financial\nreporting. FinTruthQA can be used by auditors, regulators, and financial\nanalysts for real-time monitoring and data-driven decision-making, as well as\nby researchers for advanced studies in accounting and finance, ultimately\nfostering greater trust and efficiency in the financial markets.",
        "chunk-id": 3,
        "chunk": "techniques for the automatic quality assessment of information disclosure in\nfinancial Q&A data. FinTruthQA comprises 6,000 real-world financial Q&A entries\nand each Q&A was manually annotated based on four conceptual dimensions of\naccounting. We benchmarked various NLP techniques on FinTruthQA, including\nstatistical machine learning models, pre-trained language model and their",
        "authors": [
            "Ziyue Xu",
            "Peilin Zhou",
            "Xinyu Shi",
            "Jiageng Wu",
            "Yikang Jiang",
            "Bin Ke",
            "Jie Yang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T18:25:02+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12009v1",
        "arxiv_link": "http://arxiv.org/abs/2406.12009v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000078,
        "doi": null,
        "title": "FinTruthQA: A Benchmark Dataset for Evaluating the Quality of Financial Information Disclosure",
        "abstract": "Accurate and transparent financial information disclosure is crucial in the\nfields of accounting and finance, ensuring market efficiency and investor\nconfidence. Among many information disclosure platforms, the Chinese stock\nexchanges' investor interactive platform provides a novel and interactive way\nfor listed firms to disclose information of interest to investors through an\nonline question-and-answer (Q&A) format. However, it is common for listed firms\nto respond to questions with limited or no substantive information, and\nautomatically evaluating the quality of financial information disclosure on\nlarge amounts of Q&A pairs is challenging. This paper builds a benchmark\nFinTruthQA, that can evaluate advanced natural language processing (NLP)\ntechniques for the automatic quality assessment of information disclosure in\nfinancial Q&A data. FinTruthQA comprises 6,000 real-world financial Q&A entries\nand each Q&A was manually annotated based on four conceptual dimensions of\naccounting. We benchmarked various NLP techniques on FinTruthQA, including\nstatistical machine learning models, pre-trained language model and their\nfine-tuned versions, as well as the large language model GPT-4. Experiments\nshowed that existing NLP models have strong predictive ability for real\nquestion identification and question relevance tasks, but are suboptimal for\nanswer relevance and answer readability tasks. By establishing this benchmark,\nwe provide a robust foundation for the automatic evaluation of information\ndisclosure, significantly enhancing the transparency and quality of financial\nreporting. FinTruthQA can be used by auditors, regulators, and financial\nanalysts for real-time monitoring and data-driven decision-making, as well as\nby researchers for advanced studies in accounting and finance, ultimately\nfostering greater trust and efficiency in the financial markets.",
        "chunk-id": 4,
        "chunk": "fine-tuned versions, as well as the large language model GPT-4. Experiments\nshowed that existing NLP models have strong predictive ability for real\nquestion identification and question relevance tasks, but are suboptimal for\nanswer relevance and answer readability tasks. By establishing this benchmark,\nwe provide a robust foundation for the automatic evaluation of information",
        "authors": [
            "Ziyue Xu",
            "Peilin Zhou",
            "Xinyu Shi",
            "Jiageng Wu",
            "Yikang Jiang",
            "Bin Ke",
            "Jie Yang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T18:25:02+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12009v1",
        "arxiv_link": "http://arxiv.org/abs/2406.12009v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000078,
        "doi": null,
        "title": "FinTruthQA: A Benchmark Dataset for Evaluating the Quality of Financial Information Disclosure",
        "abstract": "Accurate and transparent financial information disclosure is crucial in the\nfields of accounting and finance, ensuring market efficiency and investor\nconfidence. Among many information disclosure platforms, the Chinese stock\nexchanges' investor interactive platform provides a novel and interactive way\nfor listed firms to disclose information of interest to investors through an\nonline question-and-answer (Q&A) format. However, it is common for listed firms\nto respond to questions with limited or no substantive information, and\nautomatically evaluating the quality of financial information disclosure on\nlarge amounts of Q&A pairs is challenging. This paper builds a benchmark\nFinTruthQA, that can evaluate advanced natural language processing (NLP)\ntechniques for the automatic quality assessment of information disclosure in\nfinancial Q&A data. FinTruthQA comprises 6,000 real-world financial Q&A entries\nand each Q&A was manually annotated based on four conceptual dimensions of\naccounting. We benchmarked various NLP techniques on FinTruthQA, including\nstatistical machine learning models, pre-trained language model and their\nfine-tuned versions, as well as the large language model GPT-4. Experiments\nshowed that existing NLP models have strong predictive ability for real\nquestion identification and question relevance tasks, but are suboptimal for\nanswer relevance and answer readability tasks. By establishing this benchmark,\nwe provide a robust foundation for the automatic evaluation of information\ndisclosure, significantly enhancing the transparency and quality of financial\nreporting. FinTruthQA can be used by auditors, regulators, and financial\nanalysts for real-time monitoring and data-driven decision-making, as well as\nby researchers for advanced studies in accounting and finance, ultimately\nfostering greater trust and efficiency in the financial markets.",
        "chunk-id": 5,
        "chunk": "disclosure, significantly enhancing the transparency and quality of financial\nreporting. FinTruthQA can be used by auditors, regulators, and financial\nanalysts for real-time monitoring and data-driven decision-making, as well as\nby researchers for advanced studies in accounting and finance, ultimately\nfostering greater trust and efficiency in the financial markets.",
        "authors": [
            "Ziyue Xu",
            "Peilin Zhou",
            "Xinyu Shi",
            "Jiageng Wu",
            "Yikang Jiang",
            "Bin Ke",
            "Jie Yang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T18:25:02+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.12009v1",
        "arxiv_link": "http://arxiv.org/abs/2406.12009v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000079,
        "doi": null,
        "title": "Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs",
        "abstract": "Language Model Programs, i.e. sophisticated pipelines of modular language\nmodel (LM) calls, are increasingly advancing NLP tasks, but they require\ncrafting prompts that are jointly effective for all modules. We study prompt\noptimization for LM programs, i.e. how to update these prompts to maximize a\ndownstream metric without access to module-level labels or gradients. To make\nthis tractable, we factorize our problem into optimizing the free-form\ninstructions and few-shot demonstrations of every module and introduce several\nstrategies to craft task-grounded instructions and navigate credit assignment\nacross modules. Our strategies include (i) program- and data-aware techniques\nfor proposing effective instructions, (ii) a stochastic mini-batch evaluation\nfunction for learning a surrogate model of our objective, and (iii) a\nmeta-optimization procedure in which we refine how LMs construct proposals over\ntime. Using these insights we develop MIPRO, a novel optimizer that outperforms\nbaselines on five of six diverse LM programs using a best-in-class open-source\nmodel (Llama-3-8B), by as high as 12.9% accuracy. We will release our new\noptimizers and benchmark in DSPy at https://github.com/stanfordnlp/dspy",
        "chunk-id": 1,
        "chunk": "Language Model Programs, i.e. sophisticated pipelines of modular language\nmodel (LM) calls, are increasingly advancing NLP tasks, but they require\ncrafting prompts that are jointly effective for all modules. We study prompt\noptimization for LM programs, i.e. how to update these prompts to maximize a\ndownstream metric without access to module-level labels or gradients. To make",
        "authors": [
            "Krista Opsahl-Ong",
            "Michael J Ryan",
            "Josh Purtell",
            "David Broman",
            "Christopher Potts",
            "Matei Zaharia",
            "Omar Khattab"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T16:12:03+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11695v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11695v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Machine Learning"
        ]
    },
    {
        "id": 20000079,
        "doi": null,
        "title": "Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs",
        "abstract": "Language Model Programs, i.e. sophisticated pipelines of modular language\nmodel (LM) calls, are increasingly advancing NLP tasks, but they require\ncrafting prompts that are jointly effective for all modules. We study prompt\noptimization for LM programs, i.e. how to update these prompts to maximize a\ndownstream metric without access to module-level labels or gradients. To make\nthis tractable, we factorize our problem into optimizing the free-form\ninstructions and few-shot demonstrations of every module and introduce several\nstrategies to craft task-grounded instructions and navigate credit assignment\nacross modules. Our strategies include (i) program- and data-aware techniques\nfor proposing effective instructions, (ii) a stochastic mini-batch evaluation\nfunction for learning a surrogate model of our objective, and (iii) a\nmeta-optimization procedure in which we refine how LMs construct proposals over\ntime. Using these insights we develop MIPRO, a novel optimizer that outperforms\nbaselines on five of six diverse LM programs using a best-in-class open-source\nmodel (Llama-3-8B), by as high as 12.9% accuracy. We will release our new\noptimizers and benchmark in DSPy at https://github.com/stanfordnlp/dspy",
        "chunk-id": 2,
        "chunk": "this tractable, we factorize our problem into optimizing the free-form\ninstructions and few-shot demonstrations of every module and introduce several\nstrategies to craft task-grounded instructions and navigate credit assignment\nacross modules. Our strategies include (i) program- and data-aware techniques\nfor proposing effective instructions, (ii) a stochastic mini-batch evaluation",
        "authors": [
            "Krista Opsahl-Ong",
            "Michael J Ryan",
            "Josh Purtell",
            "David Broman",
            "Christopher Potts",
            "Matei Zaharia",
            "Omar Khattab"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T16:12:03+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11695v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11695v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Machine Learning"
        ]
    },
    {
        "id": 20000079,
        "doi": null,
        "title": "Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs",
        "abstract": "Language Model Programs, i.e. sophisticated pipelines of modular language\nmodel (LM) calls, are increasingly advancing NLP tasks, but they require\ncrafting prompts that are jointly effective for all modules. We study prompt\noptimization for LM programs, i.e. how to update these prompts to maximize a\ndownstream metric without access to module-level labels or gradients. To make\nthis tractable, we factorize our problem into optimizing the free-form\ninstructions and few-shot demonstrations of every module and introduce several\nstrategies to craft task-grounded instructions and navigate credit assignment\nacross modules. Our strategies include (i) program- and data-aware techniques\nfor proposing effective instructions, (ii) a stochastic mini-batch evaluation\nfunction for learning a surrogate model of our objective, and (iii) a\nmeta-optimization procedure in which we refine how LMs construct proposals over\ntime. Using these insights we develop MIPRO, a novel optimizer that outperforms\nbaselines on five of six diverse LM programs using a best-in-class open-source\nmodel (Llama-3-8B), by as high as 12.9% accuracy. We will release our new\noptimizers and benchmark in DSPy at https://github.com/stanfordnlp/dspy",
        "chunk-id": 3,
        "chunk": "function for learning a surrogate model of our objective, and (iii) a\nmeta-optimization procedure in which we refine how LMs construct proposals over\ntime. Using these insights we develop MIPRO, a novel optimizer that outperforms\nbaselines on five of six diverse LM programs using a best-in-class open-source\nmodel (Llama-3-8B), by as high as 12.9% accuracy. We will release our new",
        "authors": [
            "Krista Opsahl-Ong",
            "Michael J Ryan",
            "Josh Purtell",
            "David Broman",
            "Christopher Potts",
            "Matei Zaharia",
            "Omar Khattab"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T16:12:03+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11695v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11695v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Machine Learning"
        ]
    },
    {
        "id": 20000079,
        "doi": null,
        "title": "Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs",
        "abstract": "Language Model Programs, i.e. sophisticated pipelines of modular language\nmodel (LM) calls, are increasingly advancing NLP tasks, but they require\ncrafting prompts that are jointly effective for all modules. We study prompt\noptimization for LM programs, i.e. how to update these prompts to maximize a\ndownstream metric without access to module-level labels or gradients. To make\nthis tractable, we factorize our problem into optimizing the free-form\ninstructions and few-shot demonstrations of every module and introduce several\nstrategies to craft task-grounded instructions and navigate credit assignment\nacross modules. Our strategies include (i) program- and data-aware techniques\nfor proposing effective instructions, (ii) a stochastic mini-batch evaluation\nfunction for learning a surrogate model of our objective, and (iii) a\nmeta-optimization procedure in which we refine how LMs construct proposals over\ntime. Using these insights we develop MIPRO, a novel optimizer that outperforms\nbaselines on five of six diverse LM programs using a best-in-class open-source\nmodel (Llama-3-8B), by as high as 12.9% accuracy. We will release our new\noptimizers and benchmark in DSPy at https://github.com/stanfordnlp/dspy",
        "chunk-id": 4,
        "chunk": "optimizers and benchmark in DSPy at https://github.com/stanfordnlp/dspy",
        "authors": [
            "Krista Opsahl-Ong",
            "Michael J Ryan",
            "Josh Purtell",
            "David Broman",
            "Christopher Potts",
            "Matei Zaharia",
            "Omar Khattab"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T16:12:03+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11695v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11695v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Machine Learning"
        ]
    },
    {
        "id": 20000080,
        "doi": "10.1145/3637528.3671564",
        "title": "R-Eval: A Unified Toolkit for Evaluating Domain Knowledge of Retrieval Augmented Large Language Models",
        "abstract": "Large language models have achieved remarkable success on general NLP tasks,\nbut they may fall short for domain-specific problems. Recently, various\nRetrieval-Augmented Large Language Models (RALLMs) are proposed to address this\nshortcoming. However, existing evaluation tools only provide a few baselines\nand evaluate them on various domains without mining the depth of domain\nknowledge. In this paper, we address the challenges of evaluating RALLMs by\nintroducing the R-Eval toolkit, a Python toolkit designed to streamline the\nevaluation of different RAG workflows in conjunction with LLMs. Our toolkit,\nwhich supports popular built-in RAG workflows and allows for the incorporation\nof customized testing data on the specific domain, is designed to be\nuser-friendly, modular, and extensible. We conduct an evaluation of 21 RALLMs\nacross three task levels and two representative domains, revealing significant\nvariations in the effectiveness of RALLMs across different tasks and domains.\nOur analysis emphasizes the importance of considering both task and domain\nrequirements when choosing a RAG workflow and LLM combination. We are committed\nto continuously maintaining our platform at https://github.com/THU-KEG/R-Eval\nto facilitate both the industry and the researchers.",
        "chunk-id": 1,
        "chunk": "Large language models have achieved remarkable success on general NLP tasks,\nbut they may fall short for domain-specific problems. Recently, various\nRetrieval-Augmented Large Language Models (RALLMs) are proposed to address this\nshortcoming. However, existing evaluation tools only provide a few baselines\nand evaluate them on various domains without mining the depth of domain",
        "authors": [
            "Shangqing Tu",
            "Yuanchun Wang",
            "Jifan Yu",
            "Yuyang Xie",
            "Yaran Shi",
            "Xiaozhi Wang",
            "Jing Zhang",
            "Lei Hou",
            "Juanzi Li"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T15:59:49+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11681v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11681v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000080,
        "doi": "10.1145/3637528.3671564",
        "title": "R-Eval: A Unified Toolkit for Evaluating Domain Knowledge of Retrieval Augmented Large Language Models",
        "abstract": "Large language models have achieved remarkable success on general NLP tasks,\nbut they may fall short for domain-specific problems. Recently, various\nRetrieval-Augmented Large Language Models (RALLMs) are proposed to address this\nshortcoming. However, existing evaluation tools only provide a few baselines\nand evaluate them on various domains without mining the depth of domain\nknowledge. In this paper, we address the challenges of evaluating RALLMs by\nintroducing the R-Eval toolkit, a Python toolkit designed to streamline the\nevaluation of different RAG workflows in conjunction with LLMs. Our toolkit,\nwhich supports popular built-in RAG workflows and allows for the incorporation\nof customized testing data on the specific domain, is designed to be\nuser-friendly, modular, and extensible. We conduct an evaluation of 21 RALLMs\nacross three task levels and two representative domains, revealing significant\nvariations in the effectiveness of RALLMs across different tasks and domains.\nOur analysis emphasizes the importance of considering both task and domain\nrequirements when choosing a RAG workflow and LLM combination. We are committed\nto continuously maintaining our platform at https://github.com/THU-KEG/R-Eval\nto facilitate both the industry and the researchers.",
        "chunk-id": 2,
        "chunk": "knowledge. In this paper, we address the challenges of evaluating RALLMs by\nintroducing the R-Eval toolkit, a Python toolkit designed to streamline the\nevaluation of different RAG workflows in conjunction with LLMs. Our toolkit,\nwhich supports popular built-in RAG workflows and allows for the incorporation\nof customized testing data on the specific domain, is designed to be",
        "authors": [
            "Shangqing Tu",
            "Yuanchun Wang",
            "Jifan Yu",
            "Yuyang Xie",
            "Yaran Shi",
            "Xiaozhi Wang",
            "Jing Zhang",
            "Lei Hou",
            "Juanzi Li"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T15:59:49+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11681v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11681v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000080,
        "doi": "10.1145/3637528.3671564",
        "title": "R-Eval: A Unified Toolkit for Evaluating Domain Knowledge of Retrieval Augmented Large Language Models",
        "abstract": "Large language models have achieved remarkable success on general NLP tasks,\nbut they may fall short for domain-specific problems. Recently, various\nRetrieval-Augmented Large Language Models (RALLMs) are proposed to address this\nshortcoming. However, existing evaluation tools only provide a few baselines\nand evaluate them on various domains without mining the depth of domain\nknowledge. In this paper, we address the challenges of evaluating RALLMs by\nintroducing the R-Eval toolkit, a Python toolkit designed to streamline the\nevaluation of different RAG workflows in conjunction with LLMs. Our toolkit,\nwhich supports popular built-in RAG workflows and allows for the incorporation\nof customized testing data on the specific domain, is designed to be\nuser-friendly, modular, and extensible. We conduct an evaluation of 21 RALLMs\nacross three task levels and two representative domains, revealing significant\nvariations in the effectiveness of RALLMs across different tasks and domains.\nOur analysis emphasizes the importance of considering both task and domain\nrequirements when choosing a RAG workflow and LLM combination. We are committed\nto continuously maintaining our platform at https://github.com/THU-KEG/R-Eval\nto facilitate both the industry and the researchers.",
        "chunk-id": 3,
        "chunk": "user-friendly, modular, and extensible. We conduct an evaluation of 21 RALLMs\nacross three task levels and two representative domains, revealing significant\nvariations in the effectiveness of RALLMs across different tasks and domains.\nOur analysis emphasizes the importance of considering both task and domain",
        "authors": [
            "Shangqing Tu",
            "Yuanchun Wang",
            "Jifan Yu",
            "Yuyang Xie",
            "Yaran Shi",
            "Xiaozhi Wang",
            "Jing Zhang",
            "Lei Hou",
            "Juanzi Li"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T15:59:49+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11681v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11681v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000080,
        "doi": "10.1145/3637528.3671564",
        "title": "R-Eval: A Unified Toolkit for Evaluating Domain Knowledge of Retrieval Augmented Large Language Models",
        "abstract": "Large language models have achieved remarkable success on general NLP tasks,\nbut they may fall short for domain-specific problems. Recently, various\nRetrieval-Augmented Large Language Models (RALLMs) are proposed to address this\nshortcoming. However, existing evaluation tools only provide a few baselines\nand evaluate them on various domains without mining the depth of domain\nknowledge. In this paper, we address the challenges of evaluating RALLMs by\nintroducing the R-Eval toolkit, a Python toolkit designed to streamline the\nevaluation of different RAG workflows in conjunction with LLMs. Our toolkit,\nwhich supports popular built-in RAG workflows and allows for the incorporation\nof customized testing data on the specific domain, is designed to be\nuser-friendly, modular, and extensible. We conduct an evaluation of 21 RALLMs\nacross three task levels and two representative domains, revealing significant\nvariations in the effectiveness of RALLMs across different tasks and domains.\nOur analysis emphasizes the importance of considering both task and domain\nrequirements when choosing a RAG workflow and LLM combination. We are committed\nto continuously maintaining our platform at https://github.com/THU-KEG/R-Eval\nto facilitate both the industry and the researchers.",
        "chunk-id": 4,
        "chunk": "requirements when choosing a RAG workflow and LLM combination. We are committed\nto continuously maintaining our platform at https://github.com/THU-KEG/R-Eval\nto facilitate both the industry and the researchers.",
        "authors": [
            "Shangqing Tu",
            "Yuanchun Wang",
            "Jifan Yu",
            "Yuyang Xie",
            "Yaran Shi",
            "Xiaozhi Wang",
            "Jing Zhang",
            "Lei Hou",
            "Juanzi Li"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T15:59:49+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11681v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11681v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000081,
        "doi": null,
        "title": "Building Knowledge-Guided Lexica to Model Cultural Variation",
        "abstract": "Cultural variation exists between nations (e.g., the United States vs.\nChina), but also within regions (e.g., California vs. Texas, Los Angeles vs.\nSan Francisco). Measuring this regional cultural variation can illuminate how\nand why people think and behave differently. Historically, it has been\ndifficult to computationally model cultural variation due to a lack of training\ndata and scalability constraints. In this work, we introduce a new research\nproblem for the NLP community: How do we measure variation in cultural\nconstructs across regions using language? We then provide a scalable solution:\nbuilding knowledge-guided lexica to model cultural variation, encouraging\nfuture work at the intersection of NLP and cultural understanding. We also\nhighlight modern LLMs' failure to measure cultural variation or generate\nculturally varied language.",
        "chunk-id": 1,
        "chunk": "Cultural variation exists between nations (e.g., the United States vs.\nChina), but also within regions (e.g., California vs. Texas, Los Angeles vs.\nSan Francisco). Measuring this regional cultural variation can illuminate how\nand why people think and behave differently. Historically, it has been\ndifficult to computationally model cultural variation due to a lack of training",
        "authors": [
            "Shreya Havaldar",
            "Salvatore Giorgi",
            "Sunny Rai",
            "Thomas Talhelm",
            "Sharath Chandra Guntuku",
            "Lyle Ungar"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T15:05:43+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11622v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11622v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000081,
        "doi": null,
        "title": "Building Knowledge-Guided Lexica to Model Cultural Variation",
        "abstract": "Cultural variation exists between nations (e.g., the United States vs.\nChina), but also within regions (e.g., California vs. Texas, Los Angeles vs.\nSan Francisco). Measuring this regional cultural variation can illuminate how\nand why people think and behave differently. Historically, it has been\ndifficult to computationally model cultural variation due to a lack of training\ndata and scalability constraints. In this work, we introduce a new research\nproblem for the NLP community: How do we measure variation in cultural\nconstructs across regions using language? We then provide a scalable solution:\nbuilding knowledge-guided lexica to model cultural variation, encouraging\nfuture work at the intersection of NLP and cultural understanding. We also\nhighlight modern LLMs' failure to measure cultural variation or generate\nculturally varied language.",
        "chunk-id": 2,
        "chunk": "data and scalability constraints. In this work, we introduce a new research\nproblem for the NLP community: How do we measure variation in cultural\nconstructs across regions using language? We then provide a scalable solution:\nbuilding knowledge-guided lexica to model cultural variation, encouraging\nfuture work at the intersection of NLP and cultural understanding. We also",
        "authors": [
            "Shreya Havaldar",
            "Salvatore Giorgi",
            "Sunny Rai",
            "Thomas Talhelm",
            "Sharath Chandra Guntuku",
            "Lyle Ungar"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T15:05:43+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11622v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11622v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000081,
        "doi": null,
        "title": "Building Knowledge-Guided Lexica to Model Cultural Variation",
        "abstract": "Cultural variation exists between nations (e.g., the United States vs.\nChina), but also within regions (e.g., California vs. Texas, Los Angeles vs.\nSan Francisco). Measuring this regional cultural variation can illuminate how\nand why people think and behave differently. Historically, it has been\ndifficult to computationally model cultural variation due to a lack of training\ndata and scalability constraints. In this work, we introduce a new research\nproblem for the NLP community: How do we measure variation in cultural\nconstructs across regions using language? We then provide a scalable solution:\nbuilding knowledge-guided lexica to model cultural variation, encouraging\nfuture work at the intersection of NLP and cultural understanding. We also\nhighlight modern LLMs' failure to measure cultural variation or generate\nculturally varied language.",
        "chunk-id": 3,
        "chunk": "highlight modern LLMs' failure to measure cultural variation or generate\nculturally varied language.",
        "authors": [
            "Shreya Havaldar",
            "Salvatore Giorgi",
            "Sunny Rai",
            "Thomas Talhelm",
            "Sharath Chandra Guntuku",
            "Lyle Ungar"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T15:05:43+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11622v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11622v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000082,
        "doi": null,
        "title": "Understanding \"Democratization\" in NLP and ML Research",
        "abstract": "Recent improvements in natural language processing (NLP) and machine learning\n(ML) and increased mainstream adoption have led to researchers frequently\ndiscussing the \"democratization\" of artificial intelligence. In this paper, we\nseek to clarify how democratization is understood in NLP and ML publications,\nthrough large-scale mixed-methods analyses of papers using the keyword\n\"democra*\" published in NLP and adjacent venues. We find that democratization\nis most frequently used to convey (ease of) access to or use of technologies,\nwithout meaningfully engaging with theories of democratization, while research\nusing other invocations of \"democra*\" tends to be grounded in theories of\ndeliberation and debate. Based on our findings, we call for researchers to\nenrich their use of the term democratization with appropriate theory, towards\ndemocratic technologies beyond superficial access.",
        "chunk-id": 1,
        "chunk": "Recent improvements in natural language processing (NLP) and machine learning\n(ML) and increased mainstream adoption have led to researchers frequently\ndiscussing the \"democratization\" of artificial intelligence. In this paper, we\nseek to clarify how democratization is understood in NLP and ML publications,\nthrough large-scale mixed-methods analyses of papers using the keyword",
        "authors": [
            "Arjun Subramonian",
            "Vagrant Gautam",
            "Dietrich Klakow",
            "Zeerak Talat"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T14:47:06+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11598v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11598v1",
        "categories": [
            "Computation and Language",
            "Computers and Society"
        ]
    },
    {
        "id": 20000082,
        "doi": null,
        "title": "Understanding \"Democratization\" in NLP and ML Research",
        "abstract": "Recent improvements in natural language processing (NLP) and machine learning\n(ML) and increased mainstream adoption have led to researchers frequently\ndiscussing the \"democratization\" of artificial intelligence. In this paper, we\nseek to clarify how democratization is understood in NLP and ML publications,\nthrough large-scale mixed-methods analyses of papers using the keyword\n\"democra*\" published in NLP and adjacent venues. We find that democratization\nis most frequently used to convey (ease of) access to or use of technologies,\nwithout meaningfully engaging with theories of democratization, while research\nusing other invocations of \"democra*\" tends to be grounded in theories of\ndeliberation and debate. Based on our findings, we call for researchers to\nenrich their use of the term democratization with appropriate theory, towards\ndemocratic technologies beyond superficial access.",
        "chunk-id": 2,
        "chunk": "\"democra*\" published in NLP and adjacent venues. We find that democratization\nis most frequently used to convey (ease of) access to or use of technologies,\nwithout meaningfully engaging with theories of democratization, while research\nusing other invocations of \"democra*\" tends to be grounded in theories of\ndeliberation and debate. Based on our findings, we call for researchers to",
        "authors": [
            "Arjun Subramonian",
            "Vagrant Gautam",
            "Dietrich Klakow",
            "Zeerak Talat"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T14:47:06+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11598v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11598v1",
        "categories": [
            "Computation and Language",
            "Computers and Society"
        ]
    },
    {
        "id": 20000082,
        "doi": null,
        "title": "Understanding \"Democratization\" in NLP and ML Research",
        "abstract": "Recent improvements in natural language processing (NLP) and machine learning\n(ML) and increased mainstream adoption have led to researchers frequently\ndiscussing the \"democratization\" of artificial intelligence. In this paper, we\nseek to clarify how democratization is understood in NLP and ML publications,\nthrough large-scale mixed-methods analyses of papers using the keyword\n\"democra*\" published in NLP and adjacent venues. We find that democratization\nis most frequently used to convey (ease of) access to or use of technologies,\nwithout meaningfully engaging with theories of democratization, while research\nusing other invocations of \"democra*\" tends to be grounded in theories of\ndeliberation and debate. Based on our findings, we call for researchers to\nenrich their use of the term democratization with appropriate theory, towards\ndemocratic technologies beyond superficial access.",
        "chunk-id": 3,
        "chunk": "enrich their use of the term democratization with appropriate theory, towards\ndemocratic technologies beyond superficial access.",
        "authors": [
            "Arjun Subramonian",
            "Vagrant Gautam",
            "Dietrich Klakow",
            "Zeerak Talat"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T14:47:06+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11598v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11598v1",
        "categories": [
            "Computation and Language",
            "Computers and Society"
        ]
    },
    {
        "id": 20000083,
        "doi": null,
        "title": "Mathematical Entities: Corpora and Benchmarks",
        "abstract": "Mathematics is a highly specialized domain with its own unique set of\nchallenges. Despite this, there has been relatively little research on natural\nlanguage processing for mathematical texts, and there are few mathematical\nlanguage resources aimed at NLP. In this paper, we aim to provide annotated\ncorpora that can be used to study the language of mathematics in different\ncontexts, ranging from fundamental concepts found in textbooks to advanced\nresearch mathematics. We preprocess the corpora with a neural parsing model and\nsome manual intervention to provide part-of-speech tags, lemmas, and dependency\ntrees. In total, we provide 182397 sentences across three corpora. We then aim\nto test and evaluate several noteworthy natural language processing models\nusing these corpora, to show how well they can adapt to the domain of\nmathematics and provide useful tools for exploring mathematical language. We\nevaluate several neural and symbolic models against benchmarks that we extract\nfrom the corpus metadata to show that terminology extraction and definition\nextraction do not easily generalize to mathematics, and that additional work is\nneeded to achieve good performance on these metrics. Finally, we provide a\nlearning assistant that grants access to the content of these corpora in a\ncontext-sensitive manner, utilizing text search and entity linking. Though our\ncorpora and benchmarks provide useful metrics for evaluating mathematical\nlanguage processing, further work is necessary to adapt models to mathematics\nin order to provide more effective learning assistants and apply NLP methods to\ndifferent mathematical domains.",
        "chunk-id": 1,
        "chunk": "Mathematics is a highly specialized domain with its own unique set of\nchallenges. Despite this, there has been relatively little research on natural\nlanguage processing for mathematical texts, and there are few mathematical\nlanguage resources aimed at NLP. In this paper, we aim to provide annotated\ncorpora that can be used to study the language of mathematics in different",
        "authors": [
            "Jacob Collard",
            "Valeria de Paiva",
            "Eswaran Subrahmanian"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T14:11:00+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11577v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11577v1",
        "categories": [
            "Computation and Language",
            "History and Overview"
        ]
    },
    {
        "id": 20000083,
        "doi": null,
        "title": "Mathematical Entities: Corpora and Benchmarks",
        "abstract": "Mathematics is a highly specialized domain with its own unique set of\nchallenges. Despite this, there has been relatively little research on natural\nlanguage processing for mathematical texts, and there are few mathematical\nlanguage resources aimed at NLP. In this paper, we aim to provide annotated\ncorpora that can be used to study the language of mathematics in different\ncontexts, ranging from fundamental concepts found in textbooks to advanced\nresearch mathematics. We preprocess the corpora with a neural parsing model and\nsome manual intervention to provide part-of-speech tags, lemmas, and dependency\ntrees. In total, we provide 182397 sentences across three corpora. We then aim\nto test and evaluate several noteworthy natural language processing models\nusing these corpora, to show how well they can adapt to the domain of\nmathematics and provide useful tools for exploring mathematical language. We\nevaluate several neural and symbolic models against benchmarks that we extract\nfrom the corpus metadata to show that terminology extraction and definition\nextraction do not easily generalize to mathematics, and that additional work is\nneeded to achieve good performance on these metrics. Finally, we provide a\nlearning assistant that grants access to the content of these corpora in a\ncontext-sensitive manner, utilizing text search and entity linking. Though our\ncorpora and benchmarks provide useful metrics for evaluating mathematical\nlanguage processing, further work is necessary to adapt models to mathematics\nin order to provide more effective learning assistants and apply NLP methods to\ndifferent mathematical domains.",
        "chunk-id": 2,
        "chunk": "contexts, ranging from fundamental concepts found in textbooks to advanced\nresearch mathematics. We preprocess the corpora with a neural parsing model and\nsome manual intervention to provide part-of-speech tags, lemmas, and dependency\ntrees. In total, we provide 182397 sentences across three corpora. We then aim",
        "authors": [
            "Jacob Collard",
            "Valeria de Paiva",
            "Eswaran Subrahmanian"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T14:11:00+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11577v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11577v1",
        "categories": [
            "Computation and Language",
            "History and Overview"
        ]
    },
    {
        "id": 20000083,
        "doi": null,
        "title": "Mathematical Entities: Corpora and Benchmarks",
        "abstract": "Mathematics is a highly specialized domain with its own unique set of\nchallenges. Despite this, there has been relatively little research on natural\nlanguage processing for mathematical texts, and there are few mathematical\nlanguage resources aimed at NLP. In this paper, we aim to provide annotated\ncorpora that can be used to study the language of mathematics in different\ncontexts, ranging from fundamental concepts found in textbooks to advanced\nresearch mathematics. We preprocess the corpora with a neural parsing model and\nsome manual intervention to provide part-of-speech tags, lemmas, and dependency\ntrees. In total, we provide 182397 sentences across three corpora. We then aim\nto test and evaluate several noteworthy natural language processing models\nusing these corpora, to show how well they can adapt to the domain of\nmathematics and provide useful tools for exploring mathematical language. We\nevaluate several neural and symbolic models against benchmarks that we extract\nfrom the corpus metadata to show that terminology extraction and definition\nextraction do not easily generalize to mathematics, and that additional work is\nneeded to achieve good performance on these metrics. Finally, we provide a\nlearning assistant that grants access to the content of these corpora in a\ncontext-sensitive manner, utilizing text search and entity linking. Though our\ncorpora and benchmarks provide useful metrics for evaluating mathematical\nlanguage processing, further work is necessary to adapt models to mathematics\nin order to provide more effective learning assistants and apply NLP methods to\ndifferent mathematical domains.",
        "chunk-id": 3,
        "chunk": "to test and evaluate several noteworthy natural language processing models\nusing these corpora, to show how well they can adapt to the domain of\nmathematics and provide useful tools for exploring mathematical language. We\nevaluate several neural and symbolic models against benchmarks that we extract\nfrom the corpus metadata to show that terminology extraction and definition",
        "authors": [
            "Jacob Collard",
            "Valeria de Paiva",
            "Eswaran Subrahmanian"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T14:11:00+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11577v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11577v1",
        "categories": [
            "Computation and Language",
            "History and Overview"
        ]
    },
    {
        "id": 20000083,
        "doi": null,
        "title": "Mathematical Entities: Corpora and Benchmarks",
        "abstract": "Mathematics is a highly specialized domain with its own unique set of\nchallenges. Despite this, there has been relatively little research on natural\nlanguage processing for mathematical texts, and there are few mathematical\nlanguage resources aimed at NLP. In this paper, we aim to provide annotated\ncorpora that can be used to study the language of mathematics in different\ncontexts, ranging from fundamental concepts found in textbooks to advanced\nresearch mathematics. We preprocess the corpora with a neural parsing model and\nsome manual intervention to provide part-of-speech tags, lemmas, and dependency\ntrees. In total, we provide 182397 sentences across three corpora. We then aim\nto test and evaluate several noteworthy natural language processing models\nusing these corpora, to show how well they can adapt to the domain of\nmathematics and provide useful tools for exploring mathematical language. We\nevaluate several neural and symbolic models against benchmarks that we extract\nfrom the corpus metadata to show that terminology extraction and definition\nextraction do not easily generalize to mathematics, and that additional work is\nneeded to achieve good performance on these metrics. Finally, we provide a\nlearning assistant that grants access to the content of these corpora in a\ncontext-sensitive manner, utilizing text search and entity linking. Though our\ncorpora and benchmarks provide useful metrics for evaluating mathematical\nlanguage processing, further work is necessary to adapt models to mathematics\nin order to provide more effective learning assistants and apply NLP methods to\ndifferent mathematical domains.",
        "chunk-id": 4,
        "chunk": "extraction do not easily generalize to mathematics, and that additional work is\nneeded to achieve good performance on these metrics. Finally, we provide a\nlearning assistant that grants access to the content of these corpora in a\ncontext-sensitive manner, utilizing text search and entity linking. Though our\ncorpora and benchmarks provide useful metrics for evaluating mathematical",
        "authors": [
            "Jacob Collard",
            "Valeria de Paiva",
            "Eswaran Subrahmanian"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T14:11:00+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11577v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11577v1",
        "categories": [
            "Computation and Language",
            "History and Overview"
        ]
    },
    {
        "id": 20000083,
        "doi": null,
        "title": "Mathematical Entities: Corpora and Benchmarks",
        "abstract": "Mathematics is a highly specialized domain with its own unique set of\nchallenges. Despite this, there has been relatively little research on natural\nlanguage processing for mathematical texts, and there are few mathematical\nlanguage resources aimed at NLP. In this paper, we aim to provide annotated\ncorpora that can be used to study the language of mathematics in different\ncontexts, ranging from fundamental concepts found in textbooks to advanced\nresearch mathematics. We preprocess the corpora with a neural parsing model and\nsome manual intervention to provide part-of-speech tags, lemmas, and dependency\ntrees. In total, we provide 182397 sentences across three corpora. We then aim\nto test and evaluate several noteworthy natural language processing models\nusing these corpora, to show how well they can adapt to the domain of\nmathematics and provide useful tools for exploring mathematical language. We\nevaluate several neural and symbolic models against benchmarks that we extract\nfrom the corpus metadata to show that terminology extraction and definition\nextraction do not easily generalize to mathematics, and that additional work is\nneeded to achieve good performance on these metrics. Finally, we provide a\nlearning assistant that grants access to the content of these corpora in a\ncontext-sensitive manner, utilizing text search and entity linking. Though our\ncorpora and benchmarks provide useful metrics for evaluating mathematical\nlanguage processing, further work is necessary to adapt models to mathematics\nin order to provide more effective learning assistants and apply NLP methods to\ndifferent mathematical domains.",
        "chunk-id": 5,
        "chunk": "language processing, further work is necessary to adapt models to mathematics\nin order to provide more effective learning assistants and apply NLP methods to\ndifferent mathematical domains.",
        "authors": [
            "Jacob Collard",
            "Valeria de Paiva",
            "Eswaran Subrahmanian"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T14:11:00+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11577v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11577v1",
        "categories": [
            "Computation and Language",
            "History and Overview"
        ]
    },
    {
        "id": 20000084,
        "doi": null,
        "title": "GECOBench: A Gender-Controlled Text Dataset and Benchmark for Quantifying Biases in Explanations",
        "abstract": "Large pre-trained language models have become popular for many applications\nand form an important backbone of many downstream tasks in natural language\nprocessing (NLP). Applying 'explainable artificial intelligence' (XAI)\ntechniques to enrich such models' outputs is considered crucial for assuring\ntheir quality and shedding light on their inner workings. However, large\nlanguage models are trained on a plethora of data containing a variety of\nbiases, such as gender biases, affecting model weights and, potentially,\nbehavior. Currently, it is unclear to what extent such biases also impact model\nexplanations in possibly unfavorable ways. We create a gender-controlled text\ndataset, GECO, in which otherwise identical sentences appear in male and female\nforms. This gives rise to ground-truth 'world explanations' for gender\nclassification tasks, enabling the objective evaluation of the correctness of\nXAI methods. We also provide GECOBench, a rigorous quantitative evaluation\nframework benchmarking popular XAI methods, applying them to pre-trained\nlanguage models fine-tuned to different degrees. This allows us to investigate\nhow pre-training induces undesirable bias in model explanations and to what\nextent fine-tuning can mitigate such explanation bias. We show a clear\ndependency between explanation performance and the number of fine-tuned layers,\nwhere XAI methods are observed to particularly benefit from fine-tuning or\ncomplete retraining of embedding layers. Remarkably, this relationship holds\nfor models achieving similar classification performance on the same task. With\nthat, we highlight the utility of the proposed gender-controlled dataset and\nnovel benchmarking approach for research and development of novel XAI methods.\nAll code including dataset generation, model training, evaluation and\nvisualization is available at: https://github.com/braindatalab/gecobench",
        "chunk-id": 1,
        "chunk": "Large pre-trained language models have become popular for many applications\nand form an important backbone of many downstream tasks in natural language\nprocessing (NLP). Applying 'explainable artificial intelligence' (XAI)\ntechniques to enrich such models' outputs is considered crucial for assuring\ntheir quality and shedding light on their inner workings. However, large",
        "authors": [
            "Rick Wilming",
            "Artur Dox",
            "Hjalmar Schulz",
            "Marta Oliveira",
            "Benedict Clark",
            "Stefan Haufe"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T13:44:37+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11547v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11547v1",
        "categories": [
            "Machine Learning",
            "Artificial Intelligence",
            "Computation and Language",
            "Computers and Society"
        ]
    },
    {
        "id": 20000084,
        "doi": null,
        "title": "GECOBench: A Gender-Controlled Text Dataset and Benchmark for Quantifying Biases in Explanations",
        "abstract": "Large pre-trained language models have become popular for many applications\nand form an important backbone of many downstream tasks in natural language\nprocessing (NLP). Applying 'explainable artificial intelligence' (XAI)\ntechniques to enrich such models' outputs is considered crucial for assuring\ntheir quality and shedding light on their inner workings. However, large\nlanguage models are trained on a plethora of data containing a variety of\nbiases, such as gender biases, affecting model weights and, potentially,\nbehavior. Currently, it is unclear to what extent such biases also impact model\nexplanations in possibly unfavorable ways. We create a gender-controlled text\ndataset, GECO, in which otherwise identical sentences appear in male and female\nforms. This gives rise to ground-truth 'world explanations' for gender\nclassification tasks, enabling the objective evaluation of the correctness of\nXAI methods. We also provide GECOBench, a rigorous quantitative evaluation\nframework benchmarking popular XAI methods, applying them to pre-trained\nlanguage models fine-tuned to different degrees. This allows us to investigate\nhow pre-training induces undesirable bias in model explanations and to what\nextent fine-tuning can mitigate such explanation bias. We show a clear\ndependency between explanation performance and the number of fine-tuned layers,\nwhere XAI methods are observed to particularly benefit from fine-tuning or\ncomplete retraining of embedding layers. Remarkably, this relationship holds\nfor models achieving similar classification performance on the same task. With\nthat, we highlight the utility of the proposed gender-controlled dataset and\nnovel benchmarking approach for research and development of novel XAI methods.\nAll code including dataset generation, model training, evaluation and\nvisualization is available at: https://github.com/braindatalab/gecobench",
        "chunk-id": 2,
        "chunk": "language models are trained on a plethora of data containing a variety of\nbiases, such as gender biases, affecting model weights and, potentially,\nbehavior. Currently, it is unclear to what extent such biases also impact model\nexplanations in possibly unfavorable ways. We create a gender-controlled text\ndataset, GECO, in which otherwise identical sentences appear in male and female",
        "authors": [
            "Rick Wilming",
            "Artur Dox",
            "Hjalmar Schulz",
            "Marta Oliveira",
            "Benedict Clark",
            "Stefan Haufe"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T13:44:37+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11547v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11547v1",
        "categories": [
            "Machine Learning",
            "Artificial Intelligence",
            "Computation and Language",
            "Computers and Society"
        ]
    },
    {
        "id": 20000084,
        "doi": null,
        "title": "GECOBench: A Gender-Controlled Text Dataset and Benchmark for Quantifying Biases in Explanations",
        "abstract": "Large pre-trained language models have become popular for many applications\nand form an important backbone of many downstream tasks in natural language\nprocessing (NLP). Applying 'explainable artificial intelligence' (XAI)\ntechniques to enrich such models' outputs is considered crucial for assuring\ntheir quality and shedding light on their inner workings. However, large\nlanguage models are trained on a plethora of data containing a variety of\nbiases, such as gender biases, affecting model weights and, potentially,\nbehavior. Currently, it is unclear to what extent such biases also impact model\nexplanations in possibly unfavorable ways. We create a gender-controlled text\ndataset, GECO, in which otherwise identical sentences appear in male and female\nforms. This gives rise to ground-truth 'world explanations' for gender\nclassification tasks, enabling the objective evaluation of the correctness of\nXAI methods. We also provide GECOBench, a rigorous quantitative evaluation\nframework benchmarking popular XAI methods, applying them to pre-trained\nlanguage models fine-tuned to different degrees. This allows us to investigate\nhow pre-training induces undesirable bias in model explanations and to what\nextent fine-tuning can mitigate such explanation bias. We show a clear\ndependency between explanation performance and the number of fine-tuned layers,\nwhere XAI methods are observed to particularly benefit from fine-tuning or\ncomplete retraining of embedding layers. Remarkably, this relationship holds\nfor models achieving similar classification performance on the same task. With\nthat, we highlight the utility of the proposed gender-controlled dataset and\nnovel benchmarking approach for research and development of novel XAI methods.\nAll code including dataset generation, model training, evaluation and\nvisualization is available at: https://github.com/braindatalab/gecobench",
        "chunk-id": 3,
        "chunk": "forms. This gives rise to ground-truth 'world explanations' for gender\nclassification tasks, enabling the objective evaluation of the correctness of\nXAI methods. We also provide GECOBench, a rigorous quantitative evaluation\nframework benchmarking popular XAI methods, applying them to pre-trained\nlanguage models fine-tuned to different degrees. This allows us to investigate",
        "authors": [
            "Rick Wilming",
            "Artur Dox",
            "Hjalmar Schulz",
            "Marta Oliveira",
            "Benedict Clark",
            "Stefan Haufe"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T13:44:37+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11547v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11547v1",
        "categories": [
            "Machine Learning",
            "Artificial Intelligence",
            "Computation and Language",
            "Computers and Society"
        ]
    },
    {
        "id": 20000084,
        "doi": null,
        "title": "GECOBench: A Gender-Controlled Text Dataset and Benchmark for Quantifying Biases in Explanations",
        "abstract": "Large pre-trained language models have become popular for many applications\nand form an important backbone of many downstream tasks in natural language\nprocessing (NLP). Applying 'explainable artificial intelligence' (XAI)\ntechniques to enrich such models' outputs is considered crucial for assuring\ntheir quality and shedding light on their inner workings. However, large\nlanguage models are trained on a plethora of data containing a variety of\nbiases, such as gender biases, affecting model weights and, potentially,\nbehavior. Currently, it is unclear to what extent such biases also impact model\nexplanations in possibly unfavorable ways. We create a gender-controlled text\ndataset, GECO, in which otherwise identical sentences appear in male and female\nforms. This gives rise to ground-truth 'world explanations' for gender\nclassification tasks, enabling the objective evaluation of the correctness of\nXAI methods. We also provide GECOBench, a rigorous quantitative evaluation\nframework benchmarking popular XAI methods, applying them to pre-trained\nlanguage models fine-tuned to different degrees. This allows us to investigate\nhow pre-training induces undesirable bias in model explanations and to what\nextent fine-tuning can mitigate such explanation bias. We show a clear\ndependency between explanation performance and the number of fine-tuned layers,\nwhere XAI methods are observed to particularly benefit from fine-tuning or\ncomplete retraining of embedding layers. Remarkably, this relationship holds\nfor models achieving similar classification performance on the same task. With\nthat, we highlight the utility of the proposed gender-controlled dataset and\nnovel benchmarking approach for research and development of novel XAI methods.\nAll code including dataset generation, model training, evaluation and\nvisualization is available at: https://github.com/braindatalab/gecobench",
        "chunk-id": 4,
        "chunk": "how pre-training induces undesirable bias in model explanations and to what\nextent fine-tuning can mitigate such explanation bias. We show a clear\ndependency between explanation performance and the number of fine-tuned layers,\nwhere XAI methods are observed to particularly benefit from fine-tuning or\ncomplete retraining of embedding layers. Remarkably, this relationship holds",
        "authors": [
            "Rick Wilming",
            "Artur Dox",
            "Hjalmar Schulz",
            "Marta Oliveira",
            "Benedict Clark",
            "Stefan Haufe"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T13:44:37+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11547v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11547v1",
        "categories": [
            "Machine Learning",
            "Artificial Intelligence",
            "Computation and Language",
            "Computers and Society"
        ]
    },
    {
        "id": 20000084,
        "doi": null,
        "title": "GECOBench: A Gender-Controlled Text Dataset and Benchmark for Quantifying Biases in Explanations",
        "abstract": "Large pre-trained language models have become popular for many applications\nand form an important backbone of many downstream tasks in natural language\nprocessing (NLP). Applying 'explainable artificial intelligence' (XAI)\ntechniques to enrich such models' outputs is considered crucial for assuring\ntheir quality and shedding light on their inner workings. However, large\nlanguage models are trained on a plethora of data containing a variety of\nbiases, such as gender biases, affecting model weights and, potentially,\nbehavior. Currently, it is unclear to what extent such biases also impact model\nexplanations in possibly unfavorable ways. We create a gender-controlled text\ndataset, GECO, in which otherwise identical sentences appear in male and female\nforms. This gives rise to ground-truth 'world explanations' for gender\nclassification tasks, enabling the objective evaluation of the correctness of\nXAI methods. We also provide GECOBench, a rigorous quantitative evaluation\nframework benchmarking popular XAI methods, applying them to pre-trained\nlanguage models fine-tuned to different degrees. This allows us to investigate\nhow pre-training induces undesirable bias in model explanations and to what\nextent fine-tuning can mitigate such explanation bias. We show a clear\ndependency between explanation performance and the number of fine-tuned layers,\nwhere XAI methods are observed to particularly benefit from fine-tuning or\ncomplete retraining of embedding layers. Remarkably, this relationship holds\nfor models achieving similar classification performance on the same task. With\nthat, we highlight the utility of the proposed gender-controlled dataset and\nnovel benchmarking approach for research and development of novel XAI methods.\nAll code including dataset generation, model training, evaluation and\nvisualization is available at: https://github.com/braindatalab/gecobench",
        "chunk-id": 5,
        "chunk": "for models achieving similar classification performance on the same task. With\nthat, we highlight the utility of the proposed gender-controlled dataset and\nnovel benchmarking approach for research and development of novel XAI methods.\nAll code including dataset generation, model training, evaluation and\nvisualization is available at: https://github.com/braindatalab/gecobench",
        "authors": [
            "Rick Wilming",
            "Artur Dox",
            "Hjalmar Schulz",
            "Marta Oliveira",
            "Benedict Clark",
            "Stefan Haufe"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T13:44:37+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11547v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11547v1",
        "categories": [
            "Machine Learning",
            "Artificial Intelligence",
            "Computation and Language",
            "Computers and Society"
        ]
    },
    {
        "id": 20000085,
        "doi": null,
        "title": "Promises, Outlooks and Challenges of Diffusion Language Modeling",
        "abstract": "The modern autoregressive Large Language Models (LLMs) have achieved\noutstanding performance on NLP benchmarks, and they are deployed in the real\nworld. However, they still suffer from limitations of the autoregressive\ntraining paradigm. For example, autoregressive token generation is notably slow\nand can be prone to \\textit{exposure bias}. The diffusion-based language models\nwere proposed as an alternative to autoregressive generation to address some of\nthese limitations. We evaluate the recently proposed Score Entropy Discrete\nDiffusion (SEDD) approach and show it is a promising alternative to\nautoregressive generation but it has some short-comings too. We empirically\ndemonstrate the advantages and challenges of SEDD, and observe that SEDD\ngenerally matches autoregressive models in perplexity and on benchmarks such as\nHellaSwag, Arc or WinoGrande. Additionally, we show that in terms of inference\nlatency, SEDD can be up to 4.5$\\times$ more efficient than GPT-2. While SEDD\nallows conditioning on tokens at abitrary positions, SEDD appears slightly\nweaker than GPT-2 for conditional generation given short prompts. Finally, we\nreproduced the main results from the original SEDD paper.",
        "chunk-id": 1,
        "chunk": "The modern autoregressive Large Language Models (LLMs) have achieved\noutstanding performance on NLP benchmarks, and they are deployed in the real\nworld. However, they still suffer from limitations of the autoregressive\ntraining paradigm. For example, autoregressive token generation is notably slow\nand can be prone to \\textit{exposure bias}. The diffusion-based language models",
        "authors": [
            "Justin Deschenaux",
            "Caglar Gulcehre"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T12:38:38+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11473v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11473v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000085,
        "doi": null,
        "title": "Promises, Outlooks and Challenges of Diffusion Language Modeling",
        "abstract": "The modern autoregressive Large Language Models (LLMs) have achieved\noutstanding performance on NLP benchmarks, and they are deployed in the real\nworld. However, they still suffer from limitations of the autoregressive\ntraining paradigm. For example, autoregressive token generation is notably slow\nand can be prone to \\textit{exposure bias}. The diffusion-based language models\nwere proposed as an alternative to autoregressive generation to address some of\nthese limitations. We evaluate the recently proposed Score Entropy Discrete\nDiffusion (SEDD) approach and show it is a promising alternative to\nautoregressive generation but it has some short-comings too. We empirically\ndemonstrate the advantages and challenges of SEDD, and observe that SEDD\ngenerally matches autoregressive models in perplexity and on benchmarks such as\nHellaSwag, Arc or WinoGrande. Additionally, we show that in terms of inference\nlatency, SEDD can be up to 4.5$\\times$ more efficient than GPT-2. While SEDD\nallows conditioning on tokens at abitrary positions, SEDD appears slightly\nweaker than GPT-2 for conditional generation given short prompts. Finally, we\nreproduced the main results from the original SEDD paper.",
        "chunk-id": 2,
        "chunk": "were proposed as an alternative to autoregressive generation to address some of\nthese limitations. We evaluate the recently proposed Score Entropy Discrete\nDiffusion (SEDD) approach and show it is a promising alternative to\nautoregressive generation but it has some short-comings too. We empirically\ndemonstrate the advantages and challenges of SEDD, and observe that SEDD",
        "authors": [
            "Justin Deschenaux",
            "Caglar Gulcehre"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T12:38:38+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11473v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11473v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000085,
        "doi": null,
        "title": "Promises, Outlooks and Challenges of Diffusion Language Modeling",
        "abstract": "The modern autoregressive Large Language Models (LLMs) have achieved\noutstanding performance on NLP benchmarks, and they are deployed in the real\nworld. However, they still suffer from limitations of the autoregressive\ntraining paradigm. For example, autoregressive token generation is notably slow\nand can be prone to \\textit{exposure bias}. The diffusion-based language models\nwere proposed as an alternative to autoregressive generation to address some of\nthese limitations. We evaluate the recently proposed Score Entropy Discrete\nDiffusion (SEDD) approach and show it is a promising alternative to\nautoregressive generation but it has some short-comings too. We empirically\ndemonstrate the advantages and challenges of SEDD, and observe that SEDD\ngenerally matches autoregressive models in perplexity and on benchmarks such as\nHellaSwag, Arc or WinoGrande. Additionally, we show that in terms of inference\nlatency, SEDD can be up to 4.5$\\times$ more efficient than GPT-2. While SEDD\nallows conditioning on tokens at abitrary positions, SEDD appears slightly\nweaker than GPT-2 for conditional generation given short prompts. Finally, we\nreproduced the main results from the original SEDD paper.",
        "chunk-id": 3,
        "chunk": "generally matches autoregressive models in perplexity and on benchmarks such as\nHellaSwag, Arc or WinoGrande. Additionally, we show that in terms of inference\nlatency, SEDD can be up to 4.5$\\times$ more efficient than GPT-2. While SEDD\nallows conditioning on tokens at abitrary positions, SEDD appears slightly",
        "authors": [
            "Justin Deschenaux",
            "Caglar Gulcehre"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T12:38:38+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11473v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11473v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000085,
        "doi": null,
        "title": "Promises, Outlooks and Challenges of Diffusion Language Modeling",
        "abstract": "The modern autoregressive Large Language Models (LLMs) have achieved\noutstanding performance on NLP benchmarks, and they are deployed in the real\nworld. However, they still suffer from limitations of the autoregressive\ntraining paradigm. For example, autoregressive token generation is notably slow\nand can be prone to \\textit{exposure bias}. The diffusion-based language models\nwere proposed as an alternative to autoregressive generation to address some of\nthese limitations. We evaluate the recently proposed Score Entropy Discrete\nDiffusion (SEDD) approach and show it is a promising alternative to\nautoregressive generation but it has some short-comings too. We empirically\ndemonstrate the advantages and challenges of SEDD, and observe that SEDD\ngenerally matches autoregressive models in perplexity and on benchmarks such as\nHellaSwag, Arc or WinoGrande. Additionally, we show that in terms of inference\nlatency, SEDD can be up to 4.5$\\times$ more efficient than GPT-2. While SEDD\nallows conditioning on tokens at abitrary positions, SEDD appears slightly\nweaker than GPT-2 for conditional generation given short prompts. Finally, we\nreproduced the main results from the original SEDD paper.",
        "chunk-id": 4,
        "chunk": "weaker than GPT-2 for conditional generation given short prompts. Finally, we\nreproduced the main results from the original SEDD paper.",
        "authors": [
            "Justin Deschenaux",
            "Caglar Gulcehre"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T12:38:38+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11473v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11473v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000086,
        "doi": null,
        "title": "JobFair: A Framework for Benchmarking Gender Hiring Bias in Large Language Models",
        "abstract": "This paper presents a novel framework for benchmarking hierarchical gender\nhiring bias in Large Language Models (LLMs) for resume scoring, revealing\nsignificant issues of reverse bias and overdebiasing. Our contributions are\nfourfold: First, we introduce a framework using a real, anonymized resume\ndataset from the Healthcare, Finance, and Construction industries, meticulously\nused to avoid confounding factors. It evaluates gender hiring biases across\nhierarchical levels, including Level bias, Spread bias, Taste-based bias, and\nStatistical bias. This framework can be generalized to other social traits and\ntasks easily. Second, we propose novel statistical and computational hiring\nbias metrics based on a counterfactual approach, including Rank After Scoring\n(RAS), Rank-based Impact Ratio, Permutation Test-Based Metrics, and Fixed\nEffects Model-based Metrics. These metrics, rooted in labor economics, NLP, and\nlaw, enable holistic evaluation of hiring biases. Third, we analyze hiring\nbiases in ten state-of-the-art LLMs. Six out of ten LLMs show significant\nbiases against males in healthcare and finance. An industry-effect regression\nreveals that the healthcare industry is the most biased against males. GPT-4o\nand GPT-3.5 are the most biased models, showing significant bias in all three\nindustries. Conversely, Gemini-1.5-Pro, Llama3-8b-Instruct, and\nLlama3-70b-Instruct are the least biased. The hiring bias of all LLMs, except\nfor Llama3-8b-Instruct and Claude-3-Sonnet, remains consistent regardless of\nrandom expansion or reduction of resume content. Finally, we offer a\nuser-friendly demo to facilitate adoption and practical application of the\nframework.",
        "chunk-id": 1,
        "chunk": "This paper presents a novel framework for benchmarking hierarchical gender\nhiring bias in Large Language Models (LLMs) for resume scoring, revealing\nsignificant issues of reverse bias and overdebiasing. Our contributions are\nfourfold: First, we introduce a framework using a real, anonymized resume\ndataset from the Healthcare, Finance, and Construction industries, meticulously",
        "authors": [
            "Ze Wang",
            "Zekun Wu",
            "Xin Guan",
            "Michael Thaler",
            "Adriano Koshiyama",
            "Skylar Lu",
            "Sachin Beepath",
            "Ediz Ertekin Jr.",
            "Maria Perez-Ortiz"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T09:15:57+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.15484v1",
        "arxiv_link": "http://arxiv.org/abs/2406.15484v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Computers and Society"
        ]
    },
    {
        "id": 20000086,
        "doi": null,
        "title": "JobFair: A Framework for Benchmarking Gender Hiring Bias in Large Language Models",
        "abstract": "This paper presents a novel framework for benchmarking hierarchical gender\nhiring bias in Large Language Models (LLMs) for resume scoring, revealing\nsignificant issues of reverse bias and overdebiasing. Our contributions are\nfourfold: First, we introduce a framework using a real, anonymized resume\ndataset from the Healthcare, Finance, and Construction industries, meticulously\nused to avoid confounding factors. It evaluates gender hiring biases across\nhierarchical levels, including Level bias, Spread bias, Taste-based bias, and\nStatistical bias. This framework can be generalized to other social traits and\ntasks easily. Second, we propose novel statistical and computational hiring\nbias metrics based on a counterfactual approach, including Rank After Scoring\n(RAS), Rank-based Impact Ratio, Permutation Test-Based Metrics, and Fixed\nEffects Model-based Metrics. These metrics, rooted in labor economics, NLP, and\nlaw, enable holistic evaluation of hiring biases. Third, we analyze hiring\nbiases in ten state-of-the-art LLMs. Six out of ten LLMs show significant\nbiases against males in healthcare and finance. An industry-effect regression\nreveals that the healthcare industry is the most biased against males. GPT-4o\nand GPT-3.5 are the most biased models, showing significant bias in all three\nindustries. Conversely, Gemini-1.5-Pro, Llama3-8b-Instruct, and\nLlama3-70b-Instruct are the least biased. The hiring bias of all LLMs, except\nfor Llama3-8b-Instruct and Claude-3-Sonnet, remains consistent regardless of\nrandom expansion or reduction of resume content. Finally, we offer a\nuser-friendly demo to facilitate adoption and practical application of the\nframework.",
        "chunk-id": 2,
        "chunk": "used to avoid confounding factors. It evaluates gender hiring biases across\nhierarchical levels, including Level bias, Spread bias, Taste-based bias, and\nStatistical bias. This framework can be generalized to other social traits and\ntasks easily. Second, we propose novel statistical and computational hiring",
        "authors": [
            "Ze Wang",
            "Zekun Wu",
            "Xin Guan",
            "Michael Thaler",
            "Adriano Koshiyama",
            "Skylar Lu",
            "Sachin Beepath",
            "Ediz Ertekin Jr.",
            "Maria Perez-Ortiz"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T09:15:57+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.15484v1",
        "arxiv_link": "http://arxiv.org/abs/2406.15484v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Computers and Society"
        ]
    },
    {
        "id": 20000086,
        "doi": null,
        "title": "JobFair: A Framework for Benchmarking Gender Hiring Bias in Large Language Models",
        "abstract": "This paper presents a novel framework for benchmarking hierarchical gender\nhiring bias in Large Language Models (LLMs) for resume scoring, revealing\nsignificant issues of reverse bias and overdebiasing. Our contributions are\nfourfold: First, we introduce a framework using a real, anonymized resume\ndataset from the Healthcare, Finance, and Construction industries, meticulously\nused to avoid confounding factors. It evaluates gender hiring biases across\nhierarchical levels, including Level bias, Spread bias, Taste-based bias, and\nStatistical bias. This framework can be generalized to other social traits and\ntasks easily. Second, we propose novel statistical and computational hiring\nbias metrics based on a counterfactual approach, including Rank After Scoring\n(RAS), Rank-based Impact Ratio, Permutation Test-Based Metrics, and Fixed\nEffects Model-based Metrics. These metrics, rooted in labor economics, NLP, and\nlaw, enable holistic evaluation of hiring biases. Third, we analyze hiring\nbiases in ten state-of-the-art LLMs. Six out of ten LLMs show significant\nbiases against males in healthcare and finance. An industry-effect regression\nreveals that the healthcare industry is the most biased against males. GPT-4o\nand GPT-3.5 are the most biased models, showing significant bias in all three\nindustries. Conversely, Gemini-1.5-Pro, Llama3-8b-Instruct, and\nLlama3-70b-Instruct are the least biased. The hiring bias of all LLMs, except\nfor Llama3-8b-Instruct and Claude-3-Sonnet, remains consistent regardless of\nrandom expansion or reduction of resume content. Finally, we offer a\nuser-friendly demo to facilitate adoption and practical application of the\nframework.",
        "chunk-id": 3,
        "chunk": "bias metrics based on a counterfactual approach, including Rank After Scoring\n(RAS), Rank-based Impact Ratio, Permutation Test-Based Metrics, and Fixed\nEffects Model-based Metrics. These metrics, rooted in labor economics, NLP, and\nlaw, enable holistic evaluation of hiring biases. Third, we analyze hiring\nbiases in ten state-of-the-art LLMs. Six out of ten LLMs show significant",
        "authors": [
            "Ze Wang",
            "Zekun Wu",
            "Xin Guan",
            "Michael Thaler",
            "Adriano Koshiyama",
            "Skylar Lu",
            "Sachin Beepath",
            "Ediz Ertekin Jr.",
            "Maria Perez-Ortiz"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T09:15:57+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.15484v1",
        "arxiv_link": "http://arxiv.org/abs/2406.15484v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Computers and Society"
        ]
    },
    {
        "id": 20000086,
        "doi": null,
        "title": "JobFair: A Framework for Benchmarking Gender Hiring Bias in Large Language Models",
        "abstract": "This paper presents a novel framework for benchmarking hierarchical gender\nhiring bias in Large Language Models (LLMs) for resume scoring, revealing\nsignificant issues of reverse bias and overdebiasing. Our contributions are\nfourfold: First, we introduce a framework using a real, anonymized resume\ndataset from the Healthcare, Finance, and Construction industries, meticulously\nused to avoid confounding factors. It evaluates gender hiring biases across\nhierarchical levels, including Level bias, Spread bias, Taste-based bias, and\nStatistical bias. This framework can be generalized to other social traits and\ntasks easily. Second, we propose novel statistical and computational hiring\nbias metrics based on a counterfactual approach, including Rank After Scoring\n(RAS), Rank-based Impact Ratio, Permutation Test-Based Metrics, and Fixed\nEffects Model-based Metrics. These metrics, rooted in labor economics, NLP, and\nlaw, enable holistic evaluation of hiring biases. Third, we analyze hiring\nbiases in ten state-of-the-art LLMs. Six out of ten LLMs show significant\nbiases against males in healthcare and finance. An industry-effect regression\nreveals that the healthcare industry is the most biased against males. GPT-4o\nand GPT-3.5 are the most biased models, showing significant bias in all three\nindustries. Conversely, Gemini-1.5-Pro, Llama3-8b-Instruct, and\nLlama3-70b-Instruct are the least biased. The hiring bias of all LLMs, except\nfor Llama3-8b-Instruct and Claude-3-Sonnet, remains consistent regardless of\nrandom expansion or reduction of resume content. Finally, we offer a\nuser-friendly demo to facilitate adoption and practical application of the\nframework.",
        "chunk-id": 4,
        "chunk": "biases against males in healthcare and finance. An industry-effect regression\nreveals that the healthcare industry is the most biased against males. GPT-4o\nand GPT-3.5 are the most biased models, showing significant bias in all three\nindustries. Conversely, Gemini-1.5-Pro, Llama3-8b-Instruct, and\nLlama3-70b-Instruct are the least biased. The hiring bias of all LLMs, except",
        "authors": [
            "Ze Wang",
            "Zekun Wu",
            "Xin Guan",
            "Michael Thaler",
            "Adriano Koshiyama",
            "Skylar Lu",
            "Sachin Beepath",
            "Ediz Ertekin Jr.",
            "Maria Perez-Ortiz"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T09:15:57+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.15484v1",
        "arxiv_link": "http://arxiv.org/abs/2406.15484v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Computers and Society"
        ]
    },
    {
        "id": 20000086,
        "doi": null,
        "title": "JobFair: A Framework for Benchmarking Gender Hiring Bias in Large Language Models",
        "abstract": "This paper presents a novel framework for benchmarking hierarchical gender\nhiring bias in Large Language Models (LLMs) for resume scoring, revealing\nsignificant issues of reverse bias and overdebiasing. Our contributions are\nfourfold: First, we introduce a framework using a real, anonymized resume\ndataset from the Healthcare, Finance, and Construction industries, meticulously\nused to avoid confounding factors. It evaluates gender hiring biases across\nhierarchical levels, including Level bias, Spread bias, Taste-based bias, and\nStatistical bias. This framework can be generalized to other social traits and\ntasks easily. Second, we propose novel statistical and computational hiring\nbias metrics based on a counterfactual approach, including Rank After Scoring\n(RAS), Rank-based Impact Ratio, Permutation Test-Based Metrics, and Fixed\nEffects Model-based Metrics. These metrics, rooted in labor economics, NLP, and\nlaw, enable holistic evaluation of hiring biases. Third, we analyze hiring\nbiases in ten state-of-the-art LLMs. Six out of ten LLMs show significant\nbiases against males in healthcare and finance. An industry-effect regression\nreveals that the healthcare industry is the most biased against males. GPT-4o\nand GPT-3.5 are the most biased models, showing significant bias in all three\nindustries. Conversely, Gemini-1.5-Pro, Llama3-8b-Instruct, and\nLlama3-70b-Instruct are the least biased. The hiring bias of all LLMs, except\nfor Llama3-8b-Instruct and Claude-3-Sonnet, remains consistent regardless of\nrandom expansion or reduction of resume content. Finally, we offer a\nuser-friendly demo to facilitate adoption and practical application of the\nframework.",
        "chunk-id": 5,
        "chunk": "for Llama3-8b-Instruct and Claude-3-Sonnet, remains consistent regardless of\nrandom expansion or reduction of resume content. Finally, we offer a\nuser-friendly demo to facilitate adoption and practical application of the\nframework.",
        "authors": [
            "Ze Wang",
            "Zekun Wu",
            "Xin Guan",
            "Michael Thaler",
            "Adriano Koshiyama",
            "Skylar Lu",
            "Sachin Beepath",
            "Ediz Ertekin Jr.",
            "Maria Perez-Ortiz"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T09:15:57+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.15484v1",
        "arxiv_link": "http://arxiv.org/abs/2406.15484v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Computers and Society"
        ]
    },
    {
        "id": 20000087,
        "doi": null,
        "title": "A Systematic Analysis of Large Language Models as Soft Reasoners: The Case of Syllogistic Inferences",
        "abstract": "The reasoning abilities of Large Language Models (LLMs) are becoming a\ncentral focus of study in NLP. In this paper, we consider the case of\nsyllogistic reasoning, an area of deductive reasoning studied extensively in\nlogic and cognitive psychology. Previous research has shown that pre-trained\nLLMs exhibit reasoning biases, such as $\\textit{content effects}$, avoid\nanswering that $\\textit{no conclusion follows}$, display human-like\ndifficulties, and struggle with multi-step reasoning. We contribute to this\nresearch line by systematically investigating the effects of chain-of-thought\nreasoning, in-context learning (ICL), and supervised fine-tuning (SFT) on\nsyllogistic reasoning, considering syllogisms with conclusions that support or\nviolate world knowledge, as well as ones with multiple premises. Crucially, we\ngo beyond the standard focus on accuracy, with an in-depth analysis of the\nconclusions generated by the models. Our results suggest that the behavior of\npre-trained LLMs can be explained by heuristics studied in cognitive science\nand that both ICL and SFT improve model performance on valid inferences,\nalthough only the latter mitigates most reasoning biases without harming model\nconsistency.",
        "chunk-id": 1,
        "chunk": "The reasoning abilities of Large Language Models (LLMs) are becoming a\ncentral focus of study in NLP. In this paper, we consider the case of\nsyllogistic reasoning, an area of deductive reasoning studied extensively in\nlogic and cognitive psychology. Previous research has shown that pre-trained\nLLMs exhibit reasoning biases, such as $\\textit{content effects}$, avoid",
        "authors": [
            "Leonardo Bertolazzi",
            "Albert Gatt",
            "Raffaella Bernardi"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T08:59:04+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11341v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11341v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000087,
        "doi": null,
        "title": "A Systematic Analysis of Large Language Models as Soft Reasoners: The Case of Syllogistic Inferences",
        "abstract": "The reasoning abilities of Large Language Models (LLMs) are becoming a\ncentral focus of study in NLP. In this paper, we consider the case of\nsyllogistic reasoning, an area of deductive reasoning studied extensively in\nlogic and cognitive psychology. Previous research has shown that pre-trained\nLLMs exhibit reasoning biases, such as $\\textit{content effects}$, avoid\nanswering that $\\textit{no conclusion follows}$, display human-like\ndifficulties, and struggle with multi-step reasoning. We contribute to this\nresearch line by systematically investigating the effects of chain-of-thought\nreasoning, in-context learning (ICL), and supervised fine-tuning (SFT) on\nsyllogistic reasoning, considering syllogisms with conclusions that support or\nviolate world knowledge, as well as ones with multiple premises. Crucially, we\ngo beyond the standard focus on accuracy, with an in-depth analysis of the\nconclusions generated by the models. Our results suggest that the behavior of\npre-trained LLMs can be explained by heuristics studied in cognitive science\nand that both ICL and SFT improve model performance on valid inferences,\nalthough only the latter mitigates most reasoning biases without harming model\nconsistency.",
        "chunk-id": 2,
        "chunk": "answering that $\\textit{no conclusion follows}$, display human-like\ndifficulties, and struggle with multi-step reasoning. We contribute to this\nresearch line by systematically investigating the effects of chain-of-thought\nreasoning, in-context learning (ICL), and supervised fine-tuning (SFT) on\nsyllogistic reasoning, considering syllogisms with conclusions that support or",
        "authors": [
            "Leonardo Bertolazzi",
            "Albert Gatt",
            "Raffaella Bernardi"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T08:59:04+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11341v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11341v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000087,
        "doi": null,
        "title": "A Systematic Analysis of Large Language Models as Soft Reasoners: The Case of Syllogistic Inferences",
        "abstract": "The reasoning abilities of Large Language Models (LLMs) are becoming a\ncentral focus of study in NLP. In this paper, we consider the case of\nsyllogistic reasoning, an area of deductive reasoning studied extensively in\nlogic and cognitive psychology. Previous research has shown that pre-trained\nLLMs exhibit reasoning biases, such as $\\textit{content effects}$, avoid\nanswering that $\\textit{no conclusion follows}$, display human-like\ndifficulties, and struggle with multi-step reasoning. We contribute to this\nresearch line by systematically investigating the effects of chain-of-thought\nreasoning, in-context learning (ICL), and supervised fine-tuning (SFT) on\nsyllogistic reasoning, considering syllogisms with conclusions that support or\nviolate world knowledge, as well as ones with multiple premises. Crucially, we\ngo beyond the standard focus on accuracy, with an in-depth analysis of the\nconclusions generated by the models. Our results suggest that the behavior of\npre-trained LLMs can be explained by heuristics studied in cognitive science\nand that both ICL and SFT improve model performance on valid inferences,\nalthough only the latter mitigates most reasoning biases without harming model\nconsistency.",
        "chunk-id": 3,
        "chunk": "violate world knowledge, as well as ones with multiple premises. Crucially, we\ngo beyond the standard focus on accuracy, with an in-depth analysis of the\nconclusions generated by the models. Our results suggest that the behavior of\npre-trained LLMs can be explained by heuristics studied in cognitive science\nand that both ICL and SFT improve model performance on valid inferences,",
        "authors": [
            "Leonardo Bertolazzi",
            "Albert Gatt",
            "Raffaella Bernardi"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T08:59:04+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11341v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11341v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000087,
        "doi": null,
        "title": "A Systematic Analysis of Large Language Models as Soft Reasoners: The Case of Syllogistic Inferences",
        "abstract": "The reasoning abilities of Large Language Models (LLMs) are becoming a\ncentral focus of study in NLP. In this paper, we consider the case of\nsyllogistic reasoning, an area of deductive reasoning studied extensively in\nlogic and cognitive psychology. Previous research has shown that pre-trained\nLLMs exhibit reasoning biases, such as $\\textit{content effects}$, avoid\nanswering that $\\textit{no conclusion follows}$, display human-like\ndifficulties, and struggle with multi-step reasoning. We contribute to this\nresearch line by systematically investigating the effects of chain-of-thought\nreasoning, in-context learning (ICL), and supervised fine-tuning (SFT) on\nsyllogistic reasoning, considering syllogisms with conclusions that support or\nviolate world knowledge, as well as ones with multiple premises. Crucially, we\ngo beyond the standard focus on accuracy, with an in-depth analysis of the\nconclusions generated by the models. Our results suggest that the behavior of\npre-trained LLMs can be explained by heuristics studied in cognitive science\nand that both ICL and SFT improve model performance on valid inferences,\nalthough only the latter mitigates most reasoning biases without harming model\nconsistency.",
        "chunk-id": 4,
        "chunk": "although only the latter mitigates most reasoning biases without harming model\nconsistency.",
        "authors": [
            "Leonardo Bertolazzi",
            "Albert Gatt",
            "Raffaella Bernardi"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T08:59:04+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11341v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11341v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000088,
        "doi": null,
        "title": "An Empirical Investigation of Matrix Factorization Methods for Pre-trained Transformers",
        "abstract": "The increasing size of transformer-based models in NLP makes the question of\ncompressing them important. In this work, we present a comprehensive analysis\nof factorization based model compression techniques. Specifically, we focus on\ncomparing straightforward low-rank factorization against the recently\nintroduced Monarch factorization, which exhibits impressive performance\npreservation on the GLUE benchmark. To mitigate stability issues associated\nwith low-rank factorization of the matrices in pre-trained transformers, we\nintroduce a staged factorization approach wherein layers are factorized one by\none instead of being factorized simultaneously. Through this strategy we\nsignificantly enhance the stability and reliability of the compression process.\nFurther, we introduce a simple block-wise low-rank factorization method, which\nhas a close relationship to Monarch factorization. Our experiments lead to the\nsurprising conclusion that straightforward low-rank factorization consistently\noutperforms Monarch factorization across both different compression ratios and\nsix different text classification tasks.",
        "chunk-id": 1,
        "chunk": "The increasing size of transformer-based models in NLP makes the question of\ncompressing them important. In this work, we present a comprehensive analysis\nof factorization based model compression techniques. Specifically, we focus on\ncomparing straightforward low-rank factorization against the recently\nintroduced Monarch factorization, which exhibits impressive performance",
        "authors": [
            "Ashim Gupta",
            "Sina Mahdipour Saravani",
            "P. Sadayappan",
            "Vivek Srikumar"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T08:14:23+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11307v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11307v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000088,
        "doi": null,
        "title": "An Empirical Investigation of Matrix Factorization Methods for Pre-trained Transformers",
        "abstract": "The increasing size of transformer-based models in NLP makes the question of\ncompressing them important. In this work, we present a comprehensive analysis\nof factorization based model compression techniques. Specifically, we focus on\ncomparing straightforward low-rank factorization against the recently\nintroduced Monarch factorization, which exhibits impressive performance\npreservation on the GLUE benchmark. To mitigate stability issues associated\nwith low-rank factorization of the matrices in pre-trained transformers, we\nintroduce a staged factorization approach wherein layers are factorized one by\none instead of being factorized simultaneously. Through this strategy we\nsignificantly enhance the stability and reliability of the compression process.\nFurther, we introduce a simple block-wise low-rank factorization method, which\nhas a close relationship to Monarch factorization. Our experiments lead to the\nsurprising conclusion that straightforward low-rank factorization consistently\noutperforms Monarch factorization across both different compression ratios and\nsix different text classification tasks.",
        "chunk-id": 2,
        "chunk": "preservation on the GLUE benchmark. To mitigate stability issues associated\nwith low-rank factorization of the matrices in pre-trained transformers, we\nintroduce a staged factorization approach wherein layers are factorized one by\none instead of being factorized simultaneously. Through this strategy we\nsignificantly enhance the stability and reliability of the compression process.",
        "authors": [
            "Ashim Gupta",
            "Sina Mahdipour Saravani",
            "P. Sadayappan",
            "Vivek Srikumar"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T08:14:23+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11307v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11307v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000088,
        "doi": null,
        "title": "An Empirical Investigation of Matrix Factorization Methods for Pre-trained Transformers",
        "abstract": "The increasing size of transformer-based models in NLP makes the question of\ncompressing them important. In this work, we present a comprehensive analysis\nof factorization based model compression techniques. Specifically, we focus on\ncomparing straightforward low-rank factorization against the recently\nintroduced Monarch factorization, which exhibits impressive performance\npreservation on the GLUE benchmark. To mitigate stability issues associated\nwith low-rank factorization of the matrices in pre-trained transformers, we\nintroduce a staged factorization approach wherein layers are factorized one by\none instead of being factorized simultaneously. Through this strategy we\nsignificantly enhance the stability and reliability of the compression process.\nFurther, we introduce a simple block-wise low-rank factorization method, which\nhas a close relationship to Monarch factorization. Our experiments lead to the\nsurprising conclusion that straightforward low-rank factorization consistently\noutperforms Monarch factorization across both different compression ratios and\nsix different text classification tasks.",
        "chunk-id": 3,
        "chunk": "Further, we introduce a simple block-wise low-rank factorization method, which\nhas a close relationship to Monarch factorization. Our experiments lead to the\nsurprising conclusion that straightforward low-rank factorization consistently\noutperforms Monarch factorization across both different compression ratios and\nsix different text classification tasks.",
        "authors": [
            "Ashim Gupta",
            "Sina Mahdipour Saravani",
            "P. Sadayappan",
            "Vivek Srikumar"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T08:14:23+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11307v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11307v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000089,
        "doi": null,
        "title": "Duplicate Detection with GenAI",
        "abstract": "Customer data is often stored as records in Customer Relations Management\nsystems (CRMs). Data which is manually entered into such systems by one of more\nusers over time leads to data replication, partial duplication or fuzzy\nduplication. This in turn means that there no longer a single source of truth\nfor customers, contacts, accounts, etc. Downstream business processes become\nincreasing complex and contrived without a unique mapping between a record in a\nCRM and the target customer. Current methods to detect and de-duplicate records\nuse traditional Natural Language Processing techniques known as Entity\nMatching. In this paper we show how using the latest advancements in Large\nLanguage Models and Generative AI can vastly improve the identification and\nrepair of duplicated records. On common benchmark datasets we find an\nimprovement in the accuracy of data de-duplication rates from 30 percent using\nNLP techniques to almost 60 percent using our proposed method.",
        "chunk-id": 1,
        "chunk": "Customer data is often stored as records in Customer Relations Management\nsystems (CRMs). Data which is manually entered into such systems by one of more\nusers over time leads to data replication, partial duplication or fuzzy\nduplication. This in turn means that there no longer a single source of truth\nfor customers, contacts, accounts, etc. Downstream business processes become",
        "authors": [
            "Ian Ormesher"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T06:42:13+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.15483v1",
        "arxiv_link": "http://arxiv.org/abs/2406.15483v1",
        "categories": [
            "Computation and Language",
            "Databases",
            "Machine Learning"
        ]
    },
    {
        "id": 20000089,
        "doi": null,
        "title": "Duplicate Detection with GenAI",
        "abstract": "Customer data is often stored as records in Customer Relations Management\nsystems (CRMs). Data which is manually entered into such systems by one of more\nusers over time leads to data replication, partial duplication or fuzzy\nduplication. This in turn means that there no longer a single source of truth\nfor customers, contacts, accounts, etc. Downstream business processes become\nincreasing complex and contrived without a unique mapping between a record in a\nCRM and the target customer. Current methods to detect and de-duplicate records\nuse traditional Natural Language Processing techniques known as Entity\nMatching. In this paper we show how using the latest advancements in Large\nLanguage Models and Generative AI can vastly improve the identification and\nrepair of duplicated records. On common benchmark datasets we find an\nimprovement in the accuracy of data de-duplication rates from 30 percent using\nNLP techniques to almost 60 percent using our proposed method.",
        "chunk-id": 2,
        "chunk": "increasing complex and contrived without a unique mapping between a record in a\nCRM and the target customer. Current methods to detect and de-duplicate records\nuse traditional Natural Language Processing techniques known as Entity\nMatching. In this paper we show how using the latest advancements in Large\nLanguage Models and Generative AI can vastly improve the identification and",
        "authors": [
            "Ian Ormesher"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T06:42:13+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.15483v1",
        "arxiv_link": "http://arxiv.org/abs/2406.15483v1",
        "categories": [
            "Computation and Language",
            "Databases",
            "Machine Learning"
        ]
    },
    {
        "id": 20000089,
        "doi": null,
        "title": "Duplicate Detection with GenAI",
        "abstract": "Customer data is often stored as records in Customer Relations Management\nsystems (CRMs). Data which is manually entered into such systems by one of more\nusers over time leads to data replication, partial duplication or fuzzy\nduplication. This in turn means that there no longer a single source of truth\nfor customers, contacts, accounts, etc. Downstream business processes become\nincreasing complex and contrived without a unique mapping between a record in a\nCRM and the target customer. Current methods to detect and de-duplicate records\nuse traditional Natural Language Processing techniques known as Entity\nMatching. In this paper we show how using the latest advancements in Large\nLanguage Models and Generative AI can vastly improve the identification and\nrepair of duplicated records. On common benchmark datasets we find an\nimprovement in the accuracy of data de-duplication rates from 30 percent using\nNLP techniques to almost 60 percent using our proposed method.",
        "chunk-id": 3,
        "chunk": "repair of duplicated records. On common benchmark datasets we find an\nimprovement in the accuracy of data de-duplication rates from 30 percent using\nNLP techniques to almost 60 percent using our proposed method.",
        "authors": [
            "Ian Ormesher"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T06:42:13+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.15483v1",
        "arxiv_link": "http://arxiv.org/abs/2406.15483v1",
        "categories": [
            "Computation and Language",
            "Databases",
            "Machine Learning"
        ]
    },
    {
        "id": 20000090,
        "doi": null,
        "title": "Can Machines Resonate with Humans? Evaluating the Emotional and Empathic Comprehension of LMs",
        "abstract": "Empathy plays a pivotal role in fostering prosocial behavior, often triggered\nby the sharing of personal experiences through narratives. However, modeling\nempathy using NLP approaches remains challenging due to its deep\ninterconnection with human interaction dynamics. Previous approaches, which\ninvolve fine-tuning language models (LMs) on human-annotated empathic datasets,\nhave had limited success. In our pursuit of improving empathy understanding in\nLMs, we propose several strategies, including contrastive learning with masked\nLMs and supervised fine-tuning with Large Language Models (LLMs). While these\nmethods show improvements over previous methods, the overall results remain\nunsatisfactory. To better understand this trend, we performed an analysis which\nreveals a low agreement among annotators. This lack of consensus hinders\ntraining and highlights the subjective nature of the task. We also explore the\ncultural impact on annotations. To study this, we meticulously collected story\npairs in Urdu language and find that subjectivity in interpreting empathy among\nannotators appears to be independent of cultural background. The insights from\nour systematic exploration of LMs' understanding of empathy suggest that there\nis considerable room for exploration in both task formulation and modeling.",
        "chunk-id": 1,
        "chunk": "Empathy plays a pivotal role in fostering prosocial behavior, often triggered\nby the sharing of personal experiences through narratives. However, modeling\nempathy using NLP approaches remains challenging due to its deep\ninterconnection with human interaction dynamics. Previous approaches, which\ninvolve fine-tuning language models (LMs) on human-annotated empathic datasets,",
        "authors": [
            "Muhammad Arslan Manzoor",
            "Yuxia Wang",
            "Minghan Wang",
            "Preslav Nakov"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T06:22:20+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11250v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11250v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000090,
        "doi": null,
        "title": "Can Machines Resonate with Humans? Evaluating the Emotional and Empathic Comprehension of LMs",
        "abstract": "Empathy plays a pivotal role in fostering prosocial behavior, often triggered\nby the sharing of personal experiences through narratives. However, modeling\nempathy using NLP approaches remains challenging due to its deep\ninterconnection with human interaction dynamics. Previous approaches, which\ninvolve fine-tuning language models (LMs) on human-annotated empathic datasets,\nhave had limited success. In our pursuit of improving empathy understanding in\nLMs, we propose several strategies, including contrastive learning with masked\nLMs and supervised fine-tuning with Large Language Models (LLMs). While these\nmethods show improvements over previous methods, the overall results remain\nunsatisfactory. To better understand this trend, we performed an analysis which\nreveals a low agreement among annotators. This lack of consensus hinders\ntraining and highlights the subjective nature of the task. We also explore the\ncultural impact on annotations. To study this, we meticulously collected story\npairs in Urdu language and find that subjectivity in interpreting empathy among\nannotators appears to be independent of cultural background. The insights from\nour systematic exploration of LMs' understanding of empathy suggest that there\nis considerable room for exploration in both task formulation and modeling.",
        "chunk-id": 2,
        "chunk": "have had limited success. In our pursuit of improving empathy understanding in\nLMs, we propose several strategies, including contrastive learning with masked\nLMs and supervised fine-tuning with Large Language Models (LLMs). While these\nmethods show improvements over previous methods, the overall results remain",
        "authors": [
            "Muhammad Arslan Manzoor",
            "Yuxia Wang",
            "Minghan Wang",
            "Preslav Nakov"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T06:22:20+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11250v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11250v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000090,
        "doi": null,
        "title": "Can Machines Resonate with Humans? Evaluating the Emotional and Empathic Comprehension of LMs",
        "abstract": "Empathy plays a pivotal role in fostering prosocial behavior, often triggered\nby the sharing of personal experiences through narratives. However, modeling\nempathy using NLP approaches remains challenging due to its deep\ninterconnection with human interaction dynamics. Previous approaches, which\ninvolve fine-tuning language models (LMs) on human-annotated empathic datasets,\nhave had limited success. In our pursuit of improving empathy understanding in\nLMs, we propose several strategies, including contrastive learning with masked\nLMs and supervised fine-tuning with Large Language Models (LLMs). While these\nmethods show improvements over previous methods, the overall results remain\nunsatisfactory. To better understand this trend, we performed an analysis which\nreveals a low agreement among annotators. This lack of consensus hinders\ntraining and highlights the subjective nature of the task. We also explore the\ncultural impact on annotations. To study this, we meticulously collected story\npairs in Urdu language and find that subjectivity in interpreting empathy among\nannotators appears to be independent of cultural background. The insights from\nour systematic exploration of LMs' understanding of empathy suggest that there\nis considerable room for exploration in both task formulation and modeling.",
        "chunk-id": 3,
        "chunk": "unsatisfactory. To better understand this trend, we performed an analysis which\nreveals a low agreement among annotators. This lack of consensus hinders\ntraining and highlights the subjective nature of the task. We also explore the\ncultural impact on annotations. To study this, we meticulously collected story",
        "authors": [
            "Muhammad Arslan Manzoor",
            "Yuxia Wang",
            "Minghan Wang",
            "Preslav Nakov"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T06:22:20+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11250v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11250v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000090,
        "doi": null,
        "title": "Can Machines Resonate with Humans? Evaluating the Emotional and Empathic Comprehension of LMs",
        "abstract": "Empathy plays a pivotal role in fostering prosocial behavior, often triggered\nby the sharing of personal experiences through narratives. However, modeling\nempathy using NLP approaches remains challenging due to its deep\ninterconnection with human interaction dynamics. Previous approaches, which\ninvolve fine-tuning language models (LMs) on human-annotated empathic datasets,\nhave had limited success. In our pursuit of improving empathy understanding in\nLMs, we propose several strategies, including contrastive learning with masked\nLMs and supervised fine-tuning with Large Language Models (LLMs). While these\nmethods show improvements over previous methods, the overall results remain\nunsatisfactory. To better understand this trend, we performed an analysis which\nreveals a low agreement among annotators. This lack of consensus hinders\ntraining and highlights the subjective nature of the task. We also explore the\ncultural impact on annotations. To study this, we meticulously collected story\npairs in Urdu language and find that subjectivity in interpreting empathy among\nannotators appears to be independent of cultural background. The insights from\nour systematic exploration of LMs' understanding of empathy suggest that there\nis considerable room for exploration in both task formulation and modeling.",
        "chunk-id": 4,
        "chunk": "pairs in Urdu language and find that subjectivity in interpreting empathy among\nannotators appears to be independent of cultural background. The insights from\nour systematic exploration of LMs' understanding of empathy suggest that there\nis considerable room for exploration in both task formulation and modeling.",
        "authors": [
            "Muhammad Arslan Manzoor",
            "Yuxia Wang",
            "Minghan Wang",
            "Preslav Nakov"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T06:22:20+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11250v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11250v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000091,
        "doi": null,
        "title": "Breaking Boundaries: Investigating the Effects of Model Editing on Cross-linguistic Performance",
        "abstract": "The integration of pretrained language models (PLMs) like BERT and GPT has\nrevolutionized NLP, particularly for English, but it has also created\nlinguistic imbalances. This paper strategically identifies the need for\nlinguistic equity by examining several knowledge editing techniques in\nmultilingual contexts. We evaluate the performance of models such as Mistral,\nTowerInstruct, OpenHathi, Tamil-Llama, and Kan-Llama across languages including\nEnglish, German, French, Italian, Spanish, Hindi, Tamil, and Kannada. Our\nresearch identifies significant discrepancies in normal and merged models\nconcerning cross-lingual consistency. We employ strategies like 'each language\nfor itself' (ELFI) and 'each language for others' (ELFO) to stress-test these\nmodels. Our findings demonstrate the potential for LLMs to overcome linguistic\nbarriers, laying the groundwork for future research in achieving linguistic\ninclusivity in AI technologies.",
        "chunk-id": 1,
        "chunk": "The integration of pretrained language models (PLMs) like BERT and GPT has\nrevolutionized NLP, particularly for English, but it has also created\nlinguistic imbalances. This paper strategically identifies the need for\nlinguistic equity by examining several knowledge editing techniques in\nmultilingual contexts. We evaluate the performance of models such as Mistral,",
        "authors": [
            "Somnath Banerjee",
            "Avik Halder",
            "Rajarshi Mandal",
            "Sayan Layek",
            "Ian Soboroff",
            "Rima Hazra",
            "Animesh Mukherjee"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T01:54:27+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11139v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11139v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000091,
        "doi": null,
        "title": "Breaking Boundaries: Investigating the Effects of Model Editing on Cross-linguistic Performance",
        "abstract": "The integration of pretrained language models (PLMs) like BERT and GPT has\nrevolutionized NLP, particularly for English, but it has also created\nlinguistic imbalances. This paper strategically identifies the need for\nlinguistic equity by examining several knowledge editing techniques in\nmultilingual contexts. We evaluate the performance of models such as Mistral,\nTowerInstruct, OpenHathi, Tamil-Llama, and Kan-Llama across languages including\nEnglish, German, French, Italian, Spanish, Hindi, Tamil, and Kannada. Our\nresearch identifies significant discrepancies in normal and merged models\nconcerning cross-lingual consistency. We employ strategies like 'each language\nfor itself' (ELFI) and 'each language for others' (ELFO) to stress-test these\nmodels. Our findings demonstrate the potential for LLMs to overcome linguistic\nbarriers, laying the groundwork for future research in achieving linguistic\ninclusivity in AI technologies.",
        "chunk-id": 2,
        "chunk": "TowerInstruct, OpenHathi, Tamil-Llama, and Kan-Llama across languages including\nEnglish, German, French, Italian, Spanish, Hindi, Tamil, and Kannada. Our\nresearch identifies significant discrepancies in normal and merged models\nconcerning cross-lingual consistency. We employ strategies like 'each language\nfor itself' (ELFI) and 'each language for others' (ELFO) to stress-test these",
        "authors": [
            "Somnath Banerjee",
            "Avik Halder",
            "Rajarshi Mandal",
            "Sayan Layek",
            "Ian Soboroff",
            "Rima Hazra",
            "Animesh Mukherjee"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T01:54:27+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11139v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11139v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000091,
        "doi": null,
        "title": "Breaking Boundaries: Investigating the Effects of Model Editing on Cross-linguistic Performance",
        "abstract": "The integration of pretrained language models (PLMs) like BERT and GPT has\nrevolutionized NLP, particularly for English, but it has also created\nlinguistic imbalances. This paper strategically identifies the need for\nlinguistic equity by examining several knowledge editing techniques in\nmultilingual contexts. We evaluate the performance of models such as Mistral,\nTowerInstruct, OpenHathi, Tamil-Llama, and Kan-Llama across languages including\nEnglish, German, French, Italian, Spanish, Hindi, Tamil, and Kannada. Our\nresearch identifies significant discrepancies in normal and merged models\nconcerning cross-lingual consistency. We employ strategies like 'each language\nfor itself' (ELFI) and 'each language for others' (ELFO) to stress-test these\nmodels. Our findings demonstrate the potential for LLMs to overcome linguistic\nbarriers, laying the groundwork for future research in achieving linguistic\ninclusivity in AI technologies.",
        "chunk-id": 3,
        "chunk": "models. Our findings demonstrate the potential for LLMs to overcome linguistic\nbarriers, laying the groundwork for future research in achieving linguistic\ninclusivity in AI technologies.",
        "authors": [
            "Somnath Banerjee",
            "Avik Halder",
            "Rajarshi Mandal",
            "Sayan Layek",
            "Ian Soboroff",
            "Rima Hazra",
            "Animesh Mukherjee"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-17T01:54:27+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11139v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11139v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000092,
        "doi": null,
        "title": "Enhancing Class Diagram Dynamics: A Natural Language Approach with ChatGPT",
        "abstract": "Integrating artificial intelligence (AI) into software engineering can\ntransform traditional practices by enhancing efficiency, accuracy, and\ninnovation. This study explores using ChatGPT, an advanced AI language model,\nto enhance UML class diagrams dynamically, an underexplored area.\nTraditionally, creating and maintaining class diagrams are manual,\ntime-consuming, and error-prone processes. This research leverages natural\nlanguage processing (NLP) techniques to automate the extraction of methods and\ninteractions from detailed use case tables and integrate them into class\ndiagrams.\n  The methodology involves several steps: (1) developing detailed natural\nlanguage use case tables by master's degree students for a \"Waste Recycling\nPlatform,\" (2) creating an initial static class diagram based on these tables,\n(3) iteratively enriching the class diagram through ChatGPT integration to\nanalyze use cases and suggest methods, (4) reviewing and incorporating these\nmethods into the class diagram, and (5) dynamically updating the PlantUML\n\\cite{plantuml} class diagram, followed by evaluation and refinement. Findings\nindicate that the AI-driven approach significantly improves the accuracy and\ncompleteness of the class diagram. Additionally, dynamic enhancement aligns\nwell with Agile development practices, facilitating rapid iterations and\ncontinuous improvement.\n  Key contributions include demonstrating the feasibility and benefits of\nintegrating AI into software modeling tasks, providing a comprehensive\nrepresentation of system behaviors and interactions, and highlighting AI's\npotential to streamline and improve existing software engineering processes.\nFuture research should address identified limitations and explore AI\napplications in other software models.",
        "chunk-id": 1,
        "chunk": "Integrating artificial intelligence (AI) into software engineering can\ntransform traditional practices by enhancing efficiency, accuracy, and\ninnovation. This study explores using ChatGPT, an advanced AI language model,\nto enhance UML class diagrams dynamically, an underexplored area.\nTraditionally, creating and maintaining class diagrams are manual,",
        "authors": [
            "Djaber Rouabhia",
            "Ismail Hadjadj"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-16T16:30:55+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11002v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11002v1",
        "categories": [
            "Software Engineering"
        ]
    },
    {
        "id": 20000092,
        "doi": null,
        "title": "Enhancing Class Diagram Dynamics: A Natural Language Approach with ChatGPT",
        "abstract": "Integrating artificial intelligence (AI) into software engineering can\ntransform traditional practices by enhancing efficiency, accuracy, and\ninnovation. This study explores using ChatGPT, an advanced AI language model,\nto enhance UML class diagrams dynamically, an underexplored area.\nTraditionally, creating and maintaining class diagrams are manual,\ntime-consuming, and error-prone processes. This research leverages natural\nlanguage processing (NLP) techniques to automate the extraction of methods and\ninteractions from detailed use case tables and integrate them into class\ndiagrams.\n  The methodology involves several steps: (1) developing detailed natural\nlanguage use case tables by master's degree students for a \"Waste Recycling\nPlatform,\" (2) creating an initial static class diagram based on these tables,\n(3) iteratively enriching the class diagram through ChatGPT integration to\nanalyze use cases and suggest methods, (4) reviewing and incorporating these\nmethods into the class diagram, and (5) dynamically updating the PlantUML\n\\cite{plantuml} class diagram, followed by evaluation and refinement. Findings\nindicate that the AI-driven approach significantly improves the accuracy and\ncompleteness of the class diagram. Additionally, dynamic enhancement aligns\nwell with Agile development practices, facilitating rapid iterations and\ncontinuous improvement.\n  Key contributions include demonstrating the feasibility and benefits of\nintegrating AI into software modeling tasks, providing a comprehensive\nrepresentation of system behaviors and interactions, and highlighting AI's\npotential to streamline and improve existing software engineering processes.\nFuture research should address identified limitations and explore AI\napplications in other software models.",
        "chunk-id": 2,
        "chunk": "time-consuming, and error-prone processes. This research leverages natural\nlanguage processing (NLP) techniques to automate the extraction of methods and\ninteractions from detailed use case tables and integrate them into class\ndiagrams.\n  The methodology involves several steps: (1) developing detailed natural",
        "authors": [
            "Djaber Rouabhia",
            "Ismail Hadjadj"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-16T16:30:55+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11002v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11002v1",
        "categories": [
            "Software Engineering"
        ]
    },
    {
        "id": 20000092,
        "doi": null,
        "title": "Enhancing Class Diagram Dynamics: A Natural Language Approach with ChatGPT",
        "abstract": "Integrating artificial intelligence (AI) into software engineering can\ntransform traditional practices by enhancing efficiency, accuracy, and\ninnovation. This study explores using ChatGPT, an advanced AI language model,\nto enhance UML class diagrams dynamically, an underexplored area.\nTraditionally, creating and maintaining class diagrams are manual,\ntime-consuming, and error-prone processes. This research leverages natural\nlanguage processing (NLP) techniques to automate the extraction of methods and\ninteractions from detailed use case tables and integrate them into class\ndiagrams.\n  The methodology involves several steps: (1) developing detailed natural\nlanguage use case tables by master's degree students for a \"Waste Recycling\nPlatform,\" (2) creating an initial static class diagram based on these tables,\n(3) iteratively enriching the class diagram through ChatGPT integration to\nanalyze use cases and suggest methods, (4) reviewing and incorporating these\nmethods into the class diagram, and (5) dynamically updating the PlantUML\n\\cite{plantuml} class diagram, followed by evaluation and refinement. Findings\nindicate that the AI-driven approach significantly improves the accuracy and\ncompleteness of the class diagram. Additionally, dynamic enhancement aligns\nwell with Agile development practices, facilitating rapid iterations and\ncontinuous improvement.\n  Key contributions include demonstrating the feasibility and benefits of\nintegrating AI into software modeling tasks, providing a comprehensive\nrepresentation of system behaviors and interactions, and highlighting AI's\npotential to streamline and improve existing software engineering processes.\nFuture research should address identified limitations and explore AI\napplications in other software models.",
        "chunk-id": 3,
        "chunk": "language use case tables by master's degree students for a \"Waste Recycling\nPlatform,\" (2) creating an initial static class diagram based on these tables,\n(3) iteratively enriching the class diagram through ChatGPT integration to\nanalyze use cases and suggest methods, (4) reviewing and incorporating these\nmethods into the class diagram, and (5) dynamically updating the PlantUML",
        "authors": [
            "Djaber Rouabhia",
            "Ismail Hadjadj"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-16T16:30:55+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11002v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11002v1",
        "categories": [
            "Software Engineering"
        ]
    },
    {
        "id": 20000092,
        "doi": null,
        "title": "Enhancing Class Diagram Dynamics: A Natural Language Approach with ChatGPT",
        "abstract": "Integrating artificial intelligence (AI) into software engineering can\ntransform traditional practices by enhancing efficiency, accuracy, and\ninnovation. This study explores using ChatGPT, an advanced AI language model,\nto enhance UML class diagrams dynamically, an underexplored area.\nTraditionally, creating and maintaining class diagrams are manual,\ntime-consuming, and error-prone processes. This research leverages natural\nlanguage processing (NLP) techniques to automate the extraction of methods and\ninteractions from detailed use case tables and integrate them into class\ndiagrams.\n  The methodology involves several steps: (1) developing detailed natural\nlanguage use case tables by master's degree students for a \"Waste Recycling\nPlatform,\" (2) creating an initial static class diagram based on these tables,\n(3) iteratively enriching the class diagram through ChatGPT integration to\nanalyze use cases and suggest methods, (4) reviewing and incorporating these\nmethods into the class diagram, and (5) dynamically updating the PlantUML\n\\cite{plantuml} class diagram, followed by evaluation and refinement. Findings\nindicate that the AI-driven approach significantly improves the accuracy and\ncompleteness of the class diagram. Additionally, dynamic enhancement aligns\nwell with Agile development practices, facilitating rapid iterations and\ncontinuous improvement.\n  Key contributions include demonstrating the feasibility and benefits of\nintegrating AI into software modeling tasks, providing a comprehensive\nrepresentation of system behaviors and interactions, and highlighting AI's\npotential to streamline and improve existing software engineering processes.\nFuture research should address identified limitations and explore AI\napplications in other software models.",
        "chunk-id": 4,
        "chunk": "\\cite{plantuml} class diagram, followed by evaluation and refinement. Findings\nindicate that the AI-driven approach significantly improves the accuracy and\ncompleteness of the class diagram. Additionally, dynamic enhancement aligns\nwell with Agile development practices, facilitating rapid iterations and\ncontinuous improvement.",
        "authors": [
            "Djaber Rouabhia",
            "Ismail Hadjadj"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-16T16:30:55+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11002v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11002v1",
        "categories": [
            "Software Engineering"
        ]
    },
    {
        "id": 20000092,
        "doi": null,
        "title": "Enhancing Class Diagram Dynamics: A Natural Language Approach with ChatGPT",
        "abstract": "Integrating artificial intelligence (AI) into software engineering can\ntransform traditional practices by enhancing efficiency, accuracy, and\ninnovation. This study explores using ChatGPT, an advanced AI language model,\nto enhance UML class diagrams dynamically, an underexplored area.\nTraditionally, creating and maintaining class diagrams are manual,\ntime-consuming, and error-prone processes. This research leverages natural\nlanguage processing (NLP) techniques to automate the extraction of methods and\ninteractions from detailed use case tables and integrate them into class\ndiagrams.\n  The methodology involves several steps: (1) developing detailed natural\nlanguage use case tables by master's degree students for a \"Waste Recycling\nPlatform,\" (2) creating an initial static class diagram based on these tables,\n(3) iteratively enriching the class diagram through ChatGPT integration to\nanalyze use cases and suggest methods, (4) reviewing and incorporating these\nmethods into the class diagram, and (5) dynamically updating the PlantUML\n\\cite{plantuml} class diagram, followed by evaluation and refinement. Findings\nindicate that the AI-driven approach significantly improves the accuracy and\ncompleteness of the class diagram. Additionally, dynamic enhancement aligns\nwell with Agile development practices, facilitating rapid iterations and\ncontinuous improvement.\n  Key contributions include demonstrating the feasibility and benefits of\nintegrating AI into software modeling tasks, providing a comprehensive\nrepresentation of system behaviors and interactions, and highlighting AI's\npotential to streamline and improve existing software engineering processes.\nFuture research should address identified limitations and explore AI\napplications in other software models.",
        "chunk-id": 5,
        "chunk": "Key contributions include demonstrating the feasibility and benefits of\nintegrating AI into software modeling tasks, providing a comprehensive\nrepresentation of system behaviors and interactions, and highlighting AI's\npotential to streamline and improve existing software engineering processes.\nFuture research should address identified limitations and explore AI",
        "authors": [
            "Djaber Rouabhia",
            "Ismail Hadjadj"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-16T16:30:55+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11002v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11002v1",
        "categories": [
            "Software Engineering"
        ]
    },
    {
        "id": 20000092,
        "doi": null,
        "title": "Enhancing Class Diagram Dynamics: A Natural Language Approach with ChatGPT",
        "abstract": "Integrating artificial intelligence (AI) into software engineering can\ntransform traditional practices by enhancing efficiency, accuracy, and\ninnovation. This study explores using ChatGPT, an advanced AI language model,\nto enhance UML class diagrams dynamically, an underexplored area.\nTraditionally, creating and maintaining class diagrams are manual,\ntime-consuming, and error-prone processes. This research leverages natural\nlanguage processing (NLP) techniques to automate the extraction of methods and\ninteractions from detailed use case tables and integrate them into class\ndiagrams.\n  The methodology involves several steps: (1) developing detailed natural\nlanguage use case tables by master's degree students for a \"Waste Recycling\nPlatform,\" (2) creating an initial static class diagram based on these tables,\n(3) iteratively enriching the class diagram through ChatGPT integration to\nanalyze use cases and suggest methods, (4) reviewing and incorporating these\nmethods into the class diagram, and (5) dynamically updating the PlantUML\n\\cite{plantuml} class diagram, followed by evaluation and refinement. Findings\nindicate that the AI-driven approach significantly improves the accuracy and\ncompleteness of the class diagram. Additionally, dynamic enhancement aligns\nwell with Agile development practices, facilitating rapid iterations and\ncontinuous improvement.\n  Key contributions include demonstrating the feasibility and benefits of\nintegrating AI into software modeling tasks, providing a comprehensive\nrepresentation of system behaviors and interactions, and highlighting AI's\npotential to streamline and improve existing software engineering processes.\nFuture research should address identified limitations and explore AI\napplications in other software models.",
        "chunk-id": 6,
        "chunk": "applications in other software models.",
        "authors": [
            "Djaber Rouabhia",
            "Ismail Hadjadj"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-16T16:30:55+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.11002v1",
        "arxiv_link": "http://arxiv.org/abs/2406.11002v1",
        "categories": [
            "Software Engineering"
        ]
    },
    {
        "id": 20000093,
        "doi": null,
        "title": "Towards Supporting Legal Argumentation with NLP: Is More Data Really All You Need?",
        "abstract": "Modeling legal reasoning and argumentation justifying decisions in cases has\nalways been central to AI & Law, yet contemporary developments in legal NLP\nhave increasingly focused on statistically classifying legal conclusions from\ntext. While conceptually simpler, these approaches often fall short in\nproviding usable justifications connecting to appropriate legal concepts. This\npaper reviews both traditional symbolic works in AI & Law and recent advances\nin legal NLP, and distills possibilities of integrating expert-informed\nknowledge to strike a balance between scalability and explanation in symbolic\nvs. data-driven approaches. We identify open challenges and discuss the\npotential of modern NLP models and methods that integrate",
        "chunk-id": 1,
        "chunk": "Modeling legal reasoning and argumentation justifying decisions in cases has\nalways been central to AI & Law, yet contemporary developments in legal NLP\nhave increasingly focused on statistically classifying legal conclusions from\ntext. While conceptually simpler, these approaches often fall short in\nproviding usable justifications connecting to appropriate legal concepts. This",
        "authors": [
            "T. Y. S. S Santosh",
            "Kevin D. Ashley",
            "Katie Atkinson",
            "Matthias Grabmair"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-16T15:15:44+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.10974v1",
        "arxiv_link": "http://arxiv.org/abs/2406.10974v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000093,
        "doi": null,
        "title": "Towards Supporting Legal Argumentation with NLP: Is More Data Really All You Need?",
        "abstract": "Modeling legal reasoning and argumentation justifying decisions in cases has\nalways been central to AI & Law, yet contemporary developments in legal NLP\nhave increasingly focused on statistically classifying legal conclusions from\ntext. While conceptually simpler, these approaches often fall short in\nproviding usable justifications connecting to appropriate legal concepts. This\npaper reviews both traditional symbolic works in AI & Law and recent advances\nin legal NLP, and distills possibilities of integrating expert-informed\nknowledge to strike a balance between scalability and explanation in symbolic\nvs. data-driven approaches. We identify open challenges and discuss the\npotential of modern NLP models and methods that integrate",
        "chunk-id": 2,
        "chunk": "paper reviews both traditional symbolic works in AI & Law and recent advances\nin legal NLP, and distills possibilities of integrating expert-informed\nknowledge to strike a balance between scalability and explanation in symbolic\nvs. data-driven approaches. We identify open challenges and discuss the\npotential of modern NLP models and methods that integrate",
        "authors": [
            "T. Y. S. S Santosh",
            "Kevin D. Ashley",
            "Katie Atkinson",
            "Matthias Grabmair"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-16T15:15:44+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.10974v1",
        "arxiv_link": "http://arxiv.org/abs/2406.10974v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000094,
        "doi": null,
        "title": "ptt5-v2: A Closer Look at Continued Pretraining of T5 Models for the Portuguese Language",
        "abstract": "Despite advancements in Natural Language Processing (NLP) and the growing\navailability of pretrained models, the English language remains the primary\nfocus of model development. Continued pretraining on language-specific corpora\nprovides a practical solution for adapting models to other languages. However,\nthe impact of different pretraining settings on downstream tasks remains\nunderexplored. This work introduces $\\texttt{ptt5-v2}$, investigating the\ncontinued pretraining of T5 models for Portuguese. We first develop a baseline\nset of settings and pretrain models with sizes up to 3B parameters. Finetuning\non three Portuguese downstream tasks (assin2 STS, assin2 RTE, and TweetSentBR)\nyields SOTA results on the latter two. We then explore the effects of different\npretraining configurations, including quality filters, optimization strategies,\nand multi-epoch pretraining. Perhaps surprisingly, their impact remains subtle\ncompared to our baseline. We release $\\texttt{ptt5-v2}$ pretrained checkpoints\nand the finetuned MonoT5 rerankers on HuggingFace at\nhttps://huggingface.co/collections/unicamp-dl/ptt5-v2-666538a650188ba00aa8d2d0\nand\nhttps://huggingface.co/collections/unicamp-dl/monoptt5-66653981877df3ea727f720d.",
        "chunk-id": 1,
        "chunk": "Despite advancements in Natural Language Processing (NLP) and the growing\navailability of pretrained models, the English language remains the primary\nfocus of model development. Continued pretraining on language-specific corpora\nprovides a practical solution for adapting models to other languages. However,\nthe impact of different pretraining settings on downstream tasks remains",
        "authors": [
            "Marcos Piau",
            "Roberto Lotufo",
            "Rodrigo Nogueira"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-16T05:17:56+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.10806v1",
        "arxiv_link": "http://arxiv.org/abs/2406.10806v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Information Retrieval"
        ]
    },
    {
        "id": 20000094,
        "doi": null,
        "title": "ptt5-v2: A Closer Look at Continued Pretraining of T5 Models for the Portuguese Language",
        "abstract": "Despite advancements in Natural Language Processing (NLP) and the growing\navailability of pretrained models, the English language remains the primary\nfocus of model development. Continued pretraining on language-specific corpora\nprovides a practical solution for adapting models to other languages. However,\nthe impact of different pretraining settings on downstream tasks remains\nunderexplored. This work introduces $\\texttt{ptt5-v2}$, investigating the\ncontinued pretraining of T5 models for Portuguese. We first develop a baseline\nset of settings and pretrain models with sizes up to 3B parameters. Finetuning\non three Portuguese downstream tasks (assin2 STS, assin2 RTE, and TweetSentBR)\nyields SOTA results on the latter two. We then explore the effects of different\npretraining configurations, including quality filters, optimization strategies,\nand multi-epoch pretraining. Perhaps surprisingly, their impact remains subtle\ncompared to our baseline. We release $\\texttt{ptt5-v2}$ pretrained checkpoints\nand the finetuned MonoT5 rerankers on HuggingFace at\nhttps://huggingface.co/collections/unicamp-dl/ptt5-v2-666538a650188ba00aa8d2d0\nand\nhttps://huggingface.co/collections/unicamp-dl/monoptt5-66653981877df3ea727f720d.",
        "chunk-id": 2,
        "chunk": "underexplored. This work introduces $\\texttt{ptt5-v2}$, investigating the\ncontinued pretraining of T5 models for Portuguese. We first develop a baseline\nset of settings and pretrain models with sizes up to 3B parameters. Finetuning\non three Portuguese downstream tasks (assin2 STS, assin2 RTE, and TweetSentBR)",
        "authors": [
            "Marcos Piau",
            "Roberto Lotufo",
            "Rodrigo Nogueira"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-16T05:17:56+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.10806v1",
        "arxiv_link": "http://arxiv.org/abs/2406.10806v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Information Retrieval"
        ]
    },
    {
        "id": 20000094,
        "doi": null,
        "title": "ptt5-v2: A Closer Look at Continued Pretraining of T5 Models for the Portuguese Language",
        "abstract": "Despite advancements in Natural Language Processing (NLP) and the growing\navailability of pretrained models, the English language remains the primary\nfocus of model development. Continued pretraining on language-specific corpora\nprovides a practical solution for adapting models to other languages. However,\nthe impact of different pretraining settings on downstream tasks remains\nunderexplored. This work introduces $\\texttt{ptt5-v2}$, investigating the\ncontinued pretraining of T5 models for Portuguese. We first develop a baseline\nset of settings and pretrain models with sizes up to 3B parameters. Finetuning\non three Portuguese downstream tasks (assin2 STS, assin2 RTE, and TweetSentBR)\nyields SOTA results on the latter two. We then explore the effects of different\npretraining configurations, including quality filters, optimization strategies,\nand multi-epoch pretraining. Perhaps surprisingly, their impact remains subtle\ncompared to our baseline. We release $\\texttt{ptt5-v2}$ pretrained checkpoints\nand the finetuned MonoT5 rerankers on HuggingFace at\nhttps://huggingface.co/collections/unicamp-dl/ptt5-v2-666538a650188ba00aa8d2d0\nand\nhttps://huggingface.co/collections/unicamp-dl/monoptt5-66653981877df3ea727f720d.",
        "chunk-id": 3,
        "chunk": "yields SOTA results on the latter two. We then explore the effects of different\npretraining configurations, including quality filters, optimization strategies,\nand multi-epoch pretraining. Perhaps surprisingly, their impact remains subtle\ncompared to our baseline. We release $\\texttt{ptt5-v2}$ pretrained checkpoints\nand the finetuned MonoT5 rerankers on HuggingFace at",
        "authors": [
            "Marcos Piau",
            "Roberto Lotufo",
            "Rodrigo Nogueira"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-16T05:17:56+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.10806v1",
        "arxiv_link": "http://arxiv.org/abs/2406.10806v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Information Retrieval"
        ]
    },
    {
        "id": 20000094,
        "doi": null,
        "title": "ptt5-v2: A Closer Look at Continued Pretraining of T5 Models for the Portuguese Language",
        "abstract": "Despite advancements in Natural Language Processing (NLP) and the growing\navailability of pretrained models, the English language remains the primary\nfocus of model development. Continued pretraining on language-specific corpora\nprovides a practical solution for adapting models to other languages. However,\nthe impact of different pretraining settings on downstream tasks remains\nunderexplored. This work introduces $\\texttt{ptt5-v2}$, investigating the\ncontinued pretraining of T5 models for Portuguese. We first develop a baseline\nset of settings and pretrain models with sizes up to 3B parameters. Finetuning\non three Portuguese downstream tasks (assin2 STS, assin2 RTE, and TweetSentBR)\nyields SOTA results on the latter two. We then explore the effects of different\npretraining configurations, including quality filters, optimization strategies,\nand multi-epoch pretraining. Perhaps surprisingly, their impact remains subtle\ncompared to our baseline. We release $\\texttt{ptt5-v2}$ pretrained checkpoints\nand the finetuned MonoT5 rerankers on HuggingFace at\nhttps://huggingface.co/collections/unicamp-dl/ptt5-v2-666538a650188ba00aa8d2d0\nand\nhttps://huggingface.co/collections/unicamp-dl/monoptt5-66653981877df3ea727f720d.",
        "chunk-id": 4,
        "chunk": "https://huggingface.co/collections/unicamp-dl/ptt5-v2-666538a650188ba00aa8d2d0\nand\nhttps://huggingface.co/collections/unicamp-dl/monoptt5-66653981877df3ea727f720d.",
        "authors": [
            "Marcos Piau",
            "Roberto Lotufo",
            "Rodrigo Nogueira"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-16T05:17:56+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.10806v1",
        "arxiv_link": "http://arxiv.org/abs/2406.10806v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Information Retrieval"
        ]
    },
    {
        "id": 20000095,
        "doi": null,
        "title": "RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning",
        "abstract": "Pre-trained language models, trained on large-scale corpora, demonstrate\nstrong generalizability across various NLP tasks. Fine-tuning these models for\nspecific tasks typically involves updating all parameters, which is\nresource-intensive. Parameter-efficient fine-tuning (PEFT) methods, such as the\npopular LoRA family, introduce low-rank matrices to learn only a few parameters\nefficiently. However, during inference, the product of these matrices updates\nall pre-trained parameters, complicating tasks like knowledge editing that\nrequire selective updates. We propose a novel PEFT method, which conducts\n\\textbf{r}ow and c\\textbf{o}lumn-wise spar\\textbf{se}\n\\textbf{lo}w-\\textbf{r}ank \\textbf{a}daptation (RoseLoRA), to address this\nchallenge. RoseLoRA identifies and updates only the most important parameters\nfor a specific task, maintaining efficiency while preserving other model\nknowledge. By adding a sparsity constraint on the product of low-rank matrices\nand converting it to row and column-wise sparsity, we ensure efficient and\nprecise model updates. Our theoretical analysis guarantees the lower bound of\nthe sparsity with respective to the matrix product. Extensive experiments on\nfive benchmarks across twenty datasets demonstrate that RoseLoRA outperforms\nbaselines in both general fine-tuning and knowledge editing tasks.",
        "chunk-id": 1,
        "chunk": "Pre-trained language models, trained on large-scale corpora, demonstrate\nstrong generalizability across various NLP tasks. Fine-tuning these models for\nspecific tasks typically involves updating all parameters, which is\nresource-intensive. Parameter-efficient fine-tuning (PEFT) methods, such as the\npopular LoRA family, introduce low-rank matrices to learn only a few parameters",
        "authors": [
            "Haoyu Wang",
            "Tianci Liu",
            "Tuo Zhao",
            "Jing Gao"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-16T02:08:49+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.10777v1",
        "arxiv_link": "http://arxiv.org/abs/2406.10777v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000095,
        "doi": null,
        "title": "RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning",
        "abstract": "Pre-trained language models, trained on large-scale corpora, demonstrate\nstrong generalizability across various NLP tasks. Fine-tuning these models for\nspecific tasks typically involves updating all parameters, which is\nresource-intensive. Parameter-efficient fine-tuning (PEFT) methods, such as the\npopular LoRA family, introduce low-rank matrices to learn only a few parameters\nefficiently. However, during inference, the product of these matrices updates\nall pre-trained parameters, complicating tasks like knowledge editing that\nrequire selective updates. We propose a novel PEFT method, which conducts\n\\textbf{r}ow and c\\textbf{o}lumn-wise spar\\textbf{se}\n\\textbf{lo}w-\\textbf{r}ank \\textbf{a}daptation (RoseLoRA), to address this\nchallenge. RoseLoRA identifies and updates only the most important parameters\nfor a specific task, maintaining efficiency while preserving other model\nknowledge. By adding a sparsity constraint on the product of low-rank matrices\nand converting it to row and column-wise sparsity, we ensure efficient and\nprecise model updates. Our theoretical analysis guarantees the lower bound of\nthe sparsity with respective to the matrix product. Extensive experiments on\nfive benchmarks across twenty datasets demonstrate that RoseLoRA outperforms\nbaselines in both general fine-tuning and knowledge editing tasks.",
        "chunk-id": 2,
        "chunk": "efficiently. However, during inference, the product of these matrices updates\nall pre-trained parameters, complicating tasks like knowledge editing that\nrequire selective updates. We propose a novel PEFT method, which conducts\n\\textbf{r}ow and c\\textbf{o}lumn-wise spar\\textbf{se}\n\\textbf{lo}w-\\textbf{r}ank \\textbf{a}daptation (RoseLoRA), to address this",
        "authors": [
            "Haoyu Wang",
            "Tianci Liu",
            "Tuo Zhao",
            "Jing Gao"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-16T02:08:49+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.10777v1",
        "arxiv_link": "http://arxiv.org/abs/2406.10777v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000095,
        "doi": null,
        "title": "RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning",
        "abstract": "Pre-trained language models, trained on large-scale corpora, demonstrate\nstrong generalizability across various NLP tasks. Fine-tuning these models for\nspecific tasks typically involves updating all parameters, which is\nresource-intensive. Parameter-efficient fine-tuning (PEFT) methods, such as the\npopular LoRA family, introduce low-rank matrices to learn only a few parameters\nefficiently. However, during inference, the product of these matrices updates\nall pre-trained parameters, complicating tasks like knowledge editing that\nrequire selective updates. We propose a novel PEFT method, which conducts\n\\textbf{r}ow and c\\textbf{o}lumn-wise spar\\textbf{se}\n\\textbf{lo}w-\\textbf{r}ank \\textbf{a}daptation (RoseLoRA), to address this\nchallenge. RoseLoRA identifies and updates only the most important parameters\nfor a specific task, maintaining efficiency while preserving other model\nknowledge. By adding a sparsity constraint on the product of low-rank matrices\nand converting it to row and column-wise sparsity, we ensure efficient and\nprecise model updates. Our theoretical analysis guarantees the lower bound of\nthe sparsity with respective to the matrix product. Extensive experiments on\nfive benchmarks across twenty datasets demonstrate that RoseLoRA outperforms\nbaselines in both general fine-tuning and knowledge editing tasks.",
        "chunk-id": 3,
        "chunk": "challenge. RoseLoRA identifies and updates only the most important parameters\nfor a specific task, maintaining efficiency while preserving other model\nknowledge. By adding a sparsity constraint on the product of low-rank matrices\nand converting it to row and column-wise sparsity, we ensure efficient and\nprecise model updates. Our theoretical analysis guarantees the lower bound of",
        "authors": [
            "Haoyu Wang",
            "Tianci Liu",
            "Tuo Zhao",
            "Jing Gao"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-16T02:08:49+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.10777v1",
        "arxiv_link": "http://arxiv.org/abs/2406.10777v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000095,
        "doi": null,
        "title": "RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning",
        "abstract": "Pre-trained language models, trained on large-scale corpora, demonstrate\nstrong generalizability across various NLP tasks. Fine-tuning these models for\nspecific tasks typically involves updating all parameters, which is\nresource-intensive. Parameter-efficient fine-tuning (PEFT) methods, such as the\npopular LoRA family, introduce low-rank matrices to learn only a few parameters\nefficiently. However, during inference, the product of these matrices updates\nall pre-trained parameters, complicating tasks like knowledge editing that\nrequire selective updates. We propose a novel PEFT method, which conducts\n\\textbf{r}ow and c\\textbf{o}lumn-wise spar\\textbf{se}\n\\textbf{lo}w-\\textbf{r}ank \\textbf{a}daptation (RoseLoRA), to address this\nchallenge. RoseLoRA identifies and updates only the most important parameters\nfor a specific task, maintaining efficiency while preserving other model\nknowledge. By adding a sparsity constraint on the product of low-rank matrices\nand converting it to row and column-wise sparsity, we ensure efficient and\nprecise model updates. Our theoretical analysis guarantees the lower bound of\nthe sparsity with respective to the matrix product. Extensive experiments on\nfive benchmarks across twenty datasets demonstrate that RoseLoRA outperforms\nbaselines in both general fine-tuning and knowledge editing tasks.",
        "chunk-id": 4,
        "chunk": "the sparsity with respective to the matrix product. Extensive experiments on\nfive benchmarks across twenty datasets demonstrate that RoseLoRA outperforms\nbaselines in both general fine-tuning and knowledge editing tasks.",
        "authors": [
            "Haoyu Wang",
            "Tianci Liu",
            "Tuo Zhao",
            "Jing Gao"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-16T02:08:49+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.10777v1",
        "arxiv_link": "http://arxiv.org/abs/2406.10777v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000096,
        "doi": null,
        "title": "A Comprehensive Survey of Foundation Models in Medicine",
        "abstract": "Foundation models (FMs) are large-scale deep-learning models trained on\nextensive datasets using self-supervised techniques. These models serve as a\nbase for various downstream tasks, including healthcare. FMs have been adopted\nwith great success across various domains within healthcare, including natural\nlanguage processing (NLP), computer vision, graph learning, biology, and omics.\nExisting healthcare-based surveys have not yet included all of these domains.\nTherefore, this survey provides a comprehensive overview of FMs in healthcare.\nWe focus on the history, learning strategies, flagship models, applications,\nand challenges of FMs. We explore how FMs such as the BERT and GPT families are\nreshaping various healthcare domains, including clinical large language models,\nmedical image analysis, and omics data. Furthermore, we provide a detailed\ntaxonomy of healthcare applications facilitated by FMs, such as clinical NLP,\nmedical computer vision, graph learning, and other biology-related tasks.\nDespite the promising opportunities FMs provide, they also have several\nassociated challenges, which are explained in detail. We also outline potential\nfuture directions to provide researchers and practitioners with insights into\nthe potential and limitations of FMs in healthcare to advance their deployment\nand mitigate associated risks.",
        "chunk-id": 1,
        "chunk": "Foundation models (FMs) are large-scale deep-learning models trained on\nextensive datasets using self-supervised techniques. These models serve as a\nbase for various downstream tasks, including healthcare. FMs have been adopted\nwith great success across various domains within healthcare, including natural",
        "authors": [
            "Wasif Khan",
            "Seowung Leem",
            "Kyle B. See",
            "Joshua K. Wong",
            "Shaoting Zhang",
            "Ruogu Fang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-15T20:04:06+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.10729v1",
        "arxiv_link": "http://arxiv.org/abs/2406.10729v1",
        "categories": [
            "Machine Learning",
            "Artificial Intelligence",
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 20000096,
        "doi": null,
        "title": "A Comprehensive Survey of Foundation Models in Medicine",
        "abstract": "Foundation models (FMs) are large-scale deep-learning models trained on\nextensive datasets using self-supervised techniques. These models serve as a\nbase for various downstream tasks, including healthcare. FMs have been adopted\nwith great success across various domains within healthcare, including natural\nlanguage processing (NLP), computer vision, graph learning, biology, and omics.\nExisting healthcare-based surveys have not yet included all of these domains.\nTherefore, this survey provides a comprehensive overview of FMs in healthcare.\nWe focus on the history, learning strategies, flagship models, applications,\nand challenges of FMs. We explore how FMs such as the BERT and GPT families are\nreshaping various healthcare domains, including clinical large language models,\nmedical image analysis, and omics data. Furthermore, we provide a detailed\ntaxonomy of healthcare applications facilitated by FMs, such as clinical NLP,\nmedical computer vision, graph learning, and other biology-related tasks.\nDespite the promising opportunities FMs provide, they also have several\nassociated challenges, which are explained in detail. We also outline potential\nfuture directions to provide researchers and practitioners with insights into\nthe potential and limitations of FMs in healthcare to advance their deployment\nand mitigate associated risks.",
        "chunk-id": 2,
        "chunk": "language processing (NLP), computer vision, graph learning, biology, and omics.\nExisting healthcare-based surveys have not yet included all of these domains.\nTherefore, this survey provides a comprehensive overview of FMs in healthcare.\nWe focus on the history, learning strategies, flagship models, applications,",
        "authors": [
            "Wasif Khan",
            "Seowung Leem",
            "Kyle B. See",
            "Joshua K. Wong",
            "Shaoting Zhang",
            "Ruogu Fang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-15T20:04:06+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.10729v1",
        "arxiv_link": "http://arxiv.org/abs/2406.10729v1",
        "categories": [
            "Machine Learning",
            "Artificial Intelligence",
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 20000096,
        "doi": null,
        "title": "A Comprehensive Survey of Foundation Models in Medicine",
        "abstract": "Foundation models (FMs) are large-scale deep-learning models trained on\nextensive datasets using self-supervised techniques. These models serve as a\nbase for various downstream tasks, including healthcare. FMs have been adopted\nwith great success across various domains within healthcare, including natural\nlanguage processing (NLP), computer vision, graph learning, biology, and omics.\nExisting healthcare-based surveys have not yet included all of these domains.\nTherefore, this survey provides a comprehensive overview of FMs in healthcare.\nWe focus on the history, learning strategies, flagship models, applications,\nand challenges of FMs. We explore how FMs such as the BERT and GPT families are\nreshaping various healthcare domains, including clinical large language models,\nmedical image analysis, and omics data. Furthermore, we provide a detailed\ntaxonomy of healthcare applications facilitated by FMs, such as clinical NLP,\nmedical computer vision, graph learning, and other biology-related tasks.\nDespite the promising opportunities FMs provide, they also have several\nassociated challenges, which are explained in detail. We also outline potential\nfuture directions to provide researchers and practitioners with insights into\nthe potential and limitations of FMs in healthcare to advance their deployment\nand mitigate associated risks.",
        "chunk-id": 3,
        "chunk": "and challenges of FMs. We explore how FMs such as the BERT and GPT families are\nreshaping various healthcare domains, including clinical large language models,\nmedical image analysis, and omics data. Furthermore, we provide a detailed\ntaxonomy of healthcare applications facilitated by FMs, such as clinical NLP,",
        "authors": [
            "Wasif Khan",
            "Seowung Leem",
            "Kyle B. See",
            "Joshua K. Wong",
            "Shaoting Zhang",
            "Ruogu Fang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-15T20:04:06+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.10729v1",
        "arxiv_link": "http://arxiv.org/abs/2406.10729v1",
        "categories": [
            "Machine Learning",
            "Artificial Intelligence",
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 20000096,
        "doi": null,
        "title": "A Comprehensive Survey of Foundation Models in Medicine",
        "abstract": "Foundation models (FMs) are large-scale deep-learning models trained on\nextensive datasets using self-supervised techniques. These models serve as a\nbase for various downstream tasks, including healthcare. FMs have been adopted\nwith great success across various domains within healthcare, including natural\nlanguage processing (NLP), computer vision, graph learning, biology, and omics.\nExisting healthcare-based surveys have not yet included all of these domains.\nTherefore, this survey provides a comprehensive overview of FMs in healthcare.\nWe focus on the history, learning strategies, flagship models, applications,\nand challenges of FMs. We explore how FMs such as the BERT and GPT families are\nreshaping various healthcare domains, including clinical large language models,\nmedical image analysis, and omics data. Furthermore, we provide a detailed\ntaxonomy of healthcare applications facilitated by FMs, such as clinical NLP,\nmedical computer vision, graph learning, and other biology-related tasks.\nDespite the promising opportunities FMs provide, they also have several\nassociated challenges, which are explained in detail. We also outline potential\nfuture directions to provide researchers and practitioners with insights into\nthe potential and limitations of FMs in healthcare to advance their deployment\nand mitigate associated risks.",
        "chunk-id": 4,
        "chunk": "medical computer vision, graph learning, and other biology-related tasks.\nDespite the promising opportunities FMs provide, they also have several\nassociated challenges, which are explained in detail. We also outline potential\nfuture directions to provide researchers and practitioners with insights into\nthe potential and limitations of FMs in healthcare to advance their deployment",
        "authors": [
            "Wasif Khan",
            "Seowung Leem",
            "Kyle B. See",
            "Joshua K. Wong",
            "Shaoting Zhang",
            "Ruogu Fang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-15T20:04:06+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.10729v1",
        "arxiv_link": "http://arxiv.org/abs/2406.10729v1",
        "categories": [
            "Machine Learning",
            "Artificial Intelligence",
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 20000096,
        "doi": null,
        "title": "A Comprehensive Survey of Foundation Models in Medicine",
        "abstract": "Foundation models (FMs) are large-scale deep-learning models trained on\nextensive datasets using self-supervised techniques. These models serve as a\nbase for various downstream tasks, including healthcare. FMs have been adopted\nwith great success across various domains within healthcare, including natural\nlanguage processing (NLP), computer vision, graph learning, biology, and omics.\nExisting healthcare-based surveys have not yet included all of these domains.\nTherefore, this survey provides a comprehensive overview of FMs in healthcare.\nWe focus on the history, learning strategies, flagship models, applications,\nand challenges of FMs. We explore how FMs such as the BERT and GPT families are\nreshaping various healthcare domains, including clinical large language models,\nmedical image analysis, and omics data. Furthermore, we provide a detailed\ntaxonomy of healthcare applications facilitated by FMs, such as clinical NLP,\nmedical computer vision, graph learning, and other biology-related tasks.\nDespite the promising opportunities FMs provide, they also have several\nassociated challenges, which are explained in detail. We also outline potential\nfuture directions to provide researchers and practitioners with insights into\nthe potential and limitations of FMs in healthcare to advance their deployment\nand mitigate associated risks.",
        "chunk-id": 5,
        "chunk": "and mitigate associated risks.",
        "authors": [
            "Wasif Khan",
            "Seowung Leem",
            "Kyle B. See",
            "Joshua K. Wong",
            "Shaoting Zhang",
            "Ruogu Fang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-15T20:04:06+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.10729v1",
        "arxiv_link": "http://arxiv.org/abs/2406.10729v1",
        "categories": [
            "Machine Learning",
            "Artificial Intelligence",
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 20000097,
        "doi": null,
        "title": "Intertwining CP and NLP: The Generation of Unreasonably Constrained Sentences",
        "abstract": "Constrained text generation remains a challenging task, particularly when\ndealing with hard constraints. Traditional Natural Language Processing (NLP)\napproaches prioritize generating meaningful and coherent output. Also, the\ncurrent state-of-the-art methods often lack the expressiveness and constraint\nsatisfaction capabilities to handle such tasks effectively. This paper presents\nthe Constraints First Framework to remedy this issue. This framework considers\na constrained text generation problem as a discrete combinatorial optimization\nproblem. It is solved by a constraint programming method that combines\nlinguistic properties (e.g., n-grams or language level) with other more\nclassical constraints (e.g., the number of characters, syllables, or words).\nEventually, a curation phase allows for selecting the best-generated sentences\naccording to perplexity using a large language model. The effectiveness of this\napproach is demonstrated by tackling a new more tediously constrained text\ngeneration problem: the iconic RADNER sentences problem. This problem aims to\ngenerate sentences respecting a set of quite strict rules defined by their use\nin vision and clinical research. Thanks to our CP-based approach, many new\nstrongly constrained sentences have been successfully generated in an automatic\nmanner. This highlights the potential of our approach to handle unreasonably\nconstrained text generation scenarios.",
        "chunk-id": 1,
        "chunk": "Constrained text generation remains a challenging task, particularly when\ndealing with hard constraints. Traditional Natural Language Processing (NLP)\napproaches prioritize generating meaningful and coherent output. Also, the\ncurrent state-of-the-art methods often lack the expressiveness and constraint\nsatisfaction capabilities to handle such tasks effectively. This paper presents",
        "authors": [
            "Alexandre Bonlarron",
            "Jean-Charles R\u00e9gin"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-15T17:40:49+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.15473v1",
        "arxiv_link": "http://arxiv.org/abs/2406.15473v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000097,
        "doi": null,
        "title": "Intertwining CP and NLP: The Generation of Unreasonably Constrained Sentences",
        "abstract": "Constrained text generation remains a challenging task, particularly when\ndealing with hard constraints. Traditional Natural Language Processing (NLP)\napproaches prioritize generating meaningful and coherent output. Also, the\ncurrent state-of-the-art methods often lack the expressiveness and constraint\nsatisfaction capabilities to handle such tasks effectively. This paper presents\nthe Constraints First Framework to remedy this issue. This framework considers\na constrained text generation problem as a discrete combinatorial optimization\nproblem. It is solved by a constraint programming method that combines\nlinguistic properties (e.g., n-grams or language level) with other more\nclassical constraints (e.g., the number of characters, syllables, or words).\nEventually, a curation phase allows for selecting the best-generated sentences\naccording to perplexity using a large language model. The effectiveness of this\napproach is demonstrated by tackling a new more tediously constrained text\ngeneration problem: the iconic RADNER sentences problem. This problem aims to\ngenerate sentences respecting a set of quite strict rules defined by their use\nin vision and clinical research. Thanks to our CP-based approach, many new\nstrongly constrained sentences have been successfully generated in an automatic\nmanner. This highlights the potential of our approach to handle unreasonably\nconstrained text generation scenarios.",
        "chunk-id": 2,
        "chunk": "the Constraints First Framework to remedy this issue. This framework considers\na constrained text generation problem as a discrete combinatorial optimization\nproblem. It is solved by a constraint programming method that combines\nlinguistic properties (e.g., n-grams or language level) with other more\nclassical constraints (e.g., the number of characters, syllables, or words).",
        "authors": [
            "Alexandre Bonlarron",
            "Jean-Charles R\u00e9gin"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-15T17:40:49+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.15473v1",
        "arxiv_link": "http://arxiv.org/abs/2406.15473v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000097,
        "doi": null,
        "title": "Intertwining CP and NLP: The Generation of Unreasonably Constrained Sentences",
        "abstract": "Constrained text generation remains a challenging task, particularly when\ndealing with hard constraints. Traditional Natural Language Processing (NLP)\napproaches prioritize generating meaningful and coherent output. Also, the\ncurrent state-of-the-art methods often lack the expressiveness and constraint\nsatisfaction capabilities to handle such tasks effectively. This paper presents\nthe Constraints First Framework to remedy this issue. This framework considers\na constrained text generation problem as a discrete combinatorial optimization\nproblem. It is solved by a constraint programming method that combines\nlinguistic properties (e.g., n-grams or language level) with other more\nclassical constraints (e.g., the number of characters, syllables, or words).\nEventually, a curation phase allows for selecting the best-generated sentences\naccording to perplexity using a large language model. The effectiveness of this\napproach is demonstrated by tackling a new more tediously constrained text\ngeneration problem: the iconic RADNER sentences problem. This problem aims to\ngenerate sentences respecting a set of quite strict rules defined by their use\nin vision and clinical research. Thanks to our CP-based approach, many new\nstrongly constrained sentences have been successfully generated in an automatic\nmanner. This highlights the potential of our approach to handle unreasonably\nconstrained text generation scenarios.",
        "chunk-id": 3,
        "chunk": "Eventually, a curation phase allows for selecting the best-generated sentences\naccording to perplexity using a large language model. The effectiveness of this\napproach is demonstrated by tackling a new more tediously constrained text\ngeneration problem: the iconic RADNER sentences problem. This problem aims to",
        "authors": [
            "Alexandre Bonlarron",
            "Jean-Charles R\u00e9gin"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-15T17:40:49+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.15473v1",
        "arxiv_link": "http://arxiv.org/abs/2406.15473v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000097,
        "doi": null,
        "title": "Intertwining CP and NLP: The Generation of Unreasonably Constrained Sentences",
        "abstract": "Constrained text generation remains a challenging task, particularly when\ndealing with hard constraints. Traditional Natural Language Processing (NLP)\napproaches prioritize generating meaningful and coherent output. Also, the\ncurrent state-of-the-art methods often lack the expressiveness and constraint\nsatisfaction capabilities to handle such tasks effectively. This paper presents\nthe Constraints First Framework to remedy this issue. This framework considers\na constrained text generation problem as a discrete combinatorial optimization\nproblem. It is solved by a constraint programming method that combines\nlinguistic properties (e.g., n-grams or language level) with other more\nclassical constraints (e.g., the number of characters, syllables, or words).\nEventually, a curation phase allows for selecting the best-generated sentences\naccording to perplexity using a large language model. The effectiveness of this\napproach is demonstrated by tackling a new more tediously constrained text\ngeneration problem: the iconic RADNER sentences problem. This problem aims to\ngenerate sentences respecting a set of quite strict rules defined by their use\nin vision and clinical research. Thanks to our CP-based approach, many new\nstrongly constrained sentences have been successfully generated in an automatic\nmanner. This highlights the potential of our approach to handle unreasonably\nconstrained text generation scenarios.",
        "chunk-id": 4,
        "chunk": "generate sentences respecting a set of quite strict rules defined by their use\nin vision and clinical research. Thanks to our CP-based approach, many new\nstrongly constrained sentences have been successfully generated in an automatic\nmanner. This highlights the potential of our approach to handle unreasonably\nconstrained text generation scenarios.",
        "authors": [
            "Alexandre Bonlarron",
            "Jean-Charles R\u00e9gin"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-15T17:40:49+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.15473v1",
        "arxiv_link": "http://arxiv.org/abs/2406.15473v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 20000098,
        "doi": null,
        "title": "Multilingual Large Language Models and Curse of Multilinguality",
        "abstract": "Multilingual Large Language Models (LLMs) have gained large popularity among\nNatural Language Processing (NLP) researchers and practitioners. These models,\ntrained on huge datasets, show proficiency across various languages and\ndemonstrate effectiveness in numerous downstream tasks. This paper navigates\nthe landscape of multilingual LLMs, providing an introductory overview of their\ntechnical aspects. It explains underlying architectures, objective functions,\npre-training data sources, and tokenization methods. This work explores the\nunique features of different model types: encoder-only (mBERT, XLM-R),\ndecoder-only (XGLM, PALM, BLOOM, GPT-3), and encoder-decoder models (mT5,\nmBART). Additionally, it addresses one of the significant limitations of\nmultilingual LLMs - the curse of multilinguality - and discusses current\nattempts to overcome it.",
        "chunk-id": 1,
        "chunk": "Multilingual Large Language Models (LLMs) have gained large popularity among\nNatural Language Processing (NLP) researchers and practitioners. These models,\ntrained on huge datasets, show proficiency across various languages and\ndemonstrate effectiveness in numerous downstream tasks. This paper navigates\nthe landscape of multilingual LLMs, providing an introductory overview of their",
        "authors": [
            "Daniil Gurgurov",
            "Tanja B\u00e4umel",
            "Tatiana Anikina"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-15T11:31:39+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.10602v1",
        "arxiv_link": "http://arxiv.org/abs/2406.10602v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000098,
        "doi": null,
        "title": "Multilingual Large Language Models and Curse of Multilinguality",
        "abstract": "Multilingual Large Language Models (LLMs) have gained large popularity among\nNatural Language Processing (NLP) researchers and practitioners. These models,\ntrained on huge datasets, show proficiency across various languages and\ndemonstrate effectiveness in numerous downstream tasks. This paper navigates\nthe landscape of multilingual LLMs, providing an introductory overview of their\ntechnical aspects. It explains underlying architectures, objective functions,\npre-training data sources, and tokenization methods. This work explores the\nunique features of different model types: encoder-only (mBERT, XLM-R),\ndecoder-only (XGLM, PALM, BLOOM, GPT-3), and encoder-decoder models (mT5,\nmBART). Additionally, it addresses one of the significant limitations of\nmultilingual LLMs - the curse of multilinguality - and discusses current\nattempts to overcome it.",
        "chunk-id": 2,
        "chunk": "technical aspects. It explains underlying architectures, objective functions,\npre-training data sources, and tokenization methods. This work explores the\nunique features of different model types: encoder-only (mBERT, XLM-R),\ndecoder-only (XGLM, PALM, BLOOM, GPT-3), and encoder-decoder models (mT5,\nmBART). Additionally, it addresses one of the significant limitations of",
        "authors": [
            "Daniil Gurgurov",
            "Tanja B\u00e4umel",
            "Tatiana Anikina"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-15T11:31:39+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.10602v1",
        "arxiv_link": "http://arxiv.org/abs/2406.10602v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000098,
        "doi": null,
        "title": "Multilingual Large Language Models and Curse of Multilinguality",
        "abstract": "Multilingual Large Language Models (LLMs) have gained large popularity among\nNatural Language Processing (NLP) researchers and practitioners. These models,\ntrained on huge datasets, show proficiency across various languages and\ndemonstrate effectiveness in numerous downstream tasks. This paper navigates\nthe landscape of multilingual LLMs, providing an introductory overview of their\ntechnical aspects. It explains underlying architectures, objective functions,\npre-training data sources, and tokenization methods. This work explores the\nunique features of different model types: encoder-only (mBERT, XLM-R),\ndecoder-only (XGLM, PALM, BLOOM, GPT-3), and encoder-decoder models (mT5,\nmBART). Additionally, it addresses one of the significant limitations of\nmultilingual LLMs - the curse of multilinguality - and discusses current\nattempts to overcome it.",
        "chunk-id": 3,
        "chunk": "multilingual LLMs - the curse of multilinguality - and discusses current\nattempts to overcome it.",
        "authors": [
            "Daniil Gurgurov",
            "Tanja B\u00e4umel",
            "Tatiana Anikina"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-15T11:31:39+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.10602v1",
        "arxiv_link": "http://arxiv.org/abs/2406.10602v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000099,
        "doi": null,
        "title": "CancerLLM: A Large Language Model in Cancer Domain",
        "abstract": "Medical Large Language Models (LLMs) such as ClinicalCamel 70B,\nLlama3-OpenBioLLM 70B have demonstrated impressive performance on a wide\nvariety of medical NLP task.However, there still lacks a large language model\n(LLM) specifically designed for cancer domain. Moreover, these LLMs typically\nhave billions of parameters, making them computationally expensive for\nhealthcare systems.Thus, in this study, we propose CancerLLM, a model with 7\nbillion parameters and a Mistral-style architecture, pre-trained on 2,676,642\nclinical notes and 515,524 pathology reports covering 17 cancer types, followed\nby fine-tuning on three cancer-relevant tasks, including cancer phenotypes\nextraction, cancer diagnosis generation, and cancer treatment plan generation.\nOur evaluation demonstrated that CancerLLM achieves state-of-the-art results\ncompared to other existing LLMs, with an average F1 score improvement of 8.1\\%.\nAdditionally, CancerLLM outperforms other models on two proposed robustness\ntestbeds. This illustrates that CancerLLM can be effectively applied to\nclinical AI systems, enhancing clinical research and healthcare delivery in the\nfield of cancer.",
        "chunk-id": 1,
        "chunk": "Medical Large Language Models (LLMs) such as ClinicalCamel 70B,\nLlama3-OpenBioLLM 70B have demonstrated impressive performance on a wide\nvariety of medical NLP task.However, there still lacks a large language model\n(LLM) specifically designed for cancer domain. Moreover, these LLMs typically\nhave billions of parameters, making them computationally expensive for",
        "authors": [
            "Mingchen Li",
            "Anne Blaes",
            "Steven Johnson",
            "Hongfang Liu",
            "Hua Xu",
            "Rui Zhang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-15T01:02:48+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.10459v1",
        "arxiv_link": "http://arxiv.org/abs/2406.10459v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000099,
        "doi": null,
        "title": "CancerLLM: A Large Language Model in Cancer Domain",
        "abstract": "Medical Large Language Models (LLMs) such as ClinicalCamel 70B,\nLlama3-OpenBioLLM 70B have demonstrated impressive performance on a wide\nvariety of medical NLP task.However, there still lacks a large language model\n(LLM) specifically designed for cancer domain. Moreover, these LLMs typically\nhave billions of parameters, making them computationally expensive for\nhealthcare systems.Thus, in this study, we propose CancerLLM, a model with 7\nbillion parameters and a Mistral-style architecture, pre-trained on 2,676,642\nclinical notes and 515,524 pathology reports covering 17 cancer types, followed\nby fine-tuning on three cancer-relevant tasks, including cancer phenotypes\nextraction, cancer diagnosis generation, and cancer treatment plan generation.\nOur evaluation demonstrated that CancerLLM achieves state-of-the-art results\ncompared to other existing LLMs, with an average F1 score improvement of 8.1\\%.\nAdditionally, CancerLLM outperforms other models on two proposed robustness\ntestbeds. This illustrates that CancerLLM can be effectively applied to\nclinical AI systems, enhancing clinical research and healthcare delivery in the\nfield of cancer.",
        "chunk-id": 2,
        "chunk": "healthcare systems.Thus, in this study, we propose CancerLLM, a model with 7\nbillion parameters and a Mistral-style architecture, pre-trained on 2,676,642\nclinical notes and 515,524 pathology reports covering 17 cancer types, followed\nby fine-tuning on three cancer-relevant tasks, including cancer phenotypes",
        "authors": [
            "Mingchen Li",
            "Anne Blaes",
            "Steven Johnson",
            "Hongfang Liu",
            "Hua Xu",
            "Rui Zhang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-15T01:02:48+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.10459v1",
        "arxiv_link": "http://arxiv.org/abs/2406.10459v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000099,
        "doi": null,
        "title": "CancerLLM: A Large Language Model in Cancer Domain",
        "abstract": "Medical Large Language Models (LLMs) such as ClinicalCamel 70B,\nLlama3-OpenBioLLM 70B have demonstrated impressive performance on a wide\nvariety of medical NLP task.However, there still lacks a large language model\n(LLM) specifically designed for cancer domain. Moreover, these LLMs typically\nhave billions of parameters, making them computationally expensive for\nhealthcare systems.Thus, in this study, we propose CancerLLM, a model with 7\nbillion parameters and a Mistral-style architecture, pre-trained on 2,676,642\nclinical notes and 515,524 pathology reports covering 17 cancer types, followed\nby fine-tuning on three cancer-relevant tasks, including cancer phenotypes\nextraction, cancer diagnosis generation, and cancer treatment plan generation.\nOur evaluation demonstrated that CancerLLM achieves state-of-the-art results\ncompared to other existing LLMs, with an average F1 score improvement of 8.1\\%.\nAdditionally, CancerLLM outperforms other models on two proposed robustness\ntestbeds. This illustrates that CancerLLM can be effectively applied to\nclinical AI systems, enhancing clinical research and healthcare delivery in the\nfield of cancer.",
        "chunk-id": 3,
        "chunk": "extraction, cancer diagnosis generation, and cancer treatment plan generation.\nOur evaluation demonstrated that CancerLLM achieves state-of-the-art results\ncompared to other existing LLMs, with an average F1 score improvement of 8.1\\%.\nAdditionally, CancerLLM outperforms other models on two proposed robustness\ntestbeds. This illustrates that CancerLLM can be effectively applied to",
        "authors": [
            "Mingchen Li",
            "Anne Blaes",
            "Steven Johnson",
            "Hongfang Liu",
            "Hua Xu",
            "Rui Zhang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-15T01:02:48+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.10459v1",
        "arxiv_link": "http://arxiv.org/abs/2406.10459v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 20000099,
        "doi": null,
        "title": "CancerLLM: A Large Language Model in Cancer Domain",
        "abstract": "Medical Large Language Models (LLMs) such as ClinicalCamel 70B,\nLlama3-OpenBioLLM 70B have demonstrated impressive performance on a wide\nvariety of medical NLP task.However, there still lacks a large language model\n(LLM) specifically designed for cancer domain. Moreover, these LLMs typically\nhave billions of parameters, making them computationally expensive for\nhealthcare systems.Thus, in this study, we propose CancerLLM, a model with 7\nbillion parameters and a Mistral-style architecture, pre-trained on 2,676,642\nclinical notes and 515,524 pathology reports covering 17 cancer types, followed\nby fine-tuning on three cancer-relevant tasks, including cancer phenotypes\nextraction, cancer diagnosis generation, and cancer treatment plan generation.\nOur evaluation demonstrated that CancerLLM achieves state-of-the-art results\ncompared to other existing LLMs, with an average F1 score improvement of 8.1\\%.\nAdditionally, CancerLLM outperforms other models on two proposed robustness\ntestbeds. This illustrates that CancerLLM can be effectively applied to\nclinical AI systems, enhancing clinical research and healthcare delivery in the\nfield of cancer.",
        "chunk-id": 4,
        "chunk": "clinical AI systems, enhancing clinical research and healthcare delivery in the\nfield of cancer.",
        "authors": [
            "Mingchen Li",
            "Anne Blaes",
            "Steven Johnson",
            "Hongfang Liu",
            "Hua Xu",
            "Rui Zhang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-15T01:02:48+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.10459v1",
        "arxiv_link": "http://arxiv.org/abs/2406.10459v1",
        "categories": [
            "Computation and Language"
        ]
    }
]