[
    {
        "id": 30000000,
        "doi": null,
        "title": "SimLOB: Learning Representations of Limited Order Book for Financial Market Simulation",
        "abstract": "Financial market simulation (FMS) serves as a promising tool for\nunderstanding market anomalies and the underlying trading behaviors. To ensure\nhigh-fidelity simulations, it is crucial to calibrate the FMS model for\ngenerating data closely resembling the observed market data. Previous efforts\nprimarily focused on calibrating the mid-price data, leading to essential\ninformation loss of the market activities and thus biasing the calibrated\nmodel. The Limit Order Book (LOB) data is the fundamental data fully capturing\nthe market micro-structure and is adopted by worldwide exchanges. However, LOB\nis not applicable to existing calibration objective functions due to its\ntabular structure not suitable for the vectorized input requirement. This paper\nproposes to explicitly learn the vectorized representations of LOB with a\nTransformer-based autoencoder. Then the latent vector, which captures the major\ninformation of LOB, can be applied for calibration. Extensive experiments show\nthat the learned latent representation not only preserves the non-linear\nauto-correlation in the temporal axis, but the precedence between successive\nprice levels of LOB. Besides, it is verified that the performance of the\nrepresentation learning stage is consistent with the downstream calibration\ntasks. Thus, this work also progresses the FMS on LOB data, for the first time.",
        "chunk-id": 1,
        "chunk": "Financial market simulation (FMS) serves as a promising tool for\nunderstanding market anomalies and the underlying trading behaviors. To ensure\nhigh-fidelity simulations, it is crucial to calibrate the FMS model for\ngenerating data closely resembling the observed market data. Previous efforts\nprimarily focused on calibrating the mid-price data, leading to essential",
        "authors": [
            "Yuanzhe Li",
            "Yue Wu",
            "Peng Yang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:59:58+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19396v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19396v1",
        "categories": [
            "Computational Engineering, Finance, and Science"
        ]
    },
    {
        "id": 30000000,
        "doi": null,
        "title": "SimLOB: Learning Representations of Limited Order Book for Financial Market Simulation",
        "abstract": "Financial market simulation (FMS) serves as a promising tool for\nunderstanding market anomalies and the underlying trading behaviors. To ensure\nhigh-fidelity simulations, it is crucial to calibrate the FMS model for\ngenerating data closely resembling the observed market data. Previous efforts\nprimarily focused on calibrating the mid-price data, leading to essential\ninformation loss of the market activities and thus biasing the calibrated\nmodel. The Limit Order Book (LOB) data is the fundamental data fully capturing\nthe market micro-structure and is adopted by worldwide exchanges. However, LOB\nis not applicable to existing calibration objective functions due to its\ntabular structure not suitable for the vectorized input requirement. This paper\nproposes to explicitly learn the vectorized representations of LOB with a\nTransformer-based autoencoder. Then the latent vector, which captures the major\ninformation of LOB, can be applied for calibration. Extensive experiments show\nthat the learned latent representation not only preserves the non-linear\nauto-correlation in the temporal axis, but the precedence between successive\nprice levels of LOB. Besides, it is verified that the performance of the\nrepresentation learning stage is consistent with the downstream calibration\ntasks. Thus, this work also progresses the FMS on LOB data, for the first time.",
        "chunk-id": 2,
        "chunk": "information loss of the market activities and thus biasing the calibrated\nmodel. The Limit Order Book (LOB) data is the fundamental data fully capturing\nthe market micro-structure and is adopted by worldwide exchanges. However, LOB\nis not applicable to existing calibration objective functions due to its\ntabular structure not suitable for the vectorized input requirement. This paper",
        "authors": [
            "Yuanzhe Li",
            "Yue Wu",
            "Peng Yang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:59:58+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19396v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19396v1",
        "categories": [
            "Computational Engineering, Finance, and Science"
        ]
    },
    {
        "id": 30000000,
        "doi": null,
        "title": "SimLOB: Learning Representations of Limited Order Book for Financial Market Simulation",
        "abstract": "Financial market simulation (FMS) serves as a promising tool for\nunderstanding market anomalies and the underlying trading behaviors. To ensure\nhigh-fidelity simulations, it is crucial to calibrate the FMS model for\ngenerating data closely resembling the observed market data. Previous efforts\nprimarily focused on calibrating the mid-price data, leading to essential\ninformation loss of the market activities and thus biasing the calibrated\nmodel. The Limit Order Book (LOB) data is the fundamental data fully capturing\nthe market micro-structure and is adopted by worldwide exchanges. However, LOB\nis not applicable to existing calibration objective functions due to its\ntabular structure not suitable for the vectorized input requirement. This paper\nproposes to explicitly learn the vectorized representations of LOB with a\nTransformer-based autoencoder. Then the latent vector, which captures the major\ninformation of LOB, can be applied for calibration. Extensive experiments show\nthat the learned latent representation not only preserves the non-linear\nauto-correlation in the temporal axis, but the precedence between successive\nprice levels of LOB. Besides, it is verified that the performance of the\nrepresentation learning stage is consistent with the downstream calibration\ntasks. Thus, this work also progresses the FMS on LOB data, for the first time.",
        "chunk-id": 3,
        "chunk": "proposes to explicitly learn the vectorized representations of LOB with a\nTransformer-based autoencoder. Then the latent vector, which captures the major\ninformation of LOB, can be applied for calibration. Extensive experiments show\nthat the learned latent representation not only preserves the non-linear\nauto-correlation in the temporal axis, but the precedence between successive",
        "authors": [
            "Yuanzhe Li",
            "Yue Wu",
            "Peng Yang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:59:58+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19396v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19396v1",
        "categories": [
            "Computational Engineering, Finance, and Science"
        ]
    },
    {
        "id": 30000000,
        "doi": null,
        "title": "SimLOB: Learning Representations of Limited Order Book for Financial Market Simulation",
        "abstract": "Financial market simulation (FMS) serves as a promising tool for\nunderstanding market anomalies and the underlying trading behaviors. To ensure\nhigh-fidelity simulations, it is crucial to calibrate the FMS model for\ngenerating data closely resembling the observed market data. Previous efforts\nprimarily focused on calibrating the mid-price data, leading to essential\ninformation loss of the market activities and thus biasing the calibrated\nmodel. The Limit Order Book (LOB) data is the fundamental data fully capturing\nthe market micro-structure and is adopted by worldwide exchanges. However, LOB\nis not applicable to existing calibration objective functions due to its\ntabular structure not suitable for the vectorized input requirement. This paper\nproposes to explicitly learn the vectorized representations of LOB with a\nTransformer-based autoencoder. Then the latent vector, which captures the major\ninformation of LOB, can be applied for calibration. Extensive experiments show\nthat the learned latent representation not only preserves the non-linear\nauto-correlation in the temporal axis, but the precedence between successive\nprice levels of LOB. Besides, it is verified that the performance of the\nrepresentation learning stage is consistent with the downstream calibration\ntasks. Thus, this work also progresses the FMS on LOB data, for the first time.",
        "chunk-id": 4,
        "chunk": "price levels of LOB. Besides, it is verified that the performance of the\nrepresentation learning stage is consistent with the downstream calibration\ntasks. Thus, this work also progresses the FMS on LOB data, for the first time.",
        "authors": [
            "Yuanzhe Li",
            "Yue Wu",
            "Peng Yang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:59:58+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19396v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19396v1",
        "categories": [
            "Computational Engineering, Finance, and Science"
        ]
    },
    {
        "id": 30000001,
        "doi": null,
        "title": "Dataset Size Recovery from LoRA Weights",
        "abstract": "Model inversion and membership inference attacks aim to reconstruct and\nverify the data which a model was trained on. However, they are not guaranteed\nto find all training samples as they do not know the size of the training set.\nIn this paper, we introduce a new task: dataset size recovery, that aims to\ndetermine the number of samples used to train a model, directly from its\nweights. We then propose DSiRe, a method for recovering the number of images\nused to fine-tune a model, in the common case where fine-tuning uses LoRA. We\ndiscover that both the norm and the spectrum of the LoRA matrices are closely\nlinked to the fine-tuning dataset size; we leverage this finding to propose a\nsimple yet effective prediction algorithm. To evaluate dataset size recovery of\nLoRA weights, we develop and release a new benchmark, LoRA-WiSE, consisting of\nover 25000 weight snapshots from more than 2000 diverse LoRA fine-tuned models.\nOur best classifier can predict the number of fine-tuning images with a mean\nabsolute error of 0.36 images, establishing the feasibility of this attack.",
        "chunk-id": 1,
        "chunk": "Model inversion and membership inference attacks aim to reconstruct and\nverify the data which a model was trained on. However, they are not guaranteed\nto find all training samples as they do not know the size of the training set.\nIn this paper, we introduce a new task: dataset size recovery, that aims to\ndetermine the number of samples used to train a model, directly from its",
        "authors": [
            "Mohammad Salama",
            "Jonathan Kahana",
            "Eliahu Horwitz",
            "Yedid Hoshen"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:59:53+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19395v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19395v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000001,
        "doi": null,
        "title": "Dataset Size Recovery from LoRA Weights",
        "abstract": "Model inversion and membership inference attacks aim to reconstruct and\nverify the data which a model was trained on. However, they are not guaranteed\nto find all training samples as they do not know the size of the training set.\nIn this paper, we introduce a new task: dataset size recovery, that aims to\ndetermine the number of samples used to train a model, directly from its\nweights. We then propose DSiRe, a method for recovering the number of images\nused to fine-tune a model, in the common case where fine-tuning uses LoRA. We\ndiscover that both the norm and the spectrum of the LoRA matrices are closely\nlinked to the fine-tuning dataset size; we leverage this finding to propose a\nsimple yet effective prediction algorithm. To evaluate dataset size recovery of\nLoRA weights, we develop and release a new benchmark, LoRA-WiSE, consisting of\nover 25000 weight snapshots from more than 2000 diverse LoRA fine-tuned models.\nOur best classifier can predict the number of fine-tuning images with a mean\nabsolute error of 0.36 images, establishing the feasibility of this attack.",
        "chunk-id": 2,
        "chunk": "weights. We then propose DSiRe, a method for recovering the number of images\nused to fine-tune a model, in the common case where fine-tuning uses LoRA. We\ndiscover that both the norm and the spectrum of the LoRA matrices are closely\nlinked to the fine-tuning dataset size; we leverage this finding to propose a",
        "authors": [
            "Mohammad Salama",
            "Jonathan Kahana",
            "Eliahu Horwitz",
            "Yedid Hoshen"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:59:53+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19395v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19395v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000001,
        "doi": null,
        "title": "Dataset Size Recovery from LoRA Weights",
        "abstract": "Model inversion and membership inference attacks aim to reconstruct and\nverify the data which a model was trained on. However, they are not guaranteed\nto find all training samples as they do not know the size of the training set.\nIn this paper, we introduce a new task: dataset size recovery, that aims to\ndetermine the number of samples used to train a model, directly from its\nweights. We then propose DSiRe, a method for recovering the number of images\nused to fine-tune a model, in the common case where fine-tuning uses LoRA. We\ndiscover that both the norm and the spectrum of the LoRA matrices are closely\nlinked to the fine-tuning dataset size; we leverage this finding to propose a\nsimple yet effective prediction algorithm. To evaluate dataset size recovery of\nLoRA weights, we develop and release a new benchmark, LoRA-WiSE, consisting of\nover 25000 weight snapshots from more than 2000 diverse LoRA fine-tuned models.\nOur best classifier can predict the number of fine-tuning images with a mean\nabsolute error of 0.36 images, establishing the feasibility of this attack.",
        "chunk-id": 3,
        "chunk": "simple yet effective prediction algorithm. To evaluate dataset size recovery of\nLoRA weights, we develop and release a new benchmark, LoRA-WiSE, consisting of\nover 25000 weight snapshots from more than 2000 diverse LoRA fine-tuned models.\nOur best classifier can predict the number of fine-tuning images with a mean",
        "authors": [
            "Mohammad Salama",
            "Jonathan Kahana",
            "Eliahu Horwitz",
            "Yedid Hoshen"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:59:53+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19395v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19395v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000001,
        "doi": null,
        "title": "Dataset Size Recovery from LoRA Weights",
        "abstract": "Model inversion and membership inference attacks aim to reconstruct and\nverify the data which a model was trained on. However, they are not guaranteed\nto find all training samples as they do not know the size of the training set.\nIn this paper, we introduce a new task: dataset size recovery, that aims to\ndetermine the number of samples used to train a model, directly from its\nweights. We then propose DSiRe, a method for recovering the number of images\nused to fine-tune a model, in the common case where fine-tuning uses LoRA. We\ndiscover that both the norm and the spectrum of the LoRA matrices are closely\nlinked to the fine-tuning dataset size; we leverage this finding to propose a\nsimple yet effective prediction algorithm. To evaluate dataset size recovery of\nLoRA weights, we develop and release a new benchmark, LoRA-WiSE, consisting of\nover 25000 weight snapshots from more than 2000 diverse LoRA fine-tuned models.\nOur best classifier can predict the number of fine-tuning images with a mean\nabsolute error of 0.36 images, establishing the feasibility of this attack.",
        "chunk-id": 4,
        "chunk": "absolute error of 0.36 images, establishing the feasibility of this attack.",
        "authors": [
            "Mohammad Salama",
            "Jonathan Kahana",
            "Eliahu Horwitz",
            "Yedid Hoshen"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:59:53+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19395v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19395v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000002,
        "doi": null,
        "title": "HUWSOD: Holistic Self-training for Unified Weakly Supervised Object Detection",
        "abstract": "Most WSOD methods rely on traditional object proposals to generate candidate\nregions and are confronted with unstable training, which easily gets stuck in a\npoor local optimum. In this paper, we introduce a unified, high-capacity weakly\nsupervised object detection (WSOD) network called HUWSOD, which utilizes a\ncomprehensive self-training framework without needing external modules or\nadditional supervision. HUWSOD innovatively incorporates a self-supervised\nproposal generator and an autoencoder proposal generator with a multi-rate\nresampling pyramid to replace traditional object proposals, enabling end-to-end\nWSOD training and inference. Additionally, we implement a holistic\nself-training scheme that refines detection scores and coordinates through\nstep-wise entropy minimization and consistency-constraint regularization,\nensuring consistent predictions across stochastic augmentations of the same\nimage. Extensive experiments on PASCAL VOC and MS COCO demonstrate that HUWSOD\ncompetes with state-of-the-art WSOD methods, eliminating the need for offline\nproposals and additional data. The peak performance of HUWSOD approaches that\nof fully-supervised Faster R-CNN. Our findings also indicate that randomly\ninitialized boxes, although significantly different from well-designed offline\nobject proposals, are effective for WSOD training.",
        "chunk-id": 1,
        "chunk": "Most WSOD methods rely on traditional object proposals to generate candidate\nregions and are confronted with unstable training, which easily gets stuck in a\npoor local optimum. In this paper, we introduce a unified, high-capacity weakly\nsupervised object detection (WSOD) network called HUWSOD, which utilizes a",
        "authors": [
            "Liujuan Cao",
            "Jianghang Lin",
            "Zebo Hong",
            "Yunhang Shen",
            "Shaohui Lin",
            "Chao Chen",
            "Rongrong Ji"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:59:49+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19394v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19394v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000002,
        "doi": null,
        "title": "HUWSOD: Holistic Self-training for Unified Weakly Supervised Object Detection",
        "abstract": "Most WSOD methods rely on traditional object proposals to generate candidate\nregions and are confronted with unstable training, which easily gets stuck in a\npoor local optimum. In this paper, we introduce a unified, high-capacity weakly\nsupervised object detection (WSOD) network called HUWSOD, which utilizes a\ncomprehensive self-training framework without needing external modules or\nadditional supervision. HUWSOD innovatively incorporates a self-supervised\nproposal generator and an autoencoder proposal generator with a multi-rate\nresampling pyramid to replace traditional object proposals, enabling end-to-end\nWSOD training and inference. Additionally, we implement a holistic\nself-training scheme that refines detection scores and coordinates through\nstep-wise entropy minimization and consistency-constraint regularization,\nensuring consistent predictions across stochastic augmentations of the same\nimage. Extensive experiments on PASCAL VOC and MS COCO demonstrate that HUWSOD\ncompetes with state-of-the-art WSOD methods, eliminating the need for offline\nproposals and additional data. The peak performance of HUWSOD approaches that\nof fully-supervised Faster R-CNN. Our findings also indicate that randomly\ninitialized boxes, although significantly different from well-designed offline\nobject proposals, are effective for WSOD training.",
        "chunk-id": 2,
        "chunk": "comprehensive self-training framework without needing external modules or\nadditional supervision. HUWSOD innovatively incorporates a self-supervised\nproposal generator and an autoencoder proposal generator with a multi-rate\nresampling pyramid to replace traditional object proposals, enabling end-to-end\nWSOD training and inference. Additionally, we implement a holistic",
        "authors": [
            "Liujuan Cao",
            "Jianghang Lin",
            "Zebo Hong",
            "Yunhang Shen",
            "Shaohui Lin",
            "Chao Chen",
            "Rongrong Ji"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:59:49+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19394v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19394v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000002,
        "doi": null,
        "title": "HUWSOD: Holistic Self-training for Unified Weakly Supervised Object Detection",
        "abstract": "Most WSOD methods rely on traditional object proposals to generate candidate\nregions and are confronted with unstable training, which easily gets stuck in a\npoor local optimum. In this paper, we introduce a unified, high-capacity weakly\nsupervised object detection (WSOD) network called HUWSOD, which utilizes a\ncomprehensive self-training framework without needing external modules or\nadditional supervision. HUWSOD innovatively incorporates a self-supervised\nproposal generator and an autoencoder proposal generator with a multi-rate\nresampling pyramid to replace traditional object proposals, enabling end-to-end\nWSOD training and inference. Additionally, we implement a holistic\nself-training scheme that refines detection scores and coordinates through\nstep-wise entropy minimization and consistency-constraint regularization,\nensuring consistent predictions across stochastic augmentations of the same\nimage. Extensive experiments on PASCAL VOC and MS COCO demonstrate that HUWSOD\ncompetes with state-of-the-art WSOD methods, eliminating the need for offline\nproposals and additional data. The peak performance of HUWSOD approaches that\nof fully-supervised Faster R-CNN. Our findings also indicate that randomly\ninitialized boxes, although significantly different from well-designed offline\nobject proposals, are effective for WSOD training.",
        "chunk-id": 3,
        "chunk": "self-training scheme that refines detection scores and coordinates through\nstep-wise entropy minimization and consistency-constraint regularization,\nensuring consistent predictions across stochastic augmentations of the same\nimage. Extensive experiments on PASCAL VOC and MS COCO demonstrate that HUWSOD\ncompetes with state-of-the-art WSOD methods, eliminating the need for offline",
        "authors": [
            "Liujuan Cao",
            "Jianghang Lin",
            "Zebo Hong",
            "Yunhang Shen",
            "Shaohui Lin",
            "Chao Chen",
            "Rongrong Ji"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:59:49+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19394v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19394v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000002,
        "doi": null,
        "title": "HUWSOD: Holistic Self-training for Unified Weakly Supervised Object Detection",
        "abstract": "Most WSOD methods rely on traditional object proposals to generate candidate\nregions and are confronted with unstable training, which easily gets stuck in a\npoor local optimum. In this paper, we introduce a unified, high-capacity weakly\nsupervised object detection (WSOD) network called HUWSOD, which utilizes a\ncomprehensive self-training framework without needing external modules or\nadditional supervision. HUWSOD innovatively incorporates a self-supervised\nproposal generator and an autoencoder proposal generator with a multi-rate\nresampling pyramid to replace traditional object proposals, enabling end-to-end\nWSOD training and inference. Additionally, we implement a holistic\nself-training scheme that refines detection scores and coordinates through\nstep-wise entropy minimization and consistency-constraint regularization,\nensuring consistent predictions across stochastic augmentations of the same\nimage. Extensive experiments on PASCAL VOC and MS COCO demonstrate that HUWSOD\ncompetes with state-of-the-art WSOD methods, eliminating the need for offline\nproposals and additional data. The peak performance of HUWSOD approaches that\nof fully-supervised Faster R-CNN. Our findings also indicate that randomly\ninitialized boxes, although significantly different from well-designed offline\nobject proposals, are effective for WSOD training.",
        "chunk-id": 4,
        "chunk": "proposals and additional data. The peak performance of HUWSOD approaches that\nof fully-supervised Faster R-CNN. Our findings also indicate that randomly\ninitialized boxes, although significantly different from well-designed offline\nobject proposals, are effective for WSOD training.",
        "authors": [
            "Liujuan Cao",
            "Jianghang Lin",
            "Zebo Hong",
            "Yunhang Shen",
            "Shaohui Lin",
            "Chao Chen",
            "Rongrong Ji"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:59:49+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19394v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19394v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000003,
        "doi": null,
        "title": "Looking 3D: Anomaly Detection with 2D-3D Alignment",
        "abstract": "Automatic anomaly detection based on visual cues holds practical significance\nin various domains, such as manufacturing and product quality assessment. This\npaper introduces a new conditional anomaly detection problem, which involves\nidentifying anomalies in a query image by comparing it to a reference shape. To\naddress this challenge, we have created a large dataset, BrokenChairs-180K,\nconsisting of around 180K images, with diverse anomalies, geometries, and\ntextures paired with 8,143 reference 3D shapes. To tackle this task, we have\nproposed a novel transformer-based approach that explicitly learns the\ncorrespondence between the query image and reference 3D shape via feature\nalignment and leverages a customized attention mechanism for anomaly detection.\nOur approach has been rigorously evaluated through comprehensive experiments,\nserving as a benchmark for future research in this domain.",
        "chunk-id": 1,
        "chunk": "Automatic anomaly detection based on visual cues holds practical significance\nin various domains, such as manufacturing and product quality assessment. This\npaper introduces a new conditional anomaly detection problem, which involves\nidentifying anomalies in a query image by comparing it to a reference shape. To",
        "authors": [
            "Ankan Bhunia",
            "Changjian Li",
            "Hakan Bilen"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:59:46+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19393v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19393v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000003,
        "doi": null,
        "title": "Looking 3D: Anomaly Detection with 2D-3D Alignment",
        "abstract": "Automatic anomaly detection based on visual cues holds practical significance\nin various domains, such as manufacturing and product quality assessment. This\npaper introduces a new conditional anomaly detection problem, which involves\nidentifying anomalies in a query image by comparing it to a reference shape. To\naddress this challenge, we have created a large dataset, BrokenChairs-180K,\nconsisting of around 180K images, with diverse anomalies, geometries, and\ntextures paired with 8,143 reference 3D shapes. To tackle this task, we have\nproposed a novel transformer-based approach that explicitly learns the\ncorrespondence between the query image and reference 3D shape via feature\nalignment and leverages a customized attention mechanism for anomaly detection.\nOur approach has been rigorously evaluated through comprehensive experiments,\nserving as a benchmark for future research in this domain.",
        "chunk-id": 2,
        "chunk": "address this challenge, we have created a large dataset, BrokenChairs-180K,\nconsisting of around 180K images, with diverse anomalies, geometries, and\ntextures paired with 8,143 reference 3D shapes. To tackle this task, we have\nproposed a novel transformer-based approach that explicitly learns the\ncorrespondence between the query image and reference 3D shape via feature",
        "authors": [
            "Ankan Bhunia",
            "Changjian Li",
            "Hakan Bilen"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:59:46+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19393v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19393v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000003,
        "doi": null,
        "title": "Looking 3D: Anomaly Detection with 2D-3D Alignment",
        "abstract": "Automatic anomaly detection based on visual cues holds practical significance\nin various domains, such as manufacturing and product quality assessment. This\npaper introduces a new conditional anomaly detection problem, which involves\nidentifying anomalies in a query image by comparing it to a reference shape. To\naddress this challenge, we have created a large dataset, BrokenChairs-180K,\nconsisting of around 180K images, with diverse anomalies, geometries, and\ntextures paired with 8,143 reference 3D shapes. To tackle this task, we have\nproposed a novel transformer-based approach that explicitly learns the\ncorrespondence between the query image and reference 3D shape via feature\nalignment and leverages a customized attention mechanism for anomaly detection.\nOur approach has been rigorously evaluated through comprehensive experiments,\nserving as a benchmark for future research in this domain.",
        "chunk-id": 3,
        "chunk": "alignment and leverages a customized attention mechanism for anomaly detection.\nOur approach has been rigorously evaluated through comprehensive experiments,\nserving as a benchmark for future research in this domain.",
        "authors": [
            "Ankan Bhunia",
            "Changjian Li",
            "Hakan Bilen"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:59:46+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19393v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19393v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000004,
        "doi": null,
        "title": "ReXTime: A Benchmark Suite for Reasoning-Across-Time in Videos",
        "abstract": "We introduce ReXTime, a benchmark designed to rigorously test AI models'\nability to perform temporal reasoning within video events. Specifically,\nReXTime focuses on reasoning across time, i.e. human-like understanding when\nthe question and its corresponding answer occur in different video segments.\nThis form of reasoning, requiring advanced understanding of cause-and-effect\nrelationships across video segments, poses significant challenges to even the\nfrontier multimodal large language models. To facilitate this evaluation, we\ndevelop an automated pipeline for generating temporal reasoning question-answer\npairs, significantly reducing the need for labor-intensive manual annotations.\nOur benchmark includes 921 carefully vetted validation samples and 2,143 test\nsamples, each manually curated for accuracy and relevance. Evaluation results\nshow that while frontier large language models outperform academic models, they\nstill lag behind human performance by a significant 14.3% accuracy gap.\nAdditionally, our pipeline creates a training dataset of 9,695 machine\ngenerated samples without manual effort, which empirical studies suggest can\nenhance the across-time reasoning via fine-tuning.",
        "chunk-id": 1,
        "chunk": "We introduce ReXTime, a benchmark designed to rigorously test AI models'\nability to perform temporal reasoning within video events. Specifically,\nReXTime focuses on reasoning across time, i.e. human-like understanding when\nthe question and its corresponding answer occur in different video segments.\nThis form of reasoning, requiring advanced understanding of cause-and-effect",
        "authors": [
            "Jr-Jen Chen",
            "Yu-Chien Liao",
            "Hsi-Che Lin",
            "Yu-Chu Yu",
            "Yen-Chun Chen",
            "Yu-Chiang Frank Wang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:59:45+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19392v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19392v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000004,
        "doi": null,
        "title": "ReXTime: A Benchmark Suite for Reasoning-Across-Time in Videos",
        "abstract": "We introduce ReXTime, a benchmark designed to rigorously test AI models'\nability to perform temporal reasoning within video events. Specifically,\nReXTime focuses on reasoning across time, i.e. human-like understanding when\nthe question and its corresponding answer occur in different video segments.\nThis form of reasoning, requiring advanced understanding of cause-and-effect\nrelationships across video segments, poses significant challenges to even the\nfrontier multimodal large language models. To facilitate this evaluation, we\ndevelop an automated pipeline for generating temporal reasoning question-answer\npairs, significantly reducing the need for labor-intensive manual annotations.\nOur benchmark includes 921 carefully vetted validation samples and 2,143 test\nsamples, each manually curated for accuracy and relevance. Evaluation results\nshow that while frontier large language models outperform academic models, they\nstill lag behind human performance by a significant 14.3% accuracy gap.\nAdditionally, our pipeline creates a training dataset of 9,695 machine\ngenerated samples without manual effort, which empirical studies suggest can\nenhance the across-time reasoning via fine-tuning.",
        "chunk-id": 2,
        "chunk": "relationships across video segments, poses significant challenges to even the\nfrontier multimodal large language models. To facilitate this evaluation, we\ndevelop an automated pipeline for generating temporal reasoning question-answer\npairs, significantly reducing the need for labor-intensive manual annotations.",
        "authors": [
            "Jr-Jen Chen",
            "Yu-Chien Liao",
            "Hsi-Che Lin",
            "Yu-Chu Yu",
            "Yen-Chun Chen",
            "Yu-Chiang Frank Wang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:59:45+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19392v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19392v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000004,
        "doi": null,
        "title": "ReXTime: A Benchmark Suite for Reasoning-Across-Time in Videos",
        "abstract": "We introduce ReXTime, a benchmark designed to rigorously test AI models'\nability to perform temporal reasoning within video events. Specifically,\nReXTime focuses on reasoning across time, i.e. human-like understanding when\nthe question and its corresponding answer occur in different video segments.\nThis form of reasoning, requiring advanced understanding of cause-and-effect\nrelationships across video segments, poses significant challenges to even the\nfrontier multimodal large language models. To facilitate this evaluation, we\ndevelop an automated pipeline for generating temporal reasoning question-answer\npairs, significantly reducing the need for labor-intensive manual annotations.\nOur benchmark includes 921 carefully vetted validation samples and 2,143 test\nsamples, each manually curated for accuracy and relevance. Evaluation results\nshow that while frontier large language models outperform academic models, they\nstill lag behind human performance by a significant 14.3% accuracy gap.\nAdditionally, our pipeline creates a training dataset of 9,695 machine\ngenerated samples without manual effort, which empirical studies suggest can\nenhance the across-time reasoning via fine-tuning.",
        "chunk-id": 3,
        "chunk": "Our benchmark includes 921 carefully vetted validation samples and 2,143 test\nsamples, each manually curated for accuracy and relevance. Evaluation results\nshow that while frontier large language models outperform academic models, they\nstill lag behind human performance by a significant 14.3% accuracy gap.\nAdditionally, our pipeline creates a training dataset of 9,695 machine",
        "authors": [
            "Jr-Jen Chen",
            "Yu-Chien Liao",
            "Hsi-Che Lin",
            "Yu-Chu Yu",
            "Yen-Chun Chen",
            "Yu-Chiang Frank Wang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:59:45+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19392v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19392v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000004,
        "doi": null,
        "title": "ReXTime: A Benchmark Suite for Reasoning-Across-Time in Videos",
        "abstract": "We introduce ReXTime, a benchmark designed to rigorously test AI models'\nability to perform temporal reasoning within video events. Specifically,\nReXTime focuses on reasoning across time, i.e. human-like understanding when\nthe question and its corresponding answer occur in different video segments.\nThis form of reasoning, requiring advanced understanding of cause-and-effect\nrelationships across video segments, poses significant challenges to even the\nfrontier multimodal large language models. To facilitate this evaluation, we\ndevelop an automated pipeline for generating temporal reasoning question-answer\npairs, significantly reducing the need for labor-intensive manual annotations.\nOur benchmark includes 921 carefully vetted validation samples and 2,143 test\nsamples, each manually curated for accuracy and relevance. Evaluation results\nshow that while frontier large language models outperform academic models, they\nstill lag behind human performance by a significant 14.3% accuracy gap.\nAdditionally, our pipeline creates a training dataset of 9,695 machine\ngenerated samples without manual effort, which empirical studies suggest can\nenhance the across-time reasoning via fine-tuning.",
        "chunk-id": 4,
        "chunk": "generated samples without manual effort, which empirical studies suggest can\nenhance the across-time reasoning via fine-tuning.",
        "authors": [
            "Jr-Jen Chen",
            "Yu-Chien Liao",
            "Hsi-Che Lin",
            "Yu-Chu Yu",
            "Yen-Chun Chen",
            "Yu-Chiang Frank Wang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:59:45+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19392v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19392v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000005,
        "doi": null,
        "title": "Fibottention: Inceptive Visual Representation Learning with Diverse Attention Across Heads",
        "abstract": "Visual perception tasks are predominantly solved by Vision Transformer (ViT)\narchitectures, which, despite their effectiveness, encounter a computational\nbottleneck due to the quadratic complexity of computing self-attention. This\ninefficiency is largely due to the self-attention heads capturing redundant\ntoken interactions, reflecting inherent redundancy within visual data. Many\nworks have aimed to reduce the computational complexity of self-attention in\nViTs, leading to the development of efficient and sparse transformer\narchitectures. In this paper, viewing through the efficiency lens, we realized\nthat introducing any sparse self-attention strategy in ViTs can keep the\ncomputational overhead low. However, these strategies are sub-optimal as they\noften fail to capture fine-grained visual details. This observation leads us to\npropose a general, efficient, sparse architecture, named Fibottention, for\napproximating self-attention with superlinear complexity that is built upon\nFibonacci sequences. The key strategies in Fibottention include: it excludes\nproximate tokens to reduce redundancy, employs structured sparsity by design to\ndecrease computational demands, and incorporates inception-like diversity\nacross attention heads. This diversity ensures the capture of complementary\ninformation through non-overlapping token interactions, optimizing both\nperformance and resource utilization in ViTs for visual representation\nlearning. We embed our Fibottention mechanism into multiple state-of-the-art\ntransformer architectures dedicated to visual tasks. Leveraging only 2-6% of\nthe elements in the self-attention heads, Fibottention in conjunction with ViT\nand its variants, consistently achieves significant performance boosts compared\nto standard ViTs in nine datasets across three domains $\\unicode{x2013}$ image\nclassification, video understanding, and robot learning tasks.",
        "chunk-id": 1,
        "chunk": "Visual perception tasks are predominantly solved by Vision Transformer (ViT)\narchitectures, which, despite their effectiveness, encounter a computational\nbottleneck due to the quadratic complexity of computing self-attention. This\ninefficiency is largely due to the self-attention heads capturing redundant\ntoken interactions, reflecting inherent redundancy within visual data. Many",
        "authors": [
            "Ali Khaleghi Rahimian",
            "Manish Kumar Govind",
            "Subhajit Maity",
            "Dominick Reilly",
            "Christian K\u00fcmmerle",
            "Srijan Das",
            "Aritra Dutta"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:59:40+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19391v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19391v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000005,
        "doi": null,
        "title": "Fibottention: Inceptive Visual Representation Learning with Diverse Attention Across Heads",
        "abstract": "Visual perception tasks are predominantly solved by Vision Transformer (ViT)\narchitectures, which, despite their effectiveness, encounter a computational\nbottleneck due to the quadratic complexity of computing self-attention. This\ninefficiency is largely due to the self-attention heads capturing redundant\ntoken interactions, reflecting inherent redundancy within visual data. Many\nworks have aimed to reduce the computational complexity of self-attention in\nViTs, leading to the development of efficient and sparse transformer\narchitectures. In this paper, viewing through the efficiency lens, we realized\nthat introducing any sparse self-attention strategy in ViTs can keep the\ncomputational overhead low. However, these strategies are sub-optimal as they\noften fail to capture fine-grained visual details. This observation leads us to\npropose a general, efficient, sparse architecture, named Fibottention, for\napproximating self-attention with superlinear complexity that is built upon\nFibonacci sequences. The key strategies in Fibottention include: it excludes\nproximate tokens to reduce redundancy, employs structured sparsity by design to\ndecrease computational demands, and incorporates inception-like diversity\nacross attention heads. This diversity ensures the capture of complementary\ninformation through non-overlapping token interactions, optimizing both\nperformance and resource utilization in ViTs for visual representation\nlearning. We embed our Fibottention mechanism into multiple state-of-the-art\ntransformer architectures dedicated to visual tasks. Leveraging only 2-6% of\nthe elements in the self-attention heads, Fibottention in conjunction with ViT\nand its variants, consistently achieves significant performance boosts compared\nto standard ViTs in nine datasets across three domains $\\unicode{x2013}$ image\nclassification, video understanding, and robot learning tasks.",
        "chunk-id": 2,
        "chunk": "works have aimed to reduce the computational complexity of self-attention in\nViTs, leading to the development of efficient and sparse transformer\narchitectures. In this paper, viewing through the efficiency lens, we realized\nthat introducing any sparse self-attention strategy in ViTs can keep the\ncomputational overhead low. However, these strategies are sub-optimal as they",
        "authors": [
            "Ali Khaleghi Rahimian",
            "Manish Kumar Govind",
            "Subhajit Maity",
            "Dominick Reilly",
            "Christian K\u00fcmmerle",
            "Srijan Das",
            "Aritra Dutta"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:59:40+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19391v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19391v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000005,
        "doi": null,
        "title": "Fibottention: Inceptive Visual Representation Learning with Diverse Attention Across Heads",
        "abstract": "Visual perception tasks are predominantly solved by Vision Transformer (ViT)\narchitectures, which, despite their effectiveness, encounter a computational\nbottleneck due to the quadratic complexity of computing self-attention. This\ninefficiency is largely due to the self-attention heads capturing redundant\ntoken interactions, reflecting inherent redundancy within visual data. Many\nworks have aimed to reduce the computational complexity of self-attention in\nViTs, leading to the development of efficient and sparse transformer\narchitectures. In this paper, viewing through the efficiency lens, we realized\nthat introducing any sparse self-attention strategy in ViTs can keep the\ncomputational overhead low. However, these strategies are sub-optimal as they\noften fail to capture fine-grained visual details. This observation leads us to\npropose a general, efficient, sparse architecture, named Fibottention, for\napproximating self-attention with superlinear complexity that is built upon\nFibonacci sequences. The key strategies in Fibottention include: it excludes\nproximate tokens to reduce redundancy, employs structured sparsity by design to\ndecrease computational demands, and incorporates inception-like diversity\nacross attention heads. This diversity ensures the capture of complementary\ninformation through non-overlapping token interactions, optimizing both\nperformance and resource utilization in ViTs for visual representation\nlearning. We embed our Fibottention mechanism into multiple state-of-the-art\ntransformer architectures dedicated to visual tasks. Leveraging only 2-6% of\nthe elements in the self-attention heads, Fibottention in conjunction with ViT\nand its variants, consistently achieves significant performance boosts compared\nto standard ViTs in nine datasets across three domains $\\unicode{x2013}$ image\nclassification, video understanding, and robot learning tasks.",
        "chunk-id": 3,
        "chunk": "often fail to capture fine-grained visual details. This observation leads us to\npropose a general, efficient, sparse architecture, named Fibottention, for\napproximating self-attention with superlinear complexity that is built upon\nFibonacci sequences. The key strategies in Fibottention include: it excludes",
        "authors": [
            "Ali Khaleghi Rahimian",
            "Manish Kumar Govind",
            "Subhajit Maity",
            "Dominick Reilly",
            "Christian K\u00fcmmerle",
            "Srijan Das",
            "Aritra Dutta"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:59:40+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19391v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19391v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000005,
        "doi": null,
        "title": "Fibottention: Inceptive Visual Representation Learning with Diverse Attention Across Heads",
        "abstract": "Visual perception tasks are predominantly solved by Vision Transformer (ViT)\narchitectures, which, despite their effectiveness, encounter a computational\nbottleneck due to the quadratic complexity of computing self-attention. This\ninefficiency is largely due to the self-attention heads capturing redundant\ntoken interactions, reflecting inherent redundancy within visual data. Many\nworks have aimed to reduce the computational complexity of self-attention in\nViTs, leading to the development of efficient and sparse transformer\narchitectures. In this paper, viewing through the efficiency lens, we realized\nthat introducing any sparse self-attention strategy in ViTs can keep the\ncomputational overhead low. However, these strategies are sub-optimal as they\noften fail to capture fine-grained visual details. This observation leads us to\npropose a general, efficient, sparse architecture, named Fibottention, for\napproximating self-attention with superlinear complexity that is built upon\nFibonacci sequences. The key strategies in Fibottention include: it excludes\nproximate tokens to reduce redundancy, employs structured sparsity by design to\ndecrease computational demands, and incorporates inception-like diversity\nacross attention heads. This diversity ensures the capture of complementary\ninformation through non-overlapping token interactions, optimizing both\nperformance and resource utilization in ViTs for visual representation\nlearning. We embed our Fibottention mechanism into multiple state-of-the-art\ntransformer architectures dedicated to visual tasks. Leveraging only 2-6% of\nthe elements in the self-attention heads, Fibottention in conjunction with ViT\nand its variants, consistently achieves significant performance boosts compared\nto standard ViTs in nine datasets across three domains $\\unicode{x2013}$ image\nclassification, video understanding, and robot learning tasks.",
        "chunk-id": 4,
        "chunk": "proximate tokens to reduce redundancy, employs structured sparsity by design to\ndecrease computational demands, and incorporates inception-like diversity\nacross attention heads. This diversity ensures the capture of complementary\ninformation through non-overlapping token interactions, optimizing both\nperformance and resource utilization in ViTs for visual representation",
        "authors": [
            "Ali Khaleghi Rahimian",
            "Manish Kumar Govind",
            "Subhajit Maity",
            "Dominick Reilly",
            "Christian K\u00fcmmerle",
            "Srijan Das",
            "Aritra Dutta"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:59:40+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19391v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19391v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000005,
        "doi": null,
        "title": "Fibottention: Inceptive Visual Representation Learning with Diverse Attention Across Heads",
        "abstract": "Visual perception tasks are predominantly solved by Vision Transformer (ViT)\narchitectures, which, despite their effectiveness, encounter a computational\nbottleneck due to the quadratic complexity of computing self-attention. This\ninefficiency is largely due to the self-attention heads capturing redundant\ntoken interactions, reflecting inherent redundancy within visual data. Many\nworks have aimed to reduce the computational complexity of self-attention in\nViTs, leading to the development of efficient and sparse transformer\narchitectures. In this paper, viewing through the efficiency lens, we realized\nthat introducing any sparse self-attention strategy in ViTs can keep the\ncomputational overhead low. However, these strategies are sub-optimal as they\noften fail to capture fine-grained visual details. This observation leads us to\npropose a general, efficient, sparse architecture, named Fibottention, for\napproximating self-attention with superlinear complexity that is built upon\nFibonacci sequences. The key strategies in Fibottention include: it excludes\nproximate tokens to reduce redundancy, employs structured sparsity by design to\ndecrease computational demands, and incorporates inception-like diversity\nacross attention heads. This diversity ensures the capture of complementary\ninformation through non-overlapping token interactions, optimizing both\nperformance and resource utilization in ViTs for visual representation\nlearning. We embed our Fibottention mechanism into multiple state-of-the-art\ntransformer architectures dedicated to visual tasks. Leveraging only 2-6% of\nthe elements in the self-attention heads, Fibottention in conjunction with ViT\nand its variants, consistently achieves significant performance boosts compared\nto standard ViTs in nine datasets across three domains $\\unicode{x2013}$ image\nclassification, video understanding, and robot learning tasks.",
        "chunk-id": 5,
        "chunk": "learning. We embed our Fibottention mechanism into multiple state-of-the-art\ntransformer architectures dedicated to visual tasks. Leveraging only 2-6% of\nthe elements in the self-attention heads, Fibottention in conjunction with ViT\nand its variants, consistently achieves significant performance boosts compared",
        "authors": [
            "Ali Khaleghi Rahimian",
            "Manish Kumar Govind",
            "Subhajit Maity",
            "Dominick Reilly",
            "Christian K\u00fcmmerle",
            "Srijan Das",
            "Aritra Dutta"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:59:40+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19391v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19391v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000005,
        "doi": null,
        "title": "Fibottention: Inceptive Visual Representation Learning with Diverse Attention Across Heads",
        "abstract": "Visual perception tasks are predominantly solved by Vision Transformer (ViT)\narchitectures, which, despite their effectiveness, encounter a computational\nbottleneck due to the quadratic complexity of computing self-attention. This\ninefficiency is largely due to the self-attention heads capturing redundant\ntoken interactions, reflecting inherent redundancy within visual data. Many\nworks have aimed to reduce the computational complexity of self-attention in\nViTs, leading to the development of efficient and sparse transformer\narchitectures. In this paper, viewing through the efficiency lens, we realized\nthat introducing any sparse self-attention strategy in ViTs can keep the\ncomputational overhead low. However, these strategies are sub-optimal as they\noften fail to capture fine-grained visual details. This observation leads us to\npropose a general, efficient, sparse architecture, named Fibottention, for\napproximating self-attention with superlinear complexity that is built upon\nFibonacci sequences. The key strategies in Fibottention include: it excludes\nproximate tokens to reduce redundancy, employs structured sparsity by design to\ndecrease computational demands, and incorporates inception-like diversity\nacross attention heads. This diversity ensures the capture of complementary\ninformation through non-overlapping token interactions, optimizing both\nperformance and resource utilization in ViTs for visual representation\nlearning. We embed our Fibottention mechanism into multiple state-of-the-art\ntransformer architectures dedicated to visual tasks. Leveraging only 2-6% of\nthe elements in the self-attention heads, Fibottention in conjunction with ViT\nand its variants, consistently achieves significant performance boosts compared\nto standard ViTs in nine datasets across three domains $\\unicode{x2013}$ image\nclassification, video understanding, and robot learning tasks.",
        "chunk-id": 6,
        "chunk": "to standard ViTs in nine datasets across three domains $\\unicode{x2013}$ image\nclassification, video understanding, and robot learning tasks.",
        "authors": [
            "Ali Khaleghi Rahimian",
            "Manish Kumar Govind",
            "Subhajit Maity",
            "Dominick Reilly",
            "Christian K\u00fcmmerle",
            "Srijan Das",
            "Aritra Dutta"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:59:40+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19391v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19391v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000006,
        "doi": null,
        "title": "SALVe: Semantic Alignment Verification for Floorplan Reconstruction from Sparse Panoramas",
        "abstract": "We propose a new system for automatic 2D floorplan reconstruction that is\nenabled by SALVe, our novel pairwise learned alignment verifier. The inputs to\nour system are sparsely located 360$^\\circ$ panoramas, whose semantic features\n(windows, doors, and openings) are inferred and used to hypothesize pairwise\nroom adjacency or overlap. SALVe initializes a pose graph, which is\nsubsequently optimized using GTSAM. Once the room poses are computed, room\nlayouts are inferred using HorizonNet, and the floorplan is constructed by\nstitching the most confident layout boundaries. We validate our system\nqualitatively and quantitatively as well as through ablation studies, showing\nthat it outperforms state-of-the-art SfM systems in completeness by over 200%,\nwithout sacrificing accuracy. Our results point to the significance of our\nwork: poses of 81% of panoramas are localized in the first 2 connected\ncomponents (CCs), and 89% in the first 3 CCs. Code and models are publicly\navailable at https://github.com/zillow/salve.",
        "chunk-id": 1,
        "chunk": "We propose a new system for automatic 2D floorplan reconstruction that is\nenabled by SALVe, our novel pairwise learned alignment verifier. The inputs to\nour system are sparsely located 360$^\\circ$ panoramas, whose semantic features\n(windows, doors, and openings) are inferred and used to hypothesize pairwise\nroom adjacency or overlap. SALVe initializes a pose graph, which is",
        "authors": [
            "John Lambert",
            "Yuguang Li",
            "Ivaylo Boyadzhiev",
            "Lambert Wixson",
            "Manjunath Narayana",
            "Will Hutchcroft",
            "James Hays",
            "Frank Dellaert",
            "Sing Bing Kang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:59:06+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19390v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19390v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000006,
        "doi": null,
        "title": "SALVe: Semantic Alignment Verification for Floorplan Reconstruction from Sparse Panoramas",
        "abstract": "We propose a new system for automatic 2D floorplan reconstruction that is\nenabled by SALVe, our novel pairwise learned alignment verifier. The inputs to\nour system are sparsely located 360$^\\circ$ panoramas, whose semantic features\n(windows, doors, and openings) are inferred and used to hypothesize pairwise\nroom adjacency or overlap. SALVe initializes a pose graph, which is\nsubsequently optimized using GTSAM. Once the room poses are computed, room\nlayouts are inferred using HorizonNet, and the floorplan is constructed by\nstitching the most confident layout boundaries. We validate our system\nqualitatively and quantitatively as well as through ablation studies, showing\nthat it outperforms state-of-the-art SfM systems in completeness by over 200%,\nwithout sacrificing accuracy. Our results point to the significance of our\nwork: poses of 81% of panoramas are localized in the first 2 connected\ncomponents (CCs), and 89% in the first 3 CCs. Code and models are publicly\navailable at https://github.com/zillow/salve.",
        "chunk-id": 2,
        "chunk": "subsequently optimized using GTSAM. Once the room poses are computed, room\nlayouts are inferred using HorizonNet, and the floorplan is constructed by\nstitching the most confident layout boundaries. We validate our system\nqualitatively and quantitatively as well as through ablation studies, showing\nthat it outperforms state-of-the-art SfM systems in completeness by over 200%,",
        "authors": [
            "John Lambert",
            "Yuguang Li",
            "Ivaylo Boyadzhiev",
            "Lambert Wixson",
            "Manjunath Narayana",
            "Will Hutchcroft",
            "James Hays",
            "Frank Dellaert",
            "Sing Bing Kang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:59:06+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19390v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19390v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000006,
        "doi": null,
        "title": "SALVe: Semantic Alignment Verification for Floorplan Reconstruction from Sparse Panoramas",
        "abstract": "We propose a new system for automatic 2D floorplan reconstruction that is\nenabled by SALVe, our novel pairwise learned alignment verifier. The inputs to\nour system are sparsely located 360$^\\circ$ panoramas, whose semantic features\n(windows, doors, and openings) are inferred and used to hypothesize pairwise\nroom adjacency or overlap. SALVe initializes a pose graph, which is\nsubsequently optimized using GTSAM. Once the room poses are computed, room\nlayouts are inferred using HorizonNet, and the floorplan is constructed by\nstitching the most confident layout boundaries. We validate our system\nqualitatively and quantitatively as well as through ablation studies, showing\nthat it outperforms state-of-the-art SfM systems in completeness by over 200%,\nwithout sacrificing accuracy. Our results point to the significance of our\nwork: poses of 81% of panoramas are localized in the first 2 connected\ncomponents (CCs), and 89% in the first 3 CCs. Code and models are publicly\navailable at https://github.com/zillow/salve.",
        "chunk-id": 3,
        "chunk": "without sacrificing accuracy. Our results point to the significance of our\nwork: poses of 81% of panoramas are localized in the first 2 connected\ncomponents (CCs), and 89% in the first 3 CCs. Code and models are publicly\navailable at https://github.com/zillow/salve.",
        "authors": [
            "John Lambert",
            "Yuguang Li",
            "Ivaylo Boyadzhiev",
            "Lambert Wixson",
            "Manjunath Narayana",
            "Will Hutchcroft",
            "James Hays",
            "Frank Dellaert",
            "Sing Bing Kang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:59:06+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19390v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19390v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000007,
        "doi": null,
        "title": "OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and Understanding",
        "abstract": "Current universal segmentation methods demonstrate strong capabilities in\npixel-level image and video understanding. However, they lack reasoning\nabilities and cannot be controlled via text instructions. In contrast, large\nvision-language multimodal models exhibit powerful vision-based conversation\nand reasoning capabilities but lack pixel-level understanding and have\ndifficulty accepting visual prompts for flexible user interaction. This paper\nproposes OMG-LLaVA, a new and elegant framework combining powerful pixel-level\nvision understanding with reasoning abilities. It can accept various visual and\ntext prompts for flexible user interaction. Specifically, we use a universal\nsegmentation method as the visual encoder, integrating image information,\nperception priors, and visual prompts into visual tokens provided to the LLM.\nThe LLM is responsible for understanding the user's text instructions and\nproviding text responses and pixel-level segmentation results based on the\nvisual information. We propose perception prior embedding to better integrate\nperception priors with image features. OMG-LLaVA achieves image-level,\nobject-level, and pixel-level reasoning and understanding in a single model,\nmatching or surpassing the performance of specialized methods on multiple\nbenchmarks. Rather than using LLM to connect each specialist, our work aims at\nend-to-end training on one encoder, one decoder, and one LLM. The code and\nmodel have been released for further research.",
        "chunk-id": 1,
        "chunk": "Current universal segmentation methods demonstrate strong capabilities in\npixel-level image and video understanding. However, they lack reasoning\nabilities and cannot be controlled via text instructions. In contrast, large\nvision-language multimodal models exhibit powerful vision-based conversation\nand reasoning capabilities but lack pixel-level understanding and have",
        "authors": [
            "Tao Zhang",
            "Xiangtai Li",
            "Hao Fei",
            "Haobo Yuan",
            "Shengqiong Wu",
            "Shunping Ji",
            "Chen Change Loy",
            "Shuicheng Yan"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:59:01+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19389v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19389v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000007,
        "doi": null,
        "title": "OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and Understanding",
        "abstract": "Current universal segmentation methods demonstrate strong capabilities in\npixel-level image and video understanding. However, they lack reasoning\nabilities and cannot be controlled via text instructions. In contrast, large\nvision-language multimodal models exhibit powerful vision-based conversation\nand reasoning capabilities but lack pixel-level understanding and have\ndifficulty accepting visual prompts for flexible user interaction. This paper\nproposes OMG-LLaVA, a new and elegant framework combining powerful pixel-level\nvision understanding with reasoning abilities. It can accept various visual and\ntext prompts for flexible user interaction. Specifically, we use a universal\nsegmentation method as the visual encoder, integrating image information,\nperception priors, and visual prompts into visual tokens provided to the LLM.\nThe LLM is responsible for understanding the user's text instructions and\nproviding text responses and pixel-level segmentation results based on the\nvisual information. We propose perception prior embedding to better integrate\nperception priors with image features. OMG-LLaVA achieves image-level,\nobject-level, and pixel-level reasoning and understanding in a single model,\nmatching or surpassing the performance of specialized methods on multiple\nbenchmarks. Rather than using LLM to connect each specialist, our work aims at\nend-to-end training on one encoder, one decoder, and one LLM. The code and\nmodel have been released for further research.",
        "chunk-id": 2,
        "chunk": "difficulty accepting visual prompts for flexible user interaction. This paper\nproposes OMG-LLaVA, a new and elegant framework combining powerful pixel-level\nvision understanding with reasoning abilities. It can accept various visual and\ntext prompts for flexible user interaction. Specifically, we use a universal",
        "authors": [
            "Tao Zhang",
            "Xiangtai Li",
            "Hao Fei",
            "Haobo Yuan",
            "Shengqiong Wu",
            "Shunping Ji",
            "Chen Change Loy",
            "Shuicheng Yan"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:59:01+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19389v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19389v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000007,
        "doi": null,
        "title": "OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and Understanding",
        "abstract": "Current universal segmentation methods demonstrate strong capabilities in\npixel-level image and video understanding. However, they lack reasoning\nabilities and cannot be controlled via text instructions. In contrast, large\nvision-language multimodal models exhibit powerful vision-based conversation\nand reasoning capabilities but lack pixel-level understanding and have\ndifficulty accepting visual prompts for flexible user interaction. This paper\nproposes OMG-LLaVA, a new and elegant framework combining powerful pixel-level\nvision understanding with reasoning abilities. It can accept various visual and\ntext prompts for flexible user interaction. Specifically, we use a universal\nsegmentation method as the visual encoder, integrating image information,\nperception priors, and visual prompts into visual tokens provided to the LLM.\nThe LLM is responsible for understanding the user's text instructions and\nproviding text responses and pixel-level segmentation results based on the\nvisual information. We propose perception prior embedding to better integrate\nperception priors with image features. OMG-LLaVA achieves image-level,\nobject-level, and pixel-level reasoning and understanding in a single model,\nmatching or surpassing the performance of specialized methods on multiple\nbenchmarks. Rather than using LLM to connect each specialist, our work aims at\nend-to-end training on one encoder, one decoder, and one LLM. The code and\nmodel have been released for further research.",
        "chunk-id": 3,
        "chunk": "segmentation method as the visual encoder, integrating image information,\nperception priors, and visual prompts into visual tokens provided to the LLM.\nThe LLM is responsible for understanding the user's text instructions and\nproviding text responses and pixel-level segmentation results based on the\nvisual information. We propose perception prior embedding to better integrate",
        "authors": [
            "Tao Zhang",
            "Xiangtai Li",
            "Hao Fei",
            "Haobo Yuan",
            "Shengqiong Wu",
            "Shunping Ji",
            "Chen Change Loy",
            "Shuicheng Yan"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:59:01+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19389v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19389v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000007,
        "doi": null,
        "title": "OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and Understanding",
        "abstract": "Current universal segmentation methods demonstrate strong capabilities in\npixel-level image and video understanding. However, they lack reasoning\nabilities and cannot be controlled via text instructions. In contrast, large\nvision-language multimodal models exhibit powerful vision-based conversation\nand reasoning capabilities but lack pixel-level understanding and have\ndifficulty accepting visual prompts for flexible user interaction. This paper\nproposes OMG-LLaVA, a new and elegant framework combining powerful pixel-level\nvision understanding with reasoning abilities. It can accept various visual and\ntext prompts for flexible user interaction. Specifically, we use a universal\nsegmentation method as the visual encoder, integrating image information,\nperception priors, and visual prompts into visual tokens provided to the LLM.\nThe LLM is responsible for understanding the user's text instructions and\nproviding text responses and pixel-level segmentation results based on the\nvisual information. We propose perception prior embedding to better integrate\nperception priors with image features. OMG-LLaVA achieves image-level,\nobject-level, and pixel-level reasoning and understanding in a single model,\nmatching or surpassing the performance of specialized methods on multiple\nbenchmarks. Rather than using LLM to connect each specialist, our work aims at\nend-to-end training on one encoder, one decoder, and one LLM. The code and\nmodel have been released for further research.",
        "chunk-id": 4,
        "chunk": "perception priors with image features. OMG-LLaVA achieves image-level,\nobject-level, and pixel-level reasoning and understanding in a single model,\nmatching or surpassing the performance of specialized methods on multiple\nbenchmarks. Rather than using LLM to connect each specialist, our work aims at\nend-to-end training on one encoder, one decoder, and one LLM. The code and",
        "authors": [
            "Tao Zhang",
            "Xiangtai Li",
            "Hao Fei",
            "Haobo Yuan",
            "Shengqiong Wu",
            "Shunping Ji",
            "Chen Change Loy",
            "Shuicheng Yan"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:59:01+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19389v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19389v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000007,
        "doi": null,
        "title": "OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and Understanding",
        "abstract": "Current universal segmentation methods demonstrate strong capabilities in\npixel-level image and video understanding. However, they lack reasoning\nabilities and cannot be controlled via text instructions. In contrast, large\nvision-language multimodal models exhibit powerful vision-based conversation\nand reasoning capabilities but lack pixel-level understanding and have\ndifficulty accepting visual prompts for flexible user interaction. This paper\nproposes OMG-LLaVA, a new and elegant framework combining powerful pixel-level\nvision understanding with reasoning abilities. It can accept various visual and\ntext prompts for flexible user interaction. Specifically, we use a universal\nsegmentation method as the visual encoder, integrating image information,\nperception priors, and visual prompts into visual tokens provided to the LLM.\nThe LLM is responsible for understanding the user's text instructions and\nproviding text responses and pixel-level segmentation results based on the\nvisual information. We propose perception prior embedding to better integrate\nperception priors with image features. OMG-LLaVA achieves image-level,\nobject-level, and pixel-level reasoning and understanding in a single model,\nmatching or surpassing the performance of specialized methods on multiple\nbenchmarks. Rather than using LLM to connect each specialist, our work aims at\nend-to-end training on one encoder, one decoder, and one LLM. The code and\nmodel have been released for further research.",
        "chunk-id": 5,
        "chunk": "model have been released for further research.",
        "authors": [
            "Tao Zhang",
            "Xiangtai Li",
            "Hao Fei",
            "Haobo Yuan",
            "Shengqiong Wu",
            "Shunping Ji",
            "Chen Change Loy",
            "Shuicheng Yan"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:59:01+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19389v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19389v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000008,
        "doi": null,
        "title": "Taming Data and Transformers for Audio Generation",
        "abstract": "Generating ambient sounds and effects is a challenging problem due to data\nscarcity and often insufficient caption quality, making it difficult to employ\nlarge-scale generative models for the task. In this work, we tackle the problem\nby introducing two new models. First, we propose AutoCap, a high-quality and\nefficient automatic audio captioning model. We show that by leveraging metadata\navailable with the audio modality, we can substantially improve the quality of\ncaptions. AutoCap reaches CIDEr score of 83.2, marking a 3.2% improvement from\nthe best available captioning model at four times faster inference speed. We\nthen use AutoCap to caption clips from existing datasets, obtaining 761,000\naudio clips with high-quality captions, forming the largest available\naudio-text dataset. Second, we propose GenAu, a scalable transformer-based\naudio generation architecture that we scale up to 1.25B parameters and train\nwith our new dataset. When compared to state-of-the-art audio generators, GenAu\nobtains significant improvements of 15.7% in FAD score, 22.7% in IS, and 13.5%\nin CLAP score, indicating significantly improved quality of generated audio\ncompared to previous works. This shows that the quality of data is often as\nimportant as its quantity. Besides, since AutoCap is fully automatic, new audio\nsamples can be added to the training dataset, unlocking the training of even\nlarger generative models for audio synthesis.",
        "chunk-id": 1,
        "chunk": "Generating ambient sounds and effects is a challenging problem due to data\nscarcity and often insufficient caption quality, making it difficult to employ\nlarge-scale generative models for the task. In this work, we tackle the problem\nby introducing two new models. First, we propose AutoCap, a high-quality and",
        "authors": [
            "Moayed Haji-Ali",
            "Willi Menapace",
            "Aliaksandr Siarohin",
            "Guha Balakrishnan",
            "Sergey Tulyakov",
            "Vicente Ordonez"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:58:54+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19388v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19388v1",
        "categories": [
            "Sound",
            "Computation and Language",
            "Computer Vision and Pattern Recognition",
            "Multimedia",
            "Audio and Speech Processing"
        ]
    },
    {
        "id": 30000008,
        "doi": null,
        "title": "Taming Data and Transformers for Audio Generation",
        "abstract": "Generating ambient sounds and effects is a challenging problem due to data\nscarcity and often insufficient caption quality, making it difficult to employ\nlarge-scale generative models for the task. In this work, we tackle the problem\nby introducing two new models. First, we propose AutoCap, a high-quality and\nefficient automatic audio captioning model. We show that by leveraging metadata\navailable with the audio modality, we can substantially improve the quality of\ncaptions. AutoCap reaches CIDEr score of 83.2, marking a 3.2% improvement from\nthe best available captioning model at four times faster inference speed. We\nthen use AutoCap to caption clips from existing datasets, obtaining 761,000\naudio clips with high-quality captions, forming the largest available\naudio-text dataset. Second, we propose GenAu, a scalable transformer-based\naudio generation architecture that we scale up to 1.25B parameters and train\nwith our new dataset. When compared to state-of-the-art audio generators, GenAu\nobtains significant improvements of 15.7% in FAD score, 22.7% in IS, and 13.5%\nin CLAP score, indicating significantly improved quality of generated audio\ncompared to previous works. This shows that the quality of data is often as\nimportant as its quantity. Besides, since AutoCap is fully automatic, new audio\nsamples can be added to the training dataset, unlocking the training of even\nlarger generative models for audio synthesis.",
        "chunk-id": 2,
        "chunk": "efficient automatic audio captioning model. We show that by leveraging metadata\navailable with the audio modality, we can substantially improve the quality of\ncaptions. AutoCap reaches CIDEr score of 83.2, marking a 3.2% improvement from\nthe best available captioning model at four times faster inference speed. We",
        "authors": [
            "Moayed Haji-Ali",
            "Willi Menapace",
            "Aliaksandr Siarohin",
            "Guha Balakrishnan",
            "Sergey Tulyakov",
            "Vicente Ordonez"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:58:54+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19388v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19388v1",
        "categories": [
            "Sound",
            "Computation and Language",
            "Computer Vision and Pattern Recognition",
            "Multimedia",
            "Audio and Speech Processing"
        ]
    },
    {
        "id": 30000008,
        "doi": null,
        "title": "Taming Data and Transformers for Audio Generation",
        "abstract": "Generating ambient sounds and effects is a challenging problem due to data\nscarcity and often insufficient caption quality, making it difficult to employ\nlarge-scale generative models for the task. In this work, we tackle the problem\nby introducing two new models. First, we propose AutoCap, a high-quality and\nefficient automatic audio captioning model. We show that by leveraging metadata\navailable with the audio modality, we can substantially improve the quality of\ncaptions. AutoCap reaches CIDEr score of 83.2, marking a 3.2% improvement from\nthe best available captioning model at four times faster inference speed. We\nthen use AutoCap to caption clips from existing datasets, obtaining 761,000\naudio clips with high-quality captions, forming the largest available\naudio-text dataset. Second, we propose GenAu, a scalable transformer-based\naudio generation architecture that we scale up to 1.25B parameters and train\nwith our new dataset. When compared to state-of-the-art audio generators, GenAu\nobtains significant improvements of 15.7% in FAD score, 22.7% in IS, and 13.5%\nin CLAP score, indicating significantly improved quality of generated audio\ncompared to previous works. This shows that the quality of data is often as\nimportant as its quantity. Besides, since AutoCap is fully automatic, new audio\nsamples can be added to the training dataset, unlocking the training of even\nlarger generative models for audio synthesis.",
        "chunk-id": 3,
        "chunk": "then use AutoCap to caption clips from existing datasets, obtaining 761,000\naudio clips with high-quality captions, forming the largest available\naudio-text dataset. Second, we propose GenAu, a scalable transformer-based\naudio generation architecture that we scale up to 1.25B parameters and train\nwith our new dataset. When compared to state-of-the-art audio generators, GenAu",
        "authors": [
            "Moayed Haji-Ali",
            "Willi Menapace",
            "Aliaksandr Siarohin",
            "Guha Balakrishnan",
            "Sergey Tulyakov",
            "Vicente Ordonez"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:58:54+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19388v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19388v1",
        "categories": [
            "Sound",
            "Computation and Language",
            "Computer Vision and Pattern Recognition",
            "Multimedia",
            "Audio and Speech Processing"
        ]
    },
    {
        "id": 30000008,
        "doi": null,
        "title": "Taming Data and Transformers for Audio Generation",
        "abstract": "Generating ambient sounds and effects is a challenging problem due to data\nscarcity and often insufficient caption quality, making it difficult to employ\nlarge-scale generative models for the task. In this work, we tackle the problem\nby introducing two new models. First, we propose AutoCap, a high-quality and\nefficient automatic audio captioning model. We show that by leveraging metadata\navailable with the audio modality, we can substantially improve the quality of\ncaptions. AutoCap reaches CIDEr score of 83.2, marking a 3.2% improvement from\nthe best available captioning model at four times faster inference speed. We\nthen use AutoCap to caption clips from existing datasets, obtaining 761,000\naudio clips with high-quality captions, forming the largest available\naudio-text dataset. Second, we propose GenAu, a scalable transformer-based\naudio generation architecture that we scale up to 1.25B parameters and train\nwith our new dataset. When compared to state-of-the-art audio generators, GenAu\nobtains significant improvements of 15.7% in FAD score, 22.7% in IS, and 13.5%\nin CLAP score, indicating significantly improved quality of generated audio\ncompared to previous works. This shows that the quality of data is often as\nimportant as its quantity. Besides, since AutoCap is fully automatic, new audio\nsamples can be added to the training dataset, unlocking the training of even\nlarger generative models for audio synthesis.",
        "chunk-id": 4,
        "chunk": "obtains significant improvements of 15.7% in FAD score, 22.7% in IS, and 13.5%\nin CLAP score, indicating significantly improved quality of generated audio\ncompared to previous works. This shows that the quality of data is often as\nimportant as its quantity. Besides, since AutoCap is fully automatic, new audio",
        "authors": [
            "Moayed Haji-Ali",
            "Willi Menapace",
            "Aliaksandr Siarohin",
            "Guha Balakrishnan",
            "Sergey Tulyakov",
            "Vicente Ordonez"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:58:54+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19388v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19388v1",
        "categories": [
            "Sound",
            "Computation and Language",
            "Computer Vision and Pattern Recognition",
            "Multimedia",
            "Audio and Speech Processing"
        ]
    },
    {
        "id": 30000008,
        "doi": null,
        "title": "Taming Data and Transformers for Audio Generation",
        "abstract": "Generating ambient sounds and effects is a challenging problem due to data\nscarcity and often insufficient caption quality, making it difficult to employ\nlarge-scale generative models for the task. In this work, we tackle the problem\nby introducing two new models. First, we propose AutoCap, a high-quality and\nefficient automatic audio captioning model. We show that by leveraging metadata\navailable with the audio modality, we can substantially improve the quality of\ncaptions. AutoCap reaches CIDEr score of 83.2, marking a 3.2% improvement from\nthe best available captioning model at four times faster inference speed. We\nthen use AutoCap to caption clips from existing datasets, obtaining 761,000\naudio clips with high-quality captions, forming the largest available\naudio-text dataset. Second, we propose GenAu, a scalable transformer-based\naudio generation architecture that we scale up to 1.25B parameters and train\nwith our new dataset. When compared to state-of-the-art audio generators, GenAu\nobtains significant improvements of 15.7% in FAD score, 22.7% in IS, and 13.5%\nin CLAP score, indicating significantly improved quality of generated audio\ncompared to previous works. This shows that the quality of data is often as\nimportant as its quantity. Besides, since AutoCap is fully automatic, new audio\nsamples can be added to the training dataset, unlocking the training of even\nlarger generative models for audio synthesis.",
        "chunk-id": 5,
        "chunk": "samples can be added to the training dataset, unlocking the training of even\nlarger generative models for audio synthesis.",
        "authors": [
            "Moayed Haji-Ali",
            "Willi Menapace",
            "Aliaksandr Siarohin",
            "Guha Balakrishnan",
            "Sergey Tulyakov",
            "Vicente Ordonez"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:58:54+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19388v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19388v1",
        "categories": [
            "Sound",
            "Computation and Language",
            "Computer Vision and Pattern Recognition",
            "Multimedia",
            "Audio and Speech Processing"
        ]
    },
    {
        "id": 30000009,
        "doi": null,
        "title": "The Remarkable Robustness of LLMs: Stages of Inference?",
        "abstract": "We demonstrate and investigate the remarkable robustness of Large Language\nModels by deleting and swapping adjacent layers. We find that deleting and\nswapping interventions retain 72-95\\% of the original model's prediction\naccuracy without fine-tuning, whereas models with more layers exhibit more\nrobustness. Based on the results of the layer-wise intervention and further\nexperiments, we hypothesize the existence of four universal stages of inference\nacross eight different models: detokenization, feature engineering, prediction\nensembling, and residual sharpening. The first stage integrates local\ninformation, lifting raw token representations into higher-level contextual\nrepresentations. Next is the iterative refinement of task and entity-specific\nfeatures. Then, the second half of the model begins with a phase transition,\nwhere hidden representations align more with the vocabulary space due to\nspecialized model components. Finally, the last layer sharpens the following\ntoken distribution by eliminating obsolete features that add noise to the\nprediction.",
        "chunk-id": 1,
        "chunk": "We demonstrate and investigate the remarkable robustness of Large Language\nModels by deleting and swapping adjacent layers. We find that deleting and\nswapping interventions retain 72-95\\% of the original model's prediction\naccuracy without fine-tuning, whereas models with more layers exhibit more\nrobustness. Based on the results of the layer-wise intervention and further",
        "authors": [
            "Vedang Lad",
            "Wes Gurnee",
            "Max Tegmark"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:57:03+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19384v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19384v1",
        "categories": [
            "Machine Learning",
            "Artificial Intelligence",
            "Computation and Language"
        ]
    },
    {
        "id": 30000009,
        "doi": null,
        "title": "The Remarkable Robustness of LLMs: Stages of Inference?",
        "abstract": "We demonstrate and investigate the remarkable robustness of Large Language\nModels by deleting and swapping adjacent layers. We find that deleting and\nswapping interventions retain 72-95\\% of the original model's prediction\naccuracy without fine-tuning, whereas models with more layers exhibit more\nrobustness. Based on the results of the layer-wise intervention and further\nexperiments, we hypothesize the existence of four universal stages of inference\nacross eight different models: detokenization, feature engineering, prediction\nensembling, and residual sharpening. The first stage integrates local\ninformation, lifting raw token representations into higher-level contextual\nrepresentations. Next is the iterative refinement of task and entity-specific\nfeatures. Then, the second half of the model begins with a phase transition,\nwhere hidden representations align more with the vocabulary space due to\nspecialized model components. Finally, the last layer sharpens the following\ntoken distribution by eliminating obsolete features that add noise to the\nprediction.",
        "chunk-id": 2,
        "chunk": "experiments, we hypothesize the existence of four universal stages of inference\nacross eight different models: detokenization, feature engineering, prediction\nensembling, and residual sharpening. The first stage integrates local\ninformation, lifting raw token representations into higher-level contextual\nrepresentations. Next is the iterative refinement of task and entity-specific",
        "authors": [
            "Vedang Lad",
            "Wes Gurnee",
            "Max Tegmark"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:57:03+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19384v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19384v1",
        "categories": [
            "Machine Learning",
            "Artificial Intelligence",
            "Computation and Language"
        ]
    },
    {
        "id": 30000009,
        "doi": null,
        "title": "The Remarkable Robustness of LLMs: Stages of Inference?",
        "abstract": "We demonstrate and investigate the remarkable robustness of Large Language\nModels by deleting and swapping adjacent layers. We find that deleting and\nswapping interventions retain 72-95\\% of the original model's prediction\naccuracy without fine-tuning, whereas models with more layers exhibit more\nrobustness. Based on the results of the layer-wise intervention and further\nexperiments, we hypothesize the existence of four universal stages of inference\nacross eight different models: detokenization, feature engineering, prediction\nensembling, and residual sharpening. The first stage integrates local\ninformation, lifting raw token representations into higher-level contextual\nrepresentations. Next is the iterative refinement of task and entity-specific\nfeatures. Then, the second half of the model begins with a phase transition,\nwhere hidden representations align more with the vocabulary space due to\nspecialized model components. Finally, the last layer sharpens the following\ntoken distribution by eliminating obsolete features that add noise to the\nprediction.",
        "chunk-id": 3,
        "chunk": "features. Then, the second half of the model begins with a phase transition,\nwhere hidden representations align more with the vocabulary space due to\nspecialized model components. Finally, the last layer sharpens the following\ntoken distribution by eliminating obsolete features that add noise to the\nprediction.",
        "authors": [
            "Vedang Lad",
            "Wes Gurnee",
            "Max Tegmark"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:57:03+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19384v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19384v1",
        "categories": [
            "Machine Learning",
            "Artificial Intelligence",
            "Computation and Language"
        ]
    },
    {
        "id": 30000010,
        "doi": null,
        "title": "A Machine Learning Method for Monte Carlo Calculations of Radiative Processes",
        "abstract": "Radiative processes such as synchrotron radiation and Compton scattering play\nan important role in astrophysics. Radiative processes are fundamentally\nstochastic in nature, and the best tools currently used for resolving these\nprocesses computationally are Monte Carlo (MC) methods. These methods typically\ndraw a large number of samples from a complex distribution such as the\ndifferential cross section for electron-photon scattering, and then use these\nsamples to compute the radiation properties such as angular distribution,\nspectrum, and polarization. In this work we propose a machine learning (ML)\ntechnique for efficient sampling from arbitrary known probability distributions\nthat can be used to accelerate Monte Carlo calculation of radiative processes\nin astrophysical scenarios. In particular, we apply our technique to inverse\nCompton radiation and find that our ML method can be up to an order of\nmagnitude faster than traditional methods currently in use.",
        "chunk-id": 1,
        "chunk": "Radiative processes such as synchrotron radiation and Compton scattering play\nan important role in astrophysics. Radiative processes are fundamentally\nstochastic in nature, and the best tools currently used for resolving these\nprocesses computationally are Monte Carlo (MC) methods. These methods typically\ndraw a large number of samples from a complex distribution such as the",
        "authors": [
            "William Charles",
            "Alexander Y. Chen"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:57:03+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19385v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19385v1",
        "categories": [
            "High Energy Astrophysical Phenomena"
        ]
    },
    {
        "id": 30000010,
        "doi": null,
        "title": "A Machine Learning Method for Monte Carlo Calculations of Radiative Processes",
        "abstract": "Radiative processes such as synchrotron radiation and Compton scattering play\nan important role in astrophysics. Radiative processes are fundamentally\nstochastic in nature, and the best tools currently used for resolving these\nprocesses computationally are Monte Carlo (MC) methods. These methods typically\ndraw a large number of samples from a complex distribution such as the\ndifferential cross section for electron-photon scattering, and then use these\nsamples to compute the radiation properties such as angular distribution,\nspectrum, and polarization. In this work we propose a machine learning (ML)\ntechnique for efficient sampling from arbitrary known probability distributions\nthat can be used to accelerate Monte Carlo calculation of radiative processes\nin astrophysical scenarios. In particular, we apply our technique to inverse\nCompton radiation and find that our ML method can be up to an order of\nmagnitude faster than traditional methods currently in use.",
        "chunk-id": 2,
        "chunk": "differential cross section for electron-photon scattering, and then use these\nsamples to compute the radiation properties such as angular distribution,\nspectrum, and polarization. In this work we propose a machine learning (ML)\ntechnique for efficient sampling from arbitrary known probability distributions",
        "authors": [
            "William Charles",
            "Alexander Y. Chen"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:57:03+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19385v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19385v1",
        "categories": [
            "High Energy Astrophysical Phenomena"
        ]
    },
    {
        "id": 30000010,
        "doi": null,
        "title": "A Machine Learning Method for Monte Carlo Calculations of Radiative Processes",
        "abstract": "Radiative processes such as synchrotron radiation and Compton scattering play\nan important role in astrophysics. Radiative processes are fundamentally\nstochastic in nature, and the best tools currently used for resolving these\nprocesses computationally are Monte Carlo (MC) methods. These methods typically\ndraw a large number of samples from a complex distribution such as the\ndifferential cross section for electron-photon scattering, and then use these\nsamples to compute the radiation properties such as angular distribution,\nspectrum, and polarization. In this work we propose a machine learning (ML)\ntechnique for efficient sampling from arbitrary known probability distributions\nthat can be used to accelerate Monte Carlo calculation of radiative processes\nin astrophysical scenarios. In particular, we apply our technique to inverse\nCompton radiation and find that our ML method can be up to an order of\nmagnitude faster than traditional methods currently in use.",
        "chunk-id": 3,
        "chunk": "that can be used to accelerate Monte Carlo calculation of radiative processes\nin astrophysical scenarios. In particular, we apply our technique to inverse\nCompton radiation and find that our ML method can be up to an order of\nmagnitude faster than traditional methods currently in use.",
        "authors": [
            "William Charles",
            "Alexander Y. Chen"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:57:03+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19385v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19385v1",
        "categories": [
            "High Energy Astrophysical Phenomena"
        ]
    },
    {
        "id": 30000011,
        "doi": null,
        "title": "Characterizing Contextuality via Rank Separation with Applications to Cloning",
        "abstract": "Quantum contextuality is a key nonclassical feature essential for\nunderstanding advantages in quantum computation and communication. We introduce\na new framework to study contextuality based solely on information processing\nstatistics. This simple and intuitive perspective leads to a powerful criterion\ndenoted as rank separation for identifying contextuality in various quantum\nscenarios. We showcase the power of this technique through several\napplications, including a new derivation of Hardy's quantum excess-baggage\ntheorem, and a simplified proof of contextuality for minimum error quantum\nstate discrimination. Finally, we show as a prominent example that quantum\ncontextuality provides the resource in optimal phase-covariant and universal\ncloning schemes, hence establishing it as a fundamental source of\nnonclassicality in all known optimal quantum cloning scenarios.",
        "chunk-id": 1,
        "chunk": "Quantum contextuality is a key nonclassical feature essential for\nunderstanding advantages in quantum computation and communication. We introduce\na new framework to study contextuality based solely on information processing\nstatistics. This simple and intuitive perspective leads to a powerful criterion\ndenoted as rank separation for identifying contextuality in various quantum",
        "authors": [
            "Farid Shahandeh",
            "Theodoros Yianni",
            "Mina Doosti"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:56:04+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19382v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19382v1",
        "categories": [
            "Quantum Physics"
        ]
    },
    {
        "id": 30000011,
        "doi": null,
        "title": "Characterizing Contextuality via Rank Separation with Applications to Cloning",
        "abstract": "Quantum contextuality is a key nonclassical feature essential for\nunderstanding advantages in quantum computation and communication. We introduce\na new framework to study contextuality based solely on information processing\nstatistics. This simple and intuitive perspective leads to a powerful criterion\ndenoted as rank separation for identifying contextuality in various quantum\nscenarios. We showcase the power of this technique through several\napplications, including a new derivation of Hardy's quantum excess-baggage\ntheorem, and a simplified proof of contextuality for minimum error quantum\nstate discrimination. Finally, we show as a prominent example that quantum\ncontextuality provides the resource in optimal phase-covariant and universal\ncloning schemes, hence establishing it as a fundamental source of\nnonclassicality in all known optimal quantum cloning scenarios.",
        "chunk-id": 2,
        "chunk": "scenarios. We showcase the power of this technique through several\napplications, including a new derivation of Hardy's quantum excess-baggage\ntheorem, and a simplified proof of contextuality for minimum error quantum\nstate discrimination. Finally, we show as a prominent example that quantum\ncontextuality provides the resource in optimal phase-covariant and universal",
        "authors": [
            "Farid Shahandeh",
            "Theodoros Yianni",
            "Mina Doosti"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:56:04+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19382v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19382v1",
        "categories": [
            "Quantum Physics"
        ]
    },
    {
        "id": 30000011,
        "doi": null,
        "title": "Characterizing Contextuality via Rank Separation with Applications to Cloning",
        "abstract": "Quantum contextuality is a key nonclassical feature essential for\nunderstanding advantages in quantum computation and communication. We introduce\na new framework to study contextuality based solely on information processing\nstatistics. This simple and intuitive perspective leads to a powerful criterion\ndenoted as rank separation for identifying contextuality in various quantum\nscenarios. We showcase the power of this technique through several\napplications, including a new derivation of Hardy's quantum excess-baggage\ntheorem, and a simplified proof of contextuality for minimum error quantum\nstate discrimination. Finally, we show as a prominent example that quantum\ncontextuality provides the resource in optimal phase-covariant and universal\ncloning schemes, hence establishing it as a fundamental source of\nnonclassicality in all known optimal quantum cloning scenarios.",
        "chunk-id": 3,
        "chunk": "cloning schemes, hence establishing it as a fundamental source of\nnonclassicality in all known optimal quantum cloning scenarios.",
        "authors": [
            "Farid Shahandeh",
            "Theodoros Yianni",
            "Mina Doosti"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:56:04+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19382v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19382v1",
        "categories": [
            "Quantum Physics"
        ]
    },
    {
        "id": 30000012,
        "doi": null,
        "title": "Higher-Order Constrained Dependency Pairs for (Universal) Computability",
        "abstract": "Dependency pairs constitute a series of very effective techniques for the\ntermination analysis of term rewriting systems. In this paper, we adapt the\nstatic dependency pair framework to logically constrained simply-typed term\nrewriting systems (LCSTRSs), a higher-order formalism with logical constraints\nbuilt in. We also propose the concept of universal computability, which enables\na form of open-world termination analysis through the use of static dependency\npairs.",
        "chunk-id": 1,
        "chunk": "Dependency pairs constitute a series of very effective techniques for the\ntermination analysis of term rewriting systems. In this paper, we adapt the\nstatic dependency pair framework to logically constrained simply-typed term\nrewriting systems (LCSTRSs), a higher-order formalism with logical constraints\nbuilt in. We also propose the concept of universal computability, which enables",
        "authors": [
            "Liye Guo",
            "Kasper Hagens",
            "Cynthia Kop",
            "Deivid Vale"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:55:21+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19379v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19379v1",
        "categories": [
            "Logic in Computer Science"
        ]
    },
    {
        "id": 30000012,
        "doi": null,
        "title": "Higher-Order Constrained Dependency Pairs for (Universal) Computability",
        "abstract": "Dependency pairs constitute a series of very effective techniques for the\ntermination analysis of term rewriting systems. In this paper, we adapt the\nstatic dependency pair framework to logically constrained simply-typed term\nrewriting systems (LCSTRSs), a higher-order formalism with logical constraints\nbuilt in. We also propose the concept of universal computability, which enables\na form of open-world termination analysis through the use of static dependency\npairs.",
        "chunk-id": 2,
        "chunk": "a form of open-world termination analysis through the use of static dependency\npairs.",
        "authors": [
            "Liye Guo",
            "Kasper Hagens",
            "Cynthia Kop",
            "Deivid Vale"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:55:21+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19379v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19379v1",
        "categories": [
            "Logic in Computer Science"
        ]
    },
    {
        "id": 30000013,
        "doi": null,
        "title": "Quartic quantum speedups for planted inference",
        "abstract": "We describe a quantum algorithm for the Planted Noisy $k$XOR problem (also\nknown as sparse Learning Parity with Noise) that achieves a nearly quartic\n($4$th power) speedup over the best known classical algorithm while also only\nusing logarithmically many qubits. Our work generalizes and simplifies prior\nwork of Hastings, by building on his quantum algorithm for the Tensor Principal\nComponent Analysis (PCA) problem. We achieve our quantum speedup using a\ngeneral framework based on the Kikuchi Method (recovering the quartic speedup\nfor Tensor PCA), and we anticipate it will yield similar speedups for further\nplanted inference problems. These speedups rely on the fact that planted\ninference problems naturally instantiate the Guided Sparse Hamiltonian problem.\nSince the Planted Noisy $k$XOR problem has been used as a component of certain\ncryptographic constructions, our work suggests that some of these are\nsusceptible to super-quadratic quantum attacks.",
        "chunk-id": 1,
        "chunk": "We describe a quantum algorithm for the Planted Noisy $k$XOR problem (also\nknown as sparse Learning Parity with Noise) that achieves a nearly quartic\n($4$th power) speedup over the best known classical algorithm while also only\nusing logarithmically many qubits. Our work generalizes and simplifies prior\nwork of Hastings, by building on his quantum algorithm for the Tensor Principal",
        "authors": [
            "Alexander Schmidhuber",
            "Ryan O'Donnell",
            "Robin Kothari",
            "Ryan Babbush"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:54:28+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19378v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19378v1",
        "categories": [
            "Quantum Physics",
            "Computational Complexity",
            "Cryptography and Security"
        ]
    },
    {
        "id": 30000013,
        "doi": null,
        "title": "Quartic quantum speedups for planted inference",
        "abstract": "We describe a quantum algorithm for the Planted Noisy $k$XOR problem (also\nknown as sparse Learning Parity with Noise) that achieves a nearly quartic\n($4$th power) speedup over the best known classical algorithm while also only\nusing logarithmically many qubits. Our work generalizes and simplifies prior\nwork of Hastings, by building on his quantum algorithm for the Tensor Principal\nComponent Analysis (PCA) problem. We achieve our quantum speedup using a\ngeneral framework based on the Kikuchi Method (recovering the quartic speedup\nfor Tensor PCA), and we anticipate it will yield similar speedups for further\nplanted inference problems. These speedups rely on the fact that planted\ninference problems naturally instantiate the Guided Sparse Hamiltonian problem.\nSince the Planted Noisy $k$XOR problem has been used as a component of certain\ncryptographic constructions, our work suggests that some of these are\nsusceptible to super-quadratic quantum attacks.",
        "chunk-id": 2,
        "chunk": "Component Analysis (PCA) problem. We achieve our quantum speedup using a\ngeneral framework based on the Kikuchi Method (recovering the quartic speedup\nfor Tensor PCA), and we anticipate it will yield similar speedups for further\nplanted inference problems. These speedups rely on the fact that planted\ninference problems naturally instantiate the Guided Sparse Hamiltonian problem.",
        "authors": [
            "Alexander Schmidhuber",
            "Ryan O'Donnell",
            "Robin Kothari",
            "Ryan Babbush"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:54:28+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19378v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19378v1",
        "categories": [
            "Quantum Physics",
            "Computational Complexity",
            "Cryptography and Security"
        ]
    },
    {
        "id": 30000013,
        "doi": null,
        "title": "Quartic quantum speedups for planted inference",
        "abstract": "We describe a quantum algorithm for the Planted Noisy $k$XOR problem (also\nknown as sparse Learning Parity with Noise) that achieves a nearly quartic\n($4$th power) speedup over the best known classical algorithm while also only\nusing logarithmically many qubits. Our work generalizes and simplifies prior\nwork of Hastings, by building on his quantum algorithm for the Tensor Principal\nComponent Analysis (PCA) problem. We achieve our quantum speedup using a\ngeneral framework based on the Kikuchi Method (recovering the quartic speedup\nfor Tensor PCA), and we anticipate it will yield similar speedups for further\nplanted inference problems. These speedups rely on the fact that planted\ninference problems naturally instantiate the Guided Sparse Hamiltonian problem.\nSince the Planted Noisy $k$XOR problem has been used as a component of certain\ncryptographic constructions, our work suggests that some of these are\nsusceptible to super-quadratic quantum attacks.",
        "chunk-id": 3,
        "chunk": "Since the Planted Noisy $k$XOR problem has been used as a component of certain\ncryptographic constructions, our work suggests that some of these are\nsusceptible to super-quadratic quantum attacks.",
        "authors": [
            "Alexander Schmidhuber",
            "Ryan O'Donnell",
            "Robin Kothari",
            "Ryan Babbush"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:54:28+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19378v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19378v1",
        "categories": [
            "Quantum Physics",
            "Computational Complexity",
            "Cryptography and Security"
        ]
    },
    {
        "id": 30000014,
        "doi": null,
        "title": "A Novel Phase Diagram for a Spin-1 System Exhibiting a Haldane Phase",
        "abstract": "We provide the phase diagram of a 2-parameter spin-1 chain that has a\nsymmetry-protected topological (SPT) Haldane phase using computational\nalgorithms along with tensor-network tools. We improve previous results,\nshowing the existence of a new phase and new triple points. New striking\nfeatures are the triple end of the Haldane phase and the complexity of phases\nbordering the Haldane phase in proximity allowing moving to nearby non-SPT\nphases via small perturbations. These characteristics make the system, which\nappears in Rydberg excitons, e.g. in Cu$_2$O, a prime candidate for\napplications.",
        "chunk-id": 1,
        "chunk": "We provide the phase diagram of a 2-parameter spin-1 chain that has a\nsymmetry-protected topological (SPT) Haldane phase using computational\nalgorithms along with tensor-network tools. We improve previous results,\nshowing the existence of a new phase and new triple points. New striking\nfeatures are the triple end of the Haldane phase and the complexity of phases",
        "authors": [
            "Mohamad Mousa",
            "Birgit Wehefritz-Kaufmann",
            "Sabre Kais",
            "Shawn Cui",
            "Ralph Kaufmann"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:51:23+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19372v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19372v1",
        "categories": [
            "Strongly Correlated Electrons"
        ]
    },
    {
        "id": 30000014,
        "doi": null,
        "title": "A Novel Phase Diagram for a Spin-1 System Exhibiting a Haldane Phase",
        "abstract": "We provide the phase diagram of a 2-parameter spin-1 chain that has a\nsymmetry-protected topological (SPT) Haldane phase using computational\nalgorithms along with tensor-network tools. We improve previous results,\nshowing the existence of a new phase and new triple points. New striking\nfeatures are the triple end of the Haldane phase and the complexity of phases\nbordering the Haldane phase in proximity allowing moving to nearby non-SPT\nphases via small perturbations. These characteristics make the system, which\nappears in Rydberg excitons, e.g. in Cu$_2$O, a prime candidate for\napplications.",
        "chunk-id": 2,
        "chunk": "bordering the Haldane phase in proximity allowing moving to nearby non-SPT\nphases via small perturbations. These characteristics make the system, which\nappears in Rydberg excitons, e.g. in Cu$_2$O, a prime candidate for\napplications.",
        "authors": [
            "Mohamad Mousa",
            "Birgit Wehefritz-Kaufmann",
            "Sabre Kais",
            "Shawn Cui",
            "Ralph Kaufmann"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:51:23+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19372v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19372v1",
        "categories": [
            "Strongly Correlated Electrons"
        ]
    },
    {
        "id": 30000015,
        "doi": null,
        "title": "Suri: Multi-constraint Instruction Following for Long-form Text Generation",
        "abstract": "Existing research on instruction following largely focuses on tasks with\nsimple instructions and short responses. In this work, we explore\nmulti-constraint instruction following for generating long-form text. We create\nSuri, a dataset with 20K human-written long-form texts paired with\nLLM-generated backtranslated instructions that contain multiple complex\nconstraints. Because of prohibitive challenges associated with collecting human\npreference judgments on long-form texts, preference-tuning algorithms such as\nDPO are infeasible in our setting; thus, we propose Instructional ORPO\n(I-ORPO), an alignment method based on the ORPO algorithm. Instead of receiving\nnegative feedback from dispreferred responses, I-ORPO obtains negative feedback\nfrom synthetically corrupted instructions generated by an LLM. Using Suri, we\nperform supervised and I-ORPO fine-tuning on Mistral-7b-Instruct-v0.2. The\nresulting models, Suri-SFT and Suri-I-ORPO, generate significantly longer texts\n(~5K tokens) than base models without significant quality deterioration. Our\nhuman evaluation shows that while both SFT and I-ORPO models satisfy most\nconstraints, Suri-I-ORPO generations are generally preferred for their coherent\nand informative incorporation of the constraints. We release our code at\nhttps://github.com/chtmp223/suri.",
        "chunk-id": 1,
        "chunk": "Existing research on instruction following largely focuses on tasks with\nsimple instructions and short responses. In this work, we explore\nmulti-constraint instruction following for generating long-form text. We create\nSuri, a dataset with 20K human-written long-form texts paired with\nLLM-generated backtranslated instructions that contain multiple complex",
        "authors": [
            "Chau Minh Pham",
            "Simeng Sun",
            "Mohit Iyyer"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:50:35+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19371v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19371v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 30000015,
        "doi": null,
        "title": "Suri: Multi-constraint Instruction Following for Long-form Text Generation",
        "abstract": "Existing research on instruction following largely focuses on tasks with\nsimple instructions and short responses. In this work, we explore\nmulti-constraint instruction following for generating long-form text. We create\nSuri, a dataset with 20K human-written long-form texts paired with\nLLM-generated backtranslated instructions that contain multiple complex\nconstraints. Because of prohibitive challenges associated with collecting human\npreference judgments on long-form texts, preference-tuning algorithms such as\nDPO are infeasible in our setting; thus, we propose Instructional ORPO\n(I-ORPO), an alignment method based on the ORPO algorithm. Instead of receiving\nnegative feedback from dispreferred responses, I-ORPO obtains negative feedback\nfrom synthetically corrupted instructions generated by an LLM. Using Suri, we\nperform supervised and I-ORPO fine-tuning on Mistral-7b-Instruct-v0.2. The\nresulting models, Suri-SFT and Suri-I-ORPO, generate significantly longer texts\n(~5K tokens) than base models without significant quality deterioration. Our\nhuman evaluation shows that while both SFT and I-ORPO models satisfy most\nconstraints, Suri-I-ORPO generations are generally preferred for their coherent\nand informative incorporation of the constraints. We release our code at\nhttps://github.com/chtmp223/suri.",
        "chunk-id": 2,
        "chunk": "constraints. Because of prohibitive challenges associated with collecting human\npreference judgments on long-form texts, preference-tuning algorithms such as\nDPO are infeasible in our setting; thus, we propose Instructional ORPO\n(I-ORPO), an alignment method based on the ORPO algorithm. Instead of receiving",
        "authors": [
            "Chau Minh Pham",
            "Simeng Sun",
            "Mohit Iyyer"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:50:35+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19371v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19371v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 30000015,
        "doi": null,
        "title": "Suri: Multi-constraint Instruction Following for Long-form Text Generation",
        "abstract": "Existing research on instruction following largely focuses on tasks with\nsimple instructions and short responses. In this work, we explore\nmulti-constraint instruction following for generating long-form text. We create\nSuri, a dataset with 20K human-written long-form texts paired with\nLLM-generated backtranslated instructions that contain multiple complex\nconstraints. Because of prohibitive challenges associated with collecting human\npreference judgments on long-form texts, preference-tuning algorithms such as\nDPO are infeasible in our setting; thus, we propose Instructional ORPO\n(I-ORPO), an alignment method based on the ORPO algorithm. Instead of receiving\nnegative feedback from dispreferred responses, I-ORPO obtains negative feedback\nfrom synthetically corrupted instructions generated by an LLM. Using Suri, we\nperform supervised and I-ORPO fine-tuning on Mistral-7b-Instruct-v0.2. The\nresulting models, Suri-SFT and Suri-I-ORPO, generate significantly longer texts\n(~5K tokens) than base models without significant quality deterioration. Our\nhuman evaluation shows that while both SFT and I-ORPO models satisfy most\nconstraints, Suri-I-ORPO generations are generally preferred for their coherent\nand informative incorporation of the constraints. We release our code at\nhttps://github.com/chtmp223/suri.",
        "chunk-id": 3,
        "chunk": "negative feedback from dispreferred responses, I-ORPO obtains negative feedback\nfrom synthetically corrupted instructions generated by an LLM. Using Suri, we\nperform supervised and I-ORPO fine-tuning on Mistral-7b-Instruct-v0.2. The\nresulting models, Suri-SFT and Suri-I-ORPO, generate significantly longer texts",
        "authors": [
            "Chau Minh Pham",
            "Simeng Sun",
            "Mohit Iyyer"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:50:35+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19371v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19371v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 30000015,
        "doi": null,
        "title": "Suri: Multi-constraint Instruction Following for Long-form Text Generation",
        "abstract": "Existing research on instruction following largely focuses on tasks with\nsimple instructions and short responses. In this work, we explore\nmulti-constraint instruction following for generating long-form text. We create\nSuri, a dataset with 20K human-written long-form texts paired with\nLLM-generated backtranslated instructions that contain multiple complex\nconstraints. Because of prohibitive challenges associated with collecting human\npreference judgments on long-form texts, preference-tuning algorithms such as\nDPO are infeasible in our setting; thus, we propose Instructional ORPO\n(I-ORPO), an alignment method based on the ORPO algorithm. Instead of receiving\nnegative feedback from dispreferred responses, I-ORPO obtains negative feedback\nfrom synthetically corrupted instructions generated by an LLM. Using Suri, we\nperform supervised and I-ORPO fine-tuning on Mistral-7b-Instruct-v0.2. The\nresulting models, Suri-SFT and Suri-I-ORPO, generate significantly longer texts\n(~5K tokens) than base models without significant quality deterioration. Our\nhuman evaluation shows that while both SFT and I-ORPO models satisfy most\nconstraints, Suri-I-ORPO generations are generally preferred for their coherent\nand informative incorporation of the constraints. We release our code at\nhttps://github.com/chtmp223/suri.",
        "chunk-id": 4,
        "chunk": "(~5K tokens) than base models without significant quality deterioration. Our\nhuman evaluation shows that while both SFT and I-ORPO models satisfy most\nconstraints, Suri-I-ORPO generations are generally preferred for their coherent\nand informative incorporation of the constraints. We release our code at\nhttps://github.com/chtmp223/suri.",
        "authors": [
            "Chau Minh Pham",
            "Simeng Sun",
            "Mohit Iyyer"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:50:35+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19371v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19371v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 30000016,
        "doi": null,
        "title": "Mamba or RWKV: Exploring High-Quality and High-Efficiency Segment Anything Model",
        "abstract": "Transformer-based segmentation methods face the challenge of efficient\ninference when dealing with high-resolution images. Recently, several linear\nattention architectures, such as Mamba and RWKV, have attracted much attention\nas they can process long sequences efficiently. In this work, we focus on\ndesigning an efficient segment-anything model by exploring these different\narchitectures. Specifically, we design a mixed backbone that contains\nconvolution and RWKV operation, which achieves the best for both accuracy and\nefficiency. In addition, we design an efficient decoder to utilize the\nmultiscale tokens to obtain high-quality masks. We denote our method as\nRWKV-SAM, a simple, effective, fast baseline for SAM-like models. Moreover, we\nbuild a benchmark containing various high-quality segmentation datasets and\njointly train one efficient yet high-quality segmentation model using this\nbenchmark. Based on the benchmark results, our RWKV-SAM achieves outstanding\nperformance in efficiency and segmentation quality compared to transformers and\nother linear attention models. For example, compared with the same-scale\ntransformer model, RWKV-SAM achieves more than 2x speedup and can achieve\nbetter segmentation performance on various datasets. In addition, RWKV-SAM\noutperforms recent vision Mamba models with better classification and semantic\nsegmentation results. Code and models will be publicly available.",
        "chunk-id": 1,
        "chunk": "Transformer-based segmentation methods face the challenge of efficient\ninference when dealing with high-resolution images. Recently, several linear\nattention architectures, such as Mamba and RWKV, have attracted much attention\nas they can process long sequences efficiently. In this work, we focus on\ndesigning an efficient segment-anything model by exploring these different",
        "authors": [
            "Haobo Yuan",
            "Xiangtai Li",
            "Lu Qi",
            "Tao Zhang",
            "Ming-Hsuan Yang",
            "Shuicheng Yan",
            "Chen Change Loy"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:49:25+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19369v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19369v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000016,
        "doi": null,
        "title": "Mamba or RWKV: Exploring High-Quality and High-Efficiency Segment Anything Model",
        "abstract": "Transformer-based segmentation methods face the challenge of efficient\ninference when dealing with high-resolution images. Recently, several linear\nattention architectures, such as Mamba and RWKV, have attracted much attention\nas they can process long sequences efficiently. In this work, we focus on\ndesigning an efficient segment-anything model by exploring these different\narchitectures. Specifically, we design a mixed backbone that contains\nconvolution and RWKV operation, which achieves the best for both accuracy and\nefficiency. In addition, we design an efficient decoder to utilize the\nmultiscale tokens to obtain high-quality masks. We denote our method as\nRWKV-SAM, a simple, effective, fast baseline for SAM-like models. Moreover, we\nbuild a benchmark containing various high-quality segmentation datasets and\njointly train one efficient yet high-quality segmentation model using this\nbenchmark. Based on the benchmark results, our RWKV-SAM achieves outstanding\nperformance in efficiency and segmentation quality compared to transformers and\nother linear attention models. For example, compared with the same-scale\ntransformer model, RWKV-SAM achieves more than 2x speedup and can achieve\nbetter segmentation performance on various datasets. In addition, RWKV-SAM\noutperforms recent vision Mamba models with better classification and semantic\nsegmentation results. Code and models will be publicly available.",
        "chunk-id": 2,
        "chunk": "architectures. Specifically, we design a mixed backbone that contains\nconvolution and RWKV operation, which achieves the best for both accuracy and\nefficiency. In addition, we design an efficient decoder to utilize the\nmultiscale tokens to obtain high-quality masks. We denote our method as\nRWKV-SAM, a simple, effective, fast baseline for SAM-like models. Moreover, we",
        "authors": [
            "Haobo Yuan",
            "Xiangtai Li",
            "Lu Qi",
            "Tao Zhang",
            "Ming-Hsuan Yang",
            "Shuicheng Yan",
            "Chen Change Loy"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:49:25+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19369v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19369v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000016,
        "doi": null,
        "title": "Mamba or RWKV: Exploring High-Quality and High-Efficiency Segment Anything Model",
        "abstract": "Transformer-based segmentation methods face the challenge of efficient\ninference when dealing with high-resolution images. Recently, several linear\nattention architectures, such as Mamba and RWKV, have attracted much attention\nas they can process long sequences efficiently. In this work, we focus on\ndesigning an efficient segment-anything model by exploring these different\narchitectures. Specifically, we design a mixed backbone that contains\nconvolution and RWKV operation, which achieves the best for both accuracy and\nefficiency. In addition, we design an efficient decoder to utilize the\nmultiscale tokens to obtain high-quality masks. We denote our method as\nRWKV-SAM, a simple, effective, fast baseline for SAM-like models. Moreover, we\nbuild a benchmark containing various high-quality segmentation datasets and\njointly train one efficient yet high-quality segmentation model using this\nbenchmark. Based on the benchmark results, our RWKV-SAM achieves outstanding\nperformance in efficiency and segmentation quality compared to transformers and\nother linear attention models. For example, compared with the same-scale\ntransformer model, RWKV-SAM achieves more than 2x speedup and can achieve\nbetter segmentation performance on various datasets. In addition, RWKV-SAM\noutperforms recent vision Mamba models with better classification and semantic\nsegmentation results. Code and models will be publicly available.",
        "chunk-id": 3,
        "chunk": "build a benchmark containing various high-quality segmentation datasets and\njointly train one efficient yet high-quality segmentation model using this\nbenchmark. Based on the benchmark results, our RWKV-SAM achieves outstanding\nperformance in efficiency and segmentation quality compared to transformers and\nother linear attention models. For example, compared with the same-scale",
        "authors": [
            "Haobo Yuan",
            "Xiangtai Li",
            "Lu Qi",
            "Tao Zhang",
            "Ming-Hsuan Yang",
            "Shuicheng Yan",
            "Chen Change Loy"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:49:25+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19369v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19369v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000016,
        "doi": null,
        "title": "Mamba or RWKV: Exploring High-Quality and High-Efficiency Segment Anything Model",
        "abstract": "Transformer-based segmentation methods face the challenge of efficient\ninference when dealing with high-resolution images. Recently, several linear\nattention architectures, such as Mamba and RWKV, have attracted much attention\nas they can process long sequences efficiently. In this work, we focus on\ndesigning an efficient segment-anything model by exploring these different\narchitectures. Specifically, we design a mixed backbone that contains\nconvolution and RWKV operation, which achieves the best for both accuracy and\nefficiency. In addition, we design an efficient decoder to utilize the\nmultiscale tokens to obtain high-quality masks. We denote our method as\nRWKV-SAM, a simple, effective, fast baseline for SAM-like models. Moreover, we\nbuild a benchmark containing various high-quality segmentation datasets and\njointly train one efficient yet high-quality segmentation model using this\nbenchmark. Based on the benchmark results, our RWKV-SAM achieves outstanding\nperformance in efficiency and segmentation quality compared to transformers and\nother linear attention models. For example, compared with the same-scale\ntransformer model, RWKV-SAM achieves more than 2x speedup and can achieve\nbetter segmentation performance on various datasets. In addition, RWKV-SAM\noutperforms recent vision Mamba models with better classification and semantic\nsegmentation results. Code and models will be publicly available.",
        "chunk-id": 4,
        "chunk": "transformer model, RWKV-SAM achieves more than 2x speedup and can achieve\nbetter segmentation performance on various datasets. In addition, RWKV-SAM\noutperforms recent vision Mamba models with better classification and semantic\nsegmentation results. Code and models will be publicly available.",
        "authors": [
            "Haobo Yuan",
            "Xiangtai Li",
            "Lu Qi",
            "Tao Zhang",
            "Ming-Hsuan Yang",
            "Shuicheng Yan",
            "Chen Change Loy"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:49:25+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19369v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19369v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000017,
        "doi": null,
        "title": "SimTxtSeg: Weakly-Supervised Medical Image Segmentation with Simple Text Cues",
        "abstract": "Weakly-supervised medical image segmentation is a challenging task that aims\nto reduce the annotation cost while keep the segmentation performance. In this\npaper, we present a novel framework, SimTxtSeg, that leverages simple text cues\nto generate high-quality pseudo-labels and study the cross-modal fusion in\ntraining segmentation models, simultaneously. Our contribution consists of two\nkey components: an effective Textual-to-Visual Cue Converter that produces\nvisual prompts from text prompts on medical images, and a text-guided\nsegmentation model with Text-Vision Hybrid Attention that fuses text and image\nfeatures. We evaluate our framework on two medical image segmentation tasks:\ncolonic polyp segmentation and MRI brain tumor segmentation, and achieve\nconsistent state-of-the-art performance.",
        "chunk-id": 1,
        "chunk": "Weakly-supervised medical image segmentation is a challenging task that aims\nto reduce the annotation cost while keep the segmentation performance. In this\npaper, we present a novel framework, SimTxtSeg, that leverages simple text cues\nto generate high-quality pseudo-labels and study the cross-modal fusion in",
        "authors": [
            "Yuxin Xie",
            "Tao Zhou",
            "Yi Zhou",
            "Geng Chen"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:46:13+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19364v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19364v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000017,
        "doi": null,
        "title": "SimTxtSeg: Weakly-Supervised Medical Image Segmentation with Simple Text Cues",
        "abstract": "Weakly-supervised medical image segmentation is a challenging task that aims\nto reduce the annotation cost while keep the segmentation performance. In this\npaper, we present a novel framework, SimTxtSeg, that leverages simple text cues\nto generate high-quality pseudo-labels and study the cross-modal fusion in\ntraining segmentation models, simultaneously. Our contribution consists of two\nkey components: an effective Textual-to-Visual Cue Converter that produces\nvisual prompts from text prompts on medical images, and a text-guided\nsegmentation model with Text-Vision Hybrid Attention that fuses text and image\nfeatures. We evaluate our framework on two medical image segmentation tasks:\ncolonic polyp segmentation and MRI brain tumor segmentation, and achieve\nconsistent state-of-the-art performance.",
        "chunk-id": 2,
        "chunk": "training segmentation models, simultaneously. Our contribution consists of two\nkey components: an effective Textual-to-Visual Cue Converter that produces\nvisual prompts from text prompts on medical images, and a text-guided\nsegmentation model with Text-Vision Hybrid Attention that fuses text and image\nfeatures. We evaluate our framework on two medical image segmentation tasks:",
        "authors": [
            "Yuxin Xie",
            "Tao Zhou",
            "Yi Zhou",
            "Geng Chen"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:46:13+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19364v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19364v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000017,
        "doi": null,
        "title": "SimTxtSeg: Weakly-Supervised Medical Image Segmentation with Simple Text Cues",
        "abstract": "Weakly-supervised medical image segmentation is a challenging task that aims\nto reduce the annotation cost while keep the segmentation performance. In this\npaper, we present a novel framework, SimTxtSeg, that leverages simple text cues\nto generate high-quality pseudo-labels and study the cross-modal fusion in\ntraining segmentation models, simultaneously. Our contribution consists of two\nkey components: an effective Textual-to-Visual Cue Converter that produces\nvisual prompts from text prompts on medical images, and a text-guided\nsegmentation model with Text-Vision Hybrid Attention that fuses text and image\nfeatures. We evaluate our framework on two medical image segmentation tasks:\ncolonic polyp segmentation and MRI brain tumor segmentation, and achieve\nconsistent state-of-the-art performance.",
        "chunk-id": 3,
        "chunk": "colonic polyp segmentation and MRI brain tumor segmentation, and achieve\nconsistent state-of-the-art performance.",
        "authors": [
            "Yuxin Xie",
            "Tao Zhou",
            "Yi Zhou",
            "Geng Chen"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:46:13+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19364v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19364v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000018,
        "doi": "10.1109/TIV.2024.3397194",
        "title": "STAL3D: Unsupervised Domain Adaptation for 3D Object Detection via Collaborating Self-Training and Adversarial Learning",
        "abstract": "Existing 3D object detection suffers from expensive annotation costs and poor\ntransferability to unknown data due to the domain gap, Unsupervised Domain\nAdaptation (UDA) aims to generalize detection models trained in labeled source\ndomains to perform robustly on unexplored target domains, providing a promising\nsolution for cross-domain 3D object detection. Although Self-Training (ST)\nbased cross-domain 3D detection methods with the assistance of pseudo-labeling\ntechniques have achieved remarkable progress, they still face the issue of\nlow-quality pseudo-labels when there are significant domain disparities due to\nthe absence of a process for feature distribution alignment. While Adversarial\nLearning (AL) based methods can effectively align the feature distributions of\nthe source and target domains, the inability to obtain labels in the target\ndomain forces the adoption of asymmetric optimization losses, resulting in a\nchallenging issue of source domain bias. To overcome these limitations, we\npropose a novel unsupervised domain adaptation framework for 3D object\ndetection via collaborating ST and AL, dubbed as STAL3D, unleashing the\ncomplementary advantages of pseudo labels and feature distribution alignment.\nAdditionally, a Background Suppression Adversarial Learning (BS-AL) module and\na Scale Filtering Module (SFM) are designed tailored for 3D cross-domain\nscenes, effectively alleviating the issues of the large proportion of\nbackground interference and source domain size bias. Our STAL3D achieves\nstate-of-the-art performance on multiple cross-domain tasks and even surpasses\nthe Oracle results on Waymo $\\rightarrow$ KITTI and Waymo $\\rightarrow$\nKITTI-rain.",
        "chunk-id": 1,
        "chunk": "Existing 3D object detection suffers from expensive annotation costs and poor\ntransferability to unknown data due to the domain gap, Unsupervised Domain\nAdaptation (UDA) aims to generalize detection models trained in labeled source\ndomains to perform robustly on unexplored target domains, providing a promising",
        "authors": [
            "Yanan Zhang",
            "Chao Zhou",
            "Di Huang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:43:35+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19362v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19362v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000018,
        "doi": "10.1109/TIV.2024.3397194",
        "title": "STAL3D: Unsupervised Domain Adaptation for 3D Object Detection via Collaborating Self-Training and Adversarial Learning",
        "abstract": "Existing 3D object detection suffers from expensive annotation costs and poor\ntransferability to unknown data due to the domain gap, Unsupervised Domain\nAdaptation (UDA) aims to generalize detection models trained in labeled source\ndomains to perform robustly on unexplored target domains, providing a promising\nsolution for cross-domain 3D object detection. Although Self-Training (ST)\nbased cross-domain 3D detection methods with the assistance of pseudo-labeling\ntechniques have achieved remarkable progress, they still face the issue of\nlow-quality pseudo-labels when there are significant domain disparities due to\nthe absence of a process for feature distribution alignment. While Adversarial\nLearning (AL) based methods can effectively align the feature distributions of\nthe source and target domains, the inability to obtain labels in the target\ndomain forces the adoption of asymmetric optimization losses, resulting in a\nchallenging issue of source domain bias. To overcome these limitations, we\npropose a novel unsupervised domain adaptation framework for 3D object\ndetection via collaborating ST and AL, dubbed as STAL3D, unleashing the\ncomplementary advantages of pseudo labels and feature distribution alignment.\nAdditionally, a Background Suppression Adversarial Learning (BS-AL) module and\na Scale Filtering Module (SFM) are designed tailored for 3D cross-domain\nscenes, effectively alleviating the issues of the large proportion of\nbackground interference and source domain size bias. Our STAL3D achieves\nstate-of-the-art performance on multiple cross-domain tasks and even surpasses\nthe Oracle results on Waymo $\\rightarrow$ KITTI and Waymo $\\rightarrow$\nKITTI-rain.",
        "chunk-id": 2,
        "chunk": "solution for cross-domain 3D object detection. Although Self-Training (ST)\nbased cross-domain 3D detection methods with the assistance of pseudo-labeling\ntechniques have achieved remarkable progress, they still face the issue of\nlow-quality pseudo-labels when there are significant domain disparities due to",
        "authors": [
            "Yanan Zhang",
            "Chao Zhou",
            "Di Huang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:43:35+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19362v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19362v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000018,
        "doi": "10.1109/TIV.2024.3397194",
        "title": "STAL3D: Unsupervised Domain Adaptation for 3D Object Detection via Collaborating Self-Training and Adversarial Learning",
        "abstract": "Existing 3D object detection suffers from expensive annotation costs and poor\ntransferability to unknown data due to the domain gap, Unsupervised Domain\nAdaptation (UDA) aims to generalize detection models trained in labeled source\ndomains to perform robustly on unexplored target domains, providing a promising\nsolution for cross-domain 3D object detection. Although Self-Training (ST)\nbased cross-domain 3D detection methods with the assistance of pseudo-labeling\ntechniques have achieved remarkable progress, they still face the issue of\nlow-quality pseudo-labels when there are significant domain disparities due to\nthe absence of a process for feature distribution alignment. While Adversarial\nLearning (AL) based methods can effectively align the feature distributions of\nthe source and target domains, the inability to obtain labels in the target\ndomain forces the adoption of asymmetric optimization losses, resulting in a\nchallenging issue of source domain bias. To overcome these limitations, we\npropose a novel unsupervised domain adaptation framework for 3D object\ndetection via collaborating ST and AL, dubbed as STAL3D, unleashing the\ncomplementary advantages of pseudo labels and feature distribution alignment.\nAdditionally, a Background Suppression Adversarial Learning (BS-AL) module and\na Scale Filtering Module (SFM) are designed tailored for 3D cross-domain\nscenes, effectively alleviating the issues of the large proportion of\nbackground interference and source domain size bias. Our STAL3D achieves\nstate-of-the-art performance on multiple cross-domain tasks and even surpasses\nthe Oracle results on Waymo $\\rightarrow$ KITTI and Waymo $\\rightarrow$\nKITTI-rain.",
        "chunk-id": 3,
        "chunk": "the absence of a process for feature distribution alignment. While Adversarial\nLearning (AL) based methods can effectively align the feature distributions of\nthe source and target domains, the inability to obtain labels in the target\ndomain forces the adoption of asymmetric optimization losses, resulting in a",
        "authors": [
            "Yanan Zhang",
            "Chao Zhou",
            "Di Huang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:43:35+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19362v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19362v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000018,
        "doi": "10.1109/TIV.2024.3397194",
        "title": "STAL3D: Unsupervised Domain Adaptation for 3D Object Detection via Collaborating Self-Training and Adversarial Learning",
        "abstract": "Existing 3D object detection suffers from expensive annotation costs and poor\ntransferability to unknown data due to the domain gap, Unsupervised Domain\nAdaptation (UDA) aims to generalize detection models trained in labeled source\ndomains to perform robustly on unexplored target domains, providing a promising\nsolution for cross-domain 3D object detection. Although Self-Training (ST)\nbased cross-domain 3D detection methods with the assistance of pseudo-labeling\ntechniques have achieved remarkable progress, they still face the issue of\nlow-quality pseudo-labels when there are significant domain disparities due to\nthe absence of a process for feature distribution alignment. While Adversarial\nLearning (AL) based methods can effectively align the feature distributions of\nthe source and target domains, the inability to obtain labels in the target\ndomain forces the adoption of asymmetric optimization losses, resulting in a\nchallenging issue of source domain bias. To overcome these limitations, we\npropose a novel unsupervised domain adaptation framework for 3D object\ndetection via collaborating ST and AL, dubbed as STAL3D, unleashing the\ncomplementary advantages of pseudo labels and feature distribution alignment.\nAdditionally, a Background Suppression Adversarial Learning (BS-AL) module and\na Scale Filtering Module (SFM) are designed tailored for 3D cross-domain\nscenes, effectively alleviating the issues of the large proportion of\nbackground interference and source domain size bias. Our STAL3D achieves\nstate-of-the-art performance on multiple cross-domain tasks and even surpasses\nthe Oracle results on Waymo $\\rightarrow$ KITTI and Waymo $\\rightarrow$\nKITTI-rain.",
        "chunk-id": 4,
        "chunk": "challenging issue of source domain bias. To overcome these limitations, we\npropose a novel unsupervised domain adaptation framework for 3D object\ndetection via collaborating ST and AL, dubbed as STAL3D, unleashing the\ncomplementary advantages of pseudo labels and feature distribution alignment.\nAdditionally, a Background Suppression Adversarial Learning (BS-AL) module and",
        "authors": [
            "Yanan Zhang",
            "Chao Zhou",
            "Di Huang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:43:35+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19362v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19362v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000018,
        "doi": "10.1109/TIV.2024.3397194",
        "title": "STAL3D: Unsupervised Domain Adaptation for 3D Object Detection via Collaborating Self-Training and Adversarial Learning",
        "abstract": "Existing 3D object detection suffers from expensive annotation costs and poor\ntransferability to unknown data due to the domain gap, Unsupervised Domain\nAdaptation (UDA) aims to generalize detection models trained in labeled source\ndomains to perform robustly on unexplored target domains, providing a promising\nsolution for cross-domain 3D object detection. Although Self-Training (ST)\nbased cross-domain 3D detection methods with the assistance of pseudo-labeling\ntechniques have achieved remarkable progress, they still face the issue of\nlow-quality pseudo-labels when there are significant domain disparities due to\nthe absence of a process for feature distribution alignment. While Adversarial\nLearning (AL) based methods can effectively align the feature distributions of\nthe source and target domains, the inability to obtain labels in the target\ndomain forces the adoption of asymmetric optimization losses, resulting in a\nchallenging issue of source domain bias. To overcome these limitations, we\npropose a novel unsupervised domain adaptation framework for 3D object\ndetection via collaborating ST and AL, dubbed as STAL3D, unleashing the\ncomplementary advantages of pseudo labels and feature distribution alignment.\nAdditionally, a Background Suppression Adversarial Learning (BS-AL) module and\na Scale Filtering Module (SFM) are designed tailored for 3D cross-domain\nscenes, effectively alleviating the issues of the large proportion of\nbackground interference and source domain size bias. Our STAL3D achieves\nstate-of-the-art performance on multiple cross-domain tasks and even surpasses\nthe Oracle results on Waymo $\\rightarrow$ KITTI and Waymo $\\rightarrow$\nKITTI-rain.",
        "chunk-id": 5,
        "chunk": "a Scale Filtering Module (SFM) are designed tailored for 3D cross-domain\nscenes, effectively alleviating the issues of the large proportion of\nbackground interference and source domain size bias. Our STAL3D achieves\nstate-of-the-art performance on multiple cross-domain tasks and even surpasses\nthe Oracle results on Waymo $\\rightarrow$ KITTI and Waymo $\\rightarrow$\nKITTI-rain.",
        "authors": [
            "Yanan Zhang",
            "Chao Zhou",
            "Di Huang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:43:35+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19362v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19362v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000019,
        "doi": null,
        "title": "The Model Arena for Cross-lingual Sentiment Analysis: A Comparative Study in the Era of Large Language Models",
        "abstract": "Sentiment analysis serves as a pivotal component in Natural Language\nProcessing (NLP). Advancements in multilingual pre-trained models such as XLM-R\nand mT5 have contributed to the increasing interest in cross-lingual sentiment\nanalysis. The recent emergence in Large Language Models (LLM) has significantly\nadvanced general NLP tasks, however, the capability of such LLMs in\ncross-lingual sentiment analysis has not been fully studied. This work\nundertakes an empirical analysis to compare the cross-lingual transfer\ncapability of public Small Multilingual Language Models (SMLM) like XLM-R,\nagainst English-centric LLMs such as Llama-3, in the context of sentiment\nanalysis across English, Spanish, French and Chinese. Our findings reveal that\namong public models, SMLMs exhibit superior zero-shot cross-lingual performance\nrelative to LLMs. However, in few-shot cross-lingual settings, public LLMs\ndemonstrate an enhanced adaptive potential. In addition, we observe that\nproprietary GPT-3.5 and GPT-4 lead in zero-shot cross-lingual capability, but\nare outpaced by public models in few-shot scenarios.",
        "chunk-id": 1,
        "chunk": "Sentiment analysis serves as a pivotal component in Natural Language\nProcessing (NLP). Advancements in multilingual pre-trained models such as XLM-R\nand mT5 have contributed to the increasing interest in cross-lingual sentiment\nanalysis. The recent emergence in Large Language Models (LLM) has significantly\nadvanced general NLP tasks, however, the capability of such LLMs in",
        "authors": [
            "Xiliang Zhu",
            "Shayna Gardiner",
            "Tere Rold\u00e1n",
            "David Rossouw"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:38:45+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19358v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19358v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 30000019,
        "doi": null,
        "title": "The Model Arena for Cross-lingual Sentiment Analysis: A Comparative Study in the Era of Large Language Models",
        "abstract": "Sentiment analysis serves as a pivotal component in Natural Language\nProcessing (NLP). Advancements in multilingual pre-trained models such as XLM-R\nand mT5 have contributed to the increasing interest in cross-lingual sentiment\nanalysis. The recent emergence in Large Language Models (LLM) has significantly\nadvanced general NLP tasks, however, the capability of such LLMs in\ncross-lingual sentiment analysis has not been fully studied. This work\nundertakes an empirical analysis to compare the cross-lingual transfer\ncapability of public Small Multilingual Language Models (SMLM) like XLM-R,\nagainst English-centric LLMs such as Llama-3, in the context of sentiment\nanalysis across English, Spanish, French and Chinese. Our findings reveal that\namong public models, SMLMs exhibit superior zero-shot cross-lingual performance\nrelative to LLMs. However, in few-shot cross-lingual settings, public LLMs\ndemonstrate an enhanced adaptive potential. In addition, we observe that\nproprietary GPT-3.5 and GPT-4 lead in zero-shot cross-lingual capability, but\nare outpaced by public models in few-shot scenarios.",
        "chunk-id": 2,
        "chunk": "cross-lingual sentiment analysis has not been fully studied. This work\nundertakes an empirical analysis to compare the cross-lingual transfer\ncapability of public Small Multilingual Language Models (SMLM) like XLM-R,\nagainst English-centric LLMs such as Llama-3, in the context of sentiment\nanalysis across English, Spanish, French and Chinese. Our findings reveal that",
        "authors": [
            "Xiliang Zhu",
            "Shayna Gardiner",
            "Tere Rold\u00e1n",
            "David Rossouw"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:38:45+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19358v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19358v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 30000019,
        "doi": null,
        "title": "The Model Arena for Cross-lingual Sentiment Analysis: A Comparative Study in the Era of Large Language Models",
        "abstract": "Sentiment analysis serves as a pivotal component in Natural Language\nProcessing (NLP). Advancements in multilingual pre-trained models such as XLM-R\nand mT5 have contributed to the increasing interest in cross-lingual sentiment\nanalysis. The recent emergence in Large Language Models (LLM) has significantly\nadvanced general NLP tasks, however, the capability of such LLMs in\ncross-lingual sentiment analysis has not been fully studied. This work\nundertakes an empirical analysis to compare the cross-lingual transfer\ncapability of public Small Multilingual Language Models (SMLM) like XLM-R,\nagainst English-centric LLMs such as Llama-3, in the context of sentiment\nanalysis across English, Spanish, French and Chinese. Our findings reveal that\namong public models, SMLMs exhibit superior zero-shot cross-lingual performance\nrelative to LLMs. However, in few-shot cross-lingual settings, public LLMs\ndemonstrate an enhanced adaptive potential. In addition, we observe that\nproprietary GPT-3.5 and GPT-4 lead in zero-shot cross-lingual capability, but\nare outpaced by public models in few-shot scenarios.",
        "chunk-id": 3,
        "chunk": "among public models, SMLMs exhibit superior zero-shot cross-lingual performance\nrelative to LLMs. However, in few-shot cross-lingual settings, public LLMs\ndemonstrate an enhanced adaptive potential. In addition, we observe that\nproprietary GPT-3.5 and GPT-4 lead in zero-shot cross-lingual capability, but\nare outpaced by public models in few-shot scenarios.",
        "authors": [
            "Xiliang Zhu",
            "Shayna Gardiner",
            "Tere Rold\u00e1n",
            "David Rossouw"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:38:45+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19358v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19358v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 30000020,
        "doi": null,
        "title": "DiVERT: Distractor Generation with Variational Errors Represented as Text for Math Multiple-choice Questions",
        "abstract": "High-quality distractors are crucial to both the assessment and pedagogical\nvalue of multiple-choice questions (MCQs), where manually crafting ones that\nanticipate knowledge deficiencies or misconceptions among real students is\ndifficult. Meanwhile, automated distractor generation, even with the help of\nlarge language models (LLMs), remains challenging for subjects like math. It is\ncrucial to not only identify plausible distractors but also understand the\nerror behind them. In this paper, we introduce DiVERT (Distractor Generation\nwith Variational Errors Represented as Text), a novel variational approach that\nlearns an interpretable representation of errors behind distractors in math\nMCQs. Through experiments on a real-world math MCQ dataset with 1,434 questions\nused by hundreds of thousands of students, we show that DiVERT, despite using a\nbase open-source LLM with 7B parameters, outperforms state-of-the-art\napproaches using GPT-4o on downstream distractor generation. We also conduct a\nhuman evaluation with math educators and find that DiVERT leads to error labels\nthat are of comparable quality to human-authored ones.",
        "chunk-id": 1,
        "chunk": "High-quality distractors are crucial to both the assessment and pedagogical\nvalue of multiple-choice questions (MCQs), where manually crafting ones that\nanticipate knowledge deficiencies or misconceptions among real students is\ndifficult. Meanwhile, automated distractor generation, even with the help of\nlarge language models (LLMs), remains challenging for subjects like math. It is",
        "authors": [
            "Nigel Fernandez",
            "Alexander Scarlatos",
            "Simon Woodhead",
            "Andrew Lan"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:37:31+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19356v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19356v1",
        "categories": [
            "Computation and Language",
            "Computers and Society",
            "Machine Learning"
        ]
    },
    {
        "id": 30000020,
        "doi": null,
        "title": "DiVERT: Distractor Generation with Variational Errors Represented as Text for Math Multiple-choice Questions",
        "abstract": "High-quality distractors are crucial to both the assessment and pedagogical\nvalue of multiple-choice questions (MCQs), where manually crafting ones that\nanticipate knowledge deficiencies or misconceptions among real students is\ndifficult. Meanwhile, automated distractor generation, even with the help of\nlarge language models (LLMs), remains challenging for subjects like math. It is\ncrucial to not only identify plausible distractors but also understand the\nerror behind them. In this paper, we introduce DiVERT (Distractor Generation\nwith Variational Errors Represented as Text), a novel variational approach that\nlearns an interpretable representation of errors behind distractors in math\nMCQs. Through experiments on a real-world math MCQ dataset with 1,434 questions\nused by hundreds of thousands of students, we show that DiVERT, despite using a\nbase open-source LLM with 7B parameters, outperforms state-of-the-art\napproaches using GPT-4o on downstream distractor generation. We also conduct a\nhuman evaluation with math educators and find that DiVERT leads to error labels\nthat are of comparable quality to human-authored ones.",
        "chunk-id": 2,
        "chunk": "crucial to not only identify plausible distractors but also understand the\nerror behind them. In this paper, we introduce DiVERT (Distractor Generation\nwith Variational Errors Represented as Text), a novel variational approach that\nlearns an interpretable representation of errors behind distractors in math",
        "authors": [
            "Nigel Fernandez",
            "Alexander Scarlatos",
            "Simon Woodhead",
            "Andrew Lan"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:37:31+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19356v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19356v1",
        "categories": [
            "Computation and Language",
            "Computers and Society",
            "Machine Learning"
        ]
    },
    {
        "id": 30000020,
        "doi": null,
        "title": "DiVERT: Distractor Generation with Variational Errors Represented as Text for Math Multiple-choice Questions",
        "abstract": "High-quality distractors are crucial to both the assessment and pedagogical\nvalue of multiple-choice questions (MCQs), where manually crafting ones that\nanticipate knowledge deficiencies or misconceptions among real students is\ndifficult. Meanwhile, automated distractor generation, even with the help of\nlarge language models (LLMs), remains challenging for subjects like math. It is\ncrucial to not only identify plausible distractors but also understand the\nerror behind them. In this paper, we introduce DiVERT (Distractor Generation\nwith Variational Errors Represented as Text), a novel variational approach that\nlearns an interpretable representation of errors behind distractors in math\nMCQs. Through experiments on a real-world math MCQ dataset with 1,434 questions\nused by hundreds of thousands of students, we show that DiVERT, despite using a\nbase open-source LLM with 7B parameters, outperforms state-of-the-art\napproaches using GPT-4o on downstream distractor generation. We also conduct a\nhuman evaluation with math educators and find that DiVERT leads to error labels\nthat are of comparable quality to human-authored ones.",
        "chunk-id": 3,
        "chunk": "MCQs. Through experiments on a real-world math MCQ dataset with 1,434 questions\nused by hundreds of thousands of students, we show that DiVERT, despite using a\nbase open-source LLM with 7B parameters, outperforms state-of-the-art\napproaches using GPT-4o on downstream distractor generation. We also conduct a",
        "authors": [
            "Nigel Fernandez",
            "Alexander Scarlatos",
            "Simon Woodhead",
            "Andrew Lan"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:37:31+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19356v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19356v1",
        "categories": [
            "Computation and Language",
            "Computers and Society",
            "Machine Learning"
        ]
    },
    {
        "id": 30000020,
        "doi": null,
        "title": "DiVERT: Distractor Generation with Variational Errors Represented as Text for Math Multiple-choice Questions",
        "abstract": "High-quality distractors are crucial to both the assessment and pedagogical\nvalue of multiple-choice questions (MCQs), where manually crafting ones that\nanticipate knowledge deficiencies or misconceptions among real students is\ndifficult. Meanwhile, automated distractor generation, even with the help of\nlarge language models (LLMs), remains challenging for subjects like math. It is\ncrucial to not only identify plausible distractors but also understand the\nerror behind them. In this paper, we introduce DiVERT (Distractor Generation\nwith Variational Errors Represented as Text), a novel variational approach that\nlearns an interpretable representation of errors behind distractors in math\nMCQs. Through experiments on a real-world math MCQ dataset with 1,434 questions\nused by hundreds of thousands of students, we show that DiVERT, despite using a\nbase open-source LLM with 7B parameters, outperforms state-of-the-art\napproaches using GPT-4o on downstream distractor generation. We also conduct a\nhuman evaluation with math educators and find that DiVERT leads to error labels\nthat are of comparable quality to human-authored ones.",
        "chunk-id": 4,
        "chunk": "human evaluation with math educators and find that DiVERT leads to error labels\nthat are of comparable quality to human-authored ones.",
        "authors": [
            "Nigel Fernandez",
            "Alexander Scarlatos",
            "Simon Woodhead",
            "Andrew Lan"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:37:31+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19356v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19356v1",
        "categories": [
            "Computation and Language",
            "Computers and Society",
            "Machine Learning"
        ]
    },
    {
        "id": 30000021,
        "doi": null,
        "title": "Fundamental Problems With Model Editing: How Should Rational Belief Revision Work in LLMs?",
        "abstract": "The model editing problem concerns how language models should learn new facts\nabout the world over time. While empirical research on model editing has drawn\nwidespread attention, the conceptual foundations of model editing remain shaky\n-- perhaps unsurprisingly, since model editing is essentially belief revision,\na storied problem in philosophy that has eluded succinct solutions for decades.\nModel editing nonetheless demands a solution, since we need to be able to\ncontrol the knowledge within language models. With this goal in mind, this\npaper critiques the standard formulation of the model editing problem and\nproposes a formal testbed for model editing research. We first describe 12 open\nproblems with model editing, based on challenges with (1) defining the problem,\n(2) developing benchmarks, and (3) assuming LLMs have editable beliefs in the\nfirst place. Many of these challenges are extremely difficult to address, e.g.\ndetermining far-reaching consequences of edits, labeling probabilistic\nentailments between facts, and updating beliefs of agent simulators. Next, we\nintroduce a semi-synthetic dataset for model editing based on Wikidata, where\nwe can evaluate edits against labels given by an idealized Bayesian agent. This\nenables us to say exactly how belief revision in language models falls short of\na desirable epistemic standard. We encourage further research exploring\nsettings where such a gold standard can be compared against. Our code is\npublicly available at: https://github.com/peterbhase/LLM-belief-revision",
        "chunk-id": 1,
        "chunk": "The model editing problem concerns how language models should learn new facts\nabout the world over time. While empirical research on model editing has drawn\nwidespread attention, the conceptual foundations of model editing remain shaky\n-- perhaps unsurprisingly, since model editing is essentially belief revision,",
        "authors": [
            "Peter Hase",
            "Thomas Hofweber",
            "Xiang Zhou",
            "Elias Stengel-Eskin",
            "Mohit Bansal"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:33:03+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19354v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19354v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 30000021,
        "doi": null,
        "title": "Fundamental Problems With Model Editing: How Should Rational Belief Revision Work in LLMs?",
        "abstract": "The model editing problem concerns how language models should learn new facts\nabout the world over time. While empirical research on model editing has drawn\nwidespread attention, the conceptual foundations of model editing remain shaky\n-- perhaps unsurprisingly, since model editing is essentially belief revision,\na storied problem in philosophy that has eluded succinct solutions for decades.\nModel editing nonetheless demands a solution, since we need to be able to\ncontrol the knowledge within language models. With this goal in mind, this\npaper critiques the standard formulation of the model editing problem and\nproposes a formal testbed for model editing research. We first describe 12 open\nproblems with model editing, based on challenges with (1) defining the problem,\n(2) developing benchmarks, and (3) assuming LLMs have editable beliefs in the\nfirst place. Many of these challenges are extremely difficult to address, e.g.\ndetermining far-reaching consequences of edits, labeling probabilistic\nentailments between facts, and updating beliefs of agent simulators. Next, we\nintroduce a semi-synthetic dataset for model editing based on Wikidata, where\nwe can evaluate edits against labels given by an idealized Bayesian agent. This\nenables us to say exactly how belief revision in language models falls short of\na desirable epistemic standard. We encourage further research exploring\nsettings where such a gold standard can be compared against. Our code is\npublicly available at: https://github.com/peterbhase/LLM-belief-revision",
        "chunk-id": 2,
        "chunk": "a storied problem in philosophy that has eluded succinct solutions for decades.\nModel editing nonetheless demands a solution, since we need to be able to\ncontrol the knowledge within language models. With this goal in mind, this\npaper critiques the standard formulation of the model editing problem and\nproposes a formal testbed for model editing research. We first describe 12 open",
        "authors": [
            "Peter Hase",
            "Thomas Hofweber",
            "Xiang Zhou",
            "Elias Stengel-Eskin",
            "Mohit Bansal"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:33:03+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19354v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19354v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 30000021,
        "doi": null,
        "title": "Fundamental Problems With Model Editing: How Should Rational Belief Revision Work in LLMs?",
        "abstract": "The model editing problem concerns how language models should learn new facts\nabout the world over time. While empirical research on model editing has drawn\nwidespread attention, the conceptual foundations of model editing remain shaky\n-- perhaps unsurprisingly, since model editing is essentially belief revision,\na storied problem in philosophy that has eluded succinct solutions for decades.\nModel editing nonetheless demands a solution, since we need to be able to\ncontrol the knowledge within language models. With this goal in mind, this\npaper critiques the standard formulation of the model editing problem and\nproposes a formal testbed for model editing research. We first describe 12 open\nproblems with model editing, based on challenges with (1) defining the problem,\n(2) developing benchmarks, and (3) assuming LLMs have editable beliefs in the\nfirst place. Many of these challenges are extremely difficult to address, e.g.\ndetermining far-reaching consequences of edits, labeling probabilistic\nentailments between facts, and updating beliefs of agent simulators. Next, we\nintroduce a semi-synthetic dataset for model editing based on Wikidata, where\nwe can evaluate edits against labels given by an idealized Bayesian agent. This\nenables us to say exactly how belief revision in language models falls short of\na desirable epistemic standard. We encourage further research exploring\nsettings where such a gold standard can be compared against. Our code is\npublicly available at: https://github.com/peterbhase/LLM-belief-revision",
        "chunk-id": 3,
        "chunk": "problems with model editing, based on challenges with (1) defining the problem,\n(2) developing benchmarks, and (3) assuming LLMs have editable beliefs in the\nfirst place. Many of these challenges are extremely difficult to address, e.g.\ndetermining far-reaching consequences of edits, labeling probabilistic",
        "authors": [
            "Peter Hase",
            "Thomas Hofweber",
            "Xiang Zhou",
            "Elias Stengel-Eskin",
            "Mohit Bansal"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:33:03+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19354v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19354v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 30000021,
        "doi": null,
        "title": "Fundamental Problems With Model Editing: How Should Rational Belief Revision Work in LLMs?",
        "abstract": "The model editing problem concerns how language models should learn new facts\nabout the world over time. While empirical research on model editing has drawn\nwidespread attention, the conceptual foundations of model editing remain shaky\n-- perhaps unsurprisingly, since model editing is essentially belief revision,\na storied problem in philosophy that has eluded succinct solutions for decades.\nModel editing nonetheless demands a solution, since we need to be able to\ncontrol the knowledge within language models. With this goal in mind, this\npaper critiques the standard formulation of the model editing problem and\nproposes a formal testbed for model editing research. We first describe 12 open\nproblems with model editing, based on challenges with (1) defining the problem,\n(2) developing benchmarks, and (3) assuming LLMs have editable beliefs in the\nfirst place. Many of these challenges are extremely difficult to address, e.g.\ndetermining far-reaching consequences of edits, labeling probabilistic\nentailments between facts, and updating beliefs of agent simulators. Next, we\nintroduce a semi-synthetic dataset for model editing based on Wikidata, where\nwe can evaluate edits against labels given by an idealized Bayesian agent. This\nenables us to say exactly how belief revision in language models falls short of\na desirable epistemic standard. We encourage further research exploring\nsettings where such a gold standard can be compared against. Our code is\npublicly available at: https://github.com/peterbhase/LLM-belief-revision",
        "chunk-id": 4,
        "chunk": "entailments between facts, and updating beliefs of agent simulators. Next, we\nintroduce a semi-synthetic dataset for model editing based on Wikidata, where\nwe can evaluate edits against labels given by an idealized Bayesian agent. This\nenables us to say exactly how belief revision in language models falls short of",
        "authors": [
            "Peter Hase",
            "Thomas Hofweber",
            "Xiang Zhou",
            "Elias Stengel-Eskin",
            "Mohit Bansal"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:33:03+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19354v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19354v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 30000021,
        "doi": null,
        "title": "Fundamental Problems With Model Editing: How Should Rational Belief Revision Work in LLMs?",
        "abstract": "The model editing problem concerns how language models should learn new facts\nabout the world over time. While empirical research on model editing has drawn\nwidespread attention, the conceptual foundations of model editing remain shaky\n-- perhaps unsurprisingly, since model editing is essentially belief revision,\na storied problem in philosophy that has eluded succinct solutions for decades.\nModel editing nonetheless demands a solution, since we need to be able to\ncontrol the knowledge within language models. With this goal in mind, this\npaper critiques the standard formulation of the model editing problem and\nproposes a formal testbed for model editing research. We first describe 12 open\nproblems with model editing, based on challenges with (1) defining the problem,\n(2) developing benchmarks, and (3) assuming LLMs have editable beliefs in the\nfirst place. Many of these challenges are extremely difficult to address, e.g.\ndetermining far-reaching consequences of edits, labeling probabilistic\nentailments between facts, and updating beliefs of agent simulators. Next, we\nintroduce a semi-synthetic dataset for model editing based on Wikidata, where\nwe can evaluate edits against labels given by an idealized Bayesian agent. This\nenables us to say exactly how belief revision in language models falls short of\na desirable epistemic standard. We encourage further research exploring\nsettings where such a gold standard can be compared against. Our code is\npublicly available at: https://github.com/peterbhase/LLM-belief-revision",
        "chunk-id": 5,
        "chunk": "a desirable epistemic standard. We encourage further research exploring\nsettings where such a gold standard can be compared against. Our code is\npublicly available at: https://github.com/peterbhase/LLM-belief-revision",
        "authors": [
            "Peter Hase",
            "Thomas Hofweber",
            "Xiang Zhou",
            "Elias Stengel-Eskin",
            "Mohit Bansal"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:33:03+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19354v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19354v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 30000022,
        "doi": null,
        "title": "CORE4D: A 4D Human-Object-Human Interaction Dataset for Collaborative Object REarrangement",
        "abstract": "Understanding how humans cooperatively rearrange household objects is\ncritical for VR/AR and human-robot interaction. However, in-depth studies on\nmodeling these behaviors are under-researched due to the lack of relevant\ndatasets. We fill this gap by presenting CORE4D, a novel large-scale 4D\nhuman-object-human interaction dataset focusing on collaborative object\nrearrangement, which encompasses diverse compositions of various object\ngeometries, collaboration modes, and 3D scenes. With 1K human-object-human\nmotion sequences captured in the real world, we enrich CORE4D by contributing\nan iterative collaboration retargeting strategy to augment motions to a variety\nof novel objects. Leveraging this approach, CORE4D comprises a total of 11K\ncollaboration sequences spanning 3K real and virtual object shapes. Benefiting\nfrom extensive motion patterns provided by CORE4D, we benchmark two tasks\naiming at generating human-object interaction: human-object motion forecasting\nand interaction synthesis. Extensive experiments demonstrate the effectiveness\nof our collaboration retargeting strategy and indicate that CORE4D has posed\nnew challenges to existing human-object interaction generation methodologies.\nOur dataset and code are available at\nhttps://github.com/leolyliu/CORE4D-Instructions.",
        "chunk-id": 1,
        "chunk": "Understanding how humans cooperatively rearrange household objects is\ncritical for VR/AR and human-robot interaction. However, in-depth studies on\nmodeling these behaviors are under-researched due to the lack of relevant\ndatasets. We fill this gap by presenting CORE4D, a novel large-scale 4D\nhuman-object-human interaction dataset focusing on collaborative object",
        "authors": [
            "Chengwen Zhang",
            "Yun Liu",
            "Ruofan Xing",
            "Bingda Tang",
            "Li Yi"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:32:18+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19353v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19353v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000022,
        "doi": null,
        "title": "CORE4D: A 4D Human-Object-Human Interaction Dataset for Collaborative Object REarrangement",
        "abstract": "Understanding how humans cooperatively rearrange household objects is\ncritical for VR/AR and human-robot interaction. However, in-depth studies on\nmodeling these behaviors are under-researched due to the lack of relevant\ndatasets. We fill this gap by presenting CORE4D, a novel large-scale 4D\nhuman-object-human interaction dataset focusing on collaborative object\nrearrangement, which encompasses diverse compositions of various object\ngeometries, collaboration modes, and 3D scenes. With 1K human-object-human\nmotion sequences captured in the real world, we enrich CORE4D by contributing\nan iterative collaboration retargeting strategy to augment motions to a variety\nof novel objects. Leveraging this approach, CORE4D comprises a total of 11K\ncollaboration sequences spanning 3K real and virtual object shapes. Benefiting\nfrom extensive motion patterns provided by CORE4D, we benchmark two tasks\naiming at generating human-object interaction: human-object motion forecasting\nand interaction synthesis. Extensive experiments demonstrate the effectiveness\nof our collaboration retargeting strategy and indicate that CORE4D has posed\nnew challenges to existing human-object interaction generation methodologies.\nOur dataset and code are available at\nhttps://github.com/leolyliu/CORE4D-Instructions.",
        "chunk-id": 2,
        "chunk": "rearrangement, which encompasses diverse compositions of various object\ngeometries, collaboration modes, and 3D scenes. With 1K human-object-human\nmotion sequences captured in the real world, we enrich CORE4D by contributing\nan iterative collaboration retargeting strategy to augment motions to a variety\nof novel objects. Leveraging this approach, CORE4D comprises a total of 11K",
        "authors": [
            "Chengwen Zhang",
            "Yun Liu",
            "Ruofan Xing",
            "Bingda Tang",
            "Li Yi"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:32:18+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19353v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19353v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000022,
        "doi": null,
        "title": "CORE4D: A 4D Human-Object-Human Interaction Dataset for Collaborative Object REarrangement",
        "abstract": "Understanding how humans cooperatively rearrange household objects is\ncritical for VR/AR and human-robot interaction. However, in-depth studies on\nmodeling these behaviors are under-researched due to the lack of relevant\ndatasets. We fill this gap by presenting CORE4D, a novel large-scale 4D\nhuman-object-human interaction dataset focusing on collaborative object\nrearrangement, which encompasses diverse compositions of various object\ngeometries, collaboration modes, and 3D scenes. With 1K human-object-human\nmotion sequences captured in the real world, we enrich CORE4D by contributing\nan iterative collaboration retargeting strategy to augment motions to a variety\nof novel objects. Leveraging this approach, CORE4D comprises a total of 11K\ncollaboration sequences spanning 3K real and virtual object shapes. Benefiting\nfrom extensive motion patterns provided by CORE4D, we benchmark two tasks\naiming at generating human-object interaction: human-object motion forecasting\nand interaction synthesis. Extensive experiments demonstrate the effectiveness\nof our collaboration retargeting strategy and indicate that CORE4D has posed\nnew challenges to existing human-object interaction generation methodologies.\nOur dataset and code are available at\nhttps://github.com/leolyliu/CORE4D-Instructions.",
        "chunk-id": 3,
        "chunk": "collaboration sequences spanning 3K real and virtual object shapes. Benefiting\nfrom extensive motion patterns provided by CORE4D, we benchmark two tasks\naiming at generating human-object interaction: human-object motion forecasting\nand interaction synthesis. Extensive experiments demonstrate the effectiveness",
        "authors": [
            "Chengwen Zhang",
            "Yun Liu",
            "Ruofan Xing",
            "Bingda Tang",
            "Li Yi"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:32:18+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19353v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19353v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000022,
        "doi": null,
        "title": "CORE4D: A 4D Human-Object-Human Interaction Dataset for Collaborative Object REarrangement",
        "abstract": "Understanding how humans cooperatively rearrange household objects is\ncritical for VR/AR and human-robot interaction. However, in-depth studies on\nmodeling these behaviors are under-researched due to the lack of relevant\ndatasets. We fill this gap by presenting CORE4D, a novel large-scale 4D\nhuman-object-human interaction dataset focusing on collaborative object\nrearrangement, which encompasses diverse compositions of various object\ngeometries, collaboration modes, and 3D scenes. With 1K human-object-human\nmotion sequences captured in the real world, we enrich CORE4D by contributing\nan iterative collaboration retargeting strategy to augment motions to a variety\nof novel objects. Leveraging this approach, CORE4D comprises a total of 11K\ncollaboration sequences spanning 3K real and virtual object shapes. Benefiting\nfrom extensive motion patterns provided by CORE4D, we benchmark two tasks\naiming at generating human-object interaction: human-object motion forecasting\nand interaction synthesis. Extensive experiments demonstrate the effectiveness\nof our collaboration retargeting strategy and indicate that CORE4D has posed\nnew challenges to existing human-object interaction generation methodologies.\nOur dataset and code are available at\nhttps://github.com/leolyliu/CORE4D-Instructions.",
        "chunk-id": 4,
        "chunk": "of our collaboration retargeting strategy and indicate that CORE4D has posed\nnew challenges to existing human-object interaction generation methodologies.\nOur dataset and code are available at\nhttps://github.com/leolyliu/CORE4D-Instructions.",
        "authors": [
            "Chengwen Zhang",
            "Yun Liu",
            "Ruofan Xing",
            "Bingda Tang",
            "Li Yi"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:32:18+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19353v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19353v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000023,
        "doi": null,
        "title": "Periodic phenomena in equivariant stable homotopy theory",
        "abstract": "Building off of many recent advances in the subject by many different\nresearchers, we describe a picture of A-equivariant chromatic homotopy theory\nwhich mirrors the now classical non-equivariant picture of Morava,\nMiller-Ravenel-Wilson, and Devinatz-Hopkins-Smith, where A is a finite abelian\np-group. Specifically, we review the structure of the Balmer spectrum of the\ncategory of A-spectra, and the work of Hausmann-Meier connecting this to MU_A\nand equivariant formal group laws. Generalizing work of\nBhattacharya-Guillou-Li, we introduce equivariant analogs of v_n-self maps, and\ngeneralizing work of Carrick and Balderrama, we introduce equivariant analogs\nof the chromatic tower, and give equivariant analogs of the smash product and\nchromatic convergence theorems. The equivariant monochromatic theory is also\ndiscussed. We explore computational examples of this theory in the case of A =\nC_2, where we connect equivariant chromatic theory with redshift phenomena in\nMahowald invariants.",
        "chunk-id": 1,
        "chunk": "Building off of many recent advances in the subject by many different\nresearchers, we describe a picture of A-equivariant chromatic homotopy theory\nwhich mirrors the now classical non-equivariant picture of Morava,\nMiller-Ravenel-Wilson, and Devinatz-Hopkins-Smith, where A is a finite abelian\np-group. Specifically, we review the structure of the Balmer spectrum of the",
        "authors": [
            "Mark Behrens",
            "Jack Carlisle"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:31:39+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19352v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19352v1",
        "categories": [
            "Algebraic Topology"
        ]
    },
    {
        "id": 30000023,
        "doi": null,
        "title": "Periodic phenomena in equivariant stable homotopy theory",
        "abstract": "Building off of many recent advances in the subject by many different\nresearchers, we describe a picture of A-equivariant chromatic homotopy theory\nwhich mirrors the now classical non-equivariant picture of Morava,\nMiller-Ravenel-Wilson, and Devinatz-Hopkins-Smith, where A is a finite abelian\np-group. Specifically, we review the structure of the Balmer spectrum of the\ncategory of A-spectra, and the work of Hausmann-Meier connecting this to MU_A\nand equivariant formal group laws. Generalizing work of\nBhattacharya-Guillou-Li, we introduce equivariant analogs of v_n-self maps, and\ngeneralizing work of Carrick and Balderrama, we introduce equivariant analogs\nof the chromatic tower, and give equivariant analogs of the smash product and\nchromatic convergence theorems. The equivariant monochromatic theory is also\ndiscussed. We explore computational examples of this theory in the case of A =\nC_2, where we connect equivariant chromatic theory with redshift phenomena in\nMahowald invariants.",
        "chunk-id": 2,
        "chunk": "category of A-spectra, and the work of Hausmann-Meier connecting this to MU_A\nand equivariant formal group laws. Generalizing work of\nBhattacharya-Guillou-Li, we introduce equivariant analogs of v_n-self maps, and\ngeneralizing work of Carrick and Balderrama, we introduce equivariant analogs\nof the chromatic tower, and give equivariant analogs of the smash product and",
        "authors": [
            "Mark Behrens",
            "Jack Carlisle"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:31:39+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19352v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19352v1",
        "categories": [
            "Algebraic Topology"
        ]
    },
    {
        "id": 30000023,
        "doi": null,
        "title": "Periodic phenomena in equivariant stable homotopy theory",
        "abstract": "Building off of many recent advances in the subject by many different\nresearchers, we describe a picture of A-equivariant chromatic homotopy theory\nwhich mirrors the now classical non-equivariant picture of Morava,\nMiller-Ravenel-Wilson, and Devinatz-Hopkins-Smith, where A is a finite abelian\np-group. Specifically, we review the structure of the Balmer spectrum of the\ncategory of A-spectra, and the work of Hausmann-Meier connecting this to MU_A\nand equivariant formal group laws. Generalizing work of\nBhattacharya-Guillou-Li, we introduce equivariant analogs of v_n-self maps, and\ngeneralizing work of Carrick and Balderrama, we introduce equivariant analogs\nof the chromatic tower, and give equivariant analogs of the smash product and\nchromatic convergence theorems. The equivariant monochromatic theory is also\ndiscussed. We explore computational examples of this theory in the case of A =\nC_2, where we connect equivariant chromatic theory with redshift phenomena in\nMahowald invariants.",
        "chunk-id": 3,
        "chunk": "chromatic convergence theorems. The equivariant monochromatic theory is also\ndiscussed. We explore computational examples of this theory in the case of A =\nC_2, where we connect equivariant chromatic theory with redshift phenomena in\nMahowald invariants.",
        "authors": [
            "Mark Behrens",
            "Jack Carlisle"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:31:39+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19352v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19352v1",
        "categories": [
            "Algebraic Topology"
        ]
    },
    {
        "id": 30000024,
        "doi": null,
        "title": "Dynamical Analysis of Autobidding Systems",
        "abstract": "It has become the default in markets such as ad auctions for participants to\nbid in an auction through automated bidding agents (autobidders) which adjust\nbids over time to satisfy return-over-spend constraints. Despite the prominence\nof such systems for the internet economy, their resulting dynamical behavior is\nstill not well understood. Although one might hope that such relatively simple\nsystems would typically converge to the equilibria of their underlying\nauctions, we provide a plethora of results that show the emergence of complex\nbehavior, such as bi-stability, periodic orbits and quasi periodicity. We\nempirically observe how the market structure (expressed as motifs)\nqualitatively affects the behavior of the dynamics. We complement it with\ntheoretical results showing that autobidding systems can simulate both linear\ndynamical systems as well logical boolean gates.",
        "chunk-id": 1,
        "chunk": "It has become the default in markets such as ad auctions for participants to\nbid in an auction through automated bidding agents (autobidders) which adjust\nbids over time to satisfy return-over-spend constraints. Despite the prominence\nof such systems for the internet economy, their resulting dynamical behavior is",
        "authors": [
            "Renato Paes Leme",
            "Georgios Piliouras",
            "Jon Schneider",
            "Kelly Spendlove",
            "Song Zuo"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:30:10+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19350v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19350v1",
        "categories": [
            "Computer Science and Game Theory"
        ]
    },
    {
        "id": 30000024,
        "doi": null,
        "title": "Dynamical Analysis of Autobidding Systems",
        "abstract": "It has become the default in markets such as ad auctions for participants to\nbid in an auction through automated bidding agents (autobidders) which adjust\nbids over time to satisfy return-over-spend constraints. Despite the prominence\nof such systems for the internet economy, their resulting dynamical behavior is\nstill not well understood. Although one might hope that such relatively simple\nsystems would typically converge to the equilibria of their underlying\nauctions, we provide a plethora of results that show the emergence of complex\nbehavior, such as bi-stability, periodic orbits and quasi periodicity. We\nempirically observe how the market structure (expressed as motifs)\nqualitatively affects the behavior of the dynamics. We complement it with\ntheoretical results showing that autobidding systems can simulate both linear\ndynamical systems as well logical boolean gates.",
        "chunk-id": 2,
        "chunk": "still not well understood. Although one might hope that such relatively simple\nsystems would typically converge to the equilibria of their underlying\nauctions, we provide a plethora of results that show the emergence of complex\nbehavior, such as bi-stability, periodic orbits and quasi periodicity. We\nempirically observe how the market structure (expressed as motifs)",
        "authors": [
            "Renato Paes Leme",
            "Georgios Piliouras",
            "Jon Schneider",
            "Kelly Spendlove",
            "Song Zuo"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:30:10+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19350v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19350v1",
        "categories": [
            "Computer Science and Game Theory"
        ]
    },
    {
        "id": 30000024,
        "doi": null,
        "title": "Dynamical Analysis of Autobidding Systems",
        "abstract": "It has become the default in markets such as ad auctions for participants to\nbid in an auction through automated bidding agents (autobidders) which adjust\nbids over time to satisfy return-over-spend constraints. Despite the prominence\nof such systems for the internet economy, their resulting dynamical behavior is\nstill not well understood. Although one might hope that such relatively simple\nsystems would typically converge to the equilibria of their underlying\nauctions, we provide a plethora of results that show the emergence of complex\nbehavior, such as bi-stability, periodic orbits and quasi periodicity. We\nempirically observe how the market structure (expressed as motifs)\nqualitatively affects the behavior of the dynamics. We complement it with\ntheoretical results showing that autobidding systems can simulate both linear\ndynamical systems as well logical boolean gates.",
        "chunk-id": 3,
        "chunk": "qualitatively affects the behavior of the dynamics. We complement it with\ntheoretical results showing that autobidding systems can simulate both linear\ndynamical systems as well logical boolean gates.",
        "authors": [
            "Renato Paes Leme",
            "Georgios Piliouras",
            "Jon Schneider",
            "Kelly Spendlove",
            "Song Zuo"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:30:10+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19350v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19350v1",
        "categories": [
            "Computer Science and Game Theory"
        ]
    },
    {
        "id": 30000025,
        "doi": null,
        "title": "IndoToxic2024: A Demographically-Enriched Dataset of Hate Speech and Toxicity Types for Indonesian Language",
        "abstract": "Hate speech poses a significant threat to social harmony. Over the past two\nyears, Indonesia has seen a ten-fold increase in the online hate speech ratio,\nunderscoring the urgent need for effective detection mechanisms. However,\nprogress is hindered by the limited availability of labeled data for Indonesian\ntexts. The condition is even worse for marginalized minorities, such as Shia,\nLGBTQ, and other ethnic minorities because hate speech is underreported and\nless understood by detection tools. Furthermore, the lack of accommodation for\nsubjectivity in current datasets compounds this issue. To address this, we\nintroduce IndoToxic2024, a comprehensive Indonesian hate speech and toxicity\nclassification dataset. Comprising 43,692 entries annotated by 19 diverse\nindividuals, the dataset focuses on texts targeting vulnerable groups in\nIndonesia, specifically during the hottest political event in the country: the\npresidential election. We establish baselines for seven binary classification\ntasks, achieving a macro-F1 score of 0.78 with a BERT model (IndoBERTweet)\nfine-tuned for hate speech classification. Furthermore, we demonstrate how\nincorporating demographic information can enhance the zero-shot performance of\nthe large language model, gpt-3.5-turbo. However, we also caution that an\noveremphasis on demographic information can negatively impact the fine-tuned\nmodel performance due to data fragmentation.",
        "chunk-id": 1,
        "chunk": "Hate speech poses a significant threat to social harmony. Over the past two\nyears, Indonesia has seen a ten-fold increase in the online hate speech ratio,\nunderscoring the urgent need for effective detection mechanisms. However,\nprogress is hindered by the limited availability of labeled data for Indonesian",
        "authors": [
            "Lucky Susanto",
            "Musa Izzanardi Wijanarko",
            "Prasetia Anugrah Pratama",
            "Traci Hong",
            "Ika Idris",
            "Alham Fikri Aji",
            "Derry Wijaya"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:26:38+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19349v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19349v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 30000025,
        "doi": null,
        "title": "IndoToxic2024: A Demographically-Enriched Dataset of Hate Speech and Toxicity Types for Indonesian Language",
        "abstract": "Hate speech poses a significant threat to social harmony. Over the past two\nyears, Indonesia has seen a ten-fold increase in the online hate speech ratio,\nunderscoring the urgent need for effective detection mechanisms. However,\nprogress is hindered by the limited availability of labeled data for Indonesian\ntexts. The condition is even worse for marginalized minorities, such as Shia,\nLGBTQ, and other ethnic minorities because hate speech is underreported and\nless understood by detection tools. Furthermore, the lack of accommodation for\nsubjectivity in current datasets compounds this issue. To address this, we\nintroduce IndoToxic2024, a comprehensive Indonesian hate speech and toxicity\nclassification dataset. Comprising 43,692 entries annotated by 19 diverse\nindividuals, the dataset focuses on texts targeting vulnerable groups in\nIndonesia, specifically during the hottest political event in the country: the\npresidential election. We establish baselines for seven binary classification\ntasks, achieving a macro-F1 score of 0.78 with a BERT model (IndoBERTweet)\nfine-tuned for hate speech classification. Furthermore, we demonstrate how\nincorporating demographic information can enhance the zero-shot performance of\nthe large language model, gpt-3.5-turbo. However, we also caution that an\noveremphasis on demographic information can negatively impact the fine-tuned\nmodel performance due to data fragmentation.",
        "chunk-id": 2,
        "chunk": "texts. The condition is even worse for marginalized minorities, such as Shia,\nLGBTQ, and other ethnic minorities because hate speech is underreported and\nless understood by detection tools. Furthermore, the lack of accommodation for\nsubjectivity in current datasets compounds this issue. To address this, we\nintroduce IndoToxic2024, a comprehensive Indonesian hate speech and toxicity",
        "authors": [
            "Lucky Susanto",
            "Musa Izzanardi Wijanarko",
            "Prasetia Anugrah Pratama",
            "Traci Hong",
            "Ika Idris",
            "Alham Fikri Aji",
            "Derry Wijaya"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:26:38+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19349v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19349v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 30000025,
        "doi": null,
        "title": "IndoToxic2024: A Demographically-Enriched Dataset of Hate Speech and Toxicity Types for Indonesian Language",
        "abstract": "Hate speech poses a significant threat to social harmony. Over the past two\nyears, Indonesia has seen a ten-fold increase in the online hate speech ratio,\nunderscoring the urgent need for effective detection mechanisms. However,\nprogress is hindered by the limited availability of labeled data for Indonesian\ntexts. The condition is even worse for marginalized minorities, such as Shia,\nLGBTQ, and other ethnic minorities because hate speech is underreported and\nless understood by detection tools. Furthermore, the lack of accommodation for\nsubjectivity in current datasets compounds this issue. To address this, we\nintroduce IndoToxic2024, a comprehensive Indonesian hate speech and toxicity\nclassification dataset. Comprising 43,692 entries annotated by 19 diverse\nindividuals, the dataset focuses on texts targeting vulnerable groups in\nIndonesia, specifically during the hottest political event in the country: the\npresidential election. We establish baselines for seven binary classification\ntasks, achieving a macro-F1 score of 0.78 with a BERT model (IndoBERTweet)\nfine-tuned for hate speech classification. Furthermore, we demonstrate how\nincorporating demographic information can enhance the zero-shot performance of\nthe large language model, gpt-3.5-turbo. However, we also caution that an\noveremphasis on demographic information can negatively impact the fine-tuned\nmodel performance due to data fragmentation.",
        "chunk-id": 3,
        "chunk": "classification dataset. Comprising 43,692 entries annotated by 19 diverse\nindividuals, the dataset focuses on texts targeting vulnerable groups in\nIndonesia, specifically during the hottest political event in the country: the\npresidential election. We establish baselines for seven binary classification\ntasks, achieving a macro-F1 score of 0.78 with a BERT model (IndoBERTweet)",
        "authors": [
            "Lucky Susanto",
            "Musa Izzanardi Wijanarko",
            "Prasetia Anugrah Pratama",
            "Traci Hong",
            "Ika Idris",
            "Alham Fikri Aji",
            "Derry Wijaya"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:26:38+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19349v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19349v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 30000025,
        "doi": null,
        "title": "IndoToxic2024: A Demographically-Enriched Dataset of Hate Speech and Toxicity Types for Indonesian Language",
        "abstract": "Hate speech poses a significant threat to social harmony. Over the past two\nyears, Indonesia has seen a ten-fold increase in the online hate speech ratio,\nunderscoring the urgent need for effective detection mechanisms. However,\nprogress is hindered by the limited availability of labeled data for Indonesian\ntexts. The condition is even worse for marginalized minorities, such as Shia,\nLGBTQ, and other ethnic minorities because hate speech is underreported and\nless understood by detection tools. Furthermore, the lack of accommodation for\nsubjectivity in current datasets compounds this issue. To address this, we\nintroduce IndoToxic2024, a comprehensive Indonesian hate speech and toxicity\nclassification dataset. Comprising 43,692 entries annotated by 19 diverse\nindividuals, the dataset focuses on texts targeting vulnerable groups in\nIndonesia, specifically during the hottest political event in the country: the\npresidential election. We establish baselines for seven binary classification\ntasks, achieving a macro-F1 score of 0.78 with a BERT model (IndoBERTweet)\nfine-tuned for hate speech classification. Furthermore, we demonstrate how\nincorporating demographic information can enhance the zero-shot performance of\nthe large language model, gpt-3.5-turbo. However, we also caution that an\noveremphasis on demographic information can negatively impact the fine-tuned\nmodel performance due to data fragmentation.",
        "chunk-id": 4,
        "chunk": "fine-tuned for hate speech classification. Furthermore, we demonstrate how\nincorporating demographic information can enhance the zero-shot performance of\nthe large language model, gpt-3.5-turbo. However, we also caution that an\noveremphasis on demographic information can negatively impact the fine-tuned\nmodel performance due to data fragmentation.",
        "authors": [
            "Lucky Susanto",
            "Musa Izzanardi Wijanarko",
            "Prasetia Anugrah Pratama",
            "Traci Hong",
            "Ika Idris",
            "Alham Fikri Aji",
            "Derry Wijaya"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:26:38+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19349v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19349v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 30000026,
        "doi": null,
        "title": "Unconditional Stability Analysis of N-Port Networks Based on Structured Singular Value Computation",
        "abstract": "In this paper, a novel approach based on robust stability concepts and tools\nis introduced to evaluate the unconditional stability of microwave active\n$\\textit{n}$-port devices. An efficient calculation of the Structured Singular\nValue of the $\\textit{n}$x$\\textit{n}$ scattering matrix is proposed to obtain\nthe stability characteristics of the device. The presented method is validated\nin two ways. First, it is applied to a referential 4x4 scattering parameter set\nfor independent verification. Second, the method is applied to a 4-port GaAs\nFET amplifier fabricated in hybrid technology. The results confirm the validity\nand computational efficiency of the proposed approach.",
        "chunk-id": 1,
        "chunk": "In this paper, a novel approach based on robust stability concepts and tools\nis introduced to evaluate the unconditional stability of microwave active\n$\\textit{n}$-port devices. An efficient calculation of the Structured Singular\nValue of the $\\textit{n}$x$\\textit{n}$ scattering matrix is proposed to obtain",
        "authors": [
            "Aimar Mateo",
            "Ibone Lizarraga",
            "Jorge Terrer",
            "Aitziber Anakabe",
            "J. M Collantes"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:16:49+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19342v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19342v1",
        "categories": [
            "Systems and Control",
            "Systems and Control"
        ]
    },
    {
        "id": 30000026,
        "doi": null,
        "title": "Unconditional Stability Analysis of N-Port Networks Based on Structured Singular Value Computation",
        "abstract": "In this paper, a novel approach based on robust stability concepts and tools\nis introduced to evaluate the unconditional stability of microwave active\n$\\textit{n}$-port devices. An efficient calculation of the Structured Singular\nValue of the $\\textit{n}$x$\\textit{n}$ scattering matrix is proposed to obtain\nthe stability characteristics of the device. The presented method is validated\nin two ways. First, it is applied to a referential 4x4 scattering parameter set\nfor independent verification. Second, the method is applied to a 4-port GaAs\nFET amplifier fabricated in hybrid technology. The results confirm the validity\nand computational efficiency of the proposed approach.",
        "chunk-id": 2,
        "chunk": "the stability characteristics of the device. The presented method is validated\nin two ways. First, it is applied to a referential 4x4 scattering parameter set\nfor independent verification. Second, the method is applied to a 4-port GaAs\nFET amplifier fabricated in hybrid technology. The results confirm the validity\nand computational efficiency of the proposed approach.",
        "authors": [
            "Aimar Mateo",
            "Ibone Lizarraga",
            "Jorge Terrer",
            "Aitziber Anakabe",
            "J. M Collantes"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:16:49+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19342v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19342v1",
        "categories": [
            "Systems and Control",
            "Systems and Control"
        ]
    },
    {
        "id": 30000027,
        "doi": null,
        "title": "Learning Visual Conditioning Tokens to Correct Domain Shift for Fully Test-time Adaptation",
        "abstract": "Fully test-time adaptation aims to adapt the network model based on\nsequential analysis of input samples during the inference stage to address the\ncross-domain performance degradation problem of deep neural networks. This work\nis based on the following interesting finding: in transformer-based image\nclassification, the class token at the first transformer encoder layer can be\nlearned to capture the domain-specific characteristics of target samples during\ntest-time adaptation. This learned token, when combined with input image patch\nembeddings, is able to gradually remove the domain-specific information from\nthe feature representations of input samples during the transformer encoding\nprocess, thereby significantly improving the test-time adaptation performance\nof the source model across different domains. We refer to this class token as\nvisual conditioning token (VCT). To successfully learn the VCT, we propose a\nbi-level learning approach to capture the long-term variations of\ndomain-specific characteristics while accommodating local variations of\ninstance-specific characteristics. Experimental results on the benchmark\ndatasets demonstrate that our proposed bi-level visual conditioning token\nlearning method is able to achieve significantly improved test-time adaptation\nperformance by up to 1.9%.",
        "chunk-id": 1,
        "chunk": "Fully test-time adaptation aims to adapt the network model based on\nsequential analysis of input samples during the inference stage to address the\ncross-domain performance degradation problem of deep neural networks. This work\nis based on the following interesting finding: in transformer-based image\nclassification, the class token at the first transformer encoder layer can be",
        "authors": [
            "Yushun Tang",
            "Shuoshuo Chen",
            "Zhehan Kan",
            "Yi Zhang",
            "Qinghai Guo",
            "Zhihai He"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:16:23+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19341v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19341v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000027,
        "doi": null,
        "title": "Learning Visual Conditioning Tokens to Correct Domain Shift for Fully Test-time Adaptation",
        "abstract": "Fully test-time adaptation aims to adapt the network model based on\nsequential analysis of input samples during the inference stage to address the\ncross-domain performance degradation problem of deep neural networks. This work\nis based on the following interesting finding: in transformer-based image\nclassification, the class token at the first transformer encoder layer can be\nlearned to capture the domain-specific characteristics of target samples during\ntest-time adaptation. This learned token, when combined with input image patch\nembeddings, is able to gradually remove the domain-specific information from\nthe feature representations of input samples during the transformer encoding\nprocess, thereby significantly improving the test-time adaptation performance\nof the source model across different domains. We refer to this class token as\nvisual conditioning token (VCT). To successfully learn the VCT, we propose a\nbi-level learning approach to capture the long-term variations of\ndomain-specific characteristics while accommodating local variations of\ninstance-specific characteristics. Experimental results on the benchmark\ndatasets demonstrate that our proposed bi-level visual conditioning token\nlearning method is able to achieve significantly improved test-time adaptation\nperformance by up to 1.9%.",
        "chunk-id": 2,
        "chunk": "learned to capture the domain-specific characteristics of target samples during\ntest-time adaptation. This learned token, when combined with input image patch\nembeddings, is able to gradually remove the domain-specific information from\nthe feature representations of input samples during the transformer encoding",
        "authors": [
            "Yushun Tang",
            "Shuoshuo Chen",
            "Zhehan Kan",
            "Yi Zhang",
            "Qinghai Guo",
            "Zhihai He"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:16:23+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19341v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19341v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000027,
        "doi": null,
        "title": "Learning Visual Conditioning Tokens to Correct Domain Shift for Fully Test-time Adaptation",
        "abstract": "Fully test-time adaptation aims to adapt the network model based on\nsequential analysis of input samples during the inference stage to address the\ncross-domain performance degradation problem of deep neural networks. This work\nis based on the following interesting finding: in transformer-based image\nclassification, the class token at the first transformer encoder layer can be\nlearned to capture the domain-specific characteristics of target samples during\ntest-time adaptation. This learned token, when combined with input image patch\nembeddings, is able to gradually remove the domain-specific information from\nthe feature representations of input samples during the transformer encoding\nprocess, thereby significantly improving the test-time adaptation performance\nof the source model across different domains. We refer to this class token as\nvisual conditioning token (VCT). To successfully learn the VCT, we propose a\nbi-level learning approach to capture the long-term variations of\ndomain-specific characteristics while accommodating local variations of\ninstance-specific characteristics. Experimental results on the benchmark\ndatasets demonstrate that our proposed bi-level visual conditioning token\nlearning method is able to achieve significantly improved test-time adaptation\nperformance by up to 1.9%.",
        "chunk-id": 3,
        "chunk": "process, thereby significantly improving the test-time adaptation performance\nof the source model across different domains. We refer to this class token as\nvisual conditioning token (VCT). To successfully learn the VCT, we propose a\nbi-level learning approach to capture the long-term variations of\ndomain-specific characteristics while accommodating local variations of",
        "authors": [
            "Yushun Tang",
            "Shuoshuo Chen",
            "Zhehan Kan",
            "Yi Zhang",
            "Qinghai Guo",
            "Zhihai He"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:16:23+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19341v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19341v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000027,
        "doi": null,
        "title": "Learning Visual Conditioning Tokens to Correct Domain Shift for Fully Test-time Adaptation",
        "abstract": "Fully test-time adaptation aims to adapt the network model based on\nsequential analysis of input samples during the inference stage to address the\ncross-domain performance degradation problem of deep neural networks. This work\nis based on the following interesting finding: in transformer-based image\nclassification, the class token at the first transformer encoder layer can be\nlearned to capture the domain-specific characteristics of target samples during\ntest-time adaptation. This learned token, when combined with input image patch\nembeddings, is able to gradually remove the domain-specific information from\nthe feature representations of input samples during the transformer encoding\nprocess, thereby significantly improving the test-time adaptation performance\nof the source model across different domains. We refer to this class token as\nvisual conditioning token (VCT). To successfully learn the VCT, we propose a\nbi-level learning approach to capture the long-term variations of\ndomain-specific characteristics while accommodating local variations of\ninstance-specific characteristics. Experimental results on the benchmark\ndatasets demonstrate that our proposed bi-level visual conditioning token\nlearning method is able to achieve significantly improved test-time adaptation\nperformance by up to 1.9%.",
        "chunk-id": 4,
        "chunk": "instance-specific characteristics. Experimental results on the benchmark\ndatasets demonstrate that our proposed bi-level visual conditioning token\nlearning method is able to achieve significantly improved test-time adaptation\nperformance by up to 1.9%.",
        "authors": [
            "Yushun Tang",
            "Shuoshuo Chen",
            "Zhehan Kan",
            "Yi Zhang",
            "Qinghai Guo",
            "Zhihai He"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:16:23+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19341v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19341v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000028,
        "doi": null,
        "title": "LiverUSRecon: Automatic 3D Reconstruction and Volumetry of the Liver with a Few Partial Ultrasound Scans",
        "abstract": "3D reconstruction of the liver for volumetry is important for qualitative\nanalysis and disease diagnosis. Liver volumetry using ultrasound (US) scans,\nalthough advantageous due to less acquisition time and safety, is challenging\ndue to the inherent noisiness in US scans, blurry boundaries, and partial liver\nvisibility. We address these challenges by using the segmentation masks of a\nfew incomplete sagittal-plane US scans of the liver in conjunction with a\nstatistical shape model (SSM) built using a set of CT scans of the liver. We\ncompute the shape parameters needed to warp this canonical SSM to fit the US\nscans through a parametric regression network. The resulting 3D liver\nreconstruction is accurate and leads to automatic liver volume calculation. We\nevaluate the accuracy of the estimated liver volumes with respect to CT\nsegmentation volumes using RMSE. Our volume computation is statistically much\ncloser to the volume estimated using CT scans than the volume computed using\nChilds' method by radiologists: p-value of 0.094 (>0.05) says that there is no\nsignificant difference between CT segmentation volumes and ours in contrast to\nChilds' method. We validate our method using investigations (ablation studies)\non the US image resolution, the number of CT scans used for SSM, the number of\nprincipal components, and the number of input US scans. To the best of our\nknowledge, this is the first automatic liver volumetry system using a few\nincomplete US scans given a set of CT scans of livers for SSM.",
        "chunk-id": 1,
        "chunk": "3D reconstruction of the liver for volumetry is important for qualitative\nanalysis and disease diagnosis. Liver volumetry using ultrasound (US) scans,\nalthough advantageous due to less acquisition time and safety, is challenging\ndue to the inherent noisiness in US scans, blurry boundaries, and partial liver",
        "authors": [
            "Kaushalya Sivayogaraj",
            "Sahan T. Guruge",
            "Udari Liyanage",
            "Jeevani Udupihille",
            "Saroj Jayasinghe",
            "Gerard Fernando",
            "Ranga Rodrigo",
            "M. Rukshani Liyanaarachchi"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:10:10+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19336v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19336v1",
        "categories": [
            "Image and Video Processing",
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000028,
        "doi": null,
        "title": "LiverUSRecon: Automatic 3D Reconstruction and Volumetry of the Liver with a Few Partial Ultrasound Scans",
        "abstract": "3D reconstruction of the liver for volumetry is important for qualitative\nanalysis and disease diagnosis. Liver volumetry using ultrasound (US) scans,\nalthough advantageous due to less acquisition time and safety, is challenging\ndue to the inherent noisiness in US scans, blurry boundaries, and partial liver\nvisibility. We address these challenges by using the segmentation masks of a\nfew incomplete sagittal-plane US scans of the liver in conjunction with a\nstatistical shape model (SSM) built using a set of CT scans of the liver. We\ncompute the shape parameters needed to warp this canonical SSM to fit the US\nscans through a parametric regression network. The resulting 3D liver\nreconstruction is accurate and leads to automatic liver volume calculation. We\nevaluate the accuracy of the estimated liver volumes with respect to CT\nsegmentation volumes using RMSE. Our volume computation is statistically much\ncloser to the volume estimated using CT scans than the volume computed using\nChilds' method by radiologists: p-value of 0.094 (>0.05) says that there is no\nsignificant difference between CT segmentation volumes and ours in contrast to\nChilds' method. We validate our method using investigations (ablation studies)\non the US image resolution, the number of CT scans used for SSM, the number of\nprincipal components, and the number of input US scans. To the best of our\nknowledge, this is the first automatic liver volumetry system using a few\nincomplete US scans given a set of CT scans of livers for SSM.",
        "chunk-id": 2,
        "chunk": "visibility. We address these challenges by using the segmentation masks of a\nfew incomplete sagittal-plane US scans of the liver in conjunction with a\nstatistical shape model (SSM) built using a set of CT scans of the liver. We\ncompute the shape parameters needed to warp this canonical SSM to fit the US\nscans through a parametric regression network. The resulting 3D liver",
        "authors": [
            "Kaushalya Sivayogaraj",
            "Sahan T. Guruge",
            "Udari Liyanage",
            "Jeevani Udupihille",
            "Saroj Jayasinghe",
            "Gerard Fernando",
            "Ranga Rodrigo",
            "M. Rukshani Liyanaarachchi"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:10:10+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19336v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19336v1",
        "categories": [
            "Image and Video Processing",
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000028,
        "doi": null,
        "title": "LiverUSRecon: Automatic 3D Reconstruction and Volumetry of the Liver with a Few Partial Ultrasound Scans",
        "abstract": "3D reconstruction of the liver for volumetry is important for qualitative\nanalysis and disease diagnosis. Liver volumetry using ultrasound (US) scans,\nalthough advantageous due to less acquisition time and safety, is challenging\ndue to the inherent noisiness in US scans, blurry boundaries, and partial liver\nvisibility. We address these challenges by using the segmentation masks of a\nfew incomplete sagittal-plane US scans of the liver in conjunction with a\nstatistical shape model (SSM) built using a set of CT scans of the liver. We\ncompute the shape parameters needed to warp this canonical SSM to fit the US\nscans through a parametric regression network. The resulting 3D liver\nreconstruction is accurate and leads to automatic liver volume calculation. We\nevaluate the accuracy of the estimated liver volumes with respect to CT\nsegmentation volumes using RMSE. Our volume computation is statistically much\ncloser to the volume estimated using CT scans than the volume computed using\nChilds' method by radiologists: p-value of 0.094 (>0.05) says that there is no\nsignificant difference between CT segmentation volumes and ours in contrast to\nChilds' method. We validate our method using investigations (ablation studies)\non the US image resolution, the number of CT scans used for SSM, the number of\nprincipal components, and the number of input US scans. To the best of our\nknowledge, this is the first automatic liver volumetry system using a few\nincomplete US scans given a set of CT scans of livers for SSM.",
        "chunk-id": 3,
        "chunk": "reconstruction is accurate and leads to automatic liver volume calculation. We\nevaluate the accuracy of the estimated liver volumes with respect to CT\nsegmentation volumes using RMSE. Our volume computation is statistically much\ncloser to the volume estimated using CT scans than the volume computed using\nChilds' method by radiologists: p-value of 0.094 (>0.05) says that there is no",
        "authors": [
            "Kaushalya Sivayogaraj",
            "Sahan T. Guruge",
            "Udari Liyanage",
            "Jeevani Udupihille",
            "Saroj Jayasinghe",
            "Gerard Fernando",
            "Ranga Rodrigo",
            "M. Rukshani Liyanaarachchi"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:10:10+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19336v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19336v1",
        "categories": [
            "Image and Video Processing",
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000028,
        "doi": null,
        "title": "LiverUSRecon: Automatic 3D Reconstruction and Volumetry of the Liver with a Few Partial Ultrasound Scans",
        "abstract": "3D reconstruction of the liver for volumetry is important for qualitative\nanalysis and disease diagnosis. Liver volumetry using ultrasound (US) scans,\nalthough advantageous due to less acquisition time and safety, is challenging\ndue to the inherent noisiness in US scans, blurry boundaries, and partial liver\nvisibility. We address these challenges by using the segmentation masks of a\nfew incomplete sagittal-plane US scans of the liver in conjunction with a\nstatistical shape model (SSM) built using a set of CT scans of the liver. We\ncompute the shape parameters needed to warp this canonical SSM to fit the US\nscans through a parametric regression network. The resulting 3D liver\nreconstruction is accurate and leads to automatic liver volume calculation. We\nevaluate the accuracy of the estimated liver volumes with respect to CT\nsegmentation volumes using RMSE. Our volume computation is statistically much\ncloser to the volume estimated using CT scans than the volume computed using\nChilds' method by radiologists: p-value of 0.094 (>0.05) says that there is no\nsignificant difference between CT segmentation volumes and ours in contrast to\nChilds' method. We validate our method using investigations (ablation studies)\non the US image resolution, the number of CT scans used for SSM, the number of\nprincipal components, and the number of input US scans. To the best of our\nknowledge, this is the first automatic liver volumetry system using a few\nincomplete US scans given a set of CT scans of livers for SSM.",
        "chunk-id": 4,
        "chunk": "significant difference between CT segmentation volumes and ours in contrast to\nChilds' method. We validate our method using investigations (ablation studies)\non the US image resolution, the number of CT scans used for SSM, the number of\nprincipal components, and the number of input US scans. To the best of our",
        "authors": [
            "Kaushalya Sivayogaraj",
            "Sahan T. Guruge",
            "Udari Liyanage",
            "Jeevani Udupihille",
            "Saroj Jayasinghe",
            "Gerard Fernando",
            "Ranga Rodrigo",
            "M. Rukshani Liyanaarachchi"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:10:10+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19336v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19336v1",
        "categories": [
            "Image and Video Processing",
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000028,
        "doi": null,
        "title": "LiverUSRecon: Automatic 3D Reconstruction and Volumetry of the Liver with a Few Partial Ultrasound Scans",
        "abstract": "3D reconstruction of the liver for volumetry is important for qualitative\nanalysis and disease diagnosis. Liver volumetry using ultrasound (US) scans,\nalthough advantageous due to less acquisition time and safety, is challenging\ndue to the inherent noisiness in US scans, blurry boundaries, and partial liver\nvisibility. We address these challenges by using the segmentation masks of a\nfew incomplete sagittal-plane US scans of the liver in conjunction with a\nstatistical shape model (SSM) built using a set of CT scans of the liver. We\ncompute the shape parameters needed to warp this canonical SSM to fit the US\nscans through a parametric regression network. The resulting 3D liver\nreconstruction is accurate and leads to automatic liver volume calculation. We\nevaluate the accuracy of the estimated liver volumes with respect to CT\nsegmentation volumes using RMSE. Our volume computation is statistically much\ncloser to the volume estimated using CT scans than the volume computed using\nChilds' method by radiologists: p-value of 0.094 (>0.05) says that there is no\nsignificant difference between CT segmentation volumes and ours in contrast to\nChilds' method. We validate our method using investigations (ablation studies)\non the US image resolution, the number of CT scans used for SSM, the number of\nprincipal components, and the number of input US scans. To the best of our\nknowledge, this is the first automatic liver volumetry system using a few\nincomplete US scans given a set of CT scans of livers for SSM.",
        "chunk-id": 5,
        "chunk": "knowledge, this is the first automatic liver volumetry system using a few\nincomplete US scans given a set of CT scans of livers for SSM.",
        "authors": [
            "Kaushalya Sivayogaraj",
            "Sahan T. Guruge",
            "Udari Liyanage",
            "Jeevani Udupihille",
            "Saroj Jayasinghe",
            "Gerard Fernando",
            "Ranga Rodrigo",
            "M. Rukshani Liyanaarachchi"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:10:10+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19336v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19336v1",
        "categories": [
            "Image and Video Processing",
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000029,
        "doi": null,
        "title": "Accelerating Multiphase Flow Simulations with Denoising Diffusion Model Driven Initializations",
        "abstract": "This study introduces a hybrid fluid simulation approach that integrates\ngenerative diffusion models with physics-based simulations, aiming at reducing\nthe computational costs of flow simulations while still honoring all the\nphysical properties of interest. These simulations enhance our understanding of\napplications such as assessing hydrogen and CO$_2$ storage efficiency in\nunderground reservoirs. Nevertheless, they are computationally expensive and\nthe presence of nonunique solutions can require multiple simulations within a\nsingle geometry. To overcome the computational cost hurdle, we propose a hybrid\nmethod that couples generative diffusion models and physics-based modeling. We\nintroduce a system to condition the diffusion model with a geometry of\ninterest, allowing to produce variable fluid saturations in the same geometry.\nWhile training the model, we simultaneously generate initial conditions and\nperform physics-based simulations using these conditions. This integrated\napproach enables us to receive real-time feedback on a single compute node\nequipped with both CPUs and GPUs. By efficiently managing these processes\nwithin one compute node, we can continuously evaluate performance and stop\ntraining when the desired criteria are met. To test our model, we generate\nrealizations in a real Berea sandstone fracture which shows that our technique\nis up to 4.4 times faster than commonly used flow simulation initializations.",
        "chunk-id": 1,
        "chunk": "This study introduces a hybrid fluid simulation approach that integrates\ngenerative diffusion models with physics-based simulations, aiming at reducing\nthe computational costs of flow simulations while still honoring all the\nphysical properties of interest. These simulations enhance our understanding of\napplications such as assessing hydrogen and CO$_2$ storage efficiency in",
        "authors": [
            "Jaehong Chung",
            "Agnese Marcato",
            "Eric J. Guiltinan",
            "Tapan Mukerji",
            "Hari Viswanathan",
            "Yen Ting Lin",
            "Javier E. Santos"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:05:40+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19333v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19333v1",
        "categories": [
            "Geophysics",
            "Computational Physics",
            "Fluid Dynamics"
        ]
    },
    {
        "id": 30000029,
        "doi": null,
        "title": "Accelerating Multiphase Flow Simulations with Denoising Diffusion Model Driven Initializations",
        "abstract": "This study introduces a hybrid fluid simulation approach that integrates\ngenerative diffusion models with physics-based simulations, aiming at reducing\nthe computational costs of flow simulations while still honoring all the\nphysical properties of interest. These simulations enhance our understanding of\napplications such as assessing hydrogen and CO$_2$ storage efficiency in\nunderground reservoirs. Nevertheless, they are computationally expensive and\nthe presence of nonunique solutions can require multiple simulations within a\nsingle geometry. To overcome the computational cost hurdle, we propose a hybrid\nmethod that couples generative diffusion models and physics-based modeling. We\nintroduce a system to condition the diffusion model with a geometry of\ninterest, allowing to produce variable fluid saturations in the same geometry.\nWhile training the model, we simultaneously generate initial conditions and\nperform physics-based simulations using these conditions. This integrated\napproach enables us to receive real-time feedback on a single compute node\nequipped with both CPUs and GPUs. By efficiently managing these processes\nwithin one compute node, we can continuously evaluate performance and stop\ntraining when the desired criteria are met. To test our model, we generate\nrealizations in a real Berea sandstone fracture which shows that our technique\nis up to 4.4 times faster than commonly used flow simulation initializations.",
        "chunk-id": 2,
        "chunk": "underground reservoirs. Nevertheless, they are computationally expensive and\nthe presence of nonunique solutions can require multiple simulations within a\nsingle geometry. To overcome the computational cost hurdle, we propose a hybrid\nmethod that couples generative diffusion models and physics-based modeling. We\nintroduce a system to condition the diffusion model with a geometry of",
        "authors": [
            "Jaehong Chung",
            "Agnese Marcato",
            "Eric J. Guiltinan",
            "Tapan Mukerji",
            "Hari Viswanathan",
            "Yen Ting Lin",
            "Javier E. Santos"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:05:40+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19333v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19333v1",
        "categories": [
            "Geophysics",
            "Computational Physics",
            "Fluid Dynamics"
        ]
    },
    {
        "id": 30000029,
        "doi": null,
        "title": "Accelerating Multiphase Flow Simulations with Denoising Diffusion Model Driven Initializations",
        "abstract": "This study introduces a hybrid fluid simulation approach that integrates\ngenerative diffusion models with physics-based simulations, aiming at reducing\nthe computational costs of flow simulations while still honoring all the\nphysical properties of interest. These simulations enhance our understanding of\napplications such as assessing hydrogen and CO$_2$ storage efficiency in\nunderground reservoirs. Nevertheless, they are computationally expensive and\nthe presence of nonunique solutions can require multiple simulations within a\nsingle geometry. To overcome the computational cost hurdle, we propose a hybrid\nmethod that couples generative diffusion models and physics-based modeling. We\nintroduce a system to condition the diffusion model with a geometry of\ninterest, allowing to produce variable fluid saturations in the same geometry.\nWhile training the model, we simultaneously generate initial conditions and\nperform physics-based simulations using these conditions. This integrated\napproach enables us to receive real-time feedback on a single compute node\nequipped with both CPUs and GPUs. By efficiently managing these processes\nwithin one compute node, we can continuously evaluate performance and stop\ntraining when the desired criteria are met. To test our model, we generate\nrealizations in a real Berea sandstone fracture which shows that our technique\nis up to 4.4 times faster than commonly used flow simulation initializations.",
        "chunk-id": 3,
        "chunk": "interest, allowing to produce variable fluid saturations in the same geometry.\nWhile training the model, we simultaneously generate initial conditions and\nperform physics-based simulations using these conditions. This integrated\napproach enables us to receive real-time feedback on a single compute node\nequipped with both CPUs and GPUs. By efficiently managing these processes",
        "authors": [
            "Jaehong Chung",
            "Agnese Marcato",
            "Eric J. Guiltinan",
            "Tapan Mukerji",
            "Hari Viswanathan",
            "Yen Ting Lin",
            "Javier E. Santos"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:05:40+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19333v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19333v1",
        "categories": [
            "Geophysics",
            "Computational Physics",
            "Fluid Dynamics"
        ]
    },
    {
        "id": 30000029,
        "doi": null,
        "title": "Accelerating Multiphase Flow Simulations with Denoising Diffusion Model Driven Initializations",
        "abstract": "This study introduces a hybrid fluid simulation approach that integrates\ngenerative diffusion models with physics-based simulations, aiming at reducing\nthe computational costs of flow simulations while still honoring all the\nphysical properties of interest. These simulations enhance our understanding of\napplications such as assessing hydrogen and CO$_2$ storage efficiency in\nunderground reservoirs. Nevertheless, they are computationally expensive and\nthe presence of nonunique solutions can require multiple simulations within a\nsingle geometry. To overcome the computational cost hurdle, we propose a hybrid\nmethod that couples generative diffusion models and physics-based modeling. We\nintroduce a system to condition the diffusion model with a geometry of\ninterest, allowing to produce variable fluid saturations in the same geometry.\nWhile training the model, we simultaneously generate initial conditions and\nperform physics-based simulations using these conditions. This integrated\napproach enables us to receive real-time feedback on a single compute node\nequipped with both CPUs and GPUs. By efficiently managing these processes\nwithin one compute node, we can continuously evaluate performance and stop\ntraining when the desired criteria are met. To test our model, we generate\nrealizations in a real Berea sandstone fracture which shows that our technique\nis up to 4.4 times faster than commonly used flow simulation initializations.",
        "chunk-id": 4,
        "chunk": "within one compute node, we can continuously evaluate performance and stop\ntraining when the desired criteria are met. To test our model, we generate\nrealizations in a real Berea sandstone fracture which shows that our technique\nis up to 4.4 times faster than commonly used flow simulation initializations.",
        "authors": [
            "Jaehong Chung",
            "Agnese Marcato",
            "Eric J. Guiltinan",
            "Tapan Mukerji",
            "Hari Viswanathan",
            "Yen Ting Lin",
            "Javier E. Santos"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:05:40+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19333v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19333v1",
        "categories": [
            "Geophysics",
            "Computational Physics",
            "Fluid Dynamics"
        ]
    },
    {
        "id": 30000030,
        "doi": null,
        "title": "Utility of virtual qubits in trapped-ion quantum computers",
        "abstract": "We propose encoding multiple qubits inside ions in existing trapped-ion\nquantum computers to access more qubits and to simplify circuits implementing\nstandard algorithms. By using such `virtual' qubits, some inter-ion gates can\nbe replaced by intra-ion gates, reducing the use of vibrational modes of the\nion chain, leading to less noise. We discuss specific examples such as the\nBernstein-Vazirani algorithm and random circuit sampling, using a small number\nof virtual qubits. Additionally, virtual qubits enable using larger number of\ndata qubits for an error correcting code, and we consider the repetition code\nas an example. We also lay out practical considerations to be made when\nchoosing states to encode virtual qubits in $^{137}\\mathrm{Ba}^+$ ions, and for\npreparing states and performing measurements.",
        "chunk-id": 1,
        "chunk": "We propose encoding multiple qubits inside ions in existing trapped-ion\nquantum computers to access more qubits and to simplify circuits implementing\nstandard algorithms. By using such `virtual' qubits, some inter-ion gates can\nbe replaced by intra-ion gates, reducing the use of vibrational modes of the\nion chain, leading to less noise. We discuss specific examples such as the",
        "authors": [
            "Saumya Shivam",
            "Fabian Pokorny",
            "Andres Vazquez-Brennan",
            "Ana S. Sotirova",
            "Jamie D. Leppard",
            "Sophie M. Decoppet",
            "C. J. Ballance",
            "S. L. Sondhi"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:05:31+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19332v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19332v1",
        "categories": [
            "Quantum Physics",
            "Atomic Physics"
        ]
    },
    {
        "id": 30000030,
        "doi": null,
        "title": "Utility of virtual qubits in trapped-ion quantum computers",
        "abstract": "We propose encoding multiple qubits inside ions in existing trapped-ion\nquantum computers to access more qubits and to simplify circuits implementing\nstandard algorithms. By using such `virtual' qubits, some inter-ion gates can\nbe replaced by intra-ion gates, reducing the use of vibrational modes of the\nion chain, leading to less noise. We discuss specific examples such as the\nBernstein-Vazirani algorithm and random circuit sampling, using a small number\nof virtual qubits. Additionally, virtual qubits enable using larger number of\ndata qubits for an error correcting code, and we consider the repetition code\nas an example. We also lay out practical considerations to be made when\nchoosing states to encode virtual qubits in $^{137}\\mathrm{Ba}^+$ ions, and for\npreparing states and performing measurements.",
        "chunk-id": 2,
        "chunk": "Bernstein-Vazirani algorithm and random circuit sampling, using a small number\nof virtual qubits. Additionally, virtual qubits enable using larger number of\ndata qubits for an error correcting code, and we consider the repetition code\nas an example. We also lay out practical considerations to be made when",
        "authors": [
            "Saumya Shivam",
            "Fabian Pokorny",
            "Andres Vazquez-Brennan",
            "Ana S. Sotirova",
            "Jamie D. Leppard",
            "Sophie M. Decoppet",
            "C. J. Ballance",
            "S. L. Sondhi"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:05:31+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19332v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19332v1",
        "categories": [
            "Quantum Physics",
            "Atomic Physics"
        ]
    },
    {
        "id": 30000030,
        "doi": null,
        "title": "Utility of virtual qubits in trapped-ion quantum computers",
        "abstract": "We propose encoding multiple qubits inside ions in existing trapped-ion\nquantum computers to access more qubits and to simplify circuits implementing\nstandard algorithms. By using such `virtual' qubits, some inter-ion gates can\nbe replaced by intra-ion gates, reducing the use of vibrational modes of the\nion chain, leading to less noise. We discuss specific examples such as the\nBernstein-Vazirani algorithm and random circuit sampling, using a small number\nof virtual qubits. Additionally, virtual qubits enable using larger number of\ndata qubits for an error correcting code, and we consider the repetition code\nas an example. We also lay out practical considerations to be made when\nchoosing states to encode virtual qubits in $^{137}\\mathrm{Ba}^+$ ions, and for\npreparing states and performing measurements.",
        "chunk-id": 3,
        "chunk": "choosing states to encode virtual qubits in $^{137}\\mathrm{Ba}^+$ ions, and for\npreparing states and performing measurements.",
        "authors": [
            "Saumya Shivam",
            "Fabian Pokorny",
            "Andres Vazquez-Brennan",
            "Ana S. Sotirova",
            "Jamie D. Leppard",
            "Sophie M. Decoppet",
            "C. J. Ballance",
            "S. L. Sondhi"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:05:31+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19332v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19332v1",
        "categories": [
            "Quantum Physics",
            "Atomic Physics"
        ]
    },
    {
        "id": 30000031,
        "doi": null,
        "title": "Non-spinning tops are stable",
        "abstract": "We consider coupled gravitational and electromagnetic perturbations of a\nfamily of five-dimensional Einstein-Maxwell solutions that describes both\nmagnetized black strings and horizonless topological stars. We find that the\nodd perturbations of this background lead to a master equation with five\nFuchsian singularities and compute its quasinormal mode spectrum using three\nindependent methods: Leaver, WKB and numerical integration. Our analysis\nconfirms that odd perturbations always decay in time, while spherically\nsymmetric even perturbations may exhibit for certain ranges of the magnetic\nfluxes instabilities of Gregory-Laflamme type for black strings and of\nGross-Perry-Yaffe type for topological stars. This constitutes evidence that\ntopological stars and black strings are classically stable in a finite domain\nof their parameter space.",
        "chunk-id": 1,
        "chunk": "We consider coupled gravitational and electromagnetic perturbations of a\nfamily of five-dimensional Einstein-Maxwell solutions that describes both\nmagnetized black strings and horizonless topological stars. We find that the\nodd perturbations of this background lead to a master equation with five\nFuchsian singularities and compute its quasinormal mode spectrum using three",
        "authors": [
            "Iosif Bena",
            "Giorgio Di Russo",
            "Jose Francisco Morales",
            "Alejandro Ruip\u00e9rez"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:01:47+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19330v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19330v1",
        "categories": [
            "High Energy Physics - Theory"
        ]
    },
    {
        "id": 30000031,
        "doi": null,
        "title": "Non-spinning tops are stable",
        "abstract": "We consider coupled gravitational and electromagnetic perturbations of a\nfamily of five-dimensional Einstein-Maxwell solutions that describes both\nmagnetized black strings and horizonless topological stars. We find that the\nodd perturbations of this background lead to a master equation with five\nFuchsian singularities and compute its quasinormal mode spectrum using three\nindependent methods: Leaver, WKB and numerical integration. Our analysis\nconfirms that odd perturbations always decay in time, while spherically\nsymmetric even perturbations may exhibit for certain ranges of the magnetic\nfluxes instabilities of Gregory-Laflamme type for black strings and of\nGross-Perry-Yaffe type for topological stars. This constitutes evidence that\ntopological stars and black strings are classically stable in a finite domain\nof their parameter space.",
        "chunk-id": 2,
        "chunk": "independent methods: Leaver, WKB and numerical integration. Our analysis\nconfirms that odd perturbations always decay in time, while spherically\nsymmetric even perturbations may exhibit for certain ranges of the magnetic\nfluxes instabilities of Gregory-Laflamme type for black strings and of\nGross-Perry-Yaffe type for topological stars. This constitutes evidence that",
        "authors": [
            "Iosif Bena",
            "Giorgio Di Russo",
            "Jose Francisco Morales",
            "Alejandro Ruip\u00e9rez"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:01:47+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19330v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19330v1",
        "categories": [
            "High Energy Physics - Theory"
        ]
    },
    {
        "id": 30000031,
        "doi": null,
        "title": "Non-spinning tops are stable",
        "abstract": "We consider coupled gravitational and electromagnetic perturbations of a\nfamily of five-dimensional Einstein-Maxwell solutions that describes both\nmagnetized black strings and horizonless topological stars. We find that the\nodd perturbations of this background lead to a master equation with five\nFuchsian singularities and compute its quasinormal mode spectrum using three\nindependent methods: Leaver, WKB and numerical integration. Our analysis\nconfirms that odd perturbations always decay in time, while spherically\nsymmetric even perturbations may exhibit for certain ranges of the magnetic\nfluxes instabilities of Gregory-Laflamme type for black strings and of\nGross-Perry-Yaffe type for topological stars. This constitutes evidence that\ntopological stars and black strings are classically stable in a finite domain\nof their parameter space.",
        "chunk-id": 3,
        "chunk": "topological stars and black strings are classically stable in a finite domain\nof their parameter space.",
        "authors": [
            "Iosif Bena",
            "Giorgio Di Russo",
            "Jose Francisco Morales",
            "Alejandro Ruip\u00e9rez"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T17:01:47+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19330v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19330v1",
        "categories": [
            "High Energy Physics - Theory"
        ]
    },
    {
        "id": 30000032,
        "doi": null,
        "title": "Multimodal Visual-haptic pose estimation in the presence of transient occlusion",
        "abstract": "Human-robot collaboration requires the establishment of methods to guarantee\nthe safety of participating operators. A necessary part of this process is\nensuring reliable human pose estimation. Established vision-based modalities\nencounter problems when under conditions of occlusion. This article describes\nthe combination of two perception modalities for pose estimation in\nenvironments containing such transient occlusion. We first introduce a\nvision-based pose estimation method, based on a deep Predictive Coding (PC)\nmodel featuring robustness to partial occlusion. Next, capacitive sensing\nhardware capable of detecting various objects is introduced. The sensor is\ncompact enough to be mounted on the exterior of any given robotic system. The\ntechnology is particularly well-suited to detection of capacitive material,\nsuch as living tissue. Pose estimation from the two individual sensing\nmodalities is combined using a modified Luenberger observer model. We\ndemonstrate that the results offer better performance than either sensor alone.\nThe efficacy of the system is demonstrated on an environment containing a robot\narm and a human, showing the ability to estimate the pose of a human forearm\nunder varying levels of occlusion.",
        "chunk-id": 1,
        "chunk": "Human-robot collaboration requires the establishment of methods to guarantee\nthe safety of participating operators. A necessary part of this process is\nensuring reliable human pose estimation. Established vision-based modalities\nencounter problems when under conditions of occlusion. This article describes\nthe combination of two perception modalities for pose estimation in",
        "authors": [
            "Michael Zechmair",
            "Yannick Morel"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:54:56+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19323v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19323v1",
        "categories": [
            "Robotics"
        ]
    },
    {
        "id": 30000032,
        "doi": null,
        "title": "Multimodal Visual-haptic pose estimation in the presence of transient occlusion",
        "abstract": "Human-robot collaboration requires the establishment of methods to guarantee\nthe safety of participating operators. A necessary part of this process is\nensuring reliable human pose estimation. Established vision-based modalities\nencounter problems when under conditions of occlusion. This article describes\nthe combination of two perception modalities for pose estimation in\nenvironments containing such transient occlusion. We first introduce a\nvision-based pose estimation method, based on a deep Predictive Coding (PC)\nmodel featuring robustness to partial occlusion. Next, capacitive sensing\nhardware capable of detecting various objects is introduced. The sensor is\ncompact enough to be mounted on the exterior of any given robotic system. The\ntechnology is particularly well-suited to detection of capacitive material,\nsuch as living tissue. Pose estimation from the two individual sensing\nmodalities is combined using a modified Luenberger observer model. We\ndemonstrate that the results offer better performance than either sensor alone.\nThe efficacy of the system is demonstrated on an environment containing a robot\narm and a human, showing the ability to estimate the pose of a human forearm\nunder varying levels of occlusion.",
        "chunk-id": 2,
        "chunk": "environments containing such transient occlusion. We first introduce a\nvision-based pose estimation method, based on a deep Predictive Coding (PC)\nmodel featuring robustness to partial occlusion. Next, capacitive sensing\nhardware capable of detecting various objects is introduced. The sensor is\ncompact enough to be mounted on the exterior of any given robotic system. The",
        "authors": [
            "Michael Zechmair",
            "Yannick Morel"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:54:56+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19323v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19323v1",
        "categories": [
            "Robotics"
        ]
    },
    {
        "id": 30000032,
        "doi": null,
        "title": "Multimodal Visual-haptic pose estimation in the presence of transient occlusion",
        "abstract": "Human-robot collaboration requires the establishment of methods to guarantee\nthe safety of participating operators. A necessary part of this process is\nensuring reliable human pose estimation. Established vision-based modalities\nencounter problems when under conditions of occlusion. This article describes\nthe combination of two perception modalities for pose estimation in\nenvironments containing such transient occlusion. We first introduce a\nvision-based pose estimation method, based on a deep Predictive Coding (PC)\nmodel featuring robustness to partial occlusion. Next, capacitive sensing\nhardware capable of detecting various objects is introduced. The sensor is\ncompact enough to be mounted on the exterior of any given robotic system. The\ntechnology is particularly well-suited to detection of capacitive material,\nsuch as living tissue. Pose estimation from the two individual sensing\nmodalities is combined using a modified Luenberger observer model. We\ndemonstrate that the results offer better performance than either sensor alone.\nThe efficacy of the system is demonstrated on an environment containing a robot\narm and a human, showing the ability to estimate the pose of a human forearm\nunder varying levels of occlusion.",
        "chunk-id": 3,
        "chunk": "technology is particularly well-suited to detection of capacitive material,\nsuch as living tissue. Pose estimation from the two individual sensing\nmodalities is combined using a modified Luenberger observer model. We\ndemonstrate that the results offer better performance than either sensor alone.\nThe efficacy of the system is demonstrated on an environment containing a robot",
        "authors": [
            "Michael Zechmair",
            "Yannick Morel"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:54:56+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19323v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19323v1",
        "categories": [
            "Robotics"
        ]
    },
    {
        "id": 30000032,
        "doi": null,
        "title": "Multimodal Visual-haptic pose estimation in the presence of transient occlusion",
        "abstract": "Human-robot collaboration requires the establishment of methods to guarantee\nthe safety of participating operators. A necessary part of this process is\nensuring reliable human pose estimation. Established vision-based modalities\nencounter problems when under conditions of occlusion. This article describes\nthe combination of two perception modalities for pose estimation in\nenvironments containing such transient occlusion. We first introduce a\nvision-based pose estimation method, based on a deep Predictive Coding (PC)\nmodel featuring robustness to partial occlusion. Next, capacitive sensing\nhardware capable of detecting various objects is introduced. The sensor is\ncompact enough to be mounted on the exterior of any given robotic system. The\ntechnology is particularly well-suited to detection of capacitive material,\nsuch as living tissue. Pose estimation from the two individual sensing\nmodalities is combined using a modified Luenberger observer model. We\ndemonstrate that the results offer better performance than either sensor alone.\nThe efficacy of the system is demonstrated on an environment containing a robot\narm and a human, showing the ability to estimate the pose of a human forearm\nunder varying levels of occlusion.",
        "chunk-id": 4,
        "chunk": "arm and a human, showing the ability to estimate the pose of a human forearm\nunder varying levels of occlusion.",
        "authors": [
            "Michael Zechmair",
            "Yannick Morel"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:54:56+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19323v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19323v1",
        "categories": [
            "Robotics"
        ]
    },
    {
        "id": 30000033,
        "doi": null,
        "title": "Efficient World Models with Context-Aware Tokenization",
        "abstract": "Scaling up deep Reinforcement Learning (RL) methods presents a significant\nchallenge. Following developments in generative modelling, model-based RL\npositions itself as a strong contender. Recent advances in sequence modelling\nhave led to effective transformer-based world models, albeit at the price of\nheavy computations due to the long sequences of tokens required to accurately\nsimulate environments. In this work, we propose $\\Delta$-IRIS, a new agent with\na world model architecture composed of a discrete autoencoder that encodes\nstochastic deltas between time steps and an autoregressive transformer that\npredicts future deltas by summarizing the current state of the world with\ncontinuous tokens. In the Crafter benchmark, $\\Delta$-IRIS sets a new state of\nthe art at multiple frame budgets, while being an order of magnitude faster to\ntrain than previous attention-based approaches. We release our code and models\nat https://github.com/vmicheli/delta-iris.",
        "chunk-id": 1,
        "chunk": "Scaling up deep Reinforcement Learning (RL) methods presents a significant\nchallenge. Following developments in generative modelling, model-based RL\npositions itself as a strong contender. Recent advances in sequence modelling\nhave led to effective transformer-based world models, albeit at the price of\nheavy computations due to the long sequences of tokens required to accurately",
        "authors": [
            "Vincent Micheli",
            "Eloi Alonso",
            "Fran\u00e7ois Fleuret"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:54:12+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19320v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19320v1",
        "categories": [
            "Machine Learning",
            "Artificial Intelligence",
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000033,
        "doi": null,
        "title": "Efficient World Models with Context-Aware Tokenization",
        "abstract": "Scaling up deep Reinforcement Learning (RL) methods presents a significant\nchallenge. Following developments in generative modelling, model-based RL\npositions itself as a strong contender. Recent advances in sequence modelling\nhave led to effective transformer-based world models, albeit at the price of\nheavy computations due to the long sequences of tokens required to accurately\nsimulate environments. In this work, we propose $\\Delta$-IRIS, a new agent with\na world model architecture composed of a discrete autoencoder that encodes\nstochastic deltas between time steps and an autoregressive transformer that\npredicts future deltas by summarizing the current state of the world with\ncontinuous tokens. In the Crafter benchmark, $\\Delta$-IRIS sets a new state of\nthe art at multiple frame budgets, while being an order of magnitude faster to\ntrain than previous attention-based approaches. We release our code and models\nat https://github.com/vmicheli/delta-iris.",
        "chunk-id": 2,
        "chunk": "simulate environments. In this work, we propose $\\Delta$-IRIS, a new agent with\na world model architecture composed of a discrete autoencoder that encodes\nstochastic deltas between time steps and an autoregressive transformer that\npredicts future deltas by summarizing the current state of the world with\ncontinuous tokens. In the Crafter benchmark, $\\Delta$-IRIS sets a new state of",
        "authors": [
            "Vincent Micheli",
            "Eloi Alonso",
            "Fran\u00e7ois Fleuret"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:54:12+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19320v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19320v1",
        "categories": [
            "Machine Learning",
            "Artificial Intelligence",
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000033,
        "doi": null,
        "title": "Efficient World Models with Context-Aware Tokenization",
        "abstract": "Scaling up deep Reinforcement Learning (RL) methods presents a significant\nchallenge. Following developments in generative modelling, model-based RL\npositions itself as a strong contender. Recent advances in sequence modelling\nhave led to effective transformer-based world models, albeit at the price of\nheavy computations due to the long sequences of tokens required to accurately\nsimulate environments. In this work, we propose $\\Delta$-IRIS, a new agent with\na world model architecture composed of a discrete autoencoder that encodes\nstochastic deltas between time steps and an autoregressive transformer that\npredicts future deltas by summarizing the current state of the world with\ncontinuous tokens. In the Crafter benchmark, $\\Delta$-IRIS sets a new state of\nthe art at multiple frame budgets, while being an order of magnitude faster to\ntrain than previous attention-based approaches. We release our code and models\nat https://github.com/vmicheli/delta-iris.",
        "chunk-id": 3,
        "chunk": "the art at multiple frame budgets, while being an order of magnitude faster to\ntrain than previous attention-based approaches. We release our code and models\nat https://github.com/vmicheli/delta-iris.",
        "authors": [
            "Vincent Micheli",
            "Eloi Alonso",
            "Fran\u00e7ois Fleuret"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:54:12+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19320v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19320v1",
        "categories": [
            "Machine Learning",
            "Artificial Intelligence",
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000034,
        "doi": null,
        "title": "Jump Starting Bandits with LLM-Generated Prior Knowledge",
        "abstract": "We present substantial evidence demonstrating the benefits of integrating\nLarge Language Models (LLMs) with a Contextual Multi-Armed Bandit framework.\nContextual bandits have been widely used in recommendation systems to generate\npersonalized suggestions based on user-specific contexts. We show that LLMs,\npre-trained on extensive corpora rich in human knowledge and preferences, can\nsimulate human behaviours well enough to jump-start contextual multi-armed\nbandits to reduce online learning regret. We propose an initialization\nalgorithm for contextual bandits by prompting LLMs to produce a pre-training\ndataset of approximate human preferences for the bandit. This significantly\nreduces online learning regret and data-gathering costs for training such\nmodels. Our approach is validated empirically through two sets of experiments\nwith different bandit setups: one which utilizes LLMs to serve as an oracle and\na real-world experiment utilizing data from a conjoint survey experiment.",
        "chunk-id": 1,
        "chunk": "We present substantial evidence demonstrating the benefits of integrating\nLarge Language Models (LLMs) with a Contextual Multi-Armed Bandit framework.\nContextual bandits have been widely used in recommendation systems to generate\npersonalized suggestions based on user-specific contexts. We show that LLMs,\npre-trained on extensive corpora rich in human knowledge and preferences, can",
        "authors": [
            "Parand A. Alamdari",
            "Yanshuai Cao",
            "Kevin H. Wilson"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:52:19+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19317v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19317v1",
        "categories": [
            "Machine Learning",
            "Artificial Intelligence",
            "Computation and Language"
        ]
    },
    {
        "id": 30000034,
        "doi": null,
        "title": "Jump Starting Bandits with LLM-Generated Prior Knowledge",
        "abstract": "We present substantial evidence demonstrating the benefits of integrating\nLarge Language Models (LLMs) with a Contextual Multi-Armed Bandit framework.\nContextual bandits have been widely used in recommendation systems to generate\npersonalized suggestions based on user-specific contexts. We show that LLMs,\npre-trained on extensive corpora rich in human knowledge and preferences, can\nsimulate human behaviours well enough to jump-start contextual multi-armed\nbandits to reduce online learning regret. We propose an initialization\nalgorithm for contextual bandits by prompting LLMs to produce a pre-training\ndataset of approximate human preferences for the bandit. This significantly\nreduces online learning regret and data-gathering costs for training such\nmodels. Our approach is validated empirically through two sets of experiments\nwith different bandit setups: one which utilizes LLMs to serve as an oracle and\na real-world experiment utilizing data from a conjoint survey experiment.",
        "chunk-id": 2,
        "chunk": "simulate human behaviours well enough to jump-start contextual multi-armed\nbandits to reduce online learning regret. We propose an initialization\nalgorithm for contextual bandits by prompting LLMs to produce a pre-training\ndataset of approximate human preferences for the bandit. This significantly\nreduces online learning regret and data-gathering costs for training such",
        "authors": [
            "Parand A. Alamdari",
            "Yanshuai Cao",
            "Kevin H. Wilson"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:52:19+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19317v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19317v1",
        "categories": [
            "Machine Learning",
            "Artificial Intelligence",
            "Computation and Language"
        ]
    },
    {
        "id": 30000034,
        "doi": null,
        "title": "Jump Starting Bandits with LLM-Generated Prior Knowledge",
        "abstract": "We present substantial evidence demonstrating the benefits of integrating\nLarge Language Models (LLMs) with a Contextual Multi-Armed Bandit framework.\nContextual bandits have been widely used in recommendation systems to generate\npersonalized suggestions based on user-specific contexts. We show that LLMs,\npre-trained on extensive corpora rich in human knowledge and preferences, can\nsimulate human behaviours well enough to jump-start contextual multi-armed\nbandits to reduce online learning regret. We propose an initialization\nalgorithm for contextual bandits by prompting LLMs to produce a pre-training\ndataset of approximate human preferences for the bandit. This significantly\nreduces online learning regret and data-gathering costs for training such\nmodels. Our approach is validated empirically through two sets of experiments\nwith different bandit setups: one which utilizes LLMs to serve as an oracle and\na real-world experiment utilizing data from a conjoint survey experiment.",
        "chunk-id": 3,
        "chunk": "models. Our approach is validated empirically through two sets of experiments\nwith different bandit setups: one which utilizes LLMs to serve as an oracle and\na real-world experiment utilizing data from a conjoint survey experiment.",
        "authors": [
            "Parand A. Alamdari",
            "Yanshuai Cao",
            "Kevin H. Wilson"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:52:19+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19317v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19317v1",
        "categories": [
            "Machine Learning",
            "Artificial Intelligence",
            "Computation and Language"
        ]
    },
    {
        "id": 30000035,
        "doi": null,
        "title": "Enhanced Data Transfer Cooperating with Artificial Triplets for Scene Graph Generation",
        "abstract": "This work focuses on training dataset enhancement of informative relational\ntriplets for Scene Graph Generation (SGG). Due to the lack of effective\nsupervision, the current SGG model predictions perform poorly for informative\nrelational triplets with inadequate training samples. Therefore, we propose two\nnovel training dataset enhancement modules: Feature Space Triplet Augmentation\n(FSTA) and Soft Transfer. FSTA leverages a feature generator trained to\ngenerate representations of an object in relational triplets. The biased\nprediction based sampling in FSTA efficiently augments artificial triplets\nfocusing on the challenging ones. In addition, we introduce Soft Transfer,\nwhich assigns soft predicate labels to general relational triplets to make more\nsupervisions for informative predicate classes effectively. Experimental\nresults show that integrating FSTA and Soft Transfer achieve high levels of\nboth Recall and mean Recall in Visual Genome dataset. The mean of Recall and\nmean Recall is the highest among all the existing model-agnostic methods.",
        "chunk-id": 1,
        "chunk": "This work focuses on training dataset enhancement of informative relational\ntriplets for Scene Graph Generation (SGG). Due to the lack of effective\nsupervision, the current SGG model predictions perform poorly for informative\nrelational triplets with inadequate training samples. Therefore, we propose two\nnovel training dataset enhancement modules: Feature Space Triplet Augmentation",
        "authors": [
            "KuanChao Chu",
            "Satoshi Yamazaki",
            "Hideki Nakayama"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:52:01+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19316v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19316v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000035,
        "doi": null,
        "title": "Enhanced Data Transfer Cooperating with Artificial Triplets for Scene Graph Generation",
        "abstract": "This work focuses on training dataset enhancement of informative relational\ntriplets for Scene Graph Generation (SGG). Due to the lack of effective\nsupervision, the current SGG model predictions perform poorly for informative\nrelational triplets with inadequate training samples. Therefore, we propose two\nnovel training dataset enhancement modules: Feature Space Triplet Augmentation\n(FSTA) and Soft Transfer. FSTA leverages a feature generator trained to\ngenerate representations of an object in relational triplets. The biased\nprediction based sampling in FSTA efficiently augments artificial triplets\nfocusing on the challenging ones. In addition, we introduce Soft Transfer,\nwhich assigns soft predicate labels to general relational triplets to make more\nsupervisions for informative predicate classes effectively. Experimental\nresults show that integrating FSTA and Soft Transfer achieve high levels of\nboth Recall and mean Recall in Visual Genome dataset. The mean of Recall and\nmean Recall is the highest among all the existing model-agnostic methods.",
        "chunk-id": 2,
        "chunk": "(FSTA) and Soft Transfer. FSTA leverages a feature generator trained to\ngenerate representations of an object in relational triplets. The biased\nprediction based sampling in FSTA efficiently augments artificial triplets\nfocusing on the challenging ones. In addition, we introduce Soft Transfer,\nwhich assigns soft predicate labels to general relational triplets to make more",
        "authors": [
            "KuanChao Chu",
            "Satoshi Yamazaki",
            "Hideki Nakayama"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:52:01+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19316v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19316v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000035,
        "doi": null,
        "title": "Enhanced Data Transfer Cooperating with Artificial Triplets for Scene Graph Generation",
        "abstract": "This work focuses on training dataset enhancement of informative relational\ntriplets for Scene Graph Generation (SGG). Due to the lack of effective\nsupervision, the current SGG model predictions perform poorly for informative\nrelational triplets with inadequate training samples. Therefore, we propose two\nnovel training dataset enhancement modules: Feature Space Triplet Augmentation\n(FSTA) and Soft Transfer. FSTA leverages a feature generator trained to\ngenerate representations of an object in relational triplets. The biased\nprediction based sampling in FSTA efficiently augments artificial triplets\nfocusing on the challenging ones. In addition, we introduce Soft Transfer,\nwhich assigns soft predicate labels to general relational triplets to make more\nsupervisions for informative predicate classes effectively. Experimental\nresults show that integrating FSTA and Soft Transfer achieve high levels of\nboth Recall and mean Recall in Visual Genome dataset. The mean of Recall and\nmean Recall is the highest among all the existing model-agnostic methods.",
        "chunk-id": 3,
        "chunk": "supervisions for informative predicate classes effectively. Experimental\nresults show that integrating FSTA and Soft Transfer achieve high levels of\nboth Recall and mean Recall in Visual Genome dataset. The mean of Recall and\nmean Recall is the highest among all the existing model-agnostic methods.",
        "authors": [
            "KuanChao Chu",
            "Satoshi Yamazaki",
            "Hideki Nakayama"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:52:01+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19316v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19316v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000036,
        "doi": null,
        "title": "LiveBench: A Challenging, Contamination-Free LLM Benchmark",
        "abstract": "Test set contamination, wherein test data from a benchmark ends up in a newer\nmodel's training set, is a well-documented obstacle for fair LLM evaluation and\ncan quickly render benchmarks obsolete. To mitigate this, many recent\nbenchmarks crowdsource new prompts and evaluations from human or LLM judges;\nhowever, these can introduce significant biases, and break down when scoring\nhard questions. In this work, we introduce a new benchmark for LLMs designed to\nbe immune to both test set contamination and the pitfalls of LLM judging and\nhuman crowdsourcing. We release LiveBench, the first benchmark that (1)\ncontains frequently-updated questions from recent information sources, (2)\nscores answers automatically according to objective ground-truth values, and\n(3) contains a wide variety of challenging tasks, spanning math, coding,\nreasoning, language, instruction following, and data analysis. To achieve this,\nLiveBench contains questions that are based on recently-released math\ncompetitions, arXiv papers, news articles, and datasets, and it contains\nharder, contamination-free versions of tasks from previous benchmarks such as\nBig-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-source\nmodels, as well as dozens of open-source models ranging from 0.5B to 110B in\nsize. LiveBench is difficult, with top models achieving below 65% accuracy. We\nrelease all questions, code, and model answers. Questions will be added and\nupdated on a monthly basis, and we will release new tasks and harder versions\nof tasks over time so that LiveBench can distinguish between the capabilities\nof LLMs as they improve in the future. We welcome community engagement and\ncollaboration for expanding the benchmark tasks and models.",
        "chunk-id": 1,
        "chunk": "Test set contamination, wherein test data from a benchmark ends up in a newer\nmodel's training set, is a well-documented obstacle for fair LLM evaluation and\ncan quickly render benchmarks obsolete. To mitigate this, many recent\nbenchmarks crowdsource new prompts and evaluations from human or LLM judges;\nhowever, these can introduce significant biases, and break down when scoring",
        "authors": [
            "Colin White",
            "Samuel Dooley",
            "Manley Roberts",
            "Arka Pal",
            "Ben Feuer",
            "Siddhartha Jain",
            "Ravid Shwartz-Ziv",
            "Neel Jain",
            "Khalid Saifullah",
            "Siddartha Naidu",
            "Chinmay Hegde",
            "Yann LeCun",
            "Tom Goldstein",
            "Willie Neiswanger",
            "Micah Goldblum"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:47:42+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19314v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19314v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Machine Learning"
        ]
    },
    {
        "id": 30000036,
        "doi": null,
        "title": "LiveBench: A Challenging, Contamination-Free LLM Benchmark",
        "abstract": "Test set contamination, wherein test data from a benchmark ends up in a newer\nmodel's training set, is a well-documented obstacle for fair LLM evaluation and\ncan quickly render benchmarks obsolete. To mitigate this, many recent\nbenchmarks crowdsource new prompts and evaluations from human or LLM judges;\nhowever, these can introduce significant biases, and break down when scoring\nhard questions. In this work, we introduce a new benchmark for LLMs designed to\nbe immune to both test set contamination and the pitfalls of LLM judging and\nhuman crowdsourcing. We release LiveBench, the first benchmark that (1)\ncontains frequently-updated questions from recent information sources, (2)\nscores answers automatically according to objective ground-truth values, and\n(3) contains a wide variety of challenging tasks, spanning math, coding,\nreasoning, language, instruction following, and data analysis. To achieve this,\nLiveBench contains questions that are based on recently-released math\ncompetitions, arXiv papers, news articles, and datasets, and it contains\nharder, contamination-free versions of tasks from previous benchmarks such as\nBig-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-source\nmodels, as well as dozens of open-source models ranging from 0.5B to 110B in\nsize. LiveBench is difficult, with top models achieving below 65% accuracy. We\nrelease all questions, code, and model answers. Questions will be added and\nupdated on a monthly basis, and we will release new tasks and harder versions\nof tasks over time so that LiveBench can distinguish between the capabilities\nof LLMs as they improve in the future. We welcome community engagement and\ncollaboration for expanding the benchmark tasks and models.",
        "chunk-id": 2,
        "chunk": "hard questions. In this work, we introduce a new benchmark for LLMs designed to\nbe immune to both test set contamination and the pitfalls of LLM judging and\nhuman crowdsourcing. We release LiveBench, the first benchmark that (1)\ncontains frequently-updated questions from recent information sources, (2)\nscores answers automatically according to objective ground-truth values, and",
        "authors": [
            "Colin White",
            "Samuel Dooley",
            "Manley Roberts",
            "Arka Pal",
            "Ben Feuer",
            "Siddhartha Jain",
            "Ravid Shwartz-Ziv",
            "Neel Jain",
            "Khalid Saifullah",
            "Siddartha Naidu",
            "Chinmay Hegde",
            "Yann LeCun",
            "Tom Goldstein",
            "Willie Neiswanger",
            "Micah Goldblum"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:47:42+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19314v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19314v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Machine Learning"
        ]
    },
    {
        "id": 30000036,
        "doi": null,
        "title": "LiveBench: A Challenging, Contamination-Free LLM Benchmark",
        "abstract": "Test set contamination, wherein test data from a benchmark ends up in a newer\nmodel's training set, is a well-documented obstacle for fair LLM evaluation and\ncan quickly render benchmarks obsolete. To mitigate this, many recent\nbenchmarks crowdsource new prompts and evaluations from human or LLM judges;\nhowever, these can introduce significant biases, and break down when scoring\nhard questions. In this work, we introduce a new benchmark for LLMs designed to\nbe immune to both test set contamination and the pitfalls of LLM judging and\nhuman crowdsourcing. We release LiveBench, the first benchmark that (1)\ncontains frequently-updated questions from recent information sources, (2)\nscores answers automatically according to objective ground-truth values, and\n(3) contains a wide variety of challenging tasks, spanning math, coding,\nreasoning, language, instruction following, and data analysis. To achieve this,\nLiveBench contains questions that are based on recently-released math\ncompetitions, arXiv papers, news articles, and datasets, and it contains\nharder, contamination-free versions of tasks from previous benchmarks such as\nBig-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-source\nmodels, as well as dozens of open-source models ranging from 0.5B to 110B in\nsize. LiveBench is difficult, with top models achieving below 65% accuracy. We\nrelease all questions, code, and model answers. Questions will be added and\nupdated on a monthly basis, and we will release new tasks and harder versions\nof tasks over time so that LiveBench can distinguish between the capabilities\nof LLMs as they improve in the future. We welcome community engagement and\ncollaboration for expanding the benchmark tasks and models.",
        "chunk-id": 3,
        "chunk": "(3) contains a wide variety of challenging tasks, spanning math, coding,\nreasoning, language, instruction following, and data analysis. To achieve this,\nLiveBench contains questions that are based on recently-released math\ncompetitions, arXiv papers, news articles, and datasets, and it contains\nharder, contamination-free versions of tasks from previous benchmarks such as",
        "authors": [
            "Colin White",
            "Samuel Dooley",
            "Manley Roberts",
            "Arka Pal",
            "Ben Feuer",
            "Siddhartha Jain",
            "Ravid Shwartz-Ziv",
            "Neel Jain",
            "Khalid Saifullah",
            "Siddartha Naidu",
            "Chinmay Hegde",
            "Yann LeCun",
            "Tom Goldstein",
            "Willie Neiswanger",
            "Micah Goldblum"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:47:42+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19314v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19314v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Machine Learning"
        ]
    },
    {
        "id": 30000036,
        "doi": null,
        "title": "LiveBench: A Challenging, Contamination-Free LLM Benchmark",
        "abstract": "Test set contamination, wherein test data from a benchmark ends up in a newer\nmodel's training set, is a well-documented obstacle for fair LLM evaluation and\ncan quickly render benchmarks obsolete. To mitigate this, many recent\nbenchmarks crowdsource new prompts and evaluations from human or LLM judges;\nhowever, these can introduce significant biases, and break down when scoring\nhard questions. In this work, we introduce a new benchmark for LLMs designed to\nbe immune to both test set contamination and the pitfalls of LLM judging and\nhuman crowdsourcing. We release LiveBench, the first benchmark that (1)\ncontains frequently-updated questions from recent information sources, (2)\nscores answers automatically according to objective ground-truth values, and\n(3) contains a wide variety of challenging tasks, spanning math, coding,\nreasoning, language, instruction following, and data analysis. To achieve this,\nLiveBench contains questions that are based on recently-released math\ncompetitions, arXiv papers, news articles, and datasets, and it contains\nharder, contamination-free versions of tasks from previous benchmarks such as\nBig-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-source\nmodels, as well as dozens of open-source models ranging from 0.5B to 110B in\nsize. LiveBench is difficult, with top models achieving below 65% accuracy. We\nrelease all questions, code, and model answers. Questions will be added and\nupdated on a monthly basis, and we will release new tasks and harder versions\nof tasks over time so that LiveBench can distinguish between the capabilities\nof LLMs as they improve in the future. We welcome community engagement and\ncollaboration for expanding the benchmark tasks and models.",
        "chunk-id": 4,
        "chunk": "Big-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-source\nmodels, as well as dozens of open-source models ranging from 0.5B to 110B in\nsize. LiveBench is difficult, with top models achieving below 65% accuracy. We\nrelease all questions, code, and model answers. Questions will be added and\nupdated on a monthly basis, and we will release new tasks and harder versions",
        "authors": [
            "Colin White",
            "Samuel Dooley",
            "Manley Roberts",
            "Arka Pal",
            "Ben Feuer",
            "Siddhartha Jain",
            "Ravid Shwartz-Ziv",
            "Neel Jain",
            "Khalid Saifullah",
            "Siddartha Naidu",
            "Chinmay Hegde",
            "Yann LeCun",
            "Tom Goldstein",
            "Willie Neiswanger",
            "Micah Goldblum"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:47:42+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19314v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19314v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Machine Learning"
        ]
    },
    {
        "id": 30000036,
        "doi": null,
        "title": "LiveBench: A Challenging, Contamination-Free LLM Benchmark",
        "abstract": "Test set contamination, wherein test data from a benchmark ends up in a newer\nmodel's training set, is a well-documented obstacle for fair LLM evaluation and\ncan quickly render benchmarks obsolete. To mitigate this, many recent\nbenchmarks crowdsource new prompts and evaluations from human or LLM judges;\nhowever, these can introduce significant biases, and break down when scoring\nhard questions. In this work, we introduce a new benchmark for LLMs designed to\nbe immune to both test set contamination and the pitfalls of LLM judging and\nhuman crowdsourcing. We release LiveBench, the first benchmark that (1)\ncontains frequently-updated questions from recent information sources, (2)\nscores answers automatically according to objective ground-truth values, and\n(3) contains a wide variety of challenging tasks, spanning math, coding,\nreasoning, language, instruction following, and data analysis. To achieve this,\nLiveBench contains questions that are based on recently-released math\ncompetitions, arXiv papers, news articles, and datasets, and it contains\nharder, contamination-free versions of tasks from previous benchmarks such as\nBig-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-source\nmodels, as well as dozens of open-source models ranging from 0.5B to 110B in\nsize. LiveBench is difficult, with top models achieving below 65% accuracy. We\nrelease all questions, code, and model answers. Questions will be added and\nupdated on a monthly basis, and we will release new tasks and harder versions\nof tasks over time so that LiveBench can distinguish between the capabilities\nof LLMs as they improve in the future. We welcome community engagement and\ncollaboration for expanding the benchmark tasks and models.",
        "chunk-id": 5,
        "chunk": "of tasks over time so that LiveBench can distinguish between the capabilities\nof LLMs as they improve in the future. We welcome community engagement and\ncollaboration for expanding the benchmark tasks and models.",
        "authors": [
            "Colin White",
            "Samuel Dooley",
            "Manley Roberts",
            "Arka Pal",
            "Ben Feuer",
            "Siddhartha Jain",
            "Ravid Shwartz-Ziv",
            "Neel Jain",
            "Khalid Saifullah",
            "Siddartha Naidu",
            "Chinmay Hegde",
            "Yann LeCun",
            "Tom Goldstein",
            "Willie Neiswanger",
            "Micah Goldblum"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:47:42+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19314v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19314v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Machine Learning"
        ]
    },
    {
        "id": 30000037,
        "doi": null,
        "title": "Zero-Query Adversarial Attack on Black-box Automatic Speech Recognition Systems",
        "abstract": "In recent years, extensive research has been conducted on the vulnerability\nof ASR systems, revealing that black-box adversarial example attacks pose\nsignificant threats to real-world ASR systems. However, most existing black-box\nattacks rely on queries to the target ASRs, which is impractical when queries\nare not permitted. In this paper, we propose ZQ-Attack, a transfer-based\nadversarial attack on ASR systems in the zero-query black-box setting. Through\na comprehensive review and categorization of modern ASR technologies, we first\nmeticulously select surrogate ASRs of diverse types to generate adversarial\nexamples. Following this, ZQ-Attack initializes the adversarial perturbation\nwith a scaled target command audio, rendering it relatively imperceptible while\nmaintaining effectiveness. Subsequently, to achieve high transferability of\nadversarial perturbations, we propose a sequential ensemble optimization\nalgorithm, which iteratively optimizes the adversarial perturbation on each\nsurrogate model, leveraging collaborative information from other models. We\nconduct extensive experiments to evaluate ZQ-Attack. In the over-the-line\nsetting, ZQ-Attack achieves a 100% success rate of attack (SRoA) with an\naverage signal-to-noise ratio (SNR) of 21.91dB on 4 online speech recognition\nservices, and attains an average SRoA of 100% and SNR of 19.67dB on 16\nopen-source ASRs. For commercial intelligent voice control devices, ZQ-Attack\nalso achieves a 100% SRoA with an average SNR of 15.77dB in the over-the-air\nsetting.",
        "chunk-id": 1,
        "chunk": "In recent years, extensive research has been conducted on the vulnerability\nof ASR systems, revealing that black-box adversarial example attacks pose\nsignificant threats to real-world ASR systems. However, most existing black-box\nattacks rely on queries to the target ASRs, which is impractical when queries\nare not permitted. In this paper, we propose ZQ-Attack, a transfer-based",
        "authors": [
            "Zheng Fang",
            "Tao Wang",
            "Lingchen Zhao",
            "Shenyi Zhang",
            "Bowen Li",
            "Yunjie Ge",
            "Qi Li",
            "Chao Shen",
            "Qian Wang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:39:36+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19311v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19311v1",
        "categories": [
            "Cryptography and Security",
            "Sound",
            "Audio and Speech Processing"
        ]
    },
    {
        "id": 30000037,
        "doi": null,
        "title": "Zero-Query Adversarial Attack on Black-box Automatic Speech Recognition Systems",
        "abstract": "In recent years, extensive research has been conducted on the vulnerability\nof ASR systems, revealing that black-box adversarial example attacks pose\nsignificant threats to real-world ASR systems. However, most existing black-box\nattacks rely on queries to the target ASRs, which is impractical when queries\nare not permitted. In this paper, we propose ZQ-Attack, a transfer-based\nadversarial attack on ASR systems in the zero-query black-box setting. Through\na comprehensive review and categorization of modern ASR technologies, we first\nmeticulously select surrogate ASRs of diverse types to generate adversarial\nexamples. Following this, ZQ-Attack initializes the adversarial perturbation\nwith a scaled target command audio, rendering it relatively imperceptible while\nmaintaining effectiveness. Subsequently, to achieve high transferability of\nadversarial perturbations, we propose a sequential ensemble optimization\nalgorithm, which iteratively optimizes the adversarial perturbation on each\nsurrogate model, leveraging collaborative information from other models. We\nconduct extensive experiments to evaluate ZQ-Attack. In the over-the-line\nsetting, ZQ-Attack achieves a 100% success rate of attack (SRoA) with an\naverage signal-to-noise ratio (SNR) of 21.91dB on 4 online speech recognition\nservices, and attains an average SRoA of 100% and SNR of 19.67dB on 16\nopen-source ASRs. For commercial intelligent voice control devices, ZQ-Attack\nalso achieves a 100% SRoA with an average SNR of 15.77dB in the over-the-air\nsetting.",
        "chunk-id": 2,
        "chunk": "adversarial attack on ASR systems in the zero-query black-box setting. Through\na comprehensive review and categorization of modern ASR technologies, we first\nmeticulously select surrogate ASRs of diverse types to generate adversarial\nexamples. Following this, ZQ-Attack initializes the adversarial perturbation",
        "authors": [
            "Zheng Fang",
            "Tao Wang",
            "Lingchen Zhao",
            "Shenyi Zhang",
            "Bowen Li",
            "Yunjie Ge",
            "Qi Li",
            "Chao Shen",
            "Qian Wang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:39:36+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19311v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19311v1",
        "categories": [
            "Cryptography and Security",
            "Sound",
            "Audio and Speech Processing"
        ]
    },
    {
        "id": 30000037,
        "doi": null,
        "title": "Zero-Query Adversarial Attack on Black-box Automatic Speech Recognition Systems",
        "abstract": "In recent years, extensive research has been conducted on the vulnerability\nof ASR systems, revealing that black-box adversarial example attacks pose\nsignificant threats to real-world ASR systems. However, most existing black-box\nattacks rely on queries to the target ASRs, which is impractical when queries\nare not permitted. In this paper, we propose ZQ-Attack, a transfer-based\nadversarial attack on ASR systems in the zero-query black-box setting. Through\na comprehensive review and categorization of modern ASR technologies, we first\nmeticulously select surrogate ASRs of diverse types to generate adversarial\nexamples. Following this, ZQ-Attack initializes the adversarial perturbation\nwith a scaled target command audio, rendering it relatively imperceptible while\nmaintaining effectiveness. Subsequently, to achieve high transferability of\nadversarial perturbations, we propose a sequential ensemble optimization\nalgorithm, which iteratively optimizes the adversarial perturbation on each\nsurrogate model, leveraging collaborative information from other models. We\nconduct extensive experiments to evaluate ZQ-Attack. In the over-the-line\nsetting, ZQ-Attack achieves a 100% success rate of attack (SRoA) with an\naverage signal-to-noise ratio (SNR) of 21.91dB on 4 online speech recognition\nservices, and attains an average SRoA of 100% and SNR of 19.67dB on 16\nopen-source ASRs. For commercial intelligent voice control devices, ZQ-Attack\nalso achieves a 100% SRoA with an average SNR of 15.77dB in the over-the-air\nsetting.",
        "chunk-id": 3,
        "chunk": "with a scaled target command audio, rendering it relatively imperceptible while\nmaintaining effectiveness. Subsequently, to achieve high transferability of\nadversarial perturbations, we propose a sequential ensemble optimization\nalgorithm, which iteratively optimizes the adversarial perturbation on each\nsurrogate model, leveraging collaborative information from other models. We",
        "authors": [
            "Zheng Fang",
            "Tao Wang",
            "Lingchen Zhao",
            "Shenyi Zhang",
            "Bowen Li",
            "Yunjie Ge",
            "Qi Li",
            "Chao Shen",
            "Qian Wang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:39:36+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19311v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19311v1",
        "categories": [
            "Cryptography and Security",
            "Sound",
            "Audio and Speech Processing"
        ]
    },
    {
        "id": 30000037,
        "doi": null,
        "title": "Zero-Query Adversarial Attack on Black-box Automatic Speech Recognition Systems",
        "abstract": "In recent years, extensive research has been conducted on the vulnerability\nof ASR systems, revealing that black-box adversarial example attacks pose\nsignificant threats to real-world ASR systems. However, most existing black-box\nattacks rely on queries to the target ASRs, which is impractical when queries\nare not permitted. In this paper, we propose ZQ-Attack, a transfer-based\nadversarial attack on ASR systems in the zero-query black-box setting. Through\na comprehensive review and categorization of modern ASR technologies, we first\nmeticulously select surrogate ASRs of diverse types to generate adversarial\nexamples. Following this, ZQ-Attack initializes the adversarial perturbation\nwith a scaled target command audio, rendering it relatively imperceptible while\nmaintaining effectiveness. Subsequently, to achieve high transferability of\nadversarial perturbations, we propose a sequential ensemble optimization\nalgorithm, which iteratively optimizes the adversarial perturbation on each\nsurrogate model, leveraging collaborative information from other models. We\nconduct extensive experiments to evaluate ZQ-Attack. In the over-the-line\nsetting, ZQ-Attack achieves a 100% success rate of attack (SRoA) with an\naverage signal-to-noise ratio (SNR) of 21.91dB on 4 online speech recognition\nservices, and attains an average SRoA of 100% and SNR of 19.67dB on 16\nopen-source ASRs. For commercial intelligent voice control devices, ZQ-Attack\nalso achieves a 100% SRoA with an average SNR of 15.77dB in the over-the-air\nsetting.",
        "chunk-id": 4,
        "chunk": "conduct extensive experiments to evaluate ZQ-Attack. In the over-the-line\nsetting, ZQ-Attack achieves a 100% success rate of attack (SRoA) with an\naverage signal-to-noise ratio (SNR) of 21.91dB on 4 online speech recognition\nservices, and attains an average SRoA of 100% and SNR of 19.67dB on 16\nopen-source ASRs. For commercial intelligent voice control devices, ZQ-Attack",
        "authors": [
            "Zheng Fang",
            "Tao Wang",
            "Lingchen Zhao",
            "Shenyi Zhang",
            "Bowen Li",
            "Yunjie Ge",
            "Qi Li",
            "Chao Shen",
            "Qian Wang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:39:36+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19311v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19311v1",
        "categories": [
            "Cryptography and Security",
            "Sound",
            "Audio and Speech Processing"
        ]
    },
    {
        "id": 30000037,
        "doi": null,
        "title": "Zero-Query Adversarial Attack on Black-box Automatic Speech Recognition Systems",
        "abstract": "In recent years, extensive research has been conducted on the vulnerability\nof ASR systems, revealing that black-box adversarial example attacks pose\nsignificant threats to real-world ASR systems. However, most existing black-box\nattacks rely on queries to the target ASRs, which is impractical when queries\nare not permitted. In this paper, we propose ZQ-Attack, a transfer-based\nadversarial attack on ASR systems in the zero-query black-box setting. Through\na comprehensive review and categorization of modern ASR technologies, we first\nmeticulously select surrogate ASRs of diverse types to generate adversarial\nexamples. Following this, ZQ-Attack initializes the adversarial perturbation\nwith a scaled target command audio, rendering it relatively imperceptible while\nmaintaining effectiveness. Subsequently, to achieve high transferability of\nadversarial perturbations, we propose a sequential ensemble optimization\nalgorithm, which iteratively optimizes the adversarial perturbation on each\nsurrogate model, leveraging collaborative information from other models. We\nconduct extensive experiments to evaluate ZQ-Attack. In the over-the-line\nsetting, ZQ-Attack achieves a 100% success rate of attack (SRoA) with an\naverage signal-to-noise ratio (SNR) of 21.91dB on 4 online speech recognition\nservices, and attains an average SRoA of 100% and SNR of 19.67dB on 16\nopen-source ASRs. For commercial intelligent voice control devices, ZQ-Attack\nalso achieves a 100% SRoA with an average SNR of 15.77dB in the over-the-air\nsetting.",
        "chunk-id": 5,
        "chunk": "also achieves a 100% SRoA with an average SNR of 15.77dB in the over-the-air\nsetting.",
        "authors": [
            "Zheng Fang",
            "Tao Wang",
            "Lingchen Zhao",
            "Shenyi Zhang",
            "Bowen Li",
            "Yunjie Ge",
            "Qi Li",
            "Chao Shen",
            "Qian Wang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:39:36+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19311v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19311v1",
        "categories": [
            "Cryptography and Security",
            "Sound",
            "Audio and Speech Processing"
        ]
    },
    {
        "id": 30000038,
        "doi": null,
        "title": "The Odyssey of Commonsense Causality: From Foundational Benchmarks to Cutting-Edge Reasoning",
        "abstract": "Understanding commonsense causality is a unique mark of intelligence for\nhumans. It helps people understand the principles of the real world better and\nbenefits the decision-making process related to causation. For instance,\ncommonsense causality is crucial in judging whether a defendant's action causes\nthe plaintiff's loss in determining legal liability. Despite its significance,\na systematic exploration of this topic is notably lacking. Our comprehensive\nsurvey bridges this gap by focusing on taxonomies, benchmarks, acquisition\nmethods, qualitative reasoning, and quantitative measurements in commonsense\ncausality, synthesizing insights from over 200 representative articles. Our\nwork aims to provide a systematic overview, update scholars on recent\nadvancements, provide a pragmatic guide for beginners, and highlight promising\nfuture research directions in this vital field.",
        "chunk-id": 1,
        "chunk": "Understanding commonsense causality is a unique mark of intelligence for\nhumans. It helps people understand the principles of the real world better and\nbenefits the decision-making process related to causation. For instance,\ncommonsense causality is crucial in judging whether a defendant's action causes\nthe plaintiff's loss in determining legal liability. Despite its significance,",
        "authors": [
            "Shaobo Cui",
            "Zhijing Jin",
            "Bernhard Sch\u00f6lkopf",
            "Boi Faltings"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:30:50+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19307v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19307v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 30000038,
        "doi": null,
        "title": "The Odyssey of Commonsense Causality: From Foundational Benchmarks to Cutting-Edge Reasoning",
        "abstract": "Understanding commonsense causality is a unique mark of intelligence for\nhumans. It helps people understand the principles of the real world better and\nbenefits the decision-making process related to causation. For instance,\ncommonsense causality is crucial in judging whether a defendant's action causes\nthe plaintiff's loss in determining legal liability. Despite its significance,\na systematic exploration of this topic is notably lacking. Our comprehensive\nsurvey bridges this gap by focusing on taxonomies, benchmarks, acquisition\nmethods, qualitative reasoning, and quantitative measurements in commonsense\ncausality, synthesizing insights from over 200 representative articles. Our\nwork aims to provide a systematic overview, update scholars on recent\nadvancements, provide a pragmatic guide for beginners, and highlight promising\nfuture research directions in this vital field.",
        "chunk-id": 2,
        "chunk": "a systematic exploration of this topic is notably lacking. Our comprehensive\nsurvey bridges this gap by focusing on taxonomies, benchmarks, acquisition\nmethods, qualitative reasoning, and quantitative measurements in commonsense\ncausality, synthesizing insights from over 200 representative articles. Our\nwork aims to provide a systematic overview, update scholars on recent",
        "authors": [
            "Shaobo Cui",
            "Zhijing Jin",
            "Bernhard Sch\u00f6lkopf",
            "Boi Faltings"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:30:50+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19307v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19307v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 30000038,
        "doi": null,
        "title": "The Odyssey of Commonsense Causality: From Foundational Benchmarks to Cutting-Edge Reasoning",
        "abstract": "Understanding commonsense causality is a unique mark of intelligence for\nhumans. It helps people understand the principles of the real world better and\nbenefits the decision-making process related to causation. For instance,\ncommonsense causality is crucial in judging whether a defendant's action causes\nthe plaintiff's loss in determining legal liability. Despite its significance,\na systematic exploration of this topic is notably lacking. Our comprehensive\nsurvey bridges this gap by focusing on taxonomies, benchmarks, acquisition\nmethods, qualitative reasoning, and quantitative measurements in commonsense\ncausality, synthesizing insights from over 200 representative articles. Our\nwork aims to provide a systematic overview, update scholars on recent\nadvancements, provide a pragmatic guide for beginners, and highlight promising\nfuture research directions in this vital field.",
        "chunk-id": 3,
        "chunk": "advancements, provide a pragmatic guide for beginners, and highlight promising\nfuture research directions in this vital field.",
        "authors": [
            "Shaobo Cui",
            "Zhijing Jin",
            "Bernhard Sch\u00f6lkopf",
            "Boi Faltings"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:30:50+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19307v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19307v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 30000039,
        "doi": null,
        "title": "Understanding Routing-Induced Censorship Changes Globally",
        "abstract": "Internet censorship is pervasive, with significant effort dedicated to\nunderstanding what is censored, and where. Prior censorship work however have\nidentified significant inconsistencies in their results; experiments show\nunexplained non-determinism thought to be caused by censor load, end-host\ngeographic diversity, or incomplete censorship -- inconsistencies which impede\nreliable, repeatable and correct understanding of global censorship. In this\nwork we investigate the extent to which Equal-cost Multi-path (ECMP) routing is\nthe cause for these inconsistencies, developing methods to measure and\ncompensate for them. We find ECMP routing significantly changes observed\ncensorship across protocols, censor mechanisms, and in 17 countries. We\nidentify that previously observed non-determinism or regional variations are\nattributable to measurements between fixed end-hosts taking different routes\nbased on Flow-ID; i.e., choice of intra-subnet source IP or ephemeral source\nport leads to differences in observed censorship. To achieve this we develop\nnew route-stable censorship measurement methods that allow consistent\nmeasurement of DNS, HTTP, and HTTPS censorship. We find ECMP routing yields\ncensorship changes across 42% of IPs and 51% of ASes, but that impact is not\nuniform. We identify numerous causes of the behavior, ranging from likely\nfailed infrastructure, to routes to the same end-host taking geographically\ndiverse paths which experience differences in censorship en-route. Finally, we\nexplore our results in the context of prior global measurement studies,\nexploring first the applicability of our findings to prior observed variations,\nand then demonstrating how specific experiments from two studies could be\nimpacted by, and specific results are explainable by, ECMP routing. Our work\npoints to methods for improving future studies, reducing inconsistencies and\nincreasing repeatability.",
        "chunk-id": 1,
        "chunk": "Internet censorship is pervasive, with significant effort dedicated to\nunderstanding what is censored, and where. Prior censorship work however have\nidentified significant inconsistencies in their results; experiments show\nunexplained non-determinism thought to be caused by censor load, end-host\ngeographic diversity, or incomplete censorship -- inconsistencies which impede",
        "authors": [
            "Abhishek Bhaskar",
            "Paul Pearce"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:21:31+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19304v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19304v1",
        "categories": [
            "Networking and Internet Architecture",
            "Cryptography and Security"
        ]
    },
    {
        "id": 30000039,
        "doi": null,
        "title": "Understanding Routing-Induced Censorship Changes Globally",
        "abstract": "Internet censorship is pervasive, with significant effort dedicated to\nunderstanding what is censored, and where. Prior censorship work however have\nidentified significant inconsistencies in their results; experiments show\nunexplained non-determinism thought to be caused by censor load, end-host\ngeographic diversity, or incomplete censorship -- inconsistencies which impede\nreliable, repeatable and correct understanding of global censorship. In this\nwork we investigate the extent to which Equal-cost Multi-path (ECMP) routing is\nthe cause for these inconsistencies, developing methods to measure and\ncompensate for them. We find ECMP routing significantly changes observed\ncensorship across protocols, censor mechanisms, and in 17 countries. We\nidentify that previously observed non-determinism or regional variations are\nattributable to measurements between fixed end-hosts taking different routes\nbased on Flow-ID; i.e., choice of intra-subnet source IP or ephemeral source\nport leads to differences in observed censorship. To achieve this we develop\nnew route-stable censorship measurement methods that allow consistent\nmeasurement of DNS, HTTP, and HTTPS censorship. We find ECMP routing yields\ncensorship changes across 42% of IPs and 51% of ASes, but that impact is not\nuniform. We identify numerous causes of the behavior, ranging from likely\nfailed infrastructure, to routes to the same end-host taking geographically\ndiverse paths which experience differences in censorship en-route. Finally, we\nexplore our results in the context of prior global measurement studies,\nexploring first the applicability of our findings to prior observed variations,\nand then demonstrating how specific experiments from two studies could be\nimpacted by, and specific results are explainable by, ECMP routing. Our work\npoints to methods for improving future studies, reducing inconsistencies and\nincreasing repeatability.",
        "chunk-id": 2,
        "chunk": "reliable, repeatable and correct understanding of global censorship. In this\nwork we investigate the extent to which Equal-cost Multi-path (ECMP) routing is\nthe cause for these inconsistencies, developing methods to measure and\ncompensate for them. We find ECMP routing significantly changes observed\ncensorship across protocols, censor mechanisms, and in 17 countries. We",
        "authors": [
            "Abhishek Bhaskar",
            "Paul Pearce"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:21:31+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19304v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19304v1",
        "categories": [
            "Networking and Internet Architecture",
            "Cryptography and Security"
        ]
    },
    {
        "id": 30000039,
        "doi": null,
        "title": "Understanding Routing-Induced Censorship Changes Globally",
        "abstract": "Internet censorship is pervasive, with significant effort dedicated to\nunderstanding what is censored, and where. Prior censorship work however have\nidentified significant inconsistencies in their results; experiments show\nunexplained non-determinism thought to be caused by censor load, end-host\ngeographic diversity, or incomplete censorship -- inconsistencies which impede\nreliable, repeatable and correct understanding of global censorship. In this\nwork we investigate the extent to which Equal-cost Multi-path (ECMP) routing is\nthe cause for these inconsistencies, developing methods to measure and\ncompensate for them. We find ECMP routing significantly changes observed\ncensorship across protocols, censor mechanisms, and in 17 countries. We\nidentify that previously observed non-determinism or regional variations are\nattributable to measurements between fixed end-hosts taking different routes\nbased on Flow-ID; i.e., choice of intra-subnet source IP or ephemeral source\nport leads to differences in observed censorship. To achieve this we develop\nnew route-stable censorship measurement methods that allow consistent\nmeasurement of DNS, HTTP, and HTTPS censorship. We find ECMP routing yields\ncensorship changes across 42% of IPs and 51% of ASes, but that impact is not\nuniform. We identify numerous causes of the behavior, ranging from likely\nfailed infrastructure, to routes to the same end-host taking geographically\ndiverse paths which experience differences in censorship en-route. Finally, we\nexplore our results in the context of prior global measurement studies,\nexploring first the applicability of our findings to prior observed variations,\nand then demonstrating how specific experiments from two studies could be\nimpacted by, and specific results are explainable by, ECMP routing. Our work\npoints to methods for improving future studies, reducing inconsistencies and\nincreasing repeatability.",
        "chunk-id": 3,
        "chunk": "identify that previously observed non-determinism or regional variations are\nattributable to measurements between fixed end-hosts taking different routes\nbased on Flow-ID; i.e., choice of intra-subnet source IP or ephemeral source\nport leads to differences in observed censorship. To achieve this we develop\nnew route-stable censorship measurement methods that allow consistent",
        "authors": [
            "Abhishek Bhaskar",
            "Paul Pearce"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:21:31+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19304v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19304v1",
        "categories": [
            "Networking and Internet Architecture",
            "Cryptography and Security"
        ]
    },
    {
        "id": 30000039,
        "doi": null,
        "title": "Understanding Routing-Induced Censorship Changes Globally",
        "abstract": "Internet censorship is pervasive, with significant effort dedicated to\nunderstanding what is censored, and where. Prior censorship work however have\nidentified significant inconsistencies in their results; experiments show\nunexplained non-determinism thought to be caused by censor load, end-host\ngeographic diversity, or incomplete censorship -- inconsistencies which impede\nreliable, repeatable and correct understanding of global censorship. In this\nwork we investigate the extent to which Equal-cost Multi-path (ECMP) routing is\nthe cause for these inconsistencies, developing methods to measure and\ncompensate for them. We find ECMP routing significantly changes observed\ncensorship across protocols, censor mechanisms, and in 17 countries. We\nidentify that previously observed non-determinism or regional variations are\nattributable to measurements between fixed end-hosts taking different routes\nbased on Flow-ID; i.e., choice of intra-subnet source IP or ephemeral source\nport leads to differences in observed censorship. To achieve this we develop\nnew route-stable censorship measurement methods that allow consistent\nmeasurement of DNS, HTTP, and HTTPS censorship. We find ECMP routing yields\ncensorship changes across 42% of IPs and 51% of ASes, but that impact is not\nuniform. We identify numerous causes of the behavior, ranging from likely\nfailed infrastructure, to routes to the same end-host taking geographically\ndiverse paths which experience differences in censorship en-route. Finally, we\nexplore our results in the context of prior global measurement studies,\nexploring first the applicability of our findings to prior observed variations,\nand then demonstrating how specific experiments from two studies could be\nimpacted by, and specific results are explainable by, ECMP routing. Our work\npoints to methods for improving future studies, reducing inconsistencies and\nincreasing repeatability.",
        "chunk-id": 4,
        "chunk": "measurement of DNS, HTTP, and HTTPS censorship. We find ECMP routing yields\ncensorship changes across 42% of IPs and 51% of ASes, but that impact is not\nuniform. We identify numerous causes of the behavior, ranging from likely\nfailed infrastructure, to routes to the same end-host taking geographically\ndiverse paths which experience differences in censorship en-route. Finally, we",
        "authors": [
            "Abhishek Bhaskar",
            "Paul Pearce"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:21:31+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19304v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19304v1",
        "categories": [
            "Networking and Internet Architecture",
            "Cryptography and Security"
        ]
    },
    {
        "id": 30000039,
        "doi": null,
        "title": "Understanding Routing-Induced Censorship Changes Globally",
        "abstract": "Internet censorship is pervasive, with significant effort dedicated to\nunderstanding what is censored, and where. Prior censorship work however have\nidentified significant inconsistencies in their results; experiments show\nunexplained non-determinism thought to be caused by censor load, end-host\ngeographic diversity, or incomplete censorship -- inconsistencies which impede\nreliable, repeatable and correct understanding of global censorship. In this\nwork we investigate the extent to which Equal-cost Multi-path (ECMP) routing is\nthe cause for these inconsistencies, developing methods to measure and\ncompensate for them. We find ECMP routing significantly changes observed\ncensorship across protocols, censor mechanisms, and in 17 countries. We\nidentify that previously observed non-determinism or regional variations are\nattributable to measurements between fixed end-hosts taking different routes\nbased on Flow-ID; i.e., choice of intra-subnet source IP or ephemeral source\nport leads to differences in observed censorship. To achieve this we develop\nnew route-stable censorship measurement methods that allow consistent\nmeasurement of DNS, HTTP, and HTTPS censorship. We find ECMP routing yields\ncensorship changes across 42% of IPs and 51% of ASes, but that impact is not\nuniform. We identify numerous causes of the behavior, ranging from likely\nfailed infrastructure, to routes to the same end-host taking geographically\ndiverse paths which experience differences in censorship en-route. Finally, we\nexplore our results in the context of prior global measurement studies,\nexploring first the applicability of our findings to prior observed variations,\nand then demonstrating how specific experiments from two studies could be\nimpacted by, and specific results are explainable by, ECMP routing. Our work\npoints to methods for improving future studies, reducing inconsistencies and\nincreasing repeatability.",
        "chunk-id": 5,
        "chunk": "explore our results in the context of prior global measurement studies,\nexploring first the applicability of our findings to prior observed variations,\nand then demonstrating how specific experiments from two studies could be\nimpacted by, and specific results are explainable by, ECMP routing. Our work\npoints to methods for improving future studies, reducing inconsistencies and",
        "authors": [
            "Abhishek Bhaskar",
            "Paul Pearce"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:21:31+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19304v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19304v1",
        "categories": [
            "Networking and Internet Architecture",
            "Cryptography and Security"
        ]
    },
    {
        "id": 30000039,
        "doi": null,
        "title": "Understanding Routing-Induced Censorship Changes Globally",
        "abstract": "Internet censorship is pervasive, with significant effort dedicated to\nunderstanding what is censored, and where. Prior censorship work however have\nidentified significant inconsistencies in their results; experiments show\nunexplained non-determinism thought to be caused by censor load, end-host\ngeographic diversity, or incomplete censorship -- inconsistencies which impede\nreliable, repeatable and correct understanding of global censorship. In this\nwork we investigate the extent to which Equal-cost Multi-path (ECMP) routing is\nthe cause for these inconsistencies, developing methods to measure and\ncompensate for them. We find ECMP routing significantly changes observed\ncensorship across protocols, censor mechanisms, and in 17 countries. We\nidentify that previously observed non-determinism or regional variations are\nattributable to measurements between fixed end-hosts taking different routes\nbased on Flow-ID; i.e., choice of intra-subnet source IP or ephemeral source\nport leads to differences in observed censorship. To achieve this we develop\nnew route-stable censorship measurement methods that allow consistent\nmeasurement of DNS, HTTP, and HTTPS censorship. We find ECMP routing yields\ncensorship changes across 42% of IPs and 51% of ASes, but that impact is not\nuniform. We identify numerous causes of the behavior, ranging from likely\nfailed infrastructure, to routes to the same end-host taking geographically\ndiverse paths which experience differences in censorship en-route. Finally, we\nexplore our results in the context of prior global measurement studies,\nexploring first the applicability of our findings to prior observed variations,\nand then demonstrating how specific experiments from two studies could be\nimpacted by, and specific results are explainable by, ECMP routing. Our work\npoints to methods for improving future studies, reducing inconsistencies and\nincreasing repeatability.",
        "chunk-id": 6,
        "chunk": "increasing repeatability.",
        "authors": [
            "Abhishek Bhaskar",
            "Paul Pearce"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:21:31+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19304v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19304v1",
        "categories": [
            "Networking and Internet Architecture",
            "Cryptography and Security"
        ]
    },
    {
        "id": 30000040,
        "doi": null,
        "title": "Mapping Land Naturalness from Sentinel-2 using Deep Contextual and Geographical Priors",
        "abstract": "In recent decades, the causes and consequences of climate change have\naccelerated, affecting our planet on an unprecedented scale. This change is\nclosely tied to the ways in which humans alter their surroundings. As our\nactions continue to impact natural areas, using satellite images to observe and\nmeasure these effects has become crucial for understanding and combating\nclimate change. Aiming to map land naturalness on the continuum of modern human\npressure, we have developed a multi-modal supervised deep learning framework\nthat addresses the unique challenges of satellite data and the task at hand. We\nincorporate contextual and geographical priors, represented by corresponding\ncoordinate information and broader contextual information, including and\nsurrounding the immediate patch to be predicted. Our framework improves the\nmodel's predictive performance in mapping land naturalness from Sentinel-2\ndata, a type of multi-spectral optical satellite imagery. Recognizing that our\nprotective measures are only as effective as our understanding of the\necosystem, quantifying naturalness serves as a crucial step toward enhancing\nour environmental stewardship.",
        "chunk-id": 1,
        "chunk": "In recent decades, the causes and consequences of climate change have\naccelerated, affecting our planet on an unprecedented scale. This change is\nclosely tied to the ways in which humans alter their surroundings. As our\nactions continue to impact natural areas, using satellite images to observe and\nmeasure these effects has become crucial for understanding and combating",
        "authors": [
            "Burak Ekim",
            "Michael Schmitt"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:17:33+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19302v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19302v1",
        "categories": [
            "Computer Vision and Pattern Recognition",
            "Machine Learning"
        ]
    },
    {
        "id": 30000040,
        "doi": null,
        "title": "Mapping Land Naturalness from Sentinel-2 using Deep Contextual and Geographical Priors",
        "abstract": "In recent decades, the causes and consequences of climate change have\naccelerated, affecting our planet on an unprecedented scale. This change is\nclosely tied to the ways in which humans alter their surroundings. As our\nactions continue to impact natural areas, using satellite images to observe and\nmeasure these effects has become crucial for understanding and combating\nclimate change. Aiming to map land naturalness on the continuum of modern human\npressure, we have developed a multi-modal supervised deep learning framework\nthat addresses the unique challenges of satellite data and the task at hand. We\nincorporate contextual and geographical priors, represented by corresponding\ncoordinate information and broader contextual information, including and\nsurrounding the immediate patch to be predicted. Our framework improves the\nmodel's predictive performance in mapping land naturalness from Sentinel-2\ndata, a type of multi-spectral optical satellite imagery. Recognizing that our\nprotective measures are only as effective as our understanding of the\necosystem, quantifying naturalness serves as a crucial step toward enhancing\nour environmental stewardship.",
        "chunk-id": 2,
        "chunk": "climate change. Aiming to map land naturalness on the continuum of modern human\npressure, we have developed a multi-modal supervised deep learning framework\nthat addresses the unique challenges of satellite data and the task at hand. We\nincorporate contextual and geographical priors, represented by corresponding",
        "authors": [
            "Burak Ekim",
            "Michael Schmitt"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:17:33+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19302v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19302v1",
        "categories": [
            "Computer Vision and Pattern Recognition",
            "Machine Learning"
        ]
    },
    {
        "id": 30000040,
        "doi": null,
        "title": "Mapping Land Naturalness from Sentinel-2 using Deep Contextual and Geographical Priors",
        "abstract": "In recent decades, the causes and consequences of climate change have\naccelerated, affecting our planet on an unprecedented scale. This change is\nclosely tied to the ways in which humans alter their surroundings. As our\nactions continue to impact natural areas, using satellite images to observe and\nmeasure these effects has become crucial for understanding and combating\nclimate change. Aiming to map land naturalness on the continuum of modern human\npressure, we have developed a multi-modal supervised deep learning framework\nthat addresses the unique challenges of satellite data and the task at hand. We\nincorporate contextual and geographical priors, represented by corresponding\ncoordinate information and broader contextual information, including and\nsurrounding the immediate patch to be predicted. Our framework improves the\nmodel's predictive performance in mapping land naturalness from Sentinel-2\ndata, a type of multi-spectral optical satellite imagery. Recognizing that our\nprotective measures are only as effective as our understanding of the\necosystem, quantifying naturalness serves as a crucial step toward enhancing\nour environmental stewardship.",
        "chunk-id": 3,
        "chunk": "coordinate information and broader contextual information, including and\nsurrounding the immediate patch to be predicted. Our framework improves the\nmodel's predictive performance in mapping land naturalness from Sentinel-2\ndata, a type of multi-spectral optical satellite imagery. Recognizing that our\nprotective measures are only as effective as our understanding of the",
        "authors": [
            "Burak Ekim",
            "Michael Schmitt"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:17:33+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19302v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19302v1",
        "categories": [
            "Computer Vision and Pattern Recognition",
            "Machine Learning"
        ]
    },
    {
        "id": 30000040,
        "doi": null,
        "title": "Mapping Land Naturalness from Sentinel-2 using Deep Contextual and Geographical Priors",
        "abstract": "In recent decades, the causes and consequences of climate change have\naccelerated, affecting our planet on an unprecedented scale. This change is\nclosely tied to the ways in which humans alter their surroundings. As our\nactions continue to impact natural areas, using satellite images to observe and\nmeasure these effects has become crucial for understanding and combating\nclimate change. Aiming to map land naturalness on the continuum of modern human\npressure, we have developed a multi-modal supervised deep learning framework\nthat addresses the unique challenges of satellite data and the task at hand. We\nincorporate contextual and geographical priors, represented by corresponding\ncoordinate information and broader contextual information, including and\nsurrounding the immediate patch to be predicted. Our framework improves the\nmodel's predictive performance in mapping land naturalness from Sentinel-2\ndata, a type of multi-spectral optical satellite imagery. Recognizing that our\nprotective measures are only as effective as our understanding of the\necosystem, quantifying naturalness serves as a crucial step toward enhancing\nour environmental stewardship.",
        "chunk-id": 4,
        "chunk": "ecosystem, quantifying naturalness serves as a crucial step toward enhancing\nour environmental stewardship.",
        "authors": [
            "Burak Ekim",
            "Michael Schmitt"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:17:33+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19302v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19302v1",
        "categories": [
            "Computer Vision and Pattern Recognition",
            "Machine Learning"
        ]
    },
    {
        "id": 30000041,
        "doi": null,
        "title": "MCNC: Manifold Constrained Network Compression",
        "abstract": "The outstanding performance of large foundational models across diverse\ntasks-from computer vision to speech and natural language processing-has\nsignificantly increased their demand. However, storing and transmitting these\nmodels pose significant challenges due to their massive size (e.g., 350GB for\nGPT-3). Recent literature has focused on compressing the original weights or\nreducing the number of parameters required for fine-tuning these models. These\ncompression methods typically involve constraining the parameter space, for\nexample, through low-rank reparametrization (e.g., LoRA) or quantization (e.g.,\nQLoRA) during model training. In this paper, we present MCNC as a novel model\ncompression method that constrains the parameter space to low-dimensional\npre-defined and frozen nonlinear manifolds, which effectively cover this space.\nGiven the prevalence of good solutions in over-parameterized deep neural\nnetworks, we show that by constraining the parameter space to our proposed\nmanifold, we can identify high-quality solutions while achieving unprecedented\ncompression rates across a wide variety of tasks. Through extensive experiments\nin computer vision and natural language processing tasks, we demonstrate that\nour method, MCNC, significantly outperforms state-of-the-art baselines in terms\nof compression, accuracy, and/or model reconstruction time.",
        "chunk-id": 1,
        "chunk": "The outstanding performance of large foundational models across diverse\ntasks-from computer vision to speech and natural language processing-has\nsignificantly increased their demand. However, storing and transmitting these\nmodels pose significant challenges due to their massive size (e.g., 350GB for\nGPT-3). Recent literature has focused on compressing the original weights or",
        "authors": [
            "Chayne Thrash",
            "Ali Abbasi",
            "Parsa Nooralinejad",
            "Soroush Abbasi Koohpayegani",
            "Reed Andreas",
            "Hamed Pirsiavash",
            "Soheil Kolouri"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:17:26+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19301v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19301v1",
        "categories": [
            "Machine Learning"
        ]
    },
    {
        "id": 30000041,
        "doi": null,
        "title": "MCNC: Manifold Constrained Network Compression",
        "abstract": "The outstanding performance of large foundational models across diverse\ntasks-from computer vision to speech and natural language processing-has\nsignificantly increased their demand. However, storing and transmitting these\nmodels pose significant challenges due to their massive size (e.g., 350GB for\nGPT-3). Recent literature has focused on compressing the original weights or\nreducing the number of parameters required for fine-tuning these models. These\ncompression methods typically involve constraining the parameter space, for\nexample, through low-rank reparametrization (e.g., LoRA) or quantization (e.g.,\nQLoRA) during model training. In this paper, we present MCNC as a novel model\ncompression method that constrains the parameter space to low-dimensional\npre-defined and frozen nonlinear manifolds, which effectively cover this space.\nGiven the prevalence of good solutions in over-parameterized deep neural\nnetworks, we show that by constraining the parameter space to our proposed\nmanifold, we can identify high-quality solutions while achieving unprecedented\ncompression rates across a wide variety of tasks. Through extensive experiments\nin computer vision and natural language processing tasks, we demonstrate that\nour method, MCNC, significantly outperforms state-of-the-art baselines in terms\nof compression, accuracy, and/or model reconstruction time.",
        "chunk-id": 2,
        "chunk": "reducing the number of parameters required for fine-tuning these models. These\ncompression methods typically involve constraining the parameter space, for\nexample, through low-rank reparametrization (e.g., LoRA) or quantization (e.g.,\nQLoRA) during model training. In this paper, we present MCNC as a novel model",
        "authors": [
            "Chayne Thrash",
            "Ali Abbasi",
            "Parsa Nooralinejad",
            "Soroush Abbasi Koohpayegani",
            "Reed Andreas",
            "Hamed Pirsiavash",
            "Soheil Kolouri"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:17:26+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19301v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19301v1",
        "categories": [
            "Machine Learning"
        ]
    },
    {
        "id": 30000041,
        "doi": null,
        "title": "MCNC: Manifold Constrained Network Compression",
        "abstract": "The outstanding performance of large foundational models across diverse\ntasks-from computer vision to speech and natural language processing-has\nsignificantly increased their demand. However, storing and transmitting these\nmodels pose significant challenges due to their massive size (e.g., 350GB for\nGPT-3). Recent literature has focused on compressing the original weights or\nreducing the number of parameters required for fine-tuning these models. These\ncompression methods typically involve constraining the parameter space, for\nexample, through low-rank reparametrization (e.g., LoRA) or quantization (e.g.,\nQLoRA) during model training. In this paper, we present MCNC as a novel model\ncompression method that constrains the parameter space to low-dimensional\npre-defined and frozen nonlinear manifolds, which effectively cover this space.\nGiven the prevalence of good solutions in over-parameterized deep neural\nnetworks, we show that by constraining the parameter space to our proposed\nmanifold, we can identify high-quality solutions while achieving unprecedented\ncompression rates across a wide variety of tasks. Through extensive experiments\nin computer vision and natural language processing tasks, we demonstrate that\nour method, MCNC, significantly outperforms state-of-the-art baselines in terms\nof compression, accuracy, and/or model reconstruction time.",
        "chunk-id": 3,
        "chunk": "compression method that constrains the parameter space to low-dimensional\npre-defined and frozen nonlinear manifolds, which effectively cover this space.\nGiven the prevalence of good solutions in over-parameterized deep neural\nnetworks, we show that by constraining the parameter space to our proposed\nmanifold, we can identify high-quality solutions while achieving unprecedented",
        "authors": [
            "Chayne Thrash",
            "Ali Abbasi",
            "Parsa Nooralinejad",
            "Soroush Abbasi Koohpayegani",
            "Reed Andreas",
            "Hamed Pirsiavash",
            "Soheil Kolouri"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:17:26+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19301v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19301v1",
        "categories": [
            "Machine Learning"
        ]
    },
    {
        "id": 30000041,
        "doi": null,
        "title": "MCNC: Manifold Constrained Network Compression",
        "abstract": "The outstanding performance of large foundational models across diverse\ntasks-from computer vision to speech and natural language processing-has\nsignificantly increased their demand. However, storing and transmitting these\nmodels pose significant challenges due to their massive size (e.g., 350GB for\nGPT-3). Recent literature has focused on compressing the original weights or\nreducing the number of parameters required for fine-tuning these models. These\ncompression methods typically involve constraining the parameter space, for\nexample, through low-rank reparametrization (e.g., LoRA) or quantization (e.g.,\nQLoRA) during model training. In this paper, we present MCNC as a novel model\ncompression method that constrains the parameter space to low-dimensional\npre-defined and frozen nonlinear manifolds, which effectively cover this space.\nGiven the prevalence of good solutions in over-parameterized deep neural\nnetworks, we show that by constraining the parameter space to our proposed\nmanifold, we can identify high-quality solutions while achieving unprecedented\ncompression rates across a wide variety of tasks. Through extensive experiments\nin computer vision and natural language processing tasks, we demonstrate that\nour method, MCNC, significantly outperforms state-of-the-art baselines in terms\nof compression, accuracy, and/or model reconstruction time.",
        "chunk-id": 4,
        "chunk": "compression rates across a wide variety of tasks. Through extensive experiments\nin computer vision and natural language processing tasks, we demonstrate that\nour method, MCNC, significantly outperforms state-of-the-art baselines in terms\nof compression, accuracy, and/or model reconstruction time.",
        "authors": [
            "Chayne Thrash",
            "Ali Abbasi",
            "Parsa Nooralinejad",
            "Soroush Abbasi Koohpayegani",
            "Reed Andreas",
            "Hamed Pirsiavash",
            "Soheil Kolouri"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:17:26+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19301v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19301v1",
        "categories": [
            "Machine Learning"
        ]
    },
    {
        "id": 30000042,
        "doi": null,
        "title": "PNeRV: A Polynomial Neural Representation for Videos",
        "abstract": "Extracting Implicit Neural Representations (INRs) on video data poses unique\nchallenges due to the additional temporal dimension. In the context of videos,\nINRs have predominantly relied on a frame-only parameterization, which\nsacrifices the spatiotemporal continuity observed in pixel-level (spatial)\nrepresentations. To mitigate this, we introduce Polynomial Neural\nRepresentation for Videos (PNeRV), a parameter-wise efficient, patch-wise INR\nfor videos that preserves spatiotemporal continuity. PNeRV leverages the\nmodeling capabilities of Polynomial Neural Networks to perform the modulation\nof a continuous spatial (patch) signal with a continuous time (frame) signal.\nWe further propose a custom Hierarchical Patch-wise Spatial Sampling Scheme\nthat ensures spatial continuity while retaining parameter efficiency. We also\nemploy a carefully designed Positional Embedding methodology to further enhance\nPNeRV's performance. Our extensive experimentation demonstrates that PNeRV\noutperforms the baselines in conventional Implicit Neural Representation tasks\nlike compression along with downstream applications that require spatiotemporal\ncontinuity in the underlying representation. PNeRV not only addresses the\nchallenges posed by video data in the realm of INRs but also opens new avenues\nfor advanced video processing and analysis.",
        "chunk-id": 1,
        "chunk": "Extracting Implicit Neural Representations (INRs) on video data poses unique\nchallenges due to the additional temporal dimension. In the context of videos,\nINRs have predominantly relied on a frame-only parameterization, which\nsacrifices the spatiotemporal continuity observed in pixel-level (spatial)\nrepresentations. To mitigate this, we introduce Polynomial Neural",
        "authors": [
            "Sonam Gupta",
            "Snehal Singh Tomar",
            "Grigorios G Chrysos",
            "Sukhendu Das",
            "A. N. Rajagopalan"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:15:22+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19299v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19299v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000042,
        "doi": null,
        "title": "PNeRV: A Polynomial Neural Representation for Videos",
        "abstract": "Extracting Implicit Neural Representations (INRs) on video data poses unique\nchallenges due to the additional temporal dimension. In the context of videos,\nINRs have predominantly relied on a frame-only parameterization, which\nsacrifices the spatiotemporal continuity observed in pixel-level (spatial)\nrepresentations. To mitigate this, we introduce Polynomial Neural\nRepresentation for Videos (PNeRV), a parameter-wise efficient, patch-wise INR\nfor videos that preserves spatiotemporal continuity. PNeRV leverages the\nmodeling capabilities of Polynomial Neural Networks to perform the modulation\nof a continuous spatial (patch) signal with a continuous time (frame) signal.\nWe further propose a custom Hierarchical Patch-wise Spatial Sampling Scheme\nthat ensures spatial continuity while retaining parameter efficiency. We also\nemploy a carefully designed Positional Embedding methodology to further enhance\nPNeRV's performance. Our extensive experimentation demonstrates that PNeRV\noutperforms the baselines in conventional Implicit Neural Representation tasks\nlike compression along with downstream applications that require spatiotemporal\ncontinuity in the underlying representation. PNeRV not only addresses the\nchallenges posed by video data in the realm of INRs but also opens new avenues\nfor advanced video processing and analysis.",
        "chunk-id": 2,
        "chunk": "Representation for Videos (PNeRV), a parameter-wise efficient, patch-wise INR\nfor videos that preserves spatiotemporal continuity. PNeRV leverages the\nmodeling capabilities of Polynomial Neural Networks to perform the modulation\nof a continuous spatial (patch) signal with a continuous time (frame) signal.\nWe further propose a custom Hierarchical Patch-wise Spatial Sampling Scheme",
        "authors": [
            "Sonam Gupta",
            "Snehal Singh Tomar",
            "Grigorios G Chrysos",
            "Sukhendu Das",
            "A. N. Rajagopalan"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:15:22+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19299v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19299v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000042,
        "doi": null,
        "title": "PNeRV: A Polynomial Neural Representation for Videos",
        "abstract": "Extracting Implicit Neural Representations (INRs) on video data poses unique\nchallenges due to the additional temporal dimension. In the context of videos,\nINRs have predominantly relied on a frame-only parameterization, which\nsacrifices the spatiotemporal continuity observed in pixel-level (spatial)\nrepresentations. To mitigate this, we introduce Polynomial Neural\nRepresentation for Videos (PNeRV), a parameter-wise efficient, patch-wise INR\nfor videos that preserves spatiotemporal continuity. PNeRV leverages the\nmodeling capabilities of Polynomial Neural Networks to perform the modulation\nof a continuous spatial (patch) signal with a continuous time (frame) signal.\nWe further propose a custom Hierarchical Patch-wise Spatial Sampling Scheme\nthat ensures spatial continuity while retaining parameter efficiency. We also\nemploy a carefully designed Positional Embedding methodology to further enhance\nPNeRV's performance. Our extensive experimentation demonstrates that PNeRV\noutperforms the baselines in conventional Implicit Neural Representation tasks\nlike compression along with downstream applications that require spatiotemporal\ncontinuity in the underlying representation. PNeRV not only addresses the\nchallenges posed by video data in the realm of INRs but also opens new avenues\nfor advanced video processing and analysis.",
        "chunk-id": 3,
        "chunk": "that ensures spatial continuity while retaining parameter efficiency. We also\nemploy a carefully designed Positional Embedding methodology to further enhance\nPNeRV's performance. Our extensive experimentation demonstrates that PNeRV\noutperforms the baselines in conventional Implicit Neural Representation tasks",
        "authors": [
            "Sonam Gupta",
            "Snehal Singh Tomar",
            "Grigorios G Chrysos",
            "Sukhendu Das",
            "A. N. Rajagopalan"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:15:22+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19299v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19299v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000042,
        "doi": null,
        "title": "PNeRV: A Polynomial Neural Representation for Videos",
        "abstract": "Extracting Implicit Neural Representations (INRs) on video data poses unique\nchallenges due to the additional temporal dimension. In the context of videos,\nINRs have predominantly relied on a frame-only parameterization, which\nsacrifices the spatiotemporal continuity observed in pixel-level (spatial)\nrepresentations. To mitigate this, we introduce Polynomial Neural\nRepresentation for Videos (PNeRV), a parameter-wise efficient, patch-wise INR\nfor videos that preserves spatiotemporal continuity. PNeRV leverages the\nmodeling capabilities of Polynomial Neural Networks to perform the modulation\nof a continuous spatial (patch) signal with a continuous time (frame) signal.\nWe further propose a custom Hierarchical Patch-wise Spatial Sampling Scheme\nthat ensures spatial continuity while retaining parameter efficiency. We also\nemploy a carefully designed Positional Embedding methodology to further enhance\nPNeRV's performance. Our extensive experimentation demonstrates that PNeRV\noutperforms the baselines in conventional Implicit Neural Representation tasks\nlike compression along with downstream applications that require spatiotemporal\ncontinuity in the underlying representation. PNeRV not only addresses the\nchallenges posed by video data in the realm of INRs but also opens new avenues\nfor advanced video processing and analysis.",
        "chunk-id": 4,
        "chunk": "like compression along with downstream applications that require spatiotemporal\ncontinuity in the underlying representation. PNeRV not only addresses the\nchallenges posed by video data in the realm of INRs but also opens new avenues\nfor advanced video processing and analysis.",
        "authors": [
            "Sonam Gupta",
            "Snehal Singh Tomar",
            "Grigorios G Chrysos",
            "Sukhendu Das",
            "A. N. Rajagopalan"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:15:22+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19299v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19299v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000043,
        "doi": null,
        "title": "Compositional Image Decomposition with Diffusion Models",
        "abstract": "Given an image of a natural scene, we are able to quickly decompose it into a\nset of components such as objects, lighting, shadows, and foreground. We can\nthen envision a scene where we combine certain components with those from other\nimages, for instance a set of objects from our bedroom and animals from a zoo\nunder the lighting conditions of a forest, even if we have never encountered\nsuch a scene before. In this paper, we present a method to decompose an image\ninto such compositional components. Our approach, Decomp Diffusion, is an\nunsupervised method which, when given a single image, infers a set of different\ncomponents in the image, each represented by a diffusion model. We demonstrate\nhow components can capture different factors of the scene, ranging from global\nscene descriptors like shadows or facial expression to local scene descriptors\nlike constituent objects. We further illustrate how inferred factors can be\nflexibly composed, even with factors inferred from other models, to generate a\nvariety of scenes sharply different than those seen in training time. Website\nand code at https://energy-based-model.github.io/decomp-diffusion.",
        "chunk-id": 1,
        "chunk": "Given an image of a natural scene, we are able to quickly decompose it into a\nset of components such as objects, lighting, shadows, and foreground. We can\nthen envision a scene where we combine certain components with those from other\nimages, for instance a set of objects from our bedroom and animals from a zoo",
        "authors": [
            "Jocelin Su",
            "Nan Liu",
            "Yanbo Wang",
            "Joshua B. Tenenbaum",
            "Yilun Du"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:13:34+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19298v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19298v1",
        "categories": [
            "Computer Vision and Pattern Recognition",
            "Machine Learning"
        ]
    },
    {
        "id": 30000043,
        "doi": null,
        "title": "Compositional Image Decomposition with Diffusion Models",
        "abstract": "Given an image of a natural scene, we are able to quickly decompose it into a\nset of components such as objects, lighting, shadows, and foreground. We can\nthen envision a scene where we combine certain components with those from other\nimages, for instance a set of objects from our bedroom and animals from a zoo\nunder the lighting conditions of a forest, even if we have never encountered\nsuch a scene before. In this paper, we present a method to decompose an image\ninto such compositional components. Our approach, Decomp Diffusion, is an\nunsupervised method which, when given a single image, infers a set of different\ncomponents in the image, each represented by a diffusion model. We demonstrate\nhow components can capture different factors of the scene, ranging from global\nscene descriptors like shadows or facial expression to local scene descriptors\nlike constituent objects. We further illustrate how inferred factors can be\nflexibly composed, even with factors inferred from other models, to generate a\nvariety of scenes sharply different than those seen in training time. Website\nand code at https://energy-based-model.github.io/decomp-diffusion.",
        "chunk-id": 2,
        "chunk": "under the lighting conditions of a forest, even if we have never encountered\nsuch a scene before. In this paper, we present a method to decompose an image\ninto such compositional components. Our approach, Decomp Diffusion, is an\nunsupervised method which, when given a single image, infers a set of different",
        "authors": [
            "Jocelin Su",
            "Nan Liu",
            "Yanbo Wang",
            "Joshua B. Tenenbaum",
            "Yilun Du"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:13:34+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19298v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19298v1",
        "categories": [
            "Computer Vision and Pattern Recognition",
            "Machine Learning"
        ]
    },
    {
        "id": 30000043,
        "doi": null,
        "title": "Compositional Image Decomposition with Diffusion Models",
        "abstract": "Given an image of a natural scene, we are able to quickly decompose it into a\nset of components such as objects, lighting, shadows, and foreground. We can\nthen envision a scene where we combine certain components with those from other\nimages, for instance a set of objects from our bedroom and animals from a zoo\nunder the lighting conditions of a forest, even if we have never encountered\nsuch a scene before. In this paper, we present a method to decompose an image\ninto such compositional components. Our approach, Decomp Diffusion, is an\nunsupervised method which, when given a single image, infers a set of different\ncomponents in the image, each represented by a diffusion model. We demonstrate\nhow components can capture different factors of the scene, ranging from global\nscene descriptors like shadows or facial expression to local scene descriptors\nlike constituent objects. We further illustrate how inferred factors can be\nflexibly composed, even with factors inferred from other models, to generate a\nvariety of scenes sharply different than those seen in training time. Website\nand code at https://energy-based-model.github.io/decomp-diffusion.",
        "chunk-id": 3,
        "chunk": "components in the image, each represented by a diffusion model. We demonstrate\nhow components can capture different factors of the scene, ranging from global\nscene descriptors like shadows or facial expression to local scene descriptors\nlike constituent objects. We further illustrate how inferred factors can be",
        "authors": [
            "Jocelin Su",
            "Nan Liu",
            "Yanbo Wang",
            "Joshua B. Tenenbaum",
            "Yilun Du"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:13:34+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19298v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19298v1",
        "categories": [
            "Computer Vision and Pattern Recognition",
            "Machine Learning"
        ]
    },
    {
        "id": 30000043,
        "doi": null,
        "title": "Compositional Image Decomposition with Diffusion Models",
        "abstract": "Given an image of a natural scene, we are able to quickly decompose it into a\nset of components such as objects, lighting, shadows, and foreground. We can\nthen envision a scene where we combine certain components with those from other\nimages, for instance a set of objects from our bedroom and animals from a zoo\nunder the lighting conditions of a forest, even if we have never encountered\nsuch a scene before. In this paper, we present a method to decompose an image\ninto such compositional components. Our approach, Decomp Diffusion, is an\nunsupervised method which, when given a single image, infers a set of different\ncomponents in the image, each represented by a diffusion model. We demonstrate\nhow components can capture different factors of the scene, ranging from global\nscene descriptors like shadows or facial expression to local scene descriptors\nlike constituent objects. We further illustrate how inferred factors can be\nflexibly composed, even with factors inferred from other models, to generate a\nvariety of scenes sharply different than those seen in training time. Website\nand code at https://energy-based-model.github.io/decomp-diffusion.",
        "chunk-id": 4,
        "chunk": "flexibly composed, even with factors inferred from other models, to generate a\nvariety of scenes sharply different than those seen in training time. Website\nand code at https://energy-based-model.github.io/decomp-diffusion.",
        "authors": [
            "Jocelin Su",
            "Nan Liu",
            "Yanbo Wang",
            "Joshua B. Tenenbaum",
            "Yilun Du"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:13:34+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19298v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19298v1",
        "categories": [
            "Computer Vision and Pattern Recognition",
            "Machine Learning"
        ]
    },
    {
        "id": 30000044,
        "doi": null,
        "title": "Enhancing Continual Learning in Visual Question Answering with Modality-Aware Feature Distillation",
        "abstract": "Continual learning focuses on incrementally training a model on a sequence of\ntasks with the aim of learning new tasks while minimizing performance drop on\nprevious tasks. Existing approaches at the intersection of Continual Learning\nand Visual Question Answering (VQA) do not study how the multimodal nature of\nthe input affects the learning dynamics of a model. In this paper, we\ndemonstrate that each modality evolves at different rates across a continuum of\ntasks and that this behavior occurs in established encoder-only models as well\nas modern recipes for developing Vision & Language (VL) models. Motivated by\nthis observation, we propose a modality-aware feature distillation (MAFED)\napproach which outperforms existing baselines across models of varying scale in\nthree multimodal continual learning settings. Furthermore, we provide ablations\nshowcasing that modality-aware distillation complements experience replay.\nOverall, our results emphasize the importance of addressing modality-specific\ndynamics to prevent forgetting in multimodal continual learning.",
        "chunk-id": 1,
        "chunk": "Continual learning focuses on incrementally training a model on a sequence of\ntasks with the aim of learning new tasks while minimizing performance drop on\nprevious tasks. Existing approaches at the intersection of Continual Learning\nand Visual Question Answering (VQA) do not study how the multimodal nature of\nthe input affects the learning dynamics of a model. In this paper, we",
        "authors": [
            "Malvina Nikandrou",
            "Georgios Pantazopoulos",
            "Ioannis Konstas",
            "Alessandro Suglia"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:12:57+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19297v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19297v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000044,
        "doi": null,
        "title": "Enhancing Continual Learning in Visual Question Answering with Modality-Aware Feature Distillation",
        "abstract": "Continual learning focuses on incrementally training a model on a sequence of\ntasks with the aim of learning new tasks while minimizing performance drop on\nprevious tasks. Existing approaches at the intersection of Continual Learning\nand Visual Question Answering (VQA) do not study how the multimodal nature of\nthe input affects the learning dynamics of a model. In this paper, we\ndemonstrate that each modality evolves at different rates across a continuum of\ntasks and that this behavior occurs in established encoder-only models as well\nas modern recipes for developing Vision & Language (VL) models. Motivated by\nthis observation, we propose a modality-aware feature distillation (MAFED)\napproach which outperforms existing baselines across models of varying scale in\nthree multimodal continual learning settings. Furthermore, we provide ablations\nshowcasing that modality-aware distillation complements experience replay.\nOverall, our results emphasize the importance of addressing modality-specific\ndynamics to prevent forgetting in multimodal continual learning.",
        "chunk-id": 2,
        "chunk": "demonstrate that each modality evolves at different rates across a continuum of\ntasks and that this behavior occurs in established encoder-only models as well\nas modern recipes for developing Vision & Language (VL) models. Motivated by\nthis observation, we propose a modality-aware feature distillation (MAFED)",
        "authors": [
            "Malvina Nikandrou",
            "Georgios Pantazopoulos",
            "Ioannis Konstas",
            "Alessandro Suglia"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:12:57+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19297v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19297v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000044,
        "doi": null,
        "title": "Enhancing Continual Learning in Visual Question Answering with Modality-Aware Feature Distillation",
        "abstract": "Continual learning focuses on incrementally training a model on a sequence of\ntasks with the aim of learning new tasks while minimizing performance drop on\nprevious tasks. Existing approaches at the intersection of Continual Learning\nand Visual Question Answering (VQA) do not study how the multimodal nature of\nthe input affects the learning dynamics of a model. In this paper, we\ndemonstrate that each modality evolves at different rates across a continuum of\ntasks and that this behavior occurs in established encoder-only models as well\nas modern recipes for developing Vision & Language (VL) models. Motivated by\nthis observation, we propose a modality-aware feature distillation (MAFED)\napproach which outperforms existing baselines across models of varying scale in\nthree multimodal continual learning settings. Furthermore, we provide ablations\nshowcasing that modality-aware distillation complements experience replay.\nOverall, our results emphasize the importance of addressing modality-specific\ndynamics to prevent forgetting in multimodal continual learning.",
        "chunk-id": 3,
        "chunk": "approach which outperforms existing baselines across models of varying scale in\nthree multimodal continual learning settings. Furthermore, we provide ablations\nshowcasing that modality-aware distillation complements experience replay.\nOverall, our results emphasize the importance of addressing modality-specific\ndynamics to prevent forgetting in multimodal continual learning.",
        "authors": [
            "Malvina Nikandrou",
            "Georgios Pantazopoulos",
            "Ioannis Konstas",
            "Alessandro Suglia"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:12:57+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19297v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19297v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000045,
        "doi": null,
        "title": "From Artificial Needles to Real Haystacks: Improving Retrieval Capabilities in LLMs by Finetuning on Synthetic Data",
        "abstract": "Recent studies have shown that Large Language Models (LLMs) struggle to\naccurately retrieve information and maintain reasoning capabilities when\nprocessing long-context inputs. To address these limitations, we propose a\nfinetuning approach utilizing a carefully designed synthetic dataset comprising\nnumerical key-value retrieval tasks. Our experiments on models like GPT-3.5\nTurbo and Mistral 7B demonstrate that finetuning LLMs on this dataset\nsignificantly improves LLMs' information retrieval and reasoning capabilities\nin longer-context settings. We present an analysis of the finetuned models,\nillustrating the transfer of skills from synthetic to real task evaluations\n(e.g., $10.5\\%$ improvement on $20$ documents MDQA at position $10$ for GPT-3.5\nTurbo). We also find that finetuned LLMs' performance on general benchmarks\nremains almost constant while LLMs finetuned on other baseline long-context\naugmentation data can encourage hallucination (e.g., on TriviaQA, Mistral 7B\nfinetuned on our synthetic data cause no performance drop while other baseline\ndata can cause a drop that ranges from $2.33\\%$ to $6.19\\%$). Our study\nhighlights the potential of finetuning on synthetic data for improving the\nperformance of LLMs on longer-context tasks.",
        "chunk-id": 1,
        "chunk": "Recent studies have shown that Large Language Models (LLMs) struggle to\naccurately retrieve information and maintain reasoning capabilities when\nprocessing long-context inputs. To address these limitations, we propose a\nfinetuning approach utilizing a carefully designed synthetic dataset comprising\nnumerical key-value retrieval tasks. Our experiments on models like GPT-3.5",
        "authors": [
            "Zheyang Xiong",
            "Vasilis Papageorgiou",
            "Kangwook Lee",
            "Dimitris Papailiopoulos"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:05:13+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19292v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19292v1",
        "categories": [
            "Machine Learning",
            "Artificial Intelligence",
            "Computation and Language"
        ]
    },
    {
        "id": 30000045,
        "doi": null,
        "title": "From Artificial Needles to Real Haystacks: Improving Retrieval Capabilities in LLMs by Finetuning on Synthetic Data",
        "abstract": "Recent studies have shown that Large Language Models (LLMs) struggle to\naccurately retrieve information and maintain reasoning capabilities when\nprocessing long-context inputs. To address these limitations, we propose a\nfinetuning approach utilizing a carefully designed synthetic dataset comprising\nnumerical key-value retrieval tasks. Our experiments on models like GPT-3.5\nTurbo and Mistral 7B demonstrate that finetuning LLMs on this dataset\nsignificantly improves LLMs' information retrieval and reasoning capabilities\nin longer-context settings. We present an analysis of the finetuned models,\nillustrating the transfer of skills from synthetic to real task evaluations\n(e.g., $10.5\\%$ improvement on $20$ documents MDQA at position $10$ for GPT-3.5\nTurbo). We also find that finetuned LLMs' performance on general benchmarks\nremains almost constant while LLMs finetuned on other baseline long-context\naugmentation data can encourage hallucination (e.g., on TriviaQA, Mistral 7B\nfinetuned on our synthetic data cause no performance drop while other baseline\ndata can cause a drop that ranges from $2.33\\%$ to $6.19\\%$). Our study\nhighlights the potential of finetuning on synthetic data for improving the\nperformance of LLMs on longer-context tasks.",
        "chunk-id": 2,
        "chunk": "Turbo and Mistral 7B demonstrate that finetuning LLMs on this dataset\nsignificantly improves LLMs' information retrieval and reasoning capabilities\nin longer-context settings. We present an analysis of the finetuned models,\nillustrating the transfer of skills from synthetic to real task evaluations\n(e.g., $10.5\\%$ improvement on $20$ documents MDQA at position $10$ for GPT-3.5",
        "authors": [
            "Zheyang Xiong",
            "Vasilis Papageorgiou",
            "Kangwook Lee",
            "Dimitris Papailiopoulos"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:05:13+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19292v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19292v1",
        "categories": [
            "Machine Learning",
            "Artificial Intelligence",
            "Computation and Language"
        ]
    },
    {
        "id": 30000045,
        "doi": null,
        "title": "From Artificial Needles to Real Haystacks: Improving Retrieval Capabilities in LLMs by Finetuning on Synthetic Data",
        "abstract": "Recent studies have shown that Large Language Models (LLMs) struggle to\naccurately retrieve information and maintain reasoning capabilities when\nprocessing long-context inputs. To address these limitations, we propose a\nfinetuning approach utilizing a carefully designed synthetic dataset comprising\nnumerical key-value retrieval tasks. Our experiments on models like GPT-3.5\nTurbo and Mistral 7B demonstrate that finetuning LLMs on this dataset\nsignificantly improves LLMs' information retrieval and reasoning capabilities\nin longer-context settings. We present an analysis of the finetuned models,\nillustrating the transfer of skills from synthetic to real task evaluations\n(e.g., $10.5\\%$ improvement on $20$ documents MDQA at position $10$ for GPT-3.5\nTurbo). We also find that finetuned LLMs' performance on general benchmarks\nremains almost constant while LLMs finetuned on other baseline long-context\naugmentation data can encourage hallucination (e.g., on TriviaQA, Mistral 7B\nfinetuned on our synthetic data cause no performance drop while other baseline\ndata can cause a drop that ranges from $2.33\\%$ to $6.19\\%$). Our study\nhighlights the potential of finetuning on synthetic data for improving the\nperformance of LLMs on longer-context tasks.",
        "chunk-id": 3,
        "chunk": "Turbo). We also find that finetuned LLMs' performance on general benchmarks\nremains almost constant while LLMs finetuned on other baseline long-context\naugmentation data can encourage hallucination (e.g., on TriviaQA, Mistral 7B\nfinetuned on our synthetic data cause no performance drop while other baseline\ndata can cause a drop that ranges from $2.33\\%$ to $6.19\\%$). Our study",
        "authors": [
            "Zheyang Xiong",
            "Vasilis Papageorgiou",
            "Kangwook Lee",
            "Dimitris Papailiopoulos"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:05:13+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19292v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19292v1",
        "categories": [
            "Machine Learning",
            "Artificial Intelligence",
            "Computation and Language"
        ]
    },
    {
        "id": 30000045,
        "doi": null,
        "title": "From Artificial Needles to Real Haystacks: Improving Retrieval Capabilities in LLMs by Finetuning on Synthetic Data",
        "abstract": "Recent studies have shown that Large Language Models (LLMs) struggle to\naccurately retrieve information and maintain reasoning capabilities when\nprocessing long-context inputs. To address these limitations, we propose a\nfinetuning approach utilizing a carefully designed synthetic dataset comprising\nnumerical key-value retrieval tasks. Our experiments on models like GPT-3.5\nTurbo and Mistral 7B demonstrate that finetuning LLMs on this dataset\nsignificantly improves LLMs' information retrieval and reasoning capabilities\nin longer-context settings. We present an analysis of the finetuned models,\nillustrating the transfer of skills from synthetic to real task evaluations\n(e.g., $10.5\\%$ improvement on $20$ documents MDQA at position $10$ for GPT-3.5\nTurbo). We also find that finetuned LLMs' performance on general benchmarks\nremains almost constant while LLMs finetuned on other baseline long-context\naugmentation data can encourage hallucination (e.g., on TriviaQA, Mistral 7B\nfinetuned on our synthetic data cause no performance drop while other baseline\ndata can cause a drop that ranges from $2.33\\%$ to $6.19\\%$). Our study\nhighlights the potential of finetuning on synthetic data for improving the\nperformance of LLMs on longer-context tasks.",
        "chunk-id": 4,
        "chunk": "highlights the potential of finetuning on synthetic data for improving the\nperformance of LLMs on longer-context tasks.",
        "authors": [
            "Zheyang Xiong",
            "Vasilis Papageorgiou",
            "Kangwook Lee",
            "Dimitris Papailiopoulos"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:05:13+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19292v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19292v1",
        "categories": [
            "Machine Learning",
            "Artificial Intelligence",
            "Computation and Language"
        ]
    },
    {
        "id": 30000046,
        "doi": null,
        "title": "Human Modelling and Pose Estimation Overview",
        "abstract": "Human modelling and pose estimation stands at the crossroads of Computer\nVision, Computer Graphics, and Machine Learning. This paper presents a thorough\ninvestigation of this interdisciplinary field, examining various algorithms,\nmethodologies, and practical applications. It explores the diverse range of\nsensor technologies relevant to this domain and delves into a wide array of\napplication areas. Additionally, we discuss the challenges and advancements in\n2D and 3D human modelling methodologies, along with popular datasets, metrics,\nand future research directions. The main contribution of this paper lies in its\nup-to-date comparison of state-of-the-art (SOTA) human pose estimation\nalgorithms in both 2D and 3D domains. By providing this comprehensive overview,\nthe paper aims to enhance understanding of 3D human modelling and pose\nestimation, offering insights into current SOTA achievements, challenges, and\nfuture prospects within the field.",
        "chunk-id": 1,
        "chunk": "Human modelling and pose estimation stands at the crossroads of Computer\nVision, Computer Graphics, and Machine Learning. This paper presents a thorough\ninvestigation of this interdisciplinary field, examining various algorithms,\nmethodologies, and practical applications. It explores the diverse range of\nsensor technologies relevant to this domain and delves into a wide array of",
        "authors": [
            "Pawel Knap"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:04:41+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19290v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19290v1",
        "categories": [
            "Computer Vision and Pattern Recognition",
            "Scene Analysis"
        ]
    },
    {
        "id": 30000046,
        "doi": null,
        "title": "Human Modelling and Pose Estimation Overview",
        "abstract": "Human modelling and pose estimation stands at the crossroads of Computer\nVision, Computer Graphics, and Machine Learning. This paper presents a thorough\ninvestigation of this interdisciplinary field, examining various algorithms,\nmethodologies, and practical applications. It explores the diverse range of\nsensor technologies relevant to this domain and delves into a wide array of\napplication areas. Additionally, we discuss the challenges and advancements in\n2D and 3D human modelling methodologies, along with popular datasets, metrics,\nand future research directions. The main contribution of this paper lies in its\nup-to-date comparison of state-of-the-art (SOTA) human pose estimation\nalgorithms in both 2D and 3D domains. By providing this comprehensive overview,\nthe paper aims to enhance understanding of 3D human modelling and pose\nestimation, offering insights into current SOTA achievements, challenges, and\nfuture prospects within the field.",
        "chunk-id": 2,
        "chunk": "application areas. Additionally, we discuss the challenges and advancements in\n2D and 3D human modelling methodologies, along with popular datasets, metrics,\nand future research directions. The main contribution of this paper lies in its\nup-to-date comparison of state-of-the-art (SOTA) human pose estimation",
        "authors": [
            "Pawel Knap"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:04:41+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19290v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19290v1",
        "categories": [
            "Computer Vision and Pattern Recognition",
            "Scene Analysis"
        ]
    },
    {
        "id": 30000046,
        "doi": null,
        "title": "Human Modelling and Pose Estimation Overview",
        "abstract": "Human modelling and pose estimation stands at the crossroads of Computer\nVision, Computer Graphics, and Machine Learning. This paper presents a thorough\ninvestigation of this interdisciplinary field, examining various algorithms,\nmethodologies, and practical applications. It explores the diverse range of\nsensor technologies relevant to this domain and delves into a wide array of\napplication areas. Additionally, we discuss the challenges and advancements in\n2D and 3D human modelling methodologies, along with popular datasets, metrics,\nand future research directions. The main contribution of this paper lies in its\nup-to-date comparison of state-of-the-art (SOTA) human pose estimation\nalgorithms in both 2D and 3D domains. By providing this comprehensive overview,\nthe paper aims to enhance understanding of 3D human modelling and pose\nestimation, offering insights into current SOTA achievements, challenges, and\nfuture prospects within the field.",
        "chunk-id": 3,
        "chunk": "algorithms in both 2D and 3D domains. By providing this comprehensive overview,\nthe paper aims to enhance understanding of 3D human modelling and pose\nestimation, offering insights into current SOTA achievements, challenges, and\nfuture prospects within the field.",
        "authors": [
            "Pawel Knap"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:04:41+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19290v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19290v1",
        "categories": [
            "Computer Vision and Pattern Recognition",
            "Scene Analysis"
        ]
    },
    {
        "id": 30000047,
        "doi": null,
        "title": "Joint Channel and Data Estimation for Multiuser Extremely Large-Scale MIMO Systems",
        "abstract": "This paper proposes a joint channel and data estimation (JCDE) algorithm for\nuplink multiuser extremely large-scale multiple-input-multiple-output (XL-MIMO)\nsystems. The initial channel estimation is formulated as a sparse\nreconstruction problem based on the angle and distance sparsity under the\nnear-field propagation condition. This problem is solved using non-orthogonal\npilots through an efficient low complexity two-stage compressed sensing\nalgorithm. Furthermore, the initial channel estimates are refined by employing\na JCDE framework driven by both non-orthogonal pilots and estimated data. The\nJCDE problem is solved by sequential expectation propagation (EP) algorithms,\nwhere the channel and data are alternately updated in an iterative manner. In\nthe channel estimation phase, integrating Bayesian inference with a model-based\ndeterministic approach provides precise estimations to effectively exploit the\nnear-field characteristics in the beam-domain. In the data estimation phase, a\nlinear minimum mean square error (LMMSE)-based filter is designed at each\nsub-array to address the correlation due to energy leakage in the beam-domain\narising from the near-field effects. Numerical simulations reveal that the\nproposed initial channel estimation and JCDE algorithm outperforms the\nstate-ofthe-art approaches in terms of channel estimation, data detection, and\ncomputational complexity.",
        "chunk-id": 1,
        "chunk": "This paper proposes a joint channel and data estimation (JCDE) algorithm for\nuplink multiuser extremely large-scale multiple-input-multiple-output (XL-MIMO)\nsystems. The initial channel estimation is formulated as a sparse\nreconstruction problem based on the angle and distance sparsity under the\nnear-field propagation condition. This problem is solved using non-orthogonal",
        "authors": [
            "Kabuto Arai",
            "Koji Ishibashi",
            "Hiroki Iimori",
            "Valente Klaine",
            "Szabolcs Malomsoky"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:04:40+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19289v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19289v1",
        "categories": [
            "Signal Processing"
        ]
    },
    {
        "id": 30000047,
        "doi": null,
        "title": "Joint Channel and Data Estimation for Multiuser Extremely Large-Scale MIMO Systems",
        "abstract": "This paper proposes a joint channel and data estimation (JCDE) algorithm for\nuplink multiuser extremely large-scale multiple-input-multiple-output (XL-MIMO)\nsystems. The initial channel estimation is formulated as a sparse\nreconstruction problem based on the angle and distance sparsity under the\nnear-field propagation condition. This problem is solved using non-orthogonal\npilots through an efficient low complexity two-stage compressed sensing\nalgorithm. Furthermore, the initial channel estimates are refined by employing\na JCDE framework driven by both non-orthogonal pilots and estimated data. The\nJCDE problem is solved by sequential expectation propagation (EP) algorithms,\nwhere the channel and data are alternately updated in an iterative manner. In\nthe channel estimation phase, integrating Bayesian inference with a model-based\ndeterministic approach provides precise estimations to effectively exploit the\nnear-field characteristics in the beam-domain. In the data estimation phase, a\nlinear minimum mean square error (LMMSE)-based filter is designed at each\nsub-array to address the correlation due to energy leakage in the beam-domain\narising from the near-field effects. Numerical simulations reveal that the\nproposed initial channel estimation and JCDE algorithm outperforms the\nstate-ofthe-art approaches in terms of channel estimation, data detection, and\ncomputational complexity.",
        "chunk-id": 2,
        "chunk": "pilots through an efficient low complexity two-stage compressed sensing\nalgorithm. Furthermore, the initial channel estimates are refined by employing\na JCDE framework driven by both non-orthogonal pilots and estimated data. The\nJCDE problem is solved by sequential expectation propagation (EP) algorithms,\nwhere the channel and data are alternately updated in an iterative manner. In",
        "authors": [
            "Kabuto Arai",
            "Koji Ishibashi",
            "Hiroki Iimori",
            "Valente Klaine",
            "Szabolcs Malomsoky"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:04:40+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19289v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19289v1",
        "categories": [
            "Signal Processing"
        ]
    },
    {
        "id": 30000047,
        "doi": null,
        "title": "Joint Channel and Data Estimation for Multiuser Extremely Large-Scale MIMO Systems",
        "abstract": "This paper proposes a joint channel and data estimation (JCDE) algorithm for\nuplink multiuser extremely large-scale multiple-input-multiple-output (XL-MIMO)\nsystems. The initial channel estimation is formulated as a sparse\nreconstruction problem based on the angle and distance sparsity under the\nnear-field propagation condition. This problem is solved using non-orthogonal\npilots through an efficient low complexity two-stage compressed sensing\nalgorithm. Furthermore, the initial channel estimates are refined by employing\na JCDE framework driven by both non-orthogonal pilots and estimated data. The\nJCDE problem is solved by sequential expectation propagation (EP) algorithms,\nwhere the channel and data are alternately updated in an iterative manner. In\nthe channel estimation phase, integrating Bayesian inference with a model-based\ndeterministic approach provides precise estimations to effectively exploit the\nnear-field characteristics in the beam-domain. In the data estimation phase, a\nlinear minimum mean square error (LMMSE)-based filter is designed at each\nsub-array to address the correlation due to energy leakage in the beam-domain\narising from the near-field effects. Numerical simulations reveal that the\nproposed initial channel estimation and JCDE algorithm outperforms the\nstate-ofthe-art approaches in terms of channel estimation, data detection, and\ncomputational complexity.",
        "chunk-id": 3,
        "chunk": "the channel estimation phase, integrating Bayesian inference with a model-based\ndeterministic approach provides precise estimations to effectively exploit the\nnear-field characteristics in the beam-domain. In the data estimation phase, a\nlinear minimum mean square error (LMMSE)-based filter is designed at each",
        "authors": [
            "Kabuto Arai",
            "Koji Ishibashi",
            "Hiroki Iimori",
            "Valente Klaine",
            "Szabolcs Malomsoky"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:04:40+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19289v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19289v1",
        "categories": [
            "Signal Processing"
        ]
    },
    {
        "id": 30000047,
        "doi": null,
        "title": "Joint Channel and Data Estimation for Multiuser Extremely Large-Scale MIMO Systems",
        "abstract": "This paper proposes a joint channel and data estimation (JCDE) algorithm for\nuplink multiuser extremely large-scale multiple-input-multiple-output (XL-MIMO)\nsystems. The initial channel estimation is formulated as a sparse\nreconstruction problem based on the angle and distance sparsity under the\nnear-field propagation condition. This problem is solved using non-orthogonal\npilots through an efficient low complexity two-stage compressed sensing\nalgorithm. Furthermore, the initial channel estimates are refined by employing\na JCDE framework driven by both non-orthogonal pilots and estimated data. The\nJCDE problem is solved by sequential expectation propagation (EP) algorithms,\nwhere the channel and data are alternately updated in an iterative manner. In\nthe channel estimation phase, integrating Bayesian inference with a model-based\ndeterministic approach provides precise estimations to effectively exploit the\nnear-field characteristics in the beam-domain. In the data estimation phase, a\nlinear minimum mean square error (LMMSE)-based filter is designed at each\nsub-array to address the correlation due to energy leakage in the beam-domain\narising from the near-field effects. Numerical simulations reveal that the\nproposed initial channel estimation and JCDE algorithm outperforms the\nstate-ofthe-art approaches in terms of channel estimation, data detection, and\ncomputational complexity.",
        "chunk-id": 4,
        "chunk": "sub-array to address the correlation due to energy leakage in the beam-domain\narising from the near-field effects. Numerical simulations reveal that the\nproposed initial channel estimation and JCDE algorithm outperforms the\nstate-ofthe-art approaches in terms of channel estimation, data detection, and\ncomputational complexity.",
        "authors": [
            "Kabuto Arai",
            "Koji Ishibashi",
            "Hiroki Iimori",
            "Valente Klaine",
            "Szabolcs Malomsoky"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:04:40+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19289v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19289v1",
        "categories": [
            "Signal Processing"
        ]
    },
    {
        "id": 30000048,
        "doi": null,
        "title": "Task-splitting in home healthcare routing and scheduling",
        "abstract": "This paper introduces the concept of task-splitting into home healthcare\n(HHC) routing and scheduling. It focuses on the design of routes and timetables\nfor caregivers providing services at patients' homes. Task-splitting is the\ndivision of a (lengthy) patient visit into separate visits that can be\nperformed by different caregivers at different times. The resulting split parts\nmay have reduced caregiver qualification requirements, relaxed visiting time\nwindows, or a shorter/longer combined duration. However, additional temporal\ndependencies can arise between them. To incorporate task-splitting decisions\ninto the planning process, we introduce two different mixed integer linear\nprogramming formulations, a Miller-Tucker-Zemlin and a time-indexed variant.\nThese formulations aim to minimize operational costs while simultaneously\ndeciding which visits to split and imposing a potentially wide range of\ntemporal dependencies. We also propose pre-processing routines for the\ntime-indexed formulation and two heuristic procedures. These methods are\nembedded into the branch-and-bound approach as primal and improvement\nheuristics. The results of our computational study demonstrate the additional\ncomputational difficulty introduced by task-splitting and the associated\nadditional synchronization, and the usefulness of the proposed heuristic\nprocedures. From a planning perspective, our results indicate that introducing\ntask-splitting reduces staff requirements, decreases HHC operational costs, and\nallows caregivers to spend relatively more time on tasks aligned with their\nqualifications. Moreover, we observe that the potential of task-splitting is\nnot specific to the chosen planning objective; it can also be beneficial when\nminimizing travel time instead.",
        "chunk-id": 1,
        "chunk": "This paper introduces the concept of task-splitting into home healthcare\n(HHC) routing and scheduling. It focuses on the design of routes and timetables\nfor caregivers providing services at patients' homes. Task-splitting is the\ndivision of a (lengthy) patient visit into separate visits that can be\nperformed by different caregivers at different times. The resulting split parts",
        "authors": [
            "Loek van Montfort",
            "Wout Dullaert",
            "Markus Leitner"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:03:20+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19288v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19288v1",
        "categories": [
            "Optimization and Control"
        ]
    },
    {
        "id": 30000048,
        "doi": null,
        "title": "Task-splitting in home healthcare routing and scheduling",
        "abstract": "This paper introduces the concept of task-splitting into home healthcare\n(HHC) routing and scheduling. It focuses on the design of routes and timetables\nfor caregivers providing services at patients' homes. Task-splitting is the\ndivision of a (lengthy) patient visit into separate visits that can be\nperformed by different caregivers at different times. The resulting split parts\nmay have reduced caregiver qualification requirements, relaxed visiting time\nwindows, or a shorter/longer combined duration. However, additional temporal\ndependencies can arise between them. To incorporate task-splitting decisions\ninto the planning process, we introduce two different mixed integer linear\nprogramming formulations, a Miller-Tucker-Zemlin and a time-indexed variant.\nThese formulations aim to minimize operational costs while simultaneously\ndeciding which visits to split and imposing a potentially wide range of\ntemporal dependencies. We also propose pre-processing routines for the\ntime-indexed formulation and two heuristic procedures. These methods are\nembedded into the branch-and-bound approach as primal and improvement\nheuristics. The results of our computational study demonstrate the additional\ncomputational difficulty introduced by task-splitting and the associated\nadditional synchronization, and the usefulness of the proposed heuristic\nprocedures. From a planning perspective, our results indicate that introducing\ntask-splitting reduces staff requirements, decreases HHC operational costs, and\nallows caregivers to spend relatively more time on tasks aligned with their\nqualifications. Moreover, we observe that the potential of task-splitting is\nnot specific to the chosen planning objective; it can also be beneficial when\nminimizing travel time instead.",
        "chunk-id": 2,
        "chunk": "may have reduced caregiver qualification requirements, relaxed visiting time\nwindows, or a shorter/longer combined duration. However, additional temporal\ndependencies can arise between them. To incorporate task-splitting decisions\ninto the planning process, we introduce two different mixed integer linear\nprogramming formulations, a Miller-Tucker-Zemlin and a time-indexed variant.",
        "authors": [
            "Loek van Montfort",
            "Wout Dullaert",
            "Markus Leitner"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:03:20+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19288v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19288v1",
        "categories": [
            "Optimization and Control"
        ]
    },
    {
        "id": 30000048,
        "doi": null,
        "title": "Task-splitting in home healthcare routing and scheduling",
        "abstract": "This paper introduces the concept of task-splitting into home healthcare\n(HHC) routing and scheduling. It focuses on the design of routes and timetables\nfor caregivers providing services at patients' homes. Task-splitting is the\ndivision of a (lengthy) patient visit into separate visits that can be\nperformed by different caregivers at different times. The resulting split parts\nmay have reduced caregiver qualification requirements, relaxed visiting time\nwindows, or a shorter/longer combined duration. However, additional temporal\ndependencies can arise between them. To incorporate task-splitting decisions\ninto the planning process, we introduce two different mixed integer linear\nprogramming formulations, a Miller-Tucker-Zemlin and a time-indexed variant.\nThese formulations aim to minimize operational costs while simultaneously\ndeciding which visits to split and imposing a potentially wide range of\ntemporal dependencies. We also propose pre-processing routines for the\ntime-indexed formulation and two heuristic procedures. These methods are\nembedded into the branch-and-bound approach as primal and improvement\nheuristics. The results of our computational study demonstrate the additional\ncomputational difficulty introduced by task-splitting and the associated\nadditional synchronization, and the usefulness of the proposed heuristic\nprocedures. From a planning perspective, our results indicate that introducing\ntask-splitting reduces staff requirements, decreases HHC operational costs, and\nallows caregivers to spend relatively more time on tasks aligned with their\nqualifications. Moreover, we observe that the potential of task-splitting is\nnot specific to the chosen planning objective; it can also be beneficial when\nminimizing travel time instead.",
        "chunk-id": 3,
        "chunk": "These formulations aim to minimize operational costs while simultaneously\ndeciding which visits to split and imposing a potentially wide range of\ntemporal dependencies. We also propose pre-processing routines for the\ntime-indexed formulation and two heuristic procedures. These methods are\nembedded into the branch-and-bound approach as primal and improvement",
        "authors": [
            "Loek van Montfort",
            "Wout Dullaert",
            "Markus Leitner"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:03:20+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19288v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19288v1",
        "categories": [
            "Optimization and Control"
        ]
    },
    {
        "id": 30000048,
        "doi": null,
        "title": "Task-splitting in home healthcare routing and scheduling",
        "abstract": "This paper introduces the concept of task-splitting into home healthcare\n(HHC) routing and scheduling. It focuses on the design of routes and timetables\nfor caregivers providing services at patients' homes. Task-splitting is the\ndivision of a (lengthy) patient visit into separate visits that can be\nperformed by different caregivers at different times. The resulting split parts\nmay have reduced caregiver qualification requirements, relaxed visiting time\nwindows, or a shorter/longer combined duration. However, additional temporal\ndependencies can arise between them. To incorporate task-splitting decisions\ninto the planning process, we introduce two different mixed integer linear\nprogramming formulations, a Miller-Tucker-Zemlin and a time-indexed variant.\nThese formulations aim to minimize operational costs while simultaneously\ndeciding which visits to split and imposing a potentially wide range of\ntemporal dependencies. We also propose pre-processing routines for the\ntime-indexed formulation and two heuristic procedures. These methods are\nembedded into the branch-and-bound approach as primal and improvement\nheuristics. The results of our computational study demonstrate the additional\ncomputational difficulty introduced by task-splitting and the associated\nadditional synchronization, and the usefulness of the proposed heuristic\nprocedures. From a planning perspective, our results indicate that introducing\ntask-splitting reduces staff requirements, decreases HHC operational costs, and\nallows caregivers to spend relatively more time on tasks aligned with their\nqualifications. Moreover, we observe that the potential of task-splitting is\nnot specific to the chosen planning objective; it can also be beneficial when\nminimizing travel time instead.",
        "chunk-id": 4,
        "chunk": "heuristics. The results of our computational study demonstrate the additional\ncomputational difficulty introduced by task-splitting and the associated\nadditional synchronization, and the usefulness of the proposed heuristic\nprocedures. From a planning perspective, our results indicate that introducing\ntask-splitting reduces staff requirements, decreases HHC operational costs, and",
        "authors": [
            "Loek van Montfort",
            "Wout Dullaert",
            "Markus Leitner"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:03:20+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19288v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19288v1",
        "categories": [
            "Optimization and Control"
        ]
    },
    {
        "id": 30000048,
        "doi": null,
        "title": "Task-splitting in home healthcare routing and scheduling",
        "abstract": "This paper introduces the concept of task-splitting into home healthcare\n(HHC) routing and scheduling. It focuses on the design of routes and timetables\nfor caregivers providing services at patients' homes. Task-splitting is the\ndivision of a (lengthy) patient visit into separate visits that can be\nperformed by different caregivers at different times. The resulting split parts\nmay have reduced caregiver qualification requirements, relaxed visiting time\nwindows, or a shorter/longer combined duration. However, additional temporal\ndependencies can arise between them. To incorporate task-splitting decisions\ninto the planning process, we introduce two different mixed integer linear\nprogramming formulations, a Miller-Tucker-Zemlin and a time-indexed variant.\nThese formulations aim to minimize operational costs while simultaneously\ndeciding which visits to split and imposing a potentially wide range of\ntemporal dependencies. We also propose pre-processing routines for the\ntime-indexed formulation and two heuristic procedures. These methods are\nembedded into the branch-and-bound approach as primal and improvement\nheuristics. The results of our computational study demonstrate the additional\ncomputational difficulty introduced by task-splitting and the associated\nadditional synchronization, and the usefulness of the proposed heuristic\nprocedures. From a planning perspective, our results indicate that introducing\ntask-splitting reduces staff requirements, decreases HHC operational costs, and\nallows caregivers to spend relatively more time on tasks aligned with their\nqualifications. Moreover, we observe that the potential of task-splitting is\nnot specific to the chosen planning objective; it can also be beneficial when\nminimizing travel time instead.",
        "chunk-id": 5,
        "chunk": "allows caregivers to spend relatively more time on tasks aligned with their\nqualifications. Moreover, we observe that the potential of task-splitting is\nnot specific to the chosen planning objective; it can also be beneficial when\nminimizing travel time instead.",
        "authors": [
            "Loek van Montfort",
            "Wout Dullaert",
            "Markus Leitner"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T16:03:20+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19288v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19288v1",
        "categories": [
            "Optimization and Control"
        ]
    },
    {
        "id": 30000049,
        "doi": null,
        "title": "HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale",
        "abstract": "The rapid development of multimodal large language models (MLLMs), such as\nGPT-4V, has led to significant advancements. However, these models still face\nchallenges in medical multimodal capabilities due to limitations in the\nquantity and quality of medical vision-text data, stemming from data privacy\nconcerns and high annotation costs. While pioneering approaches utilize\nPubMed's large-scale, de-identified medical image-text pairs to address these\nlimitations, they still fall short due to inherent data noise. To tackle this,\nwe refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in\nan 'unblinded' capacity to denoise and reformat the data, resulting in the\ncreation of the PubMedVision dataset with 1.3 million medical VQA samples. Our\nvalidation demonstrates that: (1) PubMedVision can significantly enhance the\nmedical multimodal capabilities of current MLLMs, showing significant\nimprovement in benchmarks including the MMMU Health & Medicine track; (2)\nmanual checks by medical experts and empirical results validate the superior\ndata quality of our dataset compared to other data construction methods. Using\nPubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows\nsuperior performance in medical multimodal scenarios among open-source MLLMs.",
        "chunk-id": 1,
        "chunk": "The rapid development of multimodal large language models (MLLMs), such as\nGPT-4V, has led to significant advancements. However, these models still face\nchallenges in medical multimodal capabilities due to limitations in the\nquantity and quality of medical vision-text data, stemming from data privacy\nconcerns and high annotation costs. While pioneering approaches utilize",
        "authors": [
            "Junying Chen",
            "Ruyi Ouyang",
            "Anningzhe Gao",
            "Shunian Chen",
            "Guiming Hardy Chen",
            "Xidong Wang",
            "Ruifei Zhang",
            "Zhenyang Cai",
            "Ke Ji",
            "Guangjun Yu",
            "Xiang Wan",
            "Benyou Wang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:50:41+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19280v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19280v1",
        "categories": [
            "Computer Vision and Pattern Recognition",
            "Artificial Intelligence",
            "Computation and Language",
            "Machine Learning"
        ]
    },
    {
        "id": 30000049,
        "doi": null,
        "title": "HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale",
        "abstract": "The rapid development of multimodal large language models (MLLMs), such as\nGPT-4V, has led to significant advancements. However, these models still face\nchallenges in medical multimodal capabilities due to limitations in the\nquantity and quality of medical vision-text data, stemming from data privacy\nconcerns and high annotation costs. While pioneering approaches utilize\nPubMed's large-scale, de-identified medical image-text pairs to address these\nlimitations, they still fall short due to inherent data noise. To tackle this,\nwe refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in\nan 'unblinded' capacity to denoise and reformat the data, resulting in the\ncreation of the PubMedVision dataset with 1.3 million medical VQA samples. Our\nvalidation demonstrates that: (1) PubMedVision can significantly enhance the\nmedical multimodal capabilities of current MLLMs, showing significant\nimprovement in benchmarks including the MMMU Health & Medicine track; (2)\nmanual checks by medical experts and empirical results validate the superior\ndata quality of our dataset compared to other data construction methods. Using\nPubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows\nsuperior performance in medical multimodal scenarios among open-source MLLMs.",
        "chunk-id": 2,
        "chunk": "PubMed's large-scale, de-identified medical image-text pairs to address these\nlimitations, they still fall short due to inherent data noise. To tackle this,\nwe refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in\nan 'unblinded' capacity to denoise and reformat the data, resulting in the",
        "authors": [
            "Junying Chen",
            "Ruyi Ouyang",
            "Anningzhe Gao",
            "Shunian Chen",
            "Guiming Hardy Chen",
            "Xidong Wang",
            "Ruifei Zhang",
            "Zhenyang Cai",
            "Ke Ji",
            "Guangjun Yu",
            "Xiang Wan",
            "Benyou Wang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:50:41+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19280v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19280v1",
        "categories": [
            "Computer Vision and Pattern Recognition",
            "Artificial Intelligence",
            "Computation and Language",
            "Machine Learning"
        ]
    },
    {
        "id": 30000049,
        "doi": null,
        "title": "HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale",
        "abstract": "The rapid development of multimodal large language models (MLLMs), such as\nGPT-4V, has led to significant advancements. However, these models still face\nchallenges in medical multimodal capabilities due to limitations in the\nquantity and quality of medical vision-text data, stemming from data privacy\nconcerns and high annotation costs. While pioneering approaches utilize\nPubMed's large-scale, de-identified medical image-text pairs to address these\nlimitations, they still fall short due to inherent data noise. To tackle this,\nwe refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in\nan 'unblinded' capacity to denoise and reformat the data, resulting in the\ncreation of the PubMedVision dataset with 1.3 million medical VQA samples. Our\nvalidation demonstrates that: (1) PubMedVision can significantly enhance the\nmedical multimodal capabilities of current MLLMs, showing significant\nimprovement in benchmarks including the MMMU Health & Medicine track; (2)\nmanual checks by medical experts and empirical results validate the superior\ndata quality of our dataset compared to other data construction methods. Using\nPubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows\nsuperior performance in medical multimodal scenarios among open-source MLLMs.",
        "chunk-id": 3,
        "chunk": "creation of the PubMedVision dataset with 1.3 million medical VQA samples. Our\nvalidation demonstrates that: (1) PubMedVision can significantly enhance the\nmedical multimodal capabilities of current MLLMs, showing significant\nimprovement in benchmarks including the MMMU Health & Medicine track; (2)\nmanual checks by medical experts and empirical results validate the superior",
        "authors": [
            "Junying Chen",
            "Ruyi Ouyang",
            "Anningzhe Gao",
            "Shunian Chen",
            "Guiming Hardy Chen",
            "Xidong Wang",
            "Ruifei Zhang",
            "Zhenyang Cai",
            "Ke Ji",
            "Guangjun Yu",
            "Xiang Wan",
            "Benyou Wang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:50:41+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19280v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19280v1",
        "categories": [
            "Computer Vision and Pattern Recognition",
            "Artificial Intelligence",
            "Computation and Language",
            "Machine Learning"
        ]
    },
    {
        "id": 30000049,
        "doi": null,
        "title": "HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale",
        "abstract": "The rapid development of multimodal large language models (MLLMs), such as\nGPT-4V, has led to significant advancements. However, these models still face\nchallenges in medical multimodal capabilities due to limitations in the\nquantity and quality of medical vision-text data, stemming from data privacy\nconcerns and high annotation costs. While pioneering approaches utilize\nPubMed's large-scale, de-identified medical image-text pairs to address these\nlimitations, they still fall short due to inherent data noise. To tackle this,\nwe refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in\nan 'unblinded' capacity to denoise and reformat the data, resulting in the\ncreation of the PubMedVision dataset with 1.3 million medical VQA samples. Our\nvalidation demonstrates that: (1) PubMedVision can significantly enhance the\nmedical multimodal capabilities of current MLLMs, showing significant\nimprovement in benchmarks including the MMMU Health & Medicine track; (2)\nmanual checks by medical experts and empirical results validate the superior\ndata quality of our dataset compared to other data construction methods. Using\nPubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows\nsuperior performance in medical multimodal scenarios among open-source MLLMs.",
        "chunk-id": 4,
        "chunk": "data quality of our dataset compared to other data construction methods. Using\nPubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows\nsuperior performance in medical multimodal scenarios among open-source MLLMs.",
        "authors": [
            "Junying Chen",
            "Ruyi Ouyang",
            "Anningzhe Gao",
            "Shunian Chen",
            "Guiming Hardy Chen",
            "Xidong Wang",
            "Ruifei Zhang",
            "Zhenyang Cai",
            "Ke Ji",
            "Guangjun Yu",
            "Xiang Wan",
            "Benyou Wang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:50:41+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19280v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19280v1",
        "categories": [
            "Computer Vision and Pattern Recognition",
            "Artificial Intelligence",
            "Computation and Language",
            "Machine Learning"
        ]
    },
    {
        "id": 30000050,
        "doi": null,
        "title": "VERISCORE: Evaluating the factuality of verifiable claims in long-form text generation",
        "abstract": "Existing metrics for evaluating the factuality of long-form text, such as\nFACTSCORE (Min et al., 2023) and SAFE (Wei et al., 2024), decompose an input\ntext into \"atomic claims\" and verify each against a knowledge base like\nWikipedia. These metrics are not suitable for most generation tasks because\nthey assume that every claim is verifiable (i.e., can plausibly be proven true\nor false). We address this issue with VERISCORE, a metric for diverse long-form\ngeneration tasks that contain both verifiable and unverifiable content.\nVERISCORE can be effectively implemented with either closed or fine-tuned\nopen-weight language models, and human evaluation confirms that VERISCORE's\nextracted claims are more sensible than those from competing methods across\neight different long-form tasks. We use VERISCORE to evaluate generations from\n16 different models across multiple long-form tasks and find that while GPT-4o\nis the best-performing model overall, open-weight models such as Mixtral-8x22\nare closing the gap. We show that an LM's VERISCORE on one task (e.g.,\nbiography generation) does not necessarily correlate to its VERISCORE on a\ndifferent task (e.g., long-form QA), highlighting the need for expanding\nfactuality evaluation across tasks with varying fact density.",
        "chunk-id": 1,
        "chunk": "Existing metrics for evaluating the factuality of long-form text, such as\nFACTSCORE (Min et al., 2023) and SAFE (Wei et al., 2024), decompose an input\ntext into \"atomic claims\" and verify each against a knowledge base like\nWikipedia. These metrics are not suitable for most generation tasks because\nthey assume that every claim is verifiable (i.e., can plausibly be proven true",
        "authors": [
            "Yixiao Song",
            "Yekyung Kim",
            "Mohit Iyyer"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:43:18+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19276v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19276v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 30000050,
        "doi": null,
        "title": "VERISCORE: Evaluating the factuality of verifiable claims in long-form text generation",
        "abstract": "Existing metrics for evaluating the factuality of long-form text, such as\nFACTSCORE (Min et al., 2023) and SAFE (Wei et al., 2024), decompose an input\ntext into \"atomic claims\" and verify each against a knowledge base like\nWikipedia. These metrics are not suitable for most generation tasks because\nthey assume that every claim is verifiable (i.e., can plausibly be proven true\nor false). We address this issue with VERISCORE, a metric for diverse long-form\ngeneration tasks that contain both verifiable and unverifiable content.\nVERISCORE can be effectively implemented with either closed or fine-tuned\nopen-weight language models, and human evaluation confirms that VERISCORE's\nextracted claims are more sensible than those from competing methods across\neight different long-form tasks. We use VERISCORE to evaluate generations from\n16 different models across multiple long-form tasks and find that while GPT-4o\nis the best-performing model overall, open-weight models such as Mixtral-8x22\nare closing the gap. We show that an LM's VERISCORE on one task (e.g.,\nbiography generation) does not necessarily correlate to its VERISCORE on a\ndifferent task (e.g., long-form QA), highlighting the need for expanding\nfactuality evaluation across tasks with varying fact density.",
        "chunk-id": 2,
        "chunk": "or false). We address this issue with VERISCORE, a metric for diverse long-form\ngeneration tasks that contain both verifiable and unverifiable content.\nVERISCORE can be effectively implemented with either closed or fine-tuned\nopen-weight language models, and human evaluation confirms that VERISCORE's\nextracted claims are more sensible than those from competing methods across",
        "authors": [
            "Yixiao Song",
            "Yekyung Kim",
            "Mohit Iyyer"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:43:18+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19276v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19276v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 30000050,
        "doi": null,
        "title": "VERISCORE: Evaluating the factuality of verifiable claims in long-form text generation",
        "abstract": "Existing metrics for evaluating the factuality of long-form text, such as\nFACTSCORE (Min et al., 2023) and SAFE (Wei et al., 2024), decompose an input\ntext into \"atomic claims\" and verify each against a knowledge base like\nWikipedia. These metrics are not suitable for most generation tasks because\nthey assume that every claim is verifiable (i.e., can plausibly be proven true\nor false). We address this issue with VERISCORE, a metric for diverse long-form\ngeneration tasks that contain both verifiable and unverifiable content.\nVERISCORE can be effectively implemented with either closed or fine-tuned\nopen-weight language models, and human evaluation confirms that VERISCORE's\nextracted claims are more sensible than those from competing methods across\neight different long-form tasks. We use VERISCORE to evaluate generations from\n16 different models across multiple long-form tasks and find that while GPT-4o\nis the best-performing model overall, open-weight models such as Mixtral-8x22\nare closing the gap. We show that an LM's VERISCORE on one task (e.g.,\nbiography generation) does not necessarily correlate to its VERISCORE on a\ndifferent task (e.g., long-form QA), highlighting the need for expanding\nfactuality evaluation across tasks with varying fact density.",
        "chunk-id": 3,
        "chunk": "eight different long-form tasks. We use VERISCORE to evaluate generations from\n16 different models across multiple long-form tasks and find that while GPT-4o\nis the best-performing model overall, open-weight models such as Mixtral-8x22\nare closing the gap. We show that an LM's VERISCORE on one task (e.g.,\nbiography generation) does not necessarily correlate to its VERISCORE on a",
        "authors": [
            "Yixiao Song",
            "Yekyung Kim",
            "Mohit Iyyer"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:43:18+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19276v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19276v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 30000050,
        "doi": null,
        "title": "VERISCORE: Evaluating the factuality of verifiable claims in long-form text generation",
        "abstract": "Existing metrics for evaluating the factuality of long-form text, such as\nFACTSCORE (Min et al., 2023) and SAFE (Wei et al., 2024), decompose an input\ntext into \"atomic claims\" and verify each against a knowledge base like\nWikipedia. These metrics are not suitable for most generation tasks because\nthey assume that every claim is verifiable (i.e., can plausibly be proven true\nor false). We address this issue with VERISCORE, a metric for diverse long-form\ngeneration tasks that contain both verifiable and unverifiable content.\nVERISCORE can be effectively implemented with either closed or fine-tuned\nopen-weight language models, and human evaluation confirms that VERISCORE's\nextracted claims are more sensible than those from competing methods across\neight different long-form tasks. We use VERISCORE to evaluate generations from\n16 different models across multiple long-form tasks and find that while GPT-4o\nis the best-performing model overall, open-weight models such as Mixtral-8x22\nare closing the gap. We show that an LM's VERISCORE on one task (e.g.,\nbiography generation) does not necessarily correlate to its VERISCORE on a\ndifferent task (e.g., long-form QA), highlighting the need for expanding\nfactuality evaluation across tasks with varying fact density.",
        "chunk-id": 4,
        "chunk": "different task (e.g., long-form QA), highlighting the need for expanding\nfactuality evaluation across tasks with varying fact density.",
        "authors": [
            "Yixiao Song",
            "Yekyung Kim",
            "Mohit Iyyer"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:43:18+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19276v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19276v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 30000051,
        "doi": null,
        "title": "Insights into the Structured Coordination Game with Neutral Options through Simulation",
        "abstract": "Coordination games have been of interest to game theorists, economists, and\necologists for many years to study such problems as the emergence of local\nconventions and the evolution of cooperative behavior. Approaches for\nunderstanding the coordination game with discrete structure have been limited\nin scope, often relying on symmetric reduction of the state space, or other\nconstraints which limit the power of the model to give insight into desired\napplications. In this paper, we introduce a new way of thinking about\nequilibria of the structured coordination game with neutral strategies by means\nof graph partitioning. We begin with a few elementary game theoretical results\nand then catalogue all the Nash equilibria of the coordination game with\nneutral options for graphs with seven or fewer vertices. We extend our\nobservations through the use of simulation on larger Erd\\H{o}s-R\\'enyi random\ngraphs to form the basis for proposing some conjectures about the general\nrelationships among edge density, cluster number, and consensus stability.",
        "chunk-id": 1,
        "chunk": "Coordination games have been of interest to game theorists, economists, and\necologists for many years to study such problems as the emergence of local\nconventions and the evolution of cooperative behavior. Approaches for\nunderstanding the coordination game with discrete structure have been limited\nin scope, often relying on symmetric reduction of the state space, or other",
        "authors": [
            "John S. McAlister",
            "Nina H. Fefferman"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:39:03+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19273v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19273v1",
        "categories": [
            "Computer Science and Game Theory",
            "Dynamical Systems",
            "Dynamic games, Games involving graphs "
        ]
    },
    {
        "id": 30000051,
        "doi": null,
        "title": "Insights into the Structured Coordination Game with Neutral Options through Simulation",
        "abstract": "Coordination games have been of interest to game theorists, economists, and\necologists for many years to study such problems as the emergence of local\nconventions and the evolution of cooperative behavior. Approaches for\nunderstanding the coordination game with discrete structure have been limited\nin scope, often relying on symmetric reduction of the state space, or other\nconstraints which limit the power of the model to give insight into desired\napplications. In this paper, we introduce a new way of thinking about\nequilibria of the structured coordination game with neutral strategies by means\nof graph partitioning. We begin with a few elementary game theoretical results\nand then catalogue all the Nash equilibria of the coordination game with\nneutral options for graphs with seven or fewer vertices. We extend our\nobservations through the use of simulation on larger Erd\\H{o}s-R\\'enyi random\ngraphs to form the basis for proposing some conjectures about the general\nrelationships among edge density, cluster number, and consensus stability.",
        "chunk-id": 2,
        "chunk": "constraints which limit the power of the model to give insight into desired\napplications. In this paper, we introduce a new way of thinking about\nequilibria of the structured coordination game with neutral strategies by means\nof graph partitioning. We begin with a few elementary game theoretical results\nand then catalogue all the Nash equilibria of the coordination game with",
        "authors": [
            "John S. McAlister",
            "Nina H. Fefferman"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:39:03+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19273v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19273v1",
        "categories": [
            "Computer Science and Game Theory",
            "Dynamical Systems",
            "Dynamic games, Games involving graphs "
        ]
    },
    {
        "id": 30000051,
        "doi": null,
        "title": "Insights into the Structured Coordination Game with Neutral Options through Simulation",
        "abstract": "Coordination games have been of interest to game theorists, economists, and\necologists for many years to study such problems as the emergence of local\nconventions and the evolution of cooperative behavior. Approaches for\nunderstanding the coordination game with discrete structure have been limited\nin scope, often relying on symmetric reduction of the state space, or other\nconstraints which limit the power of the model to give insight into desired\napplications. In this paper, we introduce a new way of thinking about\nequilibria of the structured coordination game with neutral strategies by means\nof graph partitioning. We begin with a few elementary game theoretical results\nand then catalogue all the Nash equilibria of the coordination game with\nneutral options for graphs with seven or fewer vertices. We extend our\nobservations through the use of simulation on larger Erd\\H{o}s-R\\'enyi random\ngraphs to form the basis for proposing some conjectures about the general\nrelationships among edge density, cluster number, and consensus stability.",
        "chunk-id": 3,
        "chunk": "neutral options for graphs with seven or fewer vertices. We extend our\nobservations through the use of simulation on larger Erd\\H{o}s-R\\'enyi random\ngraphs to form the basis for proposing some conjectures about the general\nrelationships among edge density, cluster number, and consensus stability.",
        "authors": [
            "John S. McAlister",
            "Nina H. Fefferman"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:39:03+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19273v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19273v1",
        "categories": [
            "Computer Science and Game Theory",
            "Dynamical Systems",
            "Dynamic games, Games involving graphs "
        ]
    },
    {
        "id": 30000052,
        "doi": null,
        "title": "AutoPureData: Automated Filtering of Web Data for LLM Fine-tuning",
        "abstract": "Up-to-date and reliable Large Language Models (LLMs) are consistently sought\nafter. Typically, LLMs are trained on a fixed dataset and then deployed.\nHowever, the training data continually becomes outdated. Enable automatic\ntraining of AI using web data involves significant concerns regarding data\nquality and safety due to bias, spam, and other unsafe or unwanted text. Pure\ndata is essential for producing reliable models. Training a model on impure\ndata may result in undesirable outcomes. This research proposes a system that\ncollects web data and automatically filters out unwanted text with the\nassistance of existing trusted AI models. In the experiment, a small sample of\nweb data was collected and filtered, demonstrating the system's effectiveness\nin purifying the data.",
        "chunk-id": 1,
        "chunk": "Up-to-date and reliable Large Language Models (LLMs) are consistently sought\nafter. Typically, LLMs are trained on a fixed dataset and then deployed.\nHowever, the training data continually becomes outdated. Enable automatic\ntraining of AI using web data involves significant concerns regarding data\nquality and safety due to bias, spam, and other unsafe or unwanted text. Pure",
        "authors": [
            "Praneeth Vadlapati"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:37:57+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19271v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19271v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 30000052,
        "doi": null,
        "title": "AutoPureData: Automated Filtering of Web Data for LLM Fine-tuning",
        "abstract": "Up-to-date and reliable Large Language Models (LLMs) are consistently sought\nafter. Typically, LLMs are trained on a fixed dataset and then deployed.\nHowever, the training data continually becomes outdated. Enable automatic\ntraining of AI using web data involves significant concerns regarding data\nquality and safety due to bias, spam, and other unsafe or unwanted text. Pure\ndata is essential for producing reliable models. Training a model on impure\ndata may result in undesirable outcomes. This research proposes a system that\ncollects web data and automatically filters out unwanted text with the\nassistance of existing trusted AI models. In the experiment, a small sample of\nweb data was collected and filtered, demonstrating the system's effectiveness\nin purifying the data.",
        "chunk-id": 2,
        "chunk": "data is essential for producing reliable models. Training a model on impure\ndata may result in undesirable outcomes. This research proposes a system that\ncollects web data and automatically filters out unwanted text with the\nassistance of existing trusted AI models. In the experiment, a small sample of\nweb data was collected and filtered, demonstrating the system's effectiveness",
        "authors": [
            "Praneeth Vadlapati"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:37:57+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19271v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19271v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 30000052,
        "doi": null,
        "title": "AutoPureData: Automated Filtering of Web Data for LLM Fine-tuning",
        "abstract": "Up-to-date and reliable Large Language Models (LLMs) are consistently sought\nafter. Typically, LLMs are trained on a fixed dataset and then deployed.\nHowever, the training data continually becomes outdated. Enable automatic\ntraining of AI using web data involves significant concerns regarding data\nquality and safety due to bias, spam, and other unsafe or unwanted text. Pure\ndata is essential for producing reliable models. Training a model on impure\ndata may result in undesirable outcomes. This research proposes a system that\ncollects web data and automatically filters out unwanted text with the\nassistance of existing trusted AI models. In the experiment, a small sample of\nweb data was collected and filtered, demonstrating the system's effectiveness\nin purifying the data.",
        "chunk-id": 3,
        "chunk": "in purifying the data.",
        "authors": [
            "Praneeth Vadlapati"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:37:57+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19271v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19271v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 30000053,
        "doi": null,
        "title": "OCC-MP: A Max-Pressure framework to prioritize transit and high occupancy vehicles",
        "abstract": "Max-pressure (MP) is a decentralized adaptive traffic signal control approach\nthat has been shown to maximize throughput for private vehicles. However,\nMP-based signal control algorithms do not differentiate the movement of transit\nvehicles from private vehicles or between high and single-occupancy private\nvehicles. Prioritizing the movement of transit or other high occupancy vehicles\n(HOVs) is vital to reduce congestion and improve the reliability and efficiency\nof transit operations. This study proposes OCC-MP: a novel MP-based algorithm\nthat considers both vehicle queues and passenger occupancies in computing the\nweights of movements. By weighing movements with higher passenger occupancies\nmore heavily, transit and other HOVs are implicitly provided with priority,\nwhile accounting for any negative impacts of that priority on single occupancy\nvehicles. And, unlike rule-based transit signal priority (TSP) strategies,\nOCC-MP more naturally also accommodates conflicting transit routes at a\nsignalized intersection and facilitates their movement, even in mixed traffic\nwithout dedicated lanes. Simulations on a grid network under varying demands\nand transit configurations demonstrate the effectiveness of OCC-MP at providing\nTSP while simultaneously reducing the negative impact imparted onto lower\noccupancy private vehicles. Furthermore, OCC-MP is shown to have a larger\nstable region for demand compared to rule-based TSP strategies integrated into\nthe MP framework. The performance of OCC-MP is also shown to be robust to\nerrors in passenger occupancy information from transit vehicles and can be\napplied when passenger occupancies of private vehicles are not available.\nFinally, OCC-MP can be applied in a partially connected vehicle (CV)\nenvironment when a subset of vehicles is able to provide information to the\nsignal controller, outperforming baseline methods at low CV penetration rates.",
        "chunk-id": 1,
        "chunk": "Max-pressure (MP) is a decentralized adaptive traffic signal control approach\nthat has been shown to maximize throughput for private vehicles. However,\nMP-based signal control algorithms do not differentiate the movement of transit\nvehicles from private vehicles or between high and single-occupancy private",
        "authors": [
            "Tanveer Ahmed",
            "Hao Liu",
            "Vikash V. Gayah"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:36:08+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19269v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19269v1",
        "categories": [
            "Systems and Control",
            "Systems and Control"
        ]
    },
    {
        "id": 30000053,
        "doi": null,
        "title": "OCC-MP: A Max-Pressure framework to prioritize transit and high occupancy vehicles",
        "abstract": "Max-pressure (MP) is a decentralized adaptive traffic signal control approach\nthat has been shown to maximize throughput for private vehicles. However,\nMP-based signal control algorithms do not differentiate the movement of transit\nvehicles from private vehicles or between high and single-occupancy private\nvehicles. Prioritizing the movement of transit or other high occupancy vehicles\n(HOVs) is vital to reduce congestion and improve the reliability and efficiency\nof transit operations. This study proposes OCC-MP: a novel MP-based algorithm\nthat considers both vehicle queues and passenger occupancies in computing the\nweights of movements. By weighing movements with higher passenger occupancies\nmore heavily, transit and other HOVs are implicitly provided with priority,\nwhile accounting for any negative impacts of that priority on single occupancy\nvehicles. And, unlike rule-based transit signal priority (TSP) strategies,\nOCC-MP more naturally also accommodates conflicting transit routes at a\nsignalized intersection and facilitates their movement, even in mixed traffic\nwithout dedicated lanes. Simulations on a grid network under varying demands\nand transit configurations demonstrate the effectiveness of OCC-MP at providing\nTSP while simultaneously reducing the negative impact imparted onto lower\noccupancy private vehicles. Furthermore, OCC-MP is shown to have a larger\nstable region for demand compared to rule-based TSP strategies integrated into\nthe MP framework. The performance of OCC-MP is also shown to be robust to\nerrors in passenger occupancy information from transit vehicles and can be\napplied when passenger occupancies of private vehicles are not available.\nFinally, OCC-MP can be applied in a partially connected vehicle (CV)\nenvironment when a subset of vehicles is able to provide information to the\nsignal controller, outperforming baseline methods at low CV penetration rates.",
        "chunk-id": 2,
        "chunk": "vehicles. Prioritizing the movement of transit or other high occupancy vehicles\n(HOVs) is vital to reduce congestion and improve the reliability and efficiency\nof transit operations. This study proposes OCC-MP: a novel MP-based algorithm\nthat considers both vehicle queues and passenger occupancies in computing the",
        "authors": [
            "Tanveer Ahmed",
            "Hao Liu",
            "Vikash V. Gayah"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:36:08+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19269v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19269v1",
        "categories": [
            "Systems and Control",
            "Systems and Control"
        ]
    },
    {
        "id": 30000053,
        "doi": null,
        "title": "OCC-MP: A Max-Pressure framework to prioritize transit and high occupancy vehicles",
        "abstract": "Max-pressure (MP) is a decentralized adaptive traffic signal control approach\nthat has been shown to maximize throughput for private vehicles. However,\nMP-based signal control algorithms do not differentiate the movement of transit\nvehicles from private vehicles or between high and single-occupancy private\nvehicles. Prioritizing the movement of transit or other high occupancy vehicles\n(HOVs) is vital to reduce congestion and improve the reliability and efficiency\nof transit operations. This study proposes OCC-MP: a novel MP-based algorithm\nthat considers both vehicle queues and passenger occupancies in computing the\nweights of movements. By weighing movements with higher passenger occupancies\nmore heavily, transit and other HOVs are implicitly provided with priority,\nwhile accounting for any negative impacts of that priority on single occupancy\nvehicles. And, unlike rule-based transit signal priority (TSP) strategies,\nOCC-MP more naturally also accommodates conflicting transit routes at a\nsignalized intersection and facilitates their movement, even in mixed traffic\nwithout dedicated lanes. Simulations on a grid network under varying demands\nand transit configurations demonstrate the effectiveness of OCC-MP at providing\nTSP while simultaneously reducing the negative impact imparted onto lower\noccupancy private vehicles. Furthermore, OCC-MP is shown to have a larger\nstable region for demand compared to rule-based TSP strategies integrated into\nthe MP framework. The performance of OCC-MP is also shown to be robust to\nerrors in passenger occupancy information from transit vehicles and can be\napplied when passenger occupancies of private vehicles are not available.\nFinally, OCC-MP can be applied in a partially connected vehicle (CV)\nenvironment when a subset of vehicles is able to provide information to the\nsignal controller, outperforming baseline methods at low CV penetration rates.",
        "chunk-id": 3,
        "chunk": "weights of movements. By weighing movements with higher passenger occupancies\nmore heavily, transit and other HOVs are implicitly provided with priority,\nwhile accounting for any negative impacts of that priority on single occupancy\nvehicles. And, unlike rule-based transit signal priority (TSP) strategies,\nOCC-MP more naturally also accommodates conflicting transit routes at a",
        "authors": [
            "Tanveer Ahmed",
            "Hao Liu",
            "Vikash V. Gayah"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:36:08+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19269v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19269v1",
        "categories": [
            "Systems and Control",
            "Systems and Control"
        ]
    },
    {
        "id": 30000053,
        "doi": null,
        "title": "OCC-MP: A Max-Pressure framework to prioritize transit and high occupancy vehicles",
        "abstract": "Max-pressure (MP) is a decentralized adaptive traffic signal control approach\nthat has been shown to maximize throughput for private vehicles. However,\nMP-based signal control algorithms do not differentiate the movement of transit\nvehicles from private vehicles or between high and single-occupancy private\nvehicles. Prioritizing the movement of transit or other high occupancy vehicles\n(HOVs) is vital to reduce congestion and improve the reliability and efficiency\nof transit operations. This study proposes OCC-MP: a novel MP-based algorithm\nthat considers both vehicle queues and passenger occupancies in computing the\nweights of movements. By weighing movements with higher passenger occupancies\nmore heavily, transit and other HOVs are implicitly provided with priority,\nwhile accounting for any negative impacts of that priority on single occupancy\nvehicles. And, unlike rule-based transit signal priority (TSP) strategies,\nOCC-MP more naturally also accommodates conflicting transit routes at a\nsignalized intersection and facilitates their movement, even in mixed traffic\nwithout dedicated lanes. Simulations on a grid network under varying demands\nand transit configurations demonstrate the effectiveness of OCC-MP at providing\nTSP while simultaneously reducing the negative impact imparted onto lower\noccupancy private vehicles. Furthermore, OCC-MP is shown to have a larger\nstable region for demand compared to rule-based TSP strategies integrated into\nthe MP framework. The performance of OCC-MP is also shown to be robust to\nerrors in passenger occupancy information from transit vehicles and can be\napplied when passenger occupancies of private vehicles are not available.\nFinally, OCC-MP can be applied in a partially connected vehicle (CV)\nenvironment when a subset of vehicles is able to provide information to the\nsignal controller, outperforming baseline methods at low CV penetration rates.",
        "chunk-id": 4,
        "chunk": "signalized intersection and facilitates their movement, even in mixed traffic\nwithout dedicated lanes. Simulations on a grid network under varying demands\nand transit configurations demonstrate the effectiveness of OCC-MP at providing\nTSP while simultaneously reducing the negative impact imparted onto lower\noccupancy private vehicles. Furthermore, OCC-MP is shown to have a larger",
        "authors": [
            "Tanveer Ahmed",
            "Hao Liu",
            "Vikash V. Gayah"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:36:08+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19269v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19269v1",
        "categories": [
            "Systems and Control",
            "Systems and Control"
        ]
    },
    {
        "id": 30000053,
        "doi": null,
        "title": "OCC-MP: A Max-Pressure framework to prioritize transit and high occupancy vehicles",
        "abstract": "Max-pressure (MP) is a decentralized adaptive traffic signal control approach\nthat has been shown to maximize throughput for private vehicles. However,\nMP-based signal control algorithms do not differentiate the movement of transit\nvehicles from private vehicles or between high and single-occupancy private\nvehicles. Prioritizing the movement of transit or other high occupancy vehicles\n(HOVs) is vital to reduce congestion and improve the reliability and efficiency\nof transit operations. This study proposes OCC-MP: a novel MP-based algorithm\nthat considers both vehicle queues and passenger occupancies in computing the\nweights of movements. By weighing movements with higher passenger occupancies\nmore heavily, transit and other HOVs are implicitly provided with priority,\nwhile accounting for any negative impacts of that priority on single occupancy\nvehicles. And, unlike rule-based transit signal priority (TSP) strategies,\nOCC-MP more naturally also accommodates conflicting transit routes at a\nsignalized intersection and facilitates their movement, even in mixed traffic\nwithout dedicated lanes. Simulations on a grid network under varying demands\nand transit configurations demonstrate the effectiveness of OCC-MP at providing\nTSP while simultaneously reducing the negative impact imparted onto lower\noccupancy private vehicles. Furthermore, OCC-MP is shown to have a larger\nstable region for demand compared to rule-based TSP strategies integrated into\nthe MP framework. The performance of OCC-MP is also shown to be robust to\nerrors in passenger occupancy information from transit vehicles and can be\napplied when passenger occupancies of private vehicles are not available.\nFinally, OCC-MP can be applied in a partially connected vehicle (CV)\nenvironment when a subset of vehicles is able to provide information to the\nsignal controller, outperforming baseline methods at low CV penetration rates.",
        "chunk-id": 5,
        "chunk": "stable region for demand compared to rule-based TSP strategies integrated into\nthe MP framework. The performance of OCC-MP is also shown to be robust to\nerrors in passenger occupancy information from transit vehicles and can be\napplied when passenger occupancies of private vehicles are not available.\nFinally, OCC-MP can be applied in a partially connected vehicle (CV)",
        "authors": [
            "Tanveer Ahmed",
            "Hao Liu",
            "Vikash V. Gayah"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:36:08+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19269v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19269v1",
        "categories": [
            "Systems and Control",
            "Systems and Control"
        ]
    },
    {
        "id": 30000053,
        "doi": null,
        "title": "OCC-MP: A Max-Pressure framework to prioritize transit and high occupancy vehicles",
        "abstract": "Max-pressure (MP) is a decentralized adaptive traffic signal control approach\nthat has been shown to maximize throughput for private vehicles. However,\nMP-based signal control algorithms do not differentiate the movement of transit\nvehicles from private vehicles or between high and single-occupancy private\nvehicles. Prioritizing the movement of transit or other high occupancy vehicles\n(HOVs) is vital to reduce congestion and improve the reliability and efficiency\nof transit operations. This study proposes OCC-MP: a novel MP-based algorithm\nthat considers both vehicle queues and passenger occupancies in computing the\nweights of movements. By weighing movements with higher passenger occupancies\nmore heavily, transit and other HOVs are implicitly provided with priority,\nwhile accounting for any negative impacts of that priority on single occupancy\nvehicles. And, unlike rule-based transit signal priority (TSP) strategies,\nOCC-MP more naturally also accommodates conflicting transit routes at a\nsignalized intersection and facilitates their movement, even in mixed traffic\nwithout dedicated lanes. Simulations on a grid network under varying demands\nand transit configurations demonstrate the effectiveness of OCC-MP at providing\nTSP while simultaneously reducing the negative impact imparted onto lower\noccupancy private vehicles. Furthermore, OCC-MP is shown to have a larger\nstable region for demand compared to rule-based TSP strategies integrated into\nthe MP framework. The performance of OCC-MP is also shown to be robust to\nerrors in passenger occupancy information from transit vehicles and can be\napplied when passenger occupancies of private vehicles are not available.\nFinally, OCC-MP can be applied in a partially connected vehicle (CV)\nenvironment when a subset of vehicles is able to provide information to the\nsignal controller, outperforming baseline methods at low CV penetration rates.",
        "chunk-id": 6,
        "chunk": "environment when a subset of vehicles is able to provide information to the\nsignal controller, outperforming baseline methods at low CV penetration rates.",
        "authors": [
            "Tanveer Ahmed",
            "Hao Liu",
            "Vikash V. Gayah"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:36:08+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19269v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19269v1",
        "categories": [
            "Systems and Control",
            "Systems and Control"
        ]
    },
    {
        "id": 30000054,
        "doi": null,
        "title": "Read Anywhere Pointed: Layout-aware GUI Screen Reading with Tree-of-Lens Grounding",
        "abstract": "Graphical User Interfaces (GUIs) are central to our interaction with digital\ndevices. Recently, growing efforts have been made to build models for various\nGUI understanding tasks. However, these efforts largely overlook an important\nGUI-referring task: screen reading based on user-indicated points, which we\nname the Screen Point-and-Read (SPR) task. This task is predominantly handled\nby rigid accessible screen reading tools, in great need of new models driven by\nadvancements in Multimodal Large Language Models (MLLMs). In this paper, we\npropose a Tree-of-Lens (ToL) agent, utilizing a novel ToL grounding mechanism,\nto address the SPR task. Based on the input point coordinate and the\ncorresponding GUI screenshot, our ToL agent constructs a Hierarchical Layout\nTree. Based on the tree, our ToL agent not only comprehends the content of the\nindicated area but also articulates the layout and spatial relationships\nbetween elements. Such layout information is crucial for accurately\ninterpreting information on the screen, distinguishing our ToL agent from other\nscreen reading tools. We also thoroughly evaluate the ToL agent against other\nbaselines on a newly proposed SPR benchmark, which includes GUIs from mobile,\nweb, and operating systems. Last but not least, we test the ToL agent on mobile\nGUI navigation tasks, demonstrating its utility in identifying incorrect\nactions along the path of agent execution trajectories. Code and data:\nscreen-point-and-read.github.io",
        "chunk-id": 1,
        "chunk": "Graphical User Interfaces (GUIs) are central to our interaction with digital\ndevices. Recently, growing efforts have been made to build models for various\nGUI understanding tasks. However, these efforts largely overlook an important\nGUI-referring task: screen reading based on user-indicated points, which we",
        "authors": [
            "Yue Fan",
            "Lei Ding",
            "Ching-Chen Kuo",
            "Shan Jiang",
            "Yang Zhao",
            "Xinze Guan",
            "Jie Yang",
            "Yi Zhang",
            "Xin Eric Wang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:34:16+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19263v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19263v1",
        "categories": [
            "Computation and Language",
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000054,
        "doi": null,
        "title": "Read Anywhere Pointed: Layout-aware GUI Screen Reading with Tree-of-Lens Grounding",
        "abstract": "Graphical User Interfaces (GUIs) are central to our interaction with digital\ndevices. Recently, growing efforts have been made to build models for various\nGUI understanding tasks. However, these efforts largely overlook an important\nGUI-referring task: screen reading based on user-indicated points, which we\nname the Screen Point-and-Read (SPR) task. This task is predominantly handled\nby rigid accessible screen reading tools, in great need of new models driven by\nadvancements in Multimodal Large Language Models (MLLMs). In this paper, we\npropose a Tree-of-Lens (ToL) agent, utilizing a novel ToL grounding mechanism,\nto address the SPR task. Based on the input point coordinate and the\ncorresponding GUI screenshot, our ToL agent constructs a Hierarchical Layout\nTree. Based on the tree, our ToL agent not only comprehends the content of the\nindicated area but also articulates the layout and spatial relationships\nbetween elements. Such layout information is crucial for accurately\ninterpreting information on the screen, distinguishing our ToL agent from other\nscreen reading tools. We also thoroughly evaluate the ToL agent against other\nbaselines on a newly proposed SPR benchmark, which includes GUIs from mobile,\nweb, and operating systems. Last but not least, we test the ToL agent on mobile\nGUI navigation tasks, demonstrating its utility in identifying incorrect\nactions along the path of agent execution trajectories. Code and data:\nscreen-point-and-read.github.io",
        "chunk-id": 2,
        "chunk": "name the Screen Point-and-Read (SPR) task. This task is predominantly handled\nby rigid accessible screen reading tools, in great need of new models driven by\nadvancements in Multimodal Large Language Models (MLLMs). In this paper, we\npropose a Tree-of-Lens (ToL) agent, utilizing a novel ToL grounding mechanism,\nto address the SPR task. Based on the input point coordinate and the",
        "authors": [
            "Yue Fan",
            "Lei Ding",
            "Ching-Chen Kuo",
            "Shan Jiang",
            "Yang Zhao",
            "Xinze Guan",
            "Jie Yang",
            "Yi Zhang",
            "Xin Eric Wang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:34:16+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19263v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19263v1",
        "categories": [
            "Computation and Language",
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000054,
        "doi": null,
        "title": "Read Anywhere Pointed: Layout-aware GUI Screen Reading with Tree-of-Lens Grounding",
        "abstract": "Graphical User Interfaces (GUIs) are central to our interaction with digital\ndevices. Recently, growing efforts have been made to build models for various\nGUI understanding tasks. However, these efforts largely overlook an important\nGUI-referring task: screen reading based on user-indicated points, which we\nname the Screen Point-and-Read (SPR) task. This task is predominantly handled\nby rigid accessible screen reading tools, in great need of new models driven by\nadvancements in Multimodal Large Language Models (MLLMs). In this paper, we\npropose a Tree-of-Lens (ToL) agent, utilizing a novel ToL grounding mechanism,\nto address the SPR task. Based on the input point coordinate and the\ncorresponding GUI screenshot, our ToL agent constructs a Hierarchical Layout\nTree. Based on the tree, our ToL agent not only comprehends the content of the\nindicated area but also articulates the layout and spatial relationships\nbetween elements. Such layout information is crucial for accurately\ninterpreting information on the screen, distinguishing our ToL agent from other\nscreen reading tools. We also thoroughly evaluate the ToL agent against other\nbaselines on a newly proposed SPR benchmark, which includes GUIs from mobile,\nweb, and operating systems. Last but not least, we test the ToL agent on mobile\nGUI navigation tasks, demonstrating its utility in identifying incorrect\nactions along the path of agent execution trajectories. Code and data:\nscreen-point-and-read.github.io",
        "chunk-id": 3,
        "chunk": "corresponding GUI screenshot, our ToL agent constructs a Hierarchical Layout\nTree. Based on the tree, our ToL agent not only comprehends the content of the\nindicated area but also articulates the layout and spatial relationships\nbetween elements. Such layout information is crucial for accurately\ninterpreting information on the screen, distinguishing our ToL agent from other",
        "authors": [
            "Yue Fan",
            "Lei Ding",
            "Ching-Chen Kuo",
            "Shan Jiang",
            "Yang Zhao",
            "Xinze Guan",
            "Jie Yang",
            "Yi Zhang",
            "Xin Eric Wang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:34:16+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19263v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19263v1",
        "categories": [
            "Computation and Language",
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000054,
        "doi": null,
        "title": "Read Anywhere Pointed: Layout-aware GUI Screen Reading with Tree-of-Lens Grounding",
        "abstract": "Graphical User Interfaces (GUIs) are central to our interaction with digital\ndevices. Recently, growing efforts have been made to build models for various\nGUI understanding tasks. However, these efforts largely overlook an important\nGUI-referring task: screen reading based on user-indicated points, which we\nname the Screen Point-and-Read (SPR) task. This task is predominantly handled\nby rigid accessible screen reading tools, in great need of new models driven by\nadvancements in Multimodal Large Language Models (MLLMs). In this paper, we\npropose a Tree-of-Lens (ToL) agent, utilizing a novel ToL grounding mechanism,\nto address the SPR task. Based on the input point coordinate and the\ncorresponding GUI screenshot, our ToL agent constructs a Hierarchical Layout\nTree. Based on the tree, our ToL agent not only comprehends the content of the\nindicated area but also articulates the layout and spatial relationships\nbetween elements. Such layout information is crucial for accurately\ninterpreting information on the screen, distinguishing our ToL agent from other\nscreen reading tools. We also thoroughly evaluate the ToL agent against other\nbaselines on a newly proposed SPR benchmark, which includes GUIs from mobile,\nweb, and operating systems. Last but not least, we test the ToL agent on mobile\nGUI navigation tasks, demonstrating its utility in identifying incorrect\nactions along the path of agent execution trajectories. Code and data:\nscreen-point-and-read.github.io",
        "chunk-id": 4,
        "chunk": "screen reading tools. We also thoroughly evaluate the ToL agent against other\nbaselines on a newly proposed SPR benchmark, which includes GUIs from mobile,\nweb, and operating systems. Last but not least, we test the ToL agent on mobile\nGUI navigation tasks, demonstrating its utility in identifying incorrect\nactions along the path of agent execution trajectories. Code and data:",
        "authors": [
            "Yue Fan",
            "Lei Ding",
            "Ching-Chen Kuo",
            "Shan Jiang",
            "Yang Zhao",
            "Xinze Guan",
            "Jie Yang",
            "Yi Zhang",
            "Xin Eric Wang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:34:16+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19263v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19263v1",
        "categories": [
            "Computation and Language",
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000054,
        "doi": null,
        "title": "Read Anywhere Pointed: Layout-aware GUI Screen Reading with Tree-of-Lens Grounding",
        "abstract": "Graphical User Interfaces (GUIs) are central to our interaction with digital\ndevices. Recently, growing efforts have been made to build models for various\nGUI understanding tasks. However, these efforts largely overlook an important\nGUI-referring task: screen reading based on user-indicated points, which we\nname the Screen Point-and-Read (SPR) task. This task is predominantly handled\nby rigid accessible screen reading tools, in great need of new models driven by\nadvancements in Multimodal Large Language Models (MLLMs). In this paper, we\npropose a Tree-of-Lens (ToL) agent, utilizing a novel ToL grounding mechanism,\nto address the SPR task. Based on the input point coordinate and the\ncorresponding GUI screenshot, our ToL agent constructs a Hierarchical Layout\nTree. Based on the tree, our ToL agent not only comprehends the content of the\nindicated area but also articulates the layout and spatial relationships\nbetween elements. Such layout information is crucial for accurately\ninterpreting information on the screen, distinguishing our ToL agent from other\nscreen reading tools. We also thoroughly evaluate the ToL agent against other\nbaselines on a newly proposed SPR benchmark, which includes GUIs from mobile,\nweb, and operating systems. Last but not least, we test the ToL agent on mobile\nGUI navigation tasks, demonstrating its utility in identifying incorrect\nactions along the path of agent execution trajectories. Code and data:\nscreen-point-and-read.github.io",
        "chunk-id": 5,
        "chunk": "screen-point-and-read.github.io",
        "authors": [
            "Yue Fan",
            "Lei Ding",
            "Ching-Chen Kuo",
            "Shan Jiang",
            "Yang Zhao",
            "Xinze Guan",
            "Jie Yang",
            "Yi Zhang",
            "Xin Eric Wang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:34:16+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19263v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19263v1",
        "categories": [
            "Computation and Language",
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000055,
        "doi": null,
        "title": "Commodification of Compute",
        "abstract": "The rapid advancements in artificial intelligence, big data analytics, and\ncloud computing have precipitated an unprecedented demand for computational\nresources. However, the current landscape of computational resource allocation\nis characterized by significant inefficiencies, including underutilization and\nprice volatility. This paper addresses these challenges by introducing a novel\nglobal platform for the commodification of compute hours, termed the Global\nCompute Exchange (GCX) (Patent Pending). The GCX leverages blockchain\ntechnology and smart contracts to create a secure, transparent, and efficient\nmarketplace for buying and selling computational power. The GCX is built in a\nlayered fashion, comprising Market, App, Clearing, Risk Management, Exchange\n(Offchain), and Blockchain (Onchain) layers, each ensuring a robust and\nefficient operation. This platform aims to revolutionize the computational\nresource market by fostering a decentralized, efficient, and transparent\necosystem that ensures equitable access to computing power, stimulates\ninnovation, and supports diverse user needs on a global scale. By transforming\ncompute hours into a tradable commodity, the GCX seeks to optimize resource\nutilization, stabilize pricing, and democratize access to computational\nresources. This paper explores the technological infrastructure, market\npotential, and societal impact of the GCX, positioning it as a pioneering\nsolution poised to drive the next wave of innovation in commodities and\ncompute.",
        "chunk-id": 1,
        "chunk": "The rapid advancements in artificial intelligence, big data analytics, and\ncloud computing have precipitated an unprecedented demand for computational\nresources. However, the current landscape of computational resource allocation\nis characterized by significant inefficiencies, including underutilization and",
        "authors": [
            "Jesper Kristensen",
            "David Wender",
            "Carl Anthony"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:32:31+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19261v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19261v1",
        "categories": [
            "Computational Engineering, Finance, and Science",
            "Artificial Intelligence",
            "Computers and Society",
            "Emerging Technologies",
            "General Economics",
            "Economics"
        ]
    },
    {
        "id": 30000055,
        "doi": null,
        "title": "Commodification of Compute",
        "abstract": "The rapid advancements in artificial intelligence, big data analytics, and\ncloud computing have precipitated an unprecedented demand for computational\nresources. However, the current landscape of computational resource allocation\nis characterized by significant inefficiencies, including underutilization and\nprice volatility. This paper addresses these challenges by introducing a novel\nglobal platform for the commodification of compute hours, termed the Global\nCompute Exchange (GCX) (Patent Pending). The GCX leverages blockchain\ntechnology and smart contracts to create a secure, transparent, and efficient\nmarketplace for buying and selling computational power. The GCX is built in a\nlayered fashion, comprising Market, App, Clearing, Risk Management, Exchange\n(Offchain), and Blockchain (Onchain) layers, each ensuring a robust and\nefficient operation. This platform aims to revolutionize the computational\nresource market by fostering a decentralized, efficient, and transparent\necosystem that ensures equitable access to computing power, stimulates\ninnovation, and supports diverse user needs on a global scale. By transforming\ncompute hours into a tradable commodity, the GCX seeks to optimize resource\nutilization, stabilize pricing, and democratize access to computational\nresources. This paper explores the technological infrastructure, market\npotential, and societal impact of the GCX, positioning it as a pioneering\nsolution poised to drive the next wave of innovation in commodities and\ncompute.",
        "chunk-id": 2,
        "chunk": "price volatility. This paper addresses these challenges by introducing a novel\nglobal platform for the commodification of compute hours, termed the Global\nCompute Exchange (GCX) (Patent Pending). The GCX leverages blockchain\ntechnology and smart contracts to create a secure, transparent, and efficient\nmarketplace for buying and selling computational power. The GCX is built in a",
        "authors": [
            "Jesper Kristensen",
            "David Wender",
            "Carl Anthony"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:32:31+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19261v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19261v1",
        "categories": [
            "Computational Engineering, Finance, and Science",
            "Artificial Intelligence",
            "Computers and Society",
            "Emerging Technologies",
            "General Economics",
            "Economics"
        ]
    },
    {
        "id": 30000055,
        "doi": null,
        "title": "Commodification of Compute",
        "abstract": "The rapid advancements in artificial intelligence, big data analytics, and\ncloud computing have precipitated an unprecedented demand for computational\nresources. However, the current landscape of computational resource allocation\nis characterized by significant inefficiencies, including underutilization and\nprice volatility. This paper addresses these challenges by introducing a novel\nglobal platform for the commodification of compute hours, termed the Global\nCompute Exchange (GCX) (Patent Pending). The GCX leverages blockchain\ntechnology and smart contracts to create a secure, transparent, and efficient\nmarketplace for buying and selling computational power. The GCX is built in a\nlayered fashion, comprising Market, App, Clearing, Risk Management, Exchange\n(Offchain), and Blockchain (Onchain) layers, each ensuring a robust and\nefficient operation. This platform aims to revolutionize the computational\nresource market by fostering a decentralized, efficient, and transparent\necosystem that ensures equitable access to computing power, stimulates\ninnovation, and supports diverse user needs on a global scale. By transforming\ncompute hours into a tradable commodity, the GCX seeks to optimize resource\nutilization, stabilize pricing, and democratize access to computational\nresources. This paper explores the technological infrastructure, market\npotential, and societal impact of the GCX, positioning it as a pioneering\nsolution poised to drive the next wave of innovation in commodities and\ncompute.",
        "chunk-id": 3,
        "chunk": "layered fashion, comprising Market, App, Clearing, Risk Management, Exchange\n(Offchain), and Blockchain (Onchain) layers, each ensuring a robust and\nefficient operation. This platform aims to revolutionize the computational\nresource market by fostering a decentralized, efficient, and transparent\necosystem that ensures equitable access to computing power, stimulates",
        "authors": [
            "Jesper Kristensen",
            "David Wender",
            "Carl Anthony"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:32:31+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19261v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19261v1",
        "categories": [
            "Computational Engineering, Finance, and Science",
            "Artificial Intelligence",
            "Computers and Society",
            "Emerging Technologies",
            "General Economics",
            "Economics"
        ]
    },
    {
        "id": 30000055,
        "doi": null,
        "title": "Commodification of Compute",
        "abstract": "The rapid advancements in artificial intelligence, big data analytics, and\ncloud computing have precipitated an unprecedented demand for computational\nresources. However, the current landscape of computational resource allocation\nis characterized by significant inefficiencies, including underutilization and\nprice volatility. This paper addresses these challenges by introducing a novel\nglobal platform for the commodification of compute hours, termed the Global\nCompute Exchange (GCX) (Patent Pending). The GCX leverages blockchain\ntechnology and smart contracts to create a secure, transparent, and efficient\nmarketplace for buying and selling computational power. The GCX is built in a\nlayered fashion, comprising Market, App, Clearing, Risk Management, Exchange\n(Offchain), and Blockchain (Onchain) layers, each ensuring a robust and\nefficient operation. This platform aims to revolutionize the computational\nresource market by fostering a decentralized, efficient, and transparent\necosystem that ensures equitable access to computing power, stimulates\ninnovation, and supports diverse user needs on a global scale. By transforming\ncompute hours into a tradable commodity, the GCX seeks to optimize resource\nutilization, stabilize pricing, and democratize access to computational\nresources. This paper explores the technological infrastructure, market\npotential, and societal impact of the GCX, positioning it as a pioneering\nsolution poised to drive the next wave of innovation in commodities and\ncompute.",
        "chunk-id": 4,
        "chunk": "innovation, and supports diverse user needs on a global scale. By transforming\ncompute hours into a tradable commodity, the GCX seeks to optimize resource\nutilization, stabilize pricing, and democratize access to computational\nresources. This paper explores the technological infrastructure, market\npotential, and societal impact of the GCX, positioning it as a pioneering",
        "authors": [
            "Jesper Kristensen",
            "David Wender",
            "Carl Anthony"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:32:31+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19261v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19261v1",
        "categories": [
            "Computational Engineering, Finance, and Science",
            "Artificial Intelligence",
            "Computers and Society",
            "Emerging Technologies",
            "General Economics",
            "Economics"
        ]
    },
    {
        "id": 30000055,
        "doi": null,
        "title": "Commodification of Compute",
        "abstract": "The rapid advancements in artificial intelligence, big data analytics, and\ncloud computing have precipitated an unprecedented demand for computational\nresources. However, the current landscape of computational resource allocation\nis characterized by significant inefficiencies, including underutilization and\nprice volatility. This paper addresses these challenges by introducing a novel\nglobal platform for the commodification of compute hours, termed the Global\nCompute Exchange (GCX) (Patent Pending). The GCX leverages blockchain\ntechnology and smart contracts to create a secure, transparent, and efficient\nmarketplace for buying and selling computational power. The GCX is built in a\nlayered fashion, comprising Market, App, Clearing, Risk Management, Exchange\n(Offchain), and Blockchain (Onchain) layers, each ensuring a robust and\nefficient operation. This platform aims to revolutionize the computational\nresource market by fostering a decentralized, efficient, and transparent\necosystem that ensures equitable access to computing power, stimulates\ninnovation, and supports diverse user needs on a global scale. By transforming\ncompute hours into a tradable commodity, the GCX seeks to optimize resource\nutilization, stabilize pricing, and democratize access to computational\nresources. This paper explores the technological infrastructure, market\npotential, and societal impact of the GCX, positioning it as a pioneering\nsolution poised to drive the next wave of innovation in commodities and\ncompute.",
        "chunk-id": 5,
        "chunk": "solution poised to drive the next wave of innovation in commodities and\ncompute.",
        "authors": [
            "Jesper Kristensen",
            "David Wender",
            "Carl Anthony"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:32:31+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19261v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19261v1",
        "categories": [
            "Computational Engineering, Finance, and Science",
            "Artificial Intelligence",
            "Computers and Society",
            "Emerging Technologies",
            "General Economics",
            "Economics"
        ]
    },
    {
        "id": 30000056,
        "doi": null,
        "title": "Online sorting and online TSP: randomized, stochastic, and high-dimensional",
        "abstract": "In the online sorting problem, $n$ items are revealed one by one and have to\nbe placed (immediately and irrevocably) into empty cells of a size-$n$ array.\nThe goal is to minimize the sum of absolute differences between items in\nconsecutive cells. This natural problem was recently introduced by Aamand,\nAbrahamsen, Beretta, and Kleist (SODA 2023) as a tool in their study of online\ngeometric packing problems. They showed that when the items are reals from the\ninterval $[0,1]$ a competitive ratio of $O(\\sqrt{n})$ is achievable, and no\ndeterministic algorithm can improve this ratio asymptotically.\n  In this paper, we extend and generalize the study of online sorting in three\ndirections:\n  - randomized: we settle the open question of Aamand et al. by showing that\nthe $O(\\sqrt{n})$ competitive ratio for the online sorting of reals cannot be\nimproved even with the use of randomness;\n  - stochastic: we consider inputs consisting of $n$ samples drawn uniformly at\nrandom from an interval, and give an algorithm with an improved competitive\nratio of $\\widetilde{O}(n^{1/4})$. The result reveals connections between\nonline sorting and the design of efficient hash tables;\n  - high-dimensional: we show that $\\widetilde{O}(\\sqrt{n})$-competitive online\nsorting is possible even for items from $\\mathbb{R}^d$, for arbitrary fixed\n$d$, in an adversarial model. This can be viewed as an online variant of the\nclassical TSP problem where tasks (cities to visit) are revealed one by one and\nthe salesperson assigns each task (immediately and irrevocably) to its\ntimeslot. Along the way, we also show a tight $O(\\log{n})$-competitiveness\nresult for uniform metrics, i.e., where items are of different types and the\ngoal is to order them so as to minimize the number of switches between\nconsecutive items of different types.",
        "chunk-id": 1,
        "chunk": "In the online sorting problem, $n$ items are revealed one by one and have to\nbe placed (immediately and irrevocably) into empty cells of a size-$n$ array.\nThe goal is to minimize the sum of absolute differences between items in\nconsecutive cells. This natural problem was recently introduced by Aamand,\nAbrahamsen, Beretta, and Kleist (SODA 2023) as a tool in their study of online",
        "authors": [
            "Mikkel Abrahamsen",
            "Ioana O. Bercea",
            "Lorenzo Beretta",
            "Jonas Klausen",
            "L\u00e1szl\u00f3 Kozma"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:26:49+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19257v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19257v1",
        "categories": [
            "Data Structures and Algorithms",
            "Computational Geometry"
        ]
    },
    {
        "id": 30000056,
        "doi": null,
        "title": "Online sorting and online TSP: randomized, stochastic, and high-dimensional",
        "abstract": "In the online sorting problem, $n$ items are revealed one by one and have to\nbe placed (immediately and irrevocably) into empty cells of a size-$n$ array.\nThe goal is to minimize the sum of absolute differences between items in\nconsecutive cells. This natural problem was recently introduced by Aamand,\nAbrahamsen, Beretta, and Kleist (SODA 2023) as a tool in their study of online\ngeometric packing problems. They showed that when the items are reals from the\ninterval $[0,1]$ a competitive ratio of $O(\\sqrt{n})$ is achievable, and no\ndeterministic algorithm can improve this ratio asymptotically.\n  In this paper, we extend and generalize the study of online sorting in three\ndirections:\n  - randomized: we settle the open question of Aamand et al. by showing that\nthe $O(\\sqrt{n})$ competitive ratio for the online sorting of reals cannot be\nimproved even with the use of randomness;\n  - stochastic: we consider inputs consisting of $n$ samples drawn uniformly at\nrandom from an interval, and give an algorithm with an improved competitive\nratio of $\\widetilde{O}(n^{1/4})$. The result reveals connections between\nonline sorting and the design of efficient hash tables;\n  - high-dimensional: we show that $\\widetilde{O}(\\sqrt{n})$-competitive online\nsorting is possible even for items from $\\mathbb{R}^d$, for arbitrary fixed\n$d$, in an adversarial model. This can be viewed as an online variant of the\nclassical TSP problem where tasks (cities to visit) are revealed one by one and\nthe salesperson assigns each task (immediately and irrevocably) to its\ntimeslot. Along the way, we also show a tight $O(\\log{n})$-competitiveness\nresult for uniform metrics, i.e., where items are of different types and the\ngoal is to order them so as to minimize the number of switches between\nconsecutive items of different types.",
        "chunk-id": 2,
        "chunk": "geometric packing problems. They showed that when the items are reals from the\ninterval $[0,1]$ a competitive ratio of $O(\\sqrt{n})$ is achievable, and no\ndeterministic algorithm can improve this ratio asymptotically.\n  In this paper, we extend and generalize the study of online sorting in three\ndirections:",
        "authors": [
            "Mikkel Abrahamsen",
            "Ioana O. Bercea",
            "Lorenzo Beretta",
            "Jonas Klausen",
            "L\u00e1szl\u00f3 Kozma"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:26:49+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19257v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19257v1",
        "categories": [
            "Data Structures and Algorithms",
            "Computational Geometry"
        ]
    },
    {
        "id": 30000056,
        "doi": null,
        "title": "Online sorting and online TSP: randomized, stochastic, and high-dimensional",
        "abstract": "In the online sorting problem, $n$ items are revealed one by one and have to\nbe placed (immediately and irrevocably) into empty cells of a size-$n$ array.\nThe goal is to minimize the sum of absolute differences between items in\nconsecutive cells. This natural problem was recently introduced by Aamand,\nAbrahamsen, Beretta, and Kleist (SODA 2023) as a tool in their study of online\ngeometric packing problems. They showed that when the items are reals from the\ninterval $[0,1]$ a competitive ratio of $O(\\sqrt{n})$ is achievable, and no\ndeterministic algorithm can improve this ratio asymptotically.\n  In this paper, we extend and generalize the study of online sorting in three\ndirections:\n  - randomized: we settle the open question of Aamand et al. by showing that\nthe $O(\\sqrt{n})$ competitive ratio for the online sorting of reals cannot be\nimproved even with the use of randomness;\n  - stochastic: we consider inputs consisting of $n$ samples drawn uniformly at\nrandom from an interval, and give an algorithm with an improved competitive\nratio of $\\widetilde{O}(n^{1/4})$. The result reveals connections between\nonline sorting and the design of efficient hash tables;\n  - high-dimensional: we show that $\\widetilde{O}(\\sqrt{n})$-competitive online\nsorting is possible even for items from $\\mathbb{R}^d$, for arbitrary fixed\n$d$, in an adversarial model. This can be viewed as an online variant of the\nclassical TSP problem where tasks (cities to visit) are revealed one by one and\nthe salesperson assigns each task (immediately and irrevocably) to its\ntimeslot. Along the way, we also show a tight $O(\\log{n})$-competitiveness\nresult for uniform metrics, i.e., where items are of different types and the\ngoal is to order them so as to minimize the number of switches between\nconsecutive items of different types.",
        "chunk-id": 3,
        "chunk": "directions:\n  - randomized: we settle the open question of Aamand et al. by showing that\nthe $O(\\sqrt{n})$ competitive ratio for the online sorting of reals cannot be\nimproved even with the use of randomness;\n  - stochastic: we consider inputs consisting of $n$ samples drawn uniformly at\nrandom from an interval, and give an algorithm with an improved competitive",
        "authors": [
            "Mikkel Abrahamsen",
            "Ioana O. Bercea",
            "Lorenzo Beretta",
            "Jonas Klausen",
            "L\u00e1szl\u00f3 Kozma"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:26:49+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19257v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19257v1",
        "categories": [
            "Data Structures and Algorithms",
            "Computational Geometry"
        ]
    },
    {
        "id": 30000056,
        "doi": null,
        "title": "Online sorting and online TSP: randomized, stochastic, and high-dimensional",
        "abstract": "In the online sorting problem, $n$ items are revealed one by one and have to\nbe placed (immediately and irrevocably) into empty cells of a size-$n$ array.\nThe goal is to minimize the sum of absolute differences between items in\nconsecutive cells. This natural problem was recently introduced by Aamand,\nAbrahamsen, Beretta, and Kleist (SODA 2023) as a tool in their study of online\ngeometric packing problems. They showed that when the items are reals from the\ninterval $[0,1]$ a competitive ratio of $O(\\sqrt{n})$ is achievable, and no\ndeterministic algorithm can improve this ratio asymptotically.\n  In this paper, we extend and generalize the study of online sorting in three\ndirections:\n  - randomized: we settle the open question of Aamand et al. by showing that\nthe $O(\\sqrt{n})$ competitive ratio for the online sorting of reals cannot be\nimproved even with the use of randomness;\n  - stochastic: we consider inputs consisting of $n$ samples drawn uniformly at\nrandom from an interval, and give an algorithm with an improved competitive\nratio of $\\widetilde{O}(n^{1/4})$. The result reveals connections between\nonline sorting and the design of efficient hash tables;\n  - high-dimensional: we show that $\\widetilde{O}(\\sqrt{n})$-competitive online\nsorting is possible even for items from $\\mathbb{R}^d$, for arbitrary fixed\n$d$, in an adversarial model. This can be viewed as an online variant of the\nclassical TSP problem where tasks (cities to visit) are revealed one by one and\nthe salesperson assigns each task (immediately and irrevocably) to its\ntimeslot. Along the way, we also show a tight $O(\\log{n})$-competitiveness\nresult for uniform metrics, i.e., where items are of different types and the\ngoal is to order them so as to minimize the number of switches between\nconsecutive items of different types.",
        "chunk-id": 4,
        "chunk": "ratio of $\\widetilde{O}(n^{1/4})$. The result reveals connections between\nonline sorting and the design of efficient hash tables;\n  - high-dimensional: we show that $\\widetilde{O}(\\sqrt{n})$-competitive online\nsorting is possible even for items from $\\mathbb{R}^d$, for arbitrary fixed\n$d$, in an adversarial model. This can be viewed as an online variant of the",
        "authors": [
            "Mikkel Abrahamsen",
            "Ioana O. Bercea",
            "Lorenzo Beretta",
            "Jonas Klausen",
            "L\u00e1szl\u00f3 Kozma"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:26:49+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19257v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19257v1",
        "categories": [
            "Data Structures and Algorithms",
            "Computational Geometry"
        ]
    },
    {
        "id": 30000056,
        "doi": null,
        "title": "Online sorting and online TSP: randomized, stochastic, and high-dimensional",
        "abstract": "In the online sorting problem, $n$ items are revealed one by one and have to\nbe placed (immediately and irrevocably) into empty cells of a size-$n$ array.\nThe goal is to minimize the sum of absolute differences between items in\nconsecutive cells. This natural problem was recently introduced by Aamand,\nAbrahamsen, Beretta, and Kleist (SODA 2023) as a tool in their study of online\ngeometric packing problems. They showed that when the items are reals from the\ninterval $[0,1]$ a competitive ratio of $O(\\sqrt{n})$ is achievable, and no\ndeterministic algorithm can improve this ratio asymptotically.\n  In this paper, we extend and generalize the study of online sorting in three\ndirections:\n  - randomized: we settle the open question of Aamand et al. by showing that\nthe $O(\\sqrt{n})$ competitive ratio for the online sorting of reals cannot be\nimproved even with the use of randomness;\n  - stochastic: we consider inputs consisting of $n$ samples drawn uniformly at\nrandom from an interval, and give an algorithm with an improved competitive\nratio of $\\widetilde{O}(n^{1/4})$. The result reveals connections between\nonline sorting and the design of efficient hash tables;\n  - high-dimensional: we show that $\\widetilde{O}(\\sqrt{n})$-competitive online\nsorting is possible even for items from $\\mathbb{R}^d$, for arbitrary fixed\n$d$, in an adversarial model. This can be viewed as an online variant of the\nclassical TSP problem where tasks (cities to visit) are revealed one by one and\nthe salesperson assigns each task (immediately and irrevocably) to its\ntimeslot. Along the way, we also show a tight $O(\\log{n})$-competitiveness\nresult for uniform metrics, i.e., where items are of different types and the\ngoal is to order them so as to minimize the number of switches between\nconsecutive items of different types.",
        "chunk-id": 5,
        "chunk": "classical TSP problem where tasks (cities to visit) are revealed one by one and\nthe salesperson assigns each task (immediately and irrevocably) to its\ntimeslot. Along the way, we also show a tight $O(\\log{n})$-competitiveness\nresult for uniform metrics, i.e., where items are of different types and the\ngoal is to order them so as to minimize the number of switches between",
        "authors": [
            "Mikkel Abrahamsen",
            "Ioana O. Bercea",
            "Lorenzo Beretta",
            "Jonas Klausen",
            "L\u00e1szl\u00f3 Kozma"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:26:49+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19257v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19257v1",
        "categories": [
            "Data Structures and Algorithms",
            "Computational Geometry"
        ]
    },
    {
        "id": 30000056,
        "doi": null,
        "title": "Online sorting and online TSP: randomized, stochastic, and high-dimensional",
        "abstract": "In the online sorting problem, $n$ items are revealed one by one and have to\nbe placed (immediately and irrevocably) into empty cells of a size-$n$ array.\nThe goal is to minimize the sum of absolute differences between items in\nconsecutive cells. This natural problem was recently introduced by Aamand,\nAbrahamsen, Beretta, and Kleist (SODA 2023) as a tool in their study of online\ngeometric packing problems. They showed that when the items are reals from the\ninterval $[0,1]$ a competitive ratio of $O(\\sqrt{n})$ is achievable, and no\ndeterministic algorithm can improve this ratio asymptotically.\n  In this paper, we extend and generalize the study of online sorting in three\ndirections:\n  - randomized: we settle the open question of Aamand et al. by showing that\nthe $O(\\sqrt{n})$ competitive ratio for the online sorting of reals cannot be\nimproved even with the use of randomness;\n  - stochastic: we consider inputs consisting of $n$ samples drawn uniformly at\nrandom from an interval, and give an algorithm with an improved competitive\nratio of $\\widetilde{O}(n^{1/4})$. The result reveals connections between\nonline sorting and the design of efficient hash tables;\n  - high-dimensional: we show that $\\widetilde{O}(\\sqrt{n})$-competitive online\nsorting is possible even for items from $\\mathbb{R}^d$, for arbitrary fixed\n$d$, in an adversarial model. This can be viewed as an online variant of the\nclassical TSP problem where tasks (cities to visit) are revealed one by one and\nthe salesperson assigns each task (immediately and irrevocably) to its\ntimeslot. Along the way, we also show a tight $O(\\log{n})$-competitiveness\nresult for uniform metrics, i.e., where items are of different types and the\ngoal is to order them so as to minimize the number of switches between\nconsecutive items of different types.",
        "chunk-id": 6,
        "chunk": "consecutive items of different types.",
        "authors": [
            "Mikkel Abrahamsen",
            "Ioana O. Bercea",
            "Lorenzo Beretta",
            "Jonas Klausen",
            "L\u00e1szl\u00f3 Kozma"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:26:49+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19257v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19257v1",
        "categories": [
            "Data Structures and Algorithms",
            "Computational Geometry"
        ]
    },
    {
        "id": 30000057,
        "doi": null,
        "title": "AI Data Readiness Inspector (AIDRIN) for Quantitative Assessment of Data Readiness for AI",
        "abstract": "\"Garbage In Garbage Out\" is a universally agreed quote by computer scientists\nfrom various domains, including Artificial Intelligence (AI). As data is the\nfuel for AI, models trained on low-quality, biased data are often ineffective.\nComputer scientists who use AI invest a considerable amount of time and effort\nin preparing the data for AI. However, there are no standard methods or\nframeworks for assessing the \"readiness\" of data for AI. To provide a\nquantifiable assessment of the readiness of data for AI processes, we define\nparameters of AI data readiness and introduce AIDRIN (AI Data Readiness\nInspector). AIDRIN is a framework covering a broad range of readiness\ndimensions available in the literature that aid in evaluating the readiness of\ndata quantitatively and qualitatively. AIDRIN uses metrics in traditional data\nquality assessment such as completeness, outliers, and duplicates for data\nevaluation. Furthermore, AIDRIN uses metrics specific to assess data for AI,\nsuch as feature importance, feature correlations, class imbalance, fairness,\nprivacy, and FAIR (Findability, Accessibility, Interoperability, and\nReusability) principle compliance. AIDRIN provides visualizations and reports\nto assist data scientists in further investigating the readiness of data. The\nAIDRIN framework enhances the efficiency of the machine learning pipeline to\nmake informed decisions on data readiness for AI applications.",
        "chunk-id": 1,
        "chunk": "\"Garbage In Garbage Out\" is a universally agreed quote by computer scientists\nfrom various domains, including Artificial Intelligence (AI). As data is the\nfuel for AI, models trained on low-quality, biased data are often ineffective.\nComputer scientists who use AI invest a considerable amount of time and effort\nin preparing the data for AI. However, there are no standard methods or",
        "authors": [
            "Kaveen Hiniduma",
            "Suren Byna",
            "Jean Luca Bez",
            "Ravi Madduri"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:26:39+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19256v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19256v1",
        "categories": [
            "Artificial Intelligence"
        ]
    },
    {
        "id": 30000057,
        "doi": null,
        "title": "AI Data Readiness Inspector (AIDRIN) for Quantitative Assessment of Data Readiness for AI",
        "abstract": "\"Garbage In Garbage Out\" is a universally agreed quote by computer scientists\nfrom various domains, including Artificial Intelligence (AI). As data is the\nfuel for AI, models trained on low-quality, biased data are often ineffective.\nComputer scientists who use AI invest a considerable amount of time and effort\nin preparing the data for AI. However, there are no standard methods or\nframeworks for assessing the \"readiness\" of data for AI. To provide a\nquantifiable assessment of the readiness of data for AI processes, we define\nparameters of AI data readiness and introduce AIDRIN (AI Data Readiness\nInspector). AIDRIN is a framework covering a broad range of readiness\ndimensions available in the literature that aid in evaluating the readiness of\ndata quantitatively and qualitatively. AIDRIN uses metrics in traditional data\nquality assessment such as completeness, outliers, and duplicates for data\nevaluation. Furthermore, AIDRIN uses metrics specific to assess data for AI,\nsuch as feature importance, feature correlations, class imbalance, fairness,\nprivacy, and FAIR (Findability, Accessibility, Interoperability, and\nReusability) principle compliance. AIDRIN provides visualizations and reports\nto assist data scientists in further investigating the readiness of data. The\nAIDRIN framework enhances the efficiency of the machine learning pipeline to\nmake informed decisions on data readiness for AI applications.",
        "chunk-id": 2,
        "chunk": "frameworks for assessing the \"readiness\" of data for AI. To provide a\nquantifiable assessment of the readiness of data for AI processes, we define\nparameters of AI data readiness and introduce AIDRIN (AI Data Readiness\nInspector). AIDRIN is a framework covering a broad range of readiness\ndimensions available in the literature that aid in evaluating the readiness of",
        "authors": [
            "Kaveen Hiniduma",
            "Suren Byna",
            "Jean Luca Bez",
            "Ravi Madduri"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:26:39+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19256v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19256v1",
        "categories": [
            "Artificial Intelligence"
        ]
    },
    {
        "id": 30000057,
        "doi": null,
        "title": "AI Data Readiness Inspector (AIDRIN) for Quantitative Assessment of Data Readiness for AI",
        "abstract": "\"Garbage In Garbage Out\" is a universally agreed quote by computer scientists\nfrom various domains, including Artificial Intelligence (AI). As data is the\nfuel for AI, models trained on low-quality, biased data are often ineffective.\nComputer scientists who use AI invest a considerable amount of time and effort\nin preparing the data for AI. However, there are no standard methods or\nframeworks for assessing the \"readiness\" of data for AI. To provide a\nquantifiable assessment of the readiness of data for AI processes, we define\nparameters of AI data readiness and introduce AIDRIN (AI Data Readiness\nInspector). AIDRIN is a framework covering a broad range of readiness\ndimensions available in the literature that aid in evaluating the readiness of\ndata quantitatively and qualitatively. AIDRIN uses metrics in traditional data\nquality assessment such as completeness, outliers, and duplicates for data\nevaluation. Furthermore, AIDRIN uses metrics specific to assess data for AI,\nsuch as feature importance, feature correlations, class imbalance, fairness,\nprivacy, and FAIR (Findability, Accessibility, Interoperability, and\nReusability) principle compliance. AIDRIN provides visualizations and reports\nto assist data scientists in further investigating the readiness of data. The\nAIDRIN framework enhances the efficiency of the machine learning pipeline to\nmake informed decisions on data readiness for AI applications.",
        "chunk-id": 3,
        "chunk": "data quantitatively and qualitatively. AIDRIN uses metrics in traditional data\nquality assessment such as completeness, outliers, and duplicates for data\nevaluation. Furthermore, AIDRIN uses metrics specific to assess data for AI,\nsuch as feature importance, feature correlations, class imbalance, fairness,\nprivacy, and FAIR (Findability, Accessibility, Interoperability, and",
        "authors": [
            "Kaveen Hiniduma",
            "Suren Byna",
            "Jean Luca Bez",
            "Ravi Madduri"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:26:39+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19256v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19256v1",
        "categories": [
            "Artificial Intelligence"
        ]
    },
    {
        "id": 30000057,
        "doi": null,
        "title": "AI Data Readiness Inspector (AIDRIN) for Quantitative Assessment of Data Readiness for AI",
        "abstract": "\"Garbage In Garbage Out\" is a universally agreed quote by computer scientists\nfrom various domains, including Artificial Intelligence (AI). As data is the\nfuel for AI, models trained on low-quality, biased data are often ineffective.\nComputer scientists who use AI invest a considerable amount of time and effort\nin preparing the data for AI. However, there are no standard methods or\nframeworks for assessing the \"readiness\" of data for AI. To provide a\nquantifiable assessment of the readiness of data for AI processes, we define\nparameters of AI data readiness and introduce AIDRIN (AI Data Readiness\nInspector). AIDRIN is a framework covering a broad range of readiness\ndimensions available in the literature that aid in evaluating the readiness of\ndata quantitatively and qualitatively. AIDRIN uses metrics in traditional data\nquality assessment such as completeness, outliers, and duplicates for data\nevaluation. Furthermore, AIDRIN uses metrics specific to assess data for AI,\nsuch as feature importance, feature correlations, class imbalance, fairness,\nprivacy, and FAIR (Findability, Accessibility, Interoperability, and\nReusability) principle compliance. AIDRIN provides visualizations and reports\nto assist data scientists in further investigating the readiness of data. The\nAIDRIN framework enhances the efficiency of the machine learning pipeline to\nmake informed decisions on data readiness for AI applications.",
        "chunk-id": 4,
        "chunk": "Reusability) principle compliance. AIDRIN provides visualizations and reports\nto assist data scientists in further investigating the readiness of data. The\nAIDRIN framework enhances the efficiency of the machine learning pipeline to\nmake informed decisions on data readiness for AI applications.",
        "authors": [
            "Kaveen Hiniduma",
            "Suren Byna",
            "Jean Luca Bez",
            "Ravi Madduri"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:26:39+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19256v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19256v1",
        "categories": [
            "Artificial Intelligence"
        ]
    },
    {
        "id": 30000058,
        "doi": "10.1109/TPAMI.2024.3393452",
        "title": "Enhancing Video-Language Representations with Structural Spatio-Temporal Alignment",
        "abstract": "While pre-training large-scale video-language models (VLMs) has shown\nremarkable potential for various downstream video-language tasks, existing VLMs\ncan still suffer from certain commonly seen limitations, e.g., coarse-grained\ncross-modal aligning , under-modeling of temporal dynamics, detached\nvideo-language view. In this work, we target enhancing VLMs with a fine-grained\nstructural spatio-temporal alignment learning method (namely Finsta). First of\nall, we represent the input texts and videos with fine-grained scene graph (SG)\nstructures, both of which are further unified into a holistic SG (HSG) for\nbridging two modalities. Then, an SG-based framework is built, where the\ntextual SG (TSG) is encoded with a graph Transformer, while the video dynamic\nSG (DSG) and the HSG are modeled with a novel recurrent graph Transformer for\nspatial and temporal feature propagation. A spatial-temporal Gaussian\ndifferential graph Transformer is further devised to strengthen the sense of\nthe changes in objects across spatial and temporal dimensions. Next, based on\nthe fine-grained structural features of TSG and DSG, we perform object-centered\nspatial alignment and predicate-centered temporal alignment respectively,\nenhancing the video-language grounding in both the spatiality and temporality.\nWe design our method as a plug&play system, which can be integrated into\nexisting well-trained VLMs for further representation augmentation, without\ntraining from scratch or relying on SG annotations in downstream applications.\nOn 6 representative VL modeling tasks over 12 datasets in both standard and\nlong-form video scenarios, Finsta consistently improves the existing 13\nstrong-performing VLMs persistently, and refreshes the current state-of-the-art\nend task performance significantly in both the fine-tuning and zero-shot\nsettings.",
        "chunk-id": 1,
        "chunk": "While pre-training large-scale video-language models (VLMs) has shown\nremarkable potential for various downstream video-language tasks, existing VLMs\ncan still suffer from certain commonly seen limitations, e.g., coarse-grained\ncross-modal aligning , under-modeling of temporal dynamics, detached\nvideo-language view. In this work, we target enhancing VLMs with a fine-grained",
        "authors": [
            "Hao Fei",
            "Shengqiong Wu",
            "Meishan Zhang",
            "Min Zhang",
            "Tat-Seng Chua",
            "Shuicheng Yan"
        ],
        "journal_ref": "[J].IEEE Transactions on Pattern Analysis and Machine\n  Intelligence, 2024",
        "published": "2024-06-27T15:23:36+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19255v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19255v1",
        "categories": [
            "Computer Vision and Pattern Recognition",
            "Computation and Language"
        ]
    },
    {
        "id": 30000058,
        "doi": "10.1109/TPAMI.2024.3393452",
        "title": "Enhancing Video-Language Representations with Structural Spatio-Temporal Alignment",
        "abstract": "While pre-training large-scale video-language models (VLMs) has shown\nremarkable potential for various downstream video-language tasks, existing VLMs\ncan still suffer from certain commonly seen limitations, e.g., coarse-grained\ncross-modal aligning , under-modeling of temporal dynamics, detached\nvideo-language view. In this work, we target enhancing VLMs with a fine-grained\nstructural spatio-temporal alignment learning method (namely Finsta). First of\nall, we represent the input texts and videos with fine-grained scene graph (SG)\nstructures, both of which are further unified into a holistic SG (HSG) for\nbridging two modalities. Then, an SG-based framework is built, where the\ntextual SG (TSG) is encoded with a graph Transformer, while the video dynamic\nSG (DSG) and the HSG are modeled with a novel recurrent graph Transformer for\nspatial and temporal feature propagation. A spatial-temporal Gaussian\ndifferential graph Transformer is further devised to strengthen the sense of\nthe changes in objects across spatial and temporal dimensions. Next, based on\nthe fine-grained structural features of TSG and DSG, we perform object-centered\nspatial alignment and predicate-centered temporal alignment respectively,\nenhancing the video-language grounding in both the spatiality and temporality.\nWe design our method as a plug&play system, which can be integrated into\nexisting well-trained VLMs for further representation augmentation, without\ntraining from scratch or relying on SG annotations in downstream applications.\nOn 6 representative VL modeling tasks over 12 datasets in both standard and\nlong-form video scenarios, Finsta consistently improves the existing 13\nstrong-performing VLMs persistently, and refreshes the current state-of-the-art\nend task performance significantly in both the fine-tuning and zero-shot\nsettings.",
        "chunk-id": 2,
        "chunk": "structural spatio-temporal alignment learning method (namely Finsta). First of\nall, we represent the input texts and videos with fine-grained scene graph (SG)\nstructures, both of which are further unified into a holistic SG (HSG) for\nbridging two modalities. Then, an SG-based framework is built, where the\ntextual SG (TSG) is encoded with a graph Transformer, while the video dynamic",
        "authors": [
            "Hao Fei",
            "Shengqiong Wu",
            "Meishan Zhang",
            "Min Zhang",
            "Tat-Seng Chua",
            "Shuicheng Yan"
        ],
        "journal_ref": "[J].IEEE Transactions on Pattern Analysis and Machine\n  Intelligence, 2024",
        "published": "2024-06-27T15:23:36+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19255v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19255v1",
        "categories": [
            "Computer Vision and Pattern Recognition",
            "Computation and Language"
        ]
    },
    {
        "id": 30000058,
        "doi": "10.1109/TPAMI.2024.3393452",
        "title": "Enhancing Video-Language Representations with Structural Spatio-Temporal Alignment",
        "abstract": "While pre-training large-scale video-language models (VLMs) has shown\nremarkable potential for various downstream video-language tasks, existing VLMs\ncan still suffer from certain commonly seen limitations, e.g., coarse-grained\ncross-modal aligning , under-modeling of temporal dynamics, detached\nvideo-language view. In this work, we target enhancing VLMs with a fine-grained\nstructural spatio-temporal alignment learning method (namely Finsta). First of\nall, we represent the input texts and videos with fine-grained scene graph (SG)\nstructures, both of which are further unified into a holistic SG (HSG) for\nbridging two modalities. Then, an SG-based framework is built, where the\ntextual SG (TSG) is encoded with a graph Transformer, while the video dynamic\nSG (DSG) and the HSG are modeled with a novel recurrent graph Transformer for\nspatial and temporal feature propagation. A spatial-temporal Gaussian\ndifferential graph Transformer is further devised to strengthen the sense of\nthe changes in objects across spatial and temporal dimensions. Next, based on\nthe fine-grained structural features of TSG and DSG, we perform object-centered\nspatial alignment and predicate-centered temporal alignment respectively,\nenhancing the video-language grounding in both the spatiality and temporality.\nWe design our method as a plug&play system, which can be integrated into\nexisting well-trained VLMs for further representation augmentation, without\ntraining from scratch or relying on SG annotations in downstream applications.\nOn 6 representative VL modeling tasks over 12 datasets in both standard and\nlong-form video scenarios, Finsta consistently improves the existing 13\nstrong-performing VLMs persistently, and refreshes the current state-of-the-art\nend task performance significantly in both the fine-tuning and zero-shot\nsettings.",
        "chunk-id": 3,
        "chunk": "SG (DSG) and the HSG are modeled with a novel recurrent graph Transformer for\nspatial and temporal feature propagation. A spatial-temporal Gaussian\ndifferential graph Transformer is further devised to strengthen the sense of\nthe changes in objects across spatial and temporal dimensions. Next, based on\nthe fine-grained structural features of TSG and DSG, we perform object-centered",
        "authors": [
            "Hao Fei",
            "Shengqiong Wu",
            "Meishan Zhang",
            "Min Zhang",
            "Tat-Seng Chua",
            "Shuicheng Yan"
        ],
        "journal_ref": "[J].IEEE Transactions on Pattern Analysis and Machine\n  Intelligence, 2024",
        "published": "2024-06-27T15:23:36+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19255v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19255v1",
        "categories": [
            "Computer Vision and Pattern Recognition",
            "Computation and Language"
        ]
    },
    {
        "id": 30000058,
        "doi": "10.1109/TPAMI.2024.3393452",
        "title": "Enhancing Video-Language Representations with Structural Spatio-Temporal Alignment",
        "abstract": "While pre-training large-scale video-language models (VLMs) has shown\nremarkable potential for various downstream video-language tasks, existing VLMs\ncan still suffer from certain commonly seen limitations, e.g., coarse-grained\ncross-modal aligning , under-modeling of temporal dynamics, detached\nvideo-language view. In this work, we target enhancing VLMs with a fine-grained\nstructural spatio-temporal alignment learning method (namely Finsta). First of\nall, we represent the input texts and videos with fine-grained scene graph (SG)\nstructures, both of which are further unified into a holistic SG (HSG) for\nbridging two modalities. Then, an SG-based framework is built, where the\ntextual SG (TSG) is encoded with a graph Transformer, while the video dynamic\nSG (DSG) and the HSG are modeled with a novel recurrent graph Transformer for\nspatial and temporal feature propagation. A spatial-temporal Gaussian\ndifferential graph Transformer is further devised to strengthen the sense of\nthe changes in objects across spatial and temporal dimensions. Next, based on\nthe fine-grained structural features of TSG and DSG, we perform object-centered\nspatial alignment and predicate-centered temporal alignment respectively,\nenhancing the video-language grounding in both the spatiality and temporality.\nWe design our method as a plug&play system, which can be integrated into\nexisting well-trained VLMs for further representation augmentation, without\ntraining from scratch or relying on SG annotations in downstream applications.\nOn 6 representative VL modeling tasks over 12 datasets in both standard and\nlong-form video scenarios, Finsta consistently improves the existing 13\nstrong-performing VLMs persistently, and refreshes the current state-of-the-art\nend task performance significantly in both the fine-tuning and zero-shot\nsettings.",
        "chunk-id": 4,
        "chunk": "spatial alignment and predicate-centered temporal alignment respectively,\nenhancing the video-language grounding in both the spatiality and temporality.\nWe design our method as a plug&play system, which can be integrated into\nexisting well-trained VLMs for further representation augmentation, without\ntraining from scratch or relying on SG annotations in downstream applications.",
        "authors": [
            "Hao Fei",
            "Shengqiong Wu",
            "Meishan Zhang",
            "Min Zhang",
            "Tat-Seng Chua",
            "Shuicheng Yan"
        ],
        "journal_ref": "[J].IEEE Transactions on Pattern Analysis and Machine\n  Intelligence, 2024",
        "published": "2024-06-27T15:23:36+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19255v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19255v1",
        "categories": [
            "Computer Vision and Pattern Recognition",
            "Computation and Language"
        ]
    },
    {
        "id": 30000058,
        "doi": "10.1109/TPAMI.2024.3393452",
        "title": "Enhancing Video-Language Representations with Structural Spatio-Temporal Alignment",
        "abstract": "While pre-training large-scale video-language models (VLMs) has shown\nremarkable potential for various downstream video-language tasks, existing VLMs\ncan still suffer from certain commonly seen limitations, e.g., coarse-grained\ncross-modal aligning , under-modeling of temporal dynamics, detached\nvideo-language view. In this work, we target enhancing VLMs with a fine-grained\nstructural spatio-temporal alignment learning method (namely Finsta). First of\nall, we represent the input texts and videos with fine-grained scene graph (SG)\nstructures, both of which are further unified into a holistic SG (HSG) for\nbridging two modalities. Then, an SG-based framework is built, where the\ntextual SG (TSG) is encoded with a graph Transformer, while the video dynamic\nSG (DSG) and the HSG are modeled with a novel recurrent graph Transformer for\nspatial and temporal feature propagation. A spatial-temporal Gaussian\ndifferential graph Transformer is further devised to strengthen the sense of\nthe changes in objects across spatial and temporal dimensions. Next, based on\nthe fine-grained structural features of TSG and DSG, we perform object-centered\nspatial alignment and predicate-centered temporal alignment respectively,\nenhancing the video-language grounding in both the spatiality and temporality.\nWe design our method as a plug&play system, which can be integrated into\nexisting well-trained VLMs for further representation augmentation, without\ntraining from scratch or relying on SG annotations in downstream applications.\nOn 6 representative VL modeling tasks over 12 datasets in both standard and\nlong-form video scenarios, Finsta consistently improves the existing 13\nstrong-performing VLMs persistently, and refreshes the current state-of-the-art\nend task performance significantly in both the fine-tuning and zero-shot\nsettings.",
        "chunk-id": 5,
        "chunk": "On 6 representative VL modeling tasks over 12 datasets in both standard and\nlong-form video scenarios, Finsta consistently improves the existing 13\nstrong-performing VLMs persistently, and refreshes the current state-of-the-art\nend task performance significantly in both the fine-tuning and zero-shot\nsettings.",
        "authors": [
            "Hao Fei",
            "Shengqiong Wu",
            "Meishan Zhang",
            "Min Zhang",
            "Tat-Seng Chua",
            "Shuicheng Yan"
        ],
        "journal_ref": "[J].IEEE Transactions on Pattern Analysis and Machine\n  Intelligence, 2024",
        "published": "2024-06-27T15:23:36+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19255v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19255v1",
        "categories": [
            "Computer Vision and Pattern Recognition",
            "Computation and Language"
        ]
    },
    {
        "id": 30000059,
        "doi": null,
        "title": "AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for Retrieval-Augmented Generation",
        "abstract": "Recent advancements in Large Language Models have transformed ML/AI\ndevelopment, necessitating a reevaluation of AutoML principles for the\nRetrieval-Augmented Generation (RAG) systems. To address the challenges of\nhyper-parameter optimization and online adaptation in RAG, we propose the\nAutoRAG-HP framework, which formulates the hyper-parameter tuning as an online\nmulti-armed bandit (MAB) problem and introduces a novel two-level Hierarchical\nMAB (Hier-MAB) method for efficient exploration of large search spaces. We\nconduct extensive experiments on tuning hyper-parameters, such as top-k\nretrieved documents, prompt compression ratio, and embedding methods, using the\nALCE-ASQA and Natural Questions datasets. Our evaluation from jointly\noptimization all three hyper-parameters demonstrate that MAB-based online\nlearning methods can achieve Recall@5 $\\approx 0.8$ for scenarios with\nprominent gradients in search space, using only $\\sim20\\%$ of the LLM API calls\nrequired by the Grid Search approach. Additionally, the proposed Hier-MAB\napproach outperforms other baselines in more challenging optimization\nscenarios. The code will be made available at https://aka.ms/autorag.",
        "chunk-id": 1,
        "chunk": "Recent advancements in Large Language Models have transformed ML/AI\ndevelopment, necessitating a reevaluation of AutoML principles for the\nRetrieval-Augmented Generation (RAG) systems. To address the challenges of\nhyper-parameter optimization and online adaptation in RAG, we propose the\nAutoRAG-HP framework, which formulates the hyper-parameter tuning as an online",
        "authors": [
            "Jia Fu",
            "Xiaoting Qin",
            "Fangkai Yang",
            "Lu Wang",
            "Jue Zhang",
            "Qingwei Lin",
            "Yubo Chen",
            "Dongmei Zhang",
            "Saravan Rajmohan",
            "Qi Zhang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:18:21+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19251v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19251v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 30000059,
        "doi": null,
        "title": "AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for Retrieval-Augmented Generation",
        "abstract": "Recent advancements in Large Language Models have transformed ML/AI\ndevelopment, necessitating a reevaluation of AutoML principles for the\nRetrieval-Augmented Generation (RAG) systems. To address the challenges of\nhyper-parameter optimization and online adaptation in RAG, we propose the\nAutoRAG-HP framework, which formulates the hyper-parameter tuning as an online\nmulti-armed bandit (MAB) problem and introduces a novel two-level Hierarchical\nMAB (Hier-MAB) method for efficient exploration of large search spaces. We\nconduct extensive experiments on tuning hyper-parameters, such as top-k\nretrieved documents, prompt compression ratio, and embedding methods, using the\nALCE-ASQA and Natural Questions datasets. Our evaluation from jointly\noptimization all three hyper-parameters demonstrate that MAB-based online\nlearning methods can achieve Recall@5 $\\approx 0.8$ for scenarios with\nprominent gradients in search space, using only $\\sim20\\%$ of the LLM API calls\nrequired by the Grid Search approach. Additionally, the proposed Hier-MAB\napproach outperforms other baselines in more challenging optimization\nscenarios. The code will be made available at https://aka.ms/autorag.",
        "chunk-id": 2,
        "chunk": "multi-armed bandit (MAB) problem and introduces a novel two-level Hierarchical\nMAB (Hier-MAB) method for efficient exploration of large search spaces. We\nconduct extensive experiments on tuning hyper-parameters, such as top-k\nretrieved documents, prompt compression ratio, and embedding methods, using the\nALCE-ASQA and Natural Questions datasets. Our evaluation from jointly",
        "authors": [
            "Jia Fu",
            "Xiaoting Qin",
            "Fangkai Yang",
            "Lu Wang",
            "Jue Zhang",
            "Qingwei Lin",
            "Yubo Chen",
            "Dongmei Zhang",
            "Saravan Rajmohan",
            "Qi Zhang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:18:21+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19251v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19251v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 30000059,
        "doi": null,
        "title": "AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for Retrieval-Augmented Generation",
        "abstract": "Recent advancements in Large Language Models have transformed ML/AI\ndevelopment, necessitating a reevaluation of AutoML principles for the\nRetrieval-Augmented Generation (RAG) systems. To address the challenges of\nhyper-parameter optimization and online adaptation in RAG, we propose the\nAutoRAG-HP framework, which formulates the hyper-parameter tuning as an online\nmulti-armed bandit (MAB) problem and introduces a novel two-level Hierarchical\nMAB (Hier-MAB) method for efficient exploration of large search spaces. We\nconduct extensive experiments on tuning hyper-parameters, such as top-k\nretrieved documents, prompt compression ratio, and embedding methods, using the\nALCE-ASQA and Natural Questions datasets. Our evaluation from jointly\noptimization all three hyper-parameters demonstrate that MAB-based online\nlearning methods can achieve Recall@5 $\\approx 0.8$ for scenarios with\nprominent gradients in search space, using only $\\sim20\\%$ of the LLM API calls\nrequired by the Grid Search approach. Additionally, the proposed Hier-MAB\napproach outperforms other baselines in more challenging optimization\nscenarios. The code will be made available at https://aka.ms/autorag.",
        "chunk-id": 3,
        "chunk": "optimization all three hyper-parameters demonstrate that MAB-based online\nlearning methods can achieve Recall@5 $\\approx 0.8$ for scenarios with\nprominent gradients in search space, using only $\\sim20\\%$ of the LLM API calls\nrequired by the Grid Search approach. Additionally, the proposed Hier-MAB\napproach outperforms other baselines in more challenging optimization",
        "authors": [
            "Jia Fu",
            "Xiaoting Qin",
            "Fangkai Yang",
            "Lu Wang",
            "Jue Zhang",
            "Qingwei Lin",
            "Yubo Chen",
            "Dongmei Zhang",
            "Saravan Rajmohan",
            "Qi Zhang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:18:21+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19251v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19251v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 30000059,
        "doi": null,
        "title": "AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for Retrieval-Augmented Generation",
        "abstract": "Recent advancements in Large Language Models have transformed ML/AI\ndevelopment, necessitating a reevaluation of AutoML principles for the\nRetrieval-Augmented Generation (RAG) systems. To address the challenges of\nhyper-parameter optimization and online adaptation in RAG, we propose the\nAutoRAG-HP framework, which formulates the hyper-parameter tuning as an online\nmulti-armed bandit (MAB) problem and introduces a novel two-level Hierarchical\nMAB (Hier-MAB) method for efficient exploration of large search spaces. We\nconduct extensive experiments on tuning hyper-parameters, such as top-k\nretrieved documents, prompt compression ratio, and embedding methods, using the\nALCE-ASQA and Natural Questions datasets. Our evaluation from jointly\noptimization all three hyper-parameters demonstrate that MAB-based online\nlearning methods can achieve Recall@5 $\\approx 0.8$ for scenarios with\nprominent gradients in search space, using only $\\sim20\\%$ of the LLM API calls\nrequired by the Grid Search approach. Additionally, the proposed Hier-MAB\napproach outperforms other baselines in more challenging optimization\nscenarios. The code will be made available at https://aka.ms/autorag.",
        "chunk-id": 4,
        "chunk": "scenarios. The code will be made available at https://aka.ms/autorag.",
        "authors": [
            "Jia Fu",
            "Xiaoting Qin",
            "Fangkai Yang",
            "Lu Wang",
            "Jue Zhang",
            "Qingwei Lin",
            "Yubo Chen",
            "Dongmei Zhang",
            "Saravan Rajmohan",
            "Qi Zhang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:18:21+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19251v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19251v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 30000060,
        "doi": null,
        "title": "Local Manifold Learning for No-Reference Image Quality Assessment",
        "abstract": "Contrastive learning has considerably advanced the field of Image Quality\nAssessment (IQA), emerging as a widely adopted technique. The core mechanism of\ncontrastive learning involves minimizing the distance between quality-similar\n(positive) examples while maximizing the distance between quality-dissimilar\n(negative) examples. Despite its successes, current contrastive learning\nmethods often neglect the importance of preserving the local manifold\nstructure. This oversight can result in a high degree of similarity among hard\nexamples within the feature space, thereby impeding effective differentiation\nand assessment. To address this issue, we propose an innovative framework that\nintegrates local manifold learning with contrastive learning for No-Reference\nImage Quality Assessment (NR-IQA). Our method begins by sampling multiple crops\nfrom a given image, identifying the most visually salient crop. This crop is\nthen used to cluster other crops from the same image as the positive class,\nwhile crops from different images are treated as negative classes to increase\ninter-class distance. Uniquely, our approach also considers non-saliency crops\nfrom the same image as intra-class negative classes to preserve their\ndistinctiveness. Additionally, we employ a mutual learning framework, which\nfurther enhances the model's ability to adaptively learn and identify visual\nsaliency regions. Our approach demonstrates a better performance compared to\nstate-of-the-art methods in 7 standard datasets, achieving PLCC values of 0.942\n(compared to 0.908 in TID2013) and 0.914 (compared to 0.894 in LIVEC).",
        "chunk-id": 1,
        "chunk": "Contrastive learning has considerably advanced the field of Image Quality\nAssessment (IQA), emerging as a widely adopted technique. The core mechanism of\ncontrastive learning involves minimizing the distance between quality-similar\n(positive) examples while maximizing the distance between quality-dissimilar\n(negative) examples. Despite its successes, current contrastive learning",
        "authors": [
            "Timin Gao",
            "Wensheng Pan",
            "Yan Zhang",
            "Sicheng Zhao",
            "Shengchuan Zhang",
            "Xiawu Zheng",
            "Ke Li",
            "Liujuan Cao",
            "Rongrong Ji"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:14:23+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19247v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19247v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000060,
        "doi": null,
        "title": "Local Manifold Learning for No-Reference Image Quality Assessment",
        "abstract": "Contrastive learning has considerably advanced the field of Image Quality\nAssessment (IQA), emerging as a widely adopted technique. The core mechanism of\ncontrastive learning involves minimizing the distance between quality-similar\n(positive) examples while maximizing the distance between quality-dissimilar\n(negative) examples. Despite its successes, current contrastive learning\nmethods often neglect the importance of preserving the local manifold\nstructure. This oversight can result in a high degree of similarity among hard\nexamples within the feature space, thereby impeding effective differentiation\nand assessment. To address this issue, we propose an innovative framework that\nintegrates local manifold learning with contrastive learning for No-Reference\nImage Quality Assessment (NR-IQA). Our method begins by sampling multiple crops\nfrom a given image, identifying the most visually salient crop. This crop is\nthen used to cluster other crops from the same image as the positive class,\nwhile crops from different images are treated as negative classes to increase\ninter-class distance. Uniquely, our approach also considers non-saliency crops\nfrom the same image as intra-class negative classes to preserve their\ndistinctiveness. Additionally, we employ a mutual learning framework, which\nfurther enhances the model's ability to adaptively learn and identify visual\nsaliency regions. Our approach demonstrates a better performance compared to\nstate-of-the-art methods in 7 standard datasets, achieving PLCC values of 0.942\n(compared to 0.908 in TID2013) and 0.914 (compared to 0.894 in LIVEC).",
        "chunk-id": 2,
        "chunk": "methods often neglect the importance of preserving the local manifold\nstructure. This oversight can result in a high degree of similarity among hard\nexamples within the feature space, thereby impeding effective differentiation\nand assessment. To address this issue, we propose an innovative framework that\nintegrates local manifold learning with contrastive learning for No-Reference",
        "authors": [
            "Timin Gao",
            "Wensheng Pan",
            "Yan Zhang",
            "Sicheng Zhao",
            "Shengchuan Zhang",
            "Xiawu Zheng",
            "Ke Li",
            "Liujuan Cao",
            "Rongrong Ji"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:14:23+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19247v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19247v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000060,
        "doi": null,
        "title": "Local Manifold Learning for No-Reference Image Quality Assessment",
        "abstract": "Contrastive learning has considerably advanced the field of Image Quality\nAssessment (IQA), emerging as a widely adopted technique. The core mechanism of\ncontrastive learning involves minimizing the distance between quality-similar\n(positive) examples while maximizing the distance between quality-dissimilar\n(negative) examples. Despite its successes, current contrastive learning\nmethods often neglect the importance of preserving the local manifold\nstructure. This oversight can result in a high degree of similarity among hard\nexamples within the feature space, thereby impeding effective differentiation\nand assessment. To address this issue, we propose an innovative framework that\nintegrates local manifold learning with contrastive learning for No-Reference\nImage Quality Assessment (NR-IQA). Our method begins by sampling multiple crops\nfrom a given image, identifying the most visually salient crop. This crop is\nthen used to cluster other crops from the same image as the positive class,\nwhile crops from different images are treated as negative classes to increase\ninter-class distance. Uniquely, our approach also considers non-saliency crops\nfrom the same image as intra-class negative classes to preserve their\ndistinctiveness. Additionally, we employ a mutual learning framework, which\nfurther enhances the model's ability to adaptively learn and identify visual\nsaliency regions. Our approach demonstrates a better performance compared to\nstate-of-the-art methods in 7 standard datasets, achieving PLCC values of 0.942\n(compared to 0.908 in TID2013) and 0.914 (compared to 0.894 in LIVEC).",
        "chunk-id": 3,
        "chunk": "Image Quality Assessment (NR-IQA). Our method begins by sampling multiple crops\nfrom a given image, identifying the most visually salient crop. This crop is\nthen used to cluster other crops from the same image as the positive class,\nwhile crops from different images are treated as negative classes to increase",
        "authors": [
            "Timin Gao",
            "Wensheng Pan",
            "Yan Zhang",
            "Sicheng Zhao",
            "Shengchuan Zhang",
            "Xiawu Zheng",
            "Ke Li",
            "Liujuan Cao",
            "Rongrong Ji"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:14:23+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19247v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19247v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000060,
        "doi": null,
        "title": "Local Manifold Learning for No-Reference Image Quality Assessment",
        "abstract": "Contrastive learning has considerably advanced the field of Image Quality\nAssessment (IQA), emerging as a widely adopted technique. The core mechanism of\ncontrastive learning involves minimizing the distance between quality-similar\n(positive) examples while maximizing the distance between quality-dissimilar\n(negative) examples. Despite its successes, current contrastive learning\nmethods often neglect the importance of preserving the local manifold\nstructure. This oversight can result in a high degree of similarity among hard\nexamples within the feature space, thereby impeding effective differentiation\nand assessment. To address this issue, we propose an innovative framework that\nintegrates local manifold learning with contrastive learning for No-Reference\nImage Quality Assessment (NR-IQA). Our method begins by sampling multiple crops\nfrom a given image, identifying the most visually salient crop. This crop is\nthen used to cluster other crops from the same image as the positive class,\nwhile crops from different images are treated as negative classes to increase\ninter-class distance. Uniquely, our approach also considers non-saliency crops\nfrom the same image as intra-class negative classes to preserve their\ndistinctiveness. Additionally, we employ a mutual learning framework, which\nfurther enhances the model's ability to adaptively learn and identify visual\nsaliency regions. Our approach demonstrates a better performance compared to\nstate-of-the-art methods in 7 standard datasets, achieving PLCC values of 0.942\n(compared to 0.908 in TID2013) and 0.914 (compared to 0.894 in LIVEC).",
        "chunk-id": 4,
        "chunk": "inter-class distance. Uniquely, our approach also considers non-saliency crops\nfrom the same image as intra-class negative classes to preserve their\ndistinctiveness. Additionally, we employ a mutual learning framework, which\nfurther enhances the model's ability to adaptively learn and identify visual\nsaliency regions. Our approach demonstrates a better performance compared to",
        "authors": [
            "Timin Gao",
            "Wensheng Pan",
            "Yan Zhang",
            "Sicheng Zhao",
            "Shengchuan Zhang",
            "Xiawu Zheng",
            "Ke Li",
            "Liujuan Cao",
            "Rongrong Ji"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:14:23+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19247v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19247v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000060,
        "doi": null,
        "title": "Local Manifold Learning for No-Reference Image Quality Assessment",
        "abstract": "Contrastive learning has considerably advanced the field of Image Quality\nAssessment (IQA), emerging as a widely adopted technique. The core mechanism of\ncontrastive learning involves minimizing the distance between quality-similar\n(positive) examples while maximizing the distance between quality-dissimilar\n(negative) examples. Despite its successes, current contrastive learning\nmethods often neglect the importance of preserving the local manifold\nstructure. This oversight can result in a high degree of similarity among hard\nexamples within the feature space, thereby impeding effective differentiation\nand assessment. To address this issue, we propose an innovative framework that\nintegrates local manifold learning with contrastive learning for No-Reference\nImage Quality Assessment (NR-IQA). Our method begins by sampling multiple crops\nfrom a given image, identifying the most visually salient crop. This crop is\nthen used to cluster other crops from the same image as the positive class,\nwhile crops from different images are treated as negative classes to increase\ninter-class distance. Uniquely, our approach also considers non-saliency crops\nfrom the same image as intra-class negative classes to preserve their\ndistinctiveness. Additionally, we employ a mutual learning framework, which\nfurther enhances the model's ability to adaptively learn and identify visual\nsaliency regions. Our approach demonstrates a better performance compared to\nstate-of-the-art methods in 7 standard datasets, achieving PLCC values of 0.942\n(compared to 0.908 in TID2013) and 0.914 (compared to 0.894 in LIVEC).",
        "chunk-id": 5,
        "chunk": "state-of-the-art methods in 7 standard datasets, achieving PLCC values of 0.942\n(compared to 0.908 in TID2013) and 0.914 (compared to 0.894 in LIVEC).",
        "authors": [
            "Timin Gao",
            "Wensheng Pan",
            "Yan Zhang",
            "Sicheng Zhao",
            "Shengchuan Zhang",
            "Xiawu Zheng",
            "Ke Li",
            "Liujuan Cao",
            "Rongrong Ji"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:14:23+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19247v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19247v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000061,
        "doi": null,
        "title": "An Interpretable and Efficient Sleep Staging Algorithm: DetectsleepNet",
        "abstract": "Sleep quality directly impacts human health and quality of life, so accurate\nsleep staging is essential for assessing sleep quality. However, most\ntraditional methods are inefficient and time-consuming due to segmenting\ndifferent sleep cycles by manual labeling. In contrast, automated sleep staging\ntechnology not only directly assesses sleep quality but also helps sleep\nspecialists analyze sleep status, significantly improving efficiency and\nreducing the cost of sleep monitoring, especially for continuous sleep\nmonitoring. Most of the existing models, however, are deficient in\ncomputational efficiency, lightweight design, and model interpretability. In\nthis paper, we propose a neural network architecture based on the prior\nknowledge of sleep experts. Specifically, 1) Propose an end-to-end model named\nDetectsleepNet that uses single-channel EEG signals without additional data\nprocessing, which has achieved an impressive 80.9% accuracy on the SHHS dataset\nand an outstanding 88.0% accuracy on the Physio2018 dataset. 2) Constructure an\nefficient lightweight sleep staging model named DetectsleepNet-tiny based on\nDetectsleepNet, which has just 6% of the parameter numbers of existing models,\nbut its accuracy exceeds 99% of state-of-the-art models, 3) Introducing a\nspecific inference header to assess the attention given to a specific EEG\nsegment in each sleep frame, enhancing the transparency in the decisions of\nmodels. Our model comprises fewer parameters compared to existing ones and\nulteriorly explores the interpretability of the model to facilitate its\napplication in healthcare. The code is available at\nhttps://github.com/komdec/DetectSleepNet.git.",
        "chunk-id": 1,
        "chunk": "Sleep quality directly impacts human health and quality of life, so accurate\nsleep staging is essential for assessing sleep quality. However, most\ntraditional methods are inefficient and time-consuming due to segmenting\ndifferent sleep cycles by manual labeling. In contrast, automated sleep staging\ntechnology not only directly assesses sleep quality but also helps sleep",
        "authors": [
            "Shengwei Guo"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:13:22+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19246v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19246v1",
        "categories": [
            "Signal Processing"
        ]
    },
    {
        "id": 30000061,
        "doi": null,
        "title": "An Interpretable and Efficient Sleep Staging Algorithm: DetectsleepNet",
        "abstract": "Sleep quality directly impacts human health and quality of life, so accurate\nsleep staging is essential for assessing sleep quality. However, most\ntraditional methods are inefficient and time-consuming due to segmenting\ndifferent sleep cycles by manual labeling. In contrast, automated sleep staging\ntechnology not only directly assesses sleep quality but also helps sleep\nspecialists analyze sleep status, significantly improving efficiency and\nreducing the cost of sleep monitoring, especially for continuous sleep\nmonitoring. Most of the existing models, however, are deficient in\ncomputational efficiency, lightweight design, and model interpretability. In\nthis paper, we propose a neural network architecture based on the prior\nknowledge of sleep experts. Specifically, 1) Propose an end-to-end model named\nDetectsleepNet that uses single-channel EEG signals without additional data\nprocessing, which has achieved an impressive 80.9% accuracy on the SHHS dataset\nand an outstanding 88.0% accuracy on the Physio2018 dataset. 2) Constructure an\nefficient lightweight sleep staging model named DetectsleepNet-tiny based on\nDetectsleepNet, which has just 6% of the parameter numbers of existing models,\nbut its accuracy exceeds 99% of state-of-the-art models, 3) Introducing a\nspecific inference header to assess the attention given to a specific EEG\nsegment in each sleep frame, enhancing the transparency in the decisions of\nmodels. Our model comprises fewer parameters compared to existing ones and\nulteriorly explores the interpretability of the model to facilitate its\napplication in healthcare. The code is available at\nhttps://github.com/komdec/DetectSleepNet.git.",
        "chunk-id": 2,
        "chunk": "specialists analyze sleep status, significantly improving efficiency and\nreducing the cost of sleep monitoring, especially for continuous sleep\nmonitoring. Most of the existing models, however, are deficient in\ncomputational efficiency, lightweight design, and model interpretability. In\nthis paper, we propose a neural network architecture based on the prior",
        "authors": [
            "Shengwei Guo"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:13:22+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19246v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19246v1",
        "categories": [
            "Signal Processing"
        ]
    },
    {
        "id": 30000061,
        "doi": null,
        "title": "An Interpretable and Efficient Sleep Staging Algorithm: DetectsleepNet",
        "abstract": "Sleep quality directly impacts human health and quality of life, so accurate\nsleep staging is essential for assessing sleep quality. However, most\ntraditional methods are inefficient and time-consuming due to segmenting\ndifferent sleep cycles by manual labeling. In contrast, automated sleep staging\ntechnology not only directly assesses sleep quality but also helps sleep\nspecialists analyze sleep status, significantly improving efficiency and\nreducing the cost of sleep monitoring, especially for continuous sleep\nmonitoring. Most of the existing models, however, are deficient in\ncomputational efficiency, lightweight design, and model interpretability. In\nthis paper, we propose a neural network architecture based on the prior\nknowledge of sleep experts. Specifically, 1) Propose an end-to-end model named\nDetectsleepNet that uses single-channel EEG signals without additional data\nprocessing, which has achieved an impressive 80.9% accuracy on the SHHS dataset\nand an outstanding 88.0% accuracy on the Physio2018 dataset. 2) Constructure an\nefficient lightweight sleep staging model named DetectsleepNet-tiny based on\nDetectsleepNet, which has just 6% of the parameter numbers of existing models,\nbut its accuracy exceeds 99% of state-of-the-art models, 3) Introducing a\nspecific inference header to assess the attention given to a specific EEG\nsegment in each sleep frame, enhancing the transparency in the decisions of\nmodels. Our model comprises fewer parameters compared to existing ones and\nulteriorly explores the interpretability of the model to facilitate its\napplication in healthcare. The code is available at\nhttps://github.com/komdec/DetectSleepNet.git.",
        "chunk-id": 3,
        "chunk": "knowledge of sleep experts. Specifically, 1) Propose an end-to-end model named\nDetectsleepNet that uses single-channel EEG signals without additional data\nprocessing, which has achieved an impressive 80.9% accuracy on the SHHS dataset\nand an outstanding 88.0% accuracy on the Physio2018 dataset. 2) Constructure an",
        "authors": [
            "Shengwei Guo"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:13:22+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19246v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19246v1",
        "categories": [
            "Signal Processing"
        ]
    },
    {
        "id": 30000061,
        "doi": null,
        "title": "An Interpretable and Efficient Sleep Staging Algorithm: DetectsleepNet",
        "abstract": "Sleep quality directly impacts human health and quality of life, so accurate\nsleep staging is essential for assessing sleep quality. However, most\ntraditional methods are inefficient and time-consuming due to segmenting\ndifferent sleep cycles by manual labeling. In contrast, automated sleep staging\ntechnology not only directly assesses sleep quality but also helps sleep\nspecialists analyze sleep status, significantly improving efficiency and\nreducing the cost of sleep monitoring, especially for continuous sleep\nmonitoring. Most of the existing models, however, are deficient in\ncomputational efficiency, lightweight design, and model interpretability. In\nthis paper, we propose a neural network architecture based on the prior\nknowledge of sleep experts. Specifically, 1) Propose an end-to-end model named\nDetectsleepNet that uses single-channel EEG signals without additional data\nprocessing, which has achieved an impressive 80.9% accuracy on the SHHS dataset\nand an outstanding 88.0% accuracy on the Physio2018 dataset. 2) Constructure an\nefficient lightweight sleep staging model named DetectsleepNet-tiny based on\nDetectsleepNet, which has just 6% of the parameter numbers of existing models,\nbut its accuracy exceeds 99% of state-of-the-art models, 3) Introducing a\nspecific inference header to assess the attention given to a specific EEG\nsegment in each sleep frame, enhancing the transparency in the decisions of\nmodels. Our model comprises fewer parameters compared to existing ones and\nulteriorly explores the interpretability of the model to facilitate its\napplication in healthcare. The code is available at\nhttps://github.com/komdec/DetectSleepNet.git.",
        "chunk-id": 4,
        "chunk": "efficient lightweight sleep staging model named DetectsleepNet-tiny based on\nDetectsleepNet, which has just 6% of the parameter numbers of existing models,\nbut its accuracy exceeds 99% of state-of-the-art models, 3) Introducing a\nspecific inference header to assess the attention given to a specific EEG\nsegment in each sleep frame, enhancing the transparency in the decisions of",
        "authors": [
            "Shengwei Guo"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:13:22+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19246v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19246v1",
        "categories": [
            "Signal Processing"
        ]
    },
    {
        "id": 30000061,
        "doi": null,
        "title": "An Interpretable and Efficient Sleep Staging Algorithm: DetectsleepNet",
        "abstract": "Sleep quality directly impacts human health and quality of life, so accurate\nsleep staging is essential for assessing sleep quality. However, most\ntraditional methods are inefficient and time-consuming due to segmenting\ndifferent sleep cycles by manual labeling. In contrast, automated sleep staging\ntechnology not only directly assesses sleep quality but also helps sleep\nspecialists analyze sleep status, significantly improving efficiency and\nreducing the cost of sleep monitoring, especially for continuous sleep\nmonitoring. Most of the existing models, however, are deficient in\ncomputational efficiency, lightweight design, and model interpretability. In\nthis paper, we propose a neural network architecture based on the prior\nknowledge of sleep experts. Specifically, 1) Propose an end-to-end model named\nDetectsleepNet that uses single-channel EEG signals without additional data\nprocessing, which has achieved an impressive 80.9% accuracy on the SHHS dataset\nand an outstanding 88.0% accuracy on the Physio2018 dataset. 2) Constructure an\nefficient lightweight sleep staging model named DetectsleepNet-tiny based on\nDetectsleepNet, which has just 6% of the parameter numbers of existing models,\nbut its accuracy exceeds 99% of state-of-the-art models, 3) Introducing a\nspecific inference header to assess the attention given to a specific EEG\nsegment in each sleep frame, enhancing the transparency in the decisions of\nmodels. Our model comprises fewer parameters compared to existing ones and\nulteriorly explores the interpretability of the model to facilitate its\napplication in healthcare. The code is available at\nhttps://github.com/komdec/DetectSleepNet.git.",
        "chunk-id": 5,
        "chunk": "models. Our model comprises fewer parameters compared to existing ones and\nulteriorly explores the interpretability of the model to facilitate its\napplication in healthcare. The code is available at\nhttps://github.com/komdec/DetectSleepNet.git.",
        "authors": [
            "Shengwei Guo"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:13:22+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19246v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19246v1",
        "categories": [
            "Signal Processing"
        ]
    },
    {
        "id": 30000062,
        "doi": null,
        "title": "Stiefel-Whitney Classes for Finite Special Linear Groups of Even Rank",
        "abstract": "We compute the total Stiefel-Whitney Classes (SWCs) for orthogonal\nrepresentations of special linear groups $\\text{SL}(n,q)$ when $n$ and $q$ are\nodd. These classes are expressed in terms of character values at diagonal\nelements of order $2$. We give several consequences, and work out the $4$th SWC\nexplicitly, and the $8$th SWC when the $4$th vanishes.",
        "chunk-id": 1,
        "chunk": "We compute the total Stiefel-Whitney Classes (SWCs) for orthogonal\nrepresentations of special linear groups $\\text{SL}(n,q)$ when $n$ and $q$ are\nodd. These classes are expressed in terms of character values at diagonal\nelements of order $2$. We give several consequences, and work out the $4$th SWC\nexplicitly, and the $8$th SWC when the $4$th vanishes.",
        "authors": [
            "Neha Malik",
            "Steven Spallone"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:06:05+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19241v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19241v1",
        "categories": [
            "Representation Theory",
            "Linear algebraic groups over finite fields, Homology of classifying spaces and characteristic classes in algebraic topology"
        ]
    },
    {
        "id": 30000063,
        "doi": null,
        "title": "ALMA: a mathematics-driven approach for determining tuning parameters in generalized LASSO problems, with applications to MRI",
        "abstract": "Magnetic Resonance Imaging (MRI) is a powerful technique employed for\nnon-invasive in vivo visualization of internal structures. Sparsity is often\ndeployed to accelerate the signal acquisition or overcome the presence of\nmotion artifacts, improving the quality of image reconstruction. Image\nreconstruction algorithms use TV-regularized LASSO (Total Variation-regularized\nLASSO) to retrieve the missing information of undersampled signals, by cleaning\nthe data of noise and while optimizing sparsity. A tuning parameter moderates\nthe balance between these two aspects; its choice affecting the quality of the\nreconstructions. Currently, there is a lack of general deterministic techniques\nto choose these parameters, which are oftentimes manually selected and thus\nhinder the reliability of the reconstructions. Here, we present ALMA (Algorithm\nfor Lagrange Multipliers Approximation), an iterative mathematics-inspired\ntechnique that computes tuning parameters for generalized LASSO problems during\nMRI reconstruction. We analyze quantitatively the performance of these\nparameters for imaging reconstructions via TV-LASSO in an MRI context on\nphantoms. Although our study concentrates on TV-LASSO, the techniques developed\nhere hold significant promise for a wide array of applications. ALMA is not\nonly adaptable to more generalized LASSO problems but is also robust to\naccommodate other forms of regularization beyond total variation. Moreover, it\nextends effectively to handle non-Cartesian sampling trajectories, broadening\nits utility in complex data reconstruction scenarios. More generally, ALMA\nprovides a powerful tool for numerically solving constrained optimization\nproblems across various disciplines, offering a versatile and impactful\nsolution for advanced computational challenges.",
        "chunk-id": 1,
        "chunk": "Magnetic Resonance Imaging (MRI) is a powerful technique employed for\nnon-invasive in vivo visualization of internal structures. Sparsity is often\ndeployed to accelerate the signal acquisition or overcome the presence of\nmotion artifacts, improving the quality of image reconstruction. Image\nreconstruction algorithms use TV-regularized LASSO (Total Variation-regularized",
        "authors": [
            "Gianluca Giacchi",
            "Isidoros Iakovidis",
            "Bastien Milani",
            "Matthias Stuber",
            "Micah Murray",
            "Benedetta Franceschiello"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:02:04+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19239v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19239v1",
        "categories": [
            "Image and Video Processing",
            "Computer Vision and Pattern Recognition",
            "Signal Processing",
            "Medical Physics",
            "Biomedical imaging and signal processing, Ridge regression; shrinkage estimators (Lasso), Numerical optimization and variational techniques",
            "Compression (Coding),Reconstruction,PHYSICAL SCIENCES AND ENGINEERING"
        ]
    },
    {
        "id": 30000063,
        "doi": null,
        "title": "ALMA: a mathematics-driven approach for determining tuning parameters in generalized LASSO problems, with applications to MRI",
        "abstract": "Magnetic Resonance Imaging (MRI) is a powerful technique employed for\nnon-invasive in vivo visualization of internal structures. Sparsity is often\ndeployed to accelerate the signal acquisition or overcome the presence of\nmotion artifacts, improving the quality of image reconstruction. Image\nreconstruction algorithms use TV-regularized LASSO (Total Variation-regularized\nLASSO) to retrieve the missing information of undersampled signals, by cleaning\nthe data of noise and while optimizing sparsity. A tuning parameter moderates\nthe balance between these two aspects; its choice affecting the quality of the\nreconstructions. Currently, there is a lack of general deterministic techniques\nto choose these parameters, which are oftentimes manually selected and thus\nhinder the reliability of the reconstructions. Here, we present ALMA (Algorithm\nfor Lagrange Multipliers Approximation), an iterative mathematics-inspired\ntechnique that computes tuning parameters for generalized LASSO problems during\nMRI reconstruction. We analyze quantitatively the performance of these\nparameters for imaging reconstructions via TV-LASSO in an MRI context on\nphantoms. Although our study concentrates on TV-LASSO, the techniques developed\nhere hold significant promise for a wide array of applications. ALMA is not\nonly adaptable to more generalized LASSO problems but is also robust to\naccommodate other forms of regularization beyond total variation. Moreover, it\nextends effectively to handle non-Cartesian sampling trajectories, broadening\nits utility in complex data reconstruction scenarios. More generally, ALMA\nprovides a powerful tool for numerically solving constrained optimization\nproblems across various disciplines, offering a versatile and impactful\nsolution for advanced computational challenges.",
        "chunk-id": 2,
        "chunk": "LASSO) to retrieve the missing information of undersampled signals, by cleaning\nthe data of noise and while optimizing sparsity. A tuning parameter moderates\nthe balance between these two aspects; its choice affecting the quality of the\nreconstructions. Currently, there is a lack of general deterministic techniques",
        "authors": [
            "Gianluca Giacchi",
            "Isidoros Iakovidis",
            "Bastien Milani",
            "Matthias Stuber",
            "Micah Murray",
            "Benedetta Franceschiello"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:02:04+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19239v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19239v1",
        "categories": [
            "Image and Video Processing",
            "Computer Vision and Pattern Recognition",
            "Signal Processing",
            "Medical Physics",
            "Biomedical imaging and signal processing, Ridge regression; shrinkage estimators (Lasso), Numerical optimization and variational techniques",
            "Compression (Coding),Reconstruction,PHYSICAL SCIENCES AND ENGINEERING"
        ]
    },
    {
        "id": 30000063,
        "doi": null,
        "title": "ALMA: a mathematics-driven approach for determining tuning parameters in generalized LASSO problems, with applications to MRI",
        "abstract": "Magnetic Resonance Imaging (MRI) is a powerful technique employed for\nnon-invasive in vivo visualization of internal structures. Sparsity is often\ndeployed to accelerate the signal acquisition or overcome the presence of\nmotion artifacts, improving the quality of image reconstruction. Image\nreconstruction algorithms use TV-regularized LASSO (Total Variation-regularized\nLASSO) to retrieve the missing information of undersampled signals, by cleaning\nthe data of noise and while optimizing sparsity. A tuning parameter moderates\nthe balance between these two aspects; its choice affecting the quality of the\nreconstructions. Currently, there is a lack of general deterministic techniques\nto choose these parameters, which are oftentimes manually selected and thus\nhinder the reliability of the reconstructions. Here, we present ALMA (Algorithm\nfor Lagrange Multipliers Approximation), an iterative mathematics-inspired\ntechnique that computes tuning parameters for generalized LASSO problems during\nMRI reconstruction. We analyze quantitatively the performance of these\nparameters for imaging reconstructions via TV-LASSO in an MRI context on\nphantoms. Although our study concentrates on TV-LASSO, the techniques developed\nhere hold significant promise for a wide array of applications. ALMA is not\nonly adaptable to more generalized LASSO problems but is also robust to\naccommodate other forms of regularization beyond total variation. Moreover, it\nextends effectively to handle non-Cartesian sampling trajectories, broadening\nits utility in complex data reconstruction scenarios. More generally, ALMA\nprovides a powerful tool for numerically solving constrained optimization\nproblems across various disciplines, offering a versatile and impactful\nsolution for advanced computational challenges.",
        "chunk-id": 3,
        "chunk": "to choose these parameters, which are oftentimes manually selected and thus\nhinder the reliability of the reconstructions. Here, we present ALMA (Algorithm\nfor Lagrange Multipliers Approximation), an iterative mathematics-inspired\ntechnique that computes tuning parameters for generalized LASSO problems during\nMRI reconstruction. We analyze quantitatively the performance of these",
        "authors": [
            "Gianluca Giacchi",
            "Isidoros Iakovidis",
            "Bastien Milani",
            "Matthias Stuber",
            "Micah Murray",
            "Benedetta Franceschiello"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:02:04+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19239v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19239v1",
        "categories": [
            "Image and Video Processing",
            "Computer Vision and Pattern Recognition",
            "Signal Processing",
            "Medical Physics",
            "Biomedical imaging and signal processing, Ridge regression; shrinkage estimators (Lasso), Numerical optimization and variational techniques",
            "Compression (Coding),Reconstruction,PHYSICAL SCIENCES AND ENGINEERING"
        ]
    },
    {
        "id": 30000063,
        "doi": null,
        "title": "ALMA: a mathematics-driven approach for determining tuning parameters in generalized LASSO problems, with applications to MRI",
        "abstract": "Magnetic Resonance Imaging (MRI) is a powerful technique employed for\nnon-invasive in vivo visualization of internal structures. Sparsity is often\ndeployed to accelerate the signal acquisition or overcome the presence of\nmotion artifacts, improving the quality of image reconstruction. Image\nreconstruction algorithms use TV-regularized LASSO (Total Variation-regularized\nLASSO) to retrieve the missing information of undersampled signals, by cleaning\nthe data of noise and while optimizing sparsity. A tuning parameter moderates\nthe balance between these two aspects; its choice affecting the quality of the\nreconstructions. Currently, there is a lack of general deterministic techniques\nto choose these parameters, which are oftentimes manually selected and thus\nhinder the reliability of the reconstructions. Here, we present ALMA (Algorithm\nfor Lagrange Multipliers Approximation), an iterative mathematics-inspired\ntechnique that computes tuning parameters for generalized LASSO problems during\nMRI reconstruction. We analyze quantitatively the performance of these\nparameters for imaging reconstructions via TV-LASSO in an MRI context on\nphantoms. Although our study concentrates on TV-LASSO, the techniques developed\nhere hold significant promise for a wide array of applications. ALMA is not\nonly adaptable to more generalized LASSO problems but is also robust to\naccommodate other forms of regularization beyond total variation. Moreover, it\nextends effectively to handle non-Cartesian sampling trajectories, broadening\nits utility in complex data reconstruction scenarios. More generally, ALMA\nprovides a powerful tool for numerically solving constrained optimization\nproblems across various disciplines, offering a versatile and impactful\nsolution for advanced computational challenges.",
        "chunk-id": 4,
        "chunk": "parameters for imaging reconstructions via TV-LASSO in an MRI context on\nphantoms. Although our study concentrates on TV-LASSO, the techniques developed\nhere hold significant promise for a wide array of applications. ALMA is not\nonly adaptable to more generalized LASSO problems but is also robust to\naccommodate other forms of regularization beyond total variation. Moreover, it",
        "authors": [
            "Gianluca Giacchi",
            "Isidoros Iakovidis",
            "Bastien Milani",
            "Matthias Stuber",
            "Micah Murray",
            "Benedetta Franceschiello"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:02:04+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19239v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19239v1",
        "categories": [
            "Image and Video Processing",
            "Computer Vision and Pattern Recognition",
            "Signal Processing",
            "Medical Physics",
            "Biomedical imaging and signal processing, Ridge regression; shrinkage estimators (Lasso), Numerical optimization and variational techniques",
            "Compression (Coding),Reconstruction,PHYSICAL SCIENCES AND ENGINEERING"
        ]
    },
    {
        "id": 30000063,
        "doi": null,
        "title": "ALMA: a mathematics-driven approach for determining tuning parameters in generalized LASSO problems, with applications to MRI",
        "abstract": "Magnetic Resonance Imaging (MRI) is a powerful technique employed for\nnon-invasive in vivo visualization of internal structures. Sparsity is often\ndeployed to accelerate the signal acquisition or overcome the presence of\nmotion artifacts, improving the quality of image reconstruction. Image\nreconstruction algorithms use TV-regularized LASSO (Total Variation-regularized\nLASSO) to retrieve the missing information of undersampled signals, by cleaning\nthe data of noise and while optimizing sparsity. A tuning parameter moderates\nthe balance between these two aspects; its choice affecting the quality of the\nreconstructions. Currently, there is a lack of general deterministic techniques\nto choose these parameters, which are oftentimes manually selected and thus\nhinder the reliability of the reconstructions. Here, we present ALMA (Algorithm\nfor Lagrange Multipliers Approximation), an iterative mathematics-inspired\ntechnique that computes tuning parameters for generalized LASSO problems during\nMRI reconstruction. We analyze quantitatively the performance of these\nparameters for imaging reconstructions via TV-LASSO in an MRI context on\nphantoms. Although our study concentrates on TV-LASSO, the techniques developed\nhere hold significant promise for a wide array of applications. ALMA is not\nonly adaptable to more generalized LASSO problems but is also robust to\naccommodate other forms of regularization beyond total variation. Moreover, it\nextends effectively to handle non-Cartesian sampling trajectories, broadening\nits utility in complex data reconstruction scenarios. More generally, ALMA\nprovides a powerful tool for numerically solving constrained optimization\nproblems across various disciplines, offering a versatile and impactful\nsolution for advanced computational challenges.",
        "chunk-id": 5,
        "chunk": "extends effectively to handle non-Cartesian sampling trajectories, broadening\nits utility in complex data reconstruction scenarios. More generally, ALMA\nprovides a powerful tool for numerically solving constrained optimization\nproblems across various disciplines, offering a versatile and impactful\nsolution for advanced computational challenges.",
        "authors": [
            "Gianluca Giacchi",
            "Isidoros Iakovidis",
            "Bastien Milani",
            "Matthias Stuber",
            "Micah Murray",
            "Benedetta Franceschiello"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:02:04+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19239v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19239v1",
        "categories": [
            "Image and Video Processing",
            "Computer Vision and Pattern Recognition",
            "Signal Processing",
            "Medical Physics",
            "Biomedical imaging and signal processing, Ridge regression; shrinkage estimators (Lasso), Numerical optimization and variational techniques",
            "Compression (Coding),Reconstruction,PHYSICAL SCIENCES AND ENGINEERING"
        ]
    },
    {
        "id": 30000064,
        "doi": null,
        "title": "Revealing Fine-Grained Values and Opinions in Large Language Models",
        "abstract": "Uncovering latent values and opinions in large language models (LLMs) can\nhelp identify biases and mitigate potential harm. Recently, this has been\napproached by presenting LLMs with survey questions and quantifying their\nstances towards morally and politically charged statements. However, the\nstances generated by LLMs can vary greatly depending on how they are prompted,\nand there are many ways to argue for or against a given position. In this work,\nwe propose to address this by analysing a large and robust dataset of 156k LLM\nresponses to the 62 propositions of the Political Compass Test (PCT) generated\nby 6 LLMs using 420 prompt variations. We perform coarse-grained analysis of\ntheir generated stances and fine-grained analysis of the plain text\njustifications for those stances. For fine-grained analysis, we propose to\nidentify tropes in the responses: semantically similar phrases that are\nrecurrent and consistent across different prompts, revealing patterns in the\ntext that a given LLM is prone to produce. We find that demographic features\nadded to prompts significantly affect outcomes on the PCT, reflecting bias, as\nwell as disparities between the results of tests when eliciting closed-form vs.\nopen domain responses. Additionally, patterns in the plain text rationales via\ntropes show that similar justifications are repeatedly generated across models\nand prompts even with disparate stances.",
        "chunk-id": 1,
        "chunk": "Uncovering latent values and opinions in large language models (LLMs) can\nhelp identify biases and mitigate potential harm. Recently, this has been\napproached by presenting LLMs with survey questions and quantifying their\nstances towards morally and politically charged statements. However, the\nstances generated by LLMs can vary greatly depending on how they are prompted,",
        "authors": [
            "Dustin Wright",
            "Arnav Arora",
            "Nadav Borenstein",
            "Srishti Yadav",
            "Serge Belongie",
            "Isabelle Augenstein"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:01:53+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19238v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19238v1",
        "categories": [
            "Computation and Language",
            "Computers and Society",
            "Machine Learning"
        ]
    },
    {
        "id": 30000064,
        "doi": null,
        "title": "Revealing Fine-Grained Values and Opinions in Large Language Models",
        "abstract": "Uncovering latent values and opinions in large language models (LLMs) can\nhelp identify biases and mitigate potential harm. Recently, this has been\napproached by presenting LLMs with survey questions and quantifying their\nstances towards morally and politically charged statements. However, the\nstances generated by LLMs can vary greatly depending on how they are prompted,\nand there are many ways to argue for or against a given position. In this work,\nwe propose to address this by analysing a large and robust dataset of 156k LLM\nresponses to the 62 propositions of the Political Compass Test (PCT) generated\nby 6 LLMs using 420 prompt variations. We perform coarse-grained analysis of\ntheir generated stances and fine-grained analysis of the plain text\njustifications for those stances. For fine-grained analysis, we propose to\nidentify tropes in the responses: semantically similar phrases that are\nrecurrent and consistent across different prompts, revealing patterns in the\ntext that a given LLM is prone to produce. We find that demographic features\nadded to prompts significantly affect outcomes on the PCT, reflecting bias, as\nwell as disparities between the results of tests when eliciting closed-form vs.\nopen domain responses. Additionally, patterns in the plain text rationales via\ntropes show that similar justifications are repeatedly generated across models\nand prompts even with disparate stances.",
        "chunk-id": 2,
        "chunk": "and there are many ways to argue for or against a given position. In this work,\nwe propose to address this by analysing a large and robust dataset of 156k LLM\nresponses to the 62 propositions of the Political Compass Test (PCT) generated\nby 6 LLMs using 420 prompt variations. We perform coarse-grained analysis of\ntheir generated stances and fine-grained analysis of the plain text",
        "authors": [
            "Dustin Wright",
            "Arnav Arora",
            "Nadav Borenstein",
            "Srishti Yadav",
            "Serge Belongie",
            "Isabelle Augenstein"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:01:53+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19238v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19238v1",
        "categories": [
            "Computation and Language",
            "Computers and Society",
            "Machine Learning"
        ]
    },
    {
        "id": 30000064,
        "doi": null,
        "title": "Revealing Fine-Grained Values and Opinions in Large Language Models",
        "abstract": "Uncovering latent values and opinions in large language models (LLMs) can\nhelp identify biases and mitigate potential harm. Recently, this has been\napproached by presenting LLMs with survey questions and quantifying their\nstances towards morally and politically charged statements. However, the\nstances generated by LLMs can vary greatly depending on how they are prompted,\nand there are many ways to argue for or against a given position. In this work,\nwe propose to address this by analysing a large and robust dataset of 156k LLM\nresponses to the 62 propositions of the Political Compass Test (PCT) generated\nby 6 LLMs using 420 prompt variations. We perform coarse-grained analysis of\ntheir generated stances and fine-grained analysis of the plain text\njustifications for those stances. For fine-grained analysis, we propose to\nidentify tropes in the responses: semantically similar phrases that are\nrecurrent and consistent across different prompts, revealing patterns in the\ntext that a given LLM is prone to produce. We find that demographic features\nadded to prompts significantly affect outcomes on the PCT, reflecting bias, as\nwell as disparities between the results of tests when eliciting closed-form vs.\nopen domain responses. Additionally, patterns in the plain text rationales via\ntropes show that similar justifications are repeatedly generated across models\nand prompts even with disparate stances.",
        "chunk-id": 3,
        "chunk": "justifications for those stances. For fine-grained analysis, we propose to\nidentify tropes in the responses: semantically similar phrases that are\nrecurrent and consistent across different prompts, revealing patterns in the\ntext that a given LLM is prone to produce. We find that demographic features\nadded to prompts significantly affect outcomes on the PCT, reflecting bias, as",
        "authors": [
            "Dustin Wright",
            "Arnav Arora",
            "Nadav Borenstein",
            "Srishti Yadav",
            "Serge Belongie",
            "Isabelle Augenstein"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:01:53+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19238v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19238v1",
        "categories": [
            "Computation and Language",
            "Computers and Society",
            "Machine Learning"
        ]
    },
    {
        "id": 30000064,
        "doi": null,
        "title": "Revealing Fine-Grained Values and Opinions in Large Language Models",
        "abstract": "Uncovering latent values and opinions in large language models (LLMs) can\nhelp identify biases and mitigate potential harm. Recently, this has been\napproached by presenting LLMs with survey questions and quantifying their\nstances towards morally and politically charged statements. However, the\nstances generated by LLMs can vary greatly depending on how they are prompted,\nand there are many ways to argue for or against a given position. In this work,\nwe propose to address this by analysing a large and robust dataset of 156k LLM\nresponses to the 62 propositions of the Political Compass Test (PCT) generated\nby 6 LLMs using 420 prompt variations. We perform coarse-grained analysis of\ntheir generated stances and fine-grained analysis of the plain text\njustifications for those stances. For fine-grained analysis, we propose to\nidentify tropes in the responses: semantically similar phrases that are\nrecurrent and consistent across different prompts, revealing patterns in the\ntext that a given LLM is prone to produce. We find that demographic features\nadded to prompts significantly affect outcomes on the PCT, reflecting bias, as\nwell as disparities between the results of tests when eliciting closed-form vs.\nopen domain responses. Additionally, patterns in the plain text rationales via\ntropes show that similar justifications are repeatedly generated across models\nand prompts even with disparate stances.",
        "chunk-id": 4,
        "chunk": "well as disparities between the results of tests when eliciting closed-form vs.\nopen domain responses. Additionally, patterns in the plain text rationales via\ntropes show that similar justifications are repeatedly generated across models\nand prompts even with disparate stances.",
        "authors": [
            "Dustin Wright",
            "Arnav Arora",
            "Nadav Borenstein",
            "Srishti Yadav",
            "Serge Belongie",
            "Isabelle Augenstein"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:01:53+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19238v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19238v1",
        "categories": [
            "Computation and Language",
            "Computers and Society",
            "Machine Learning"
        ]
    },
    {
        "id": 30000065,
        "doi": null,
        "title": "FlowVQA: Mapping Multimodal Logic in Visual Question Answering with Flowcharts",
        "abstract": "Existing benchmarks for visual question answering lack in visual grounding\nand complexity, particularly in evaluating spatial reasoning skills. We\nintroduce FlowVQA, a novel benchmark aimed at assessing the capabilities of\nvisual question-answering multimodal language models in reasoning with\nflowcharts as visual contexts. FlowVQA comprises 2,272 carefully generated and\nhuman-verified flowchart images from three distinct content sources, along with\n22,413 diverse question-answer pairs, to test a spectrum of reasoning tasks,\nincluding information localization, decision-making, and logical progression.\nWe conduct a thorough baseline evaluation on a suite of both open-source and\nproprietary multimodal language models using various strategies, followed by an\nanalysis of directional bias. The results underscore the benchmark's potential\nas a vital tool for advancing the field of multimodal modeling, providing a\nfocused and challenging environment for enhancing model performance in visual\nand logical reasoning tasks.",
        "chunk-id": 1,
        "chunk": "Existing benchmarks for visual question answering lack in visual grounding\nand complexity, particularly in evaluating spatial reasoning skills. We\nintroduce FlowVQA, a novel benchmark aimed at assessing the capabilities of\nvisual question-answering multimodal language models in reasoning with\nflowcharts as visual contexts. FlowVQA comprises 2,272 carefully generated and",
        "authors": [
            "Shubhankar Singh",
            "Purvi Chaurasia",
            "Yerram Varun",
            "Pranshu Pandya",
            "Vatsal Gupta",
            "Vivek Gupta",
            "Dan Roth"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:01:48+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19237v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19237v1",
        "categories": [
            "Computation and Language",
            "Computer Vision and Pattern Recognition",
            "Information Retrieval",
            "Machine Learning"
        ]
    },
    {
        "id": 30000065,
        "doi": null,
        "title": "FlowVQA: Mapping Multimodal Logic in Visual Question Answering with Flowcharts",
        "abstract": "Existing benchmarks for visual question answering lack in visual grounding\nand complexity, particularly in evaluating spatial reasoning skills. We\nintroduce FlowVQA, a novel benchmark aimed at assessing the capabilities of\nvisual question-answering multimodal language models in reasoning with\nflowcharts as visual contexts. FlowVQA comprises 2,272 carefully generated and\nhuman-verified flowchart images from three distinct content sources, along with\n22,413 diverse question-answer pairs, to test a spectrum of reasoning tasks,\nincluding information localization, decision-making, and logical progression.\nWe conduct a thorough baseline evaluation on a suite of both open-source and\nproprietary multimodal language models using various strategies, followed by an\nanalysis of directional bias. The results underscore the benchmark's potential\nas a vital tool for advancing the field of multimodal modeling, providing a\nfocused and challenging environment for enhancing model performance in visual\nand logical reasoning tasks.",
        "chunk-id": 2,
        "chunk": "human-verified flowchart images from three distinct content sources, along with\n22,413 diverse question-answer pairs, to test a spectrum of reasoning tasks,\nincluding information localization, decision-making, and logical progression.\nWe conduct a thorough baseline evaluation on a suite of both open-source and",
        "authors": [
            "Shubhankar Singh",
            "Purvi Chaurasia",
            "Yerram Varun",
            "Pranshu Pandya",
            "Vatsal Gupta",
            "Vivek Gupta",
            "Dan Roth"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:01:48+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19237v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19237v1",
        "categories": [
            "Computation and Language",
            "Computer Vision and Pattern Recognition",
            "Information Retrieval",
            "Machine Learning"
        ]
    },
    {
        "id": 30000065,
        "doi": null,
        "title": "FlowVQA: Mapping Multimodal Logic in Visual Question Answering with Flowcharts",
        "abstract": "Existing benchmarks for visual question answering lack in visual grounding\nand complexity, particularly in evaluating spatial reasoning skills. We\nintroduce FlowVQA, a novel benchmark aimed at assessing the capabilities of\nvisual question-answering multimodal language models in reasoning with\nflowcharts as visual contexts. FlowVQA comprises 2,272 carefully generated and\nhuman-verified flowchart images from three distinct content sources, along with\n22,413 diverse question-answer pairs, to test a spectrum of reasoning tasks,\nincluding information localization, decision-making, and logical progression.\nWe conduct a thorough baseline evaluation on a suite of both open-source and\nproprietary multimodal language models using various strategies, followed by an\nanalysis of directional bias. The results underscore the benchmark's potential\nas a vital tool for advancing the field of multimodal modeling, providing a\nfocused and challenging environment for enhancing model performance in visual\nand logical reasoning tasks.",
        "chunk-id": 3,
        "chunk": "proprietary multimodal language models using various strategies, followed by an\nanalysis of directional bias. The results underscore the benchmark's potential\nas a vital tool for advancing the field of multimodal modeling, providing a\nfocused and challenging environment for enhancing model performance in visual\nand logical reasoning tasks.",
        "authors": [
            "Shubhankar Singh",
            "Purvi Chaurasia",
            "Yerram Varun",
            "Pranshu Pandya",
            "Vatsal Gupta",
            "Vivek Gupta",
            "Dan Roth"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:01:48+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19237v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19237v1",
        "categories": [
            "Computation and Language",
            "Computer Vision and Pattern Recognition",
            "Information Retrieval",
            "Machine Learning"
        ]
    },
    {
        "id": 30000066,
        "doi": null,
        "title": "Human-Aware Vision-and-Language Navigation: Bridging Simulation to Reality with Dynamic Human Interactions",
        "abstract": "Vision-and-Language Navigation (VLN) aims to develop embodied agents that\nnavigate based on human instructions. However, current VLN frameworks often\nrely on static environments and optimal expert supervision, limiting their\nreal-world applicability. To address this, we introduce Human-Aware\nVision-and-Language Navigation (HA-VLN), extending traditional VLN by\nincorporating dynamic human activities and relaxing key assumptions. We propose\nthe Human-Aware 3D (HA3D) simulator, which combines dynamic human activities\nwith the Matterport3D dataset, and the Human-Aware Room-to-Room (HA-R2R)\ndataset, extending R2R with human activity descriptions. To tackle HA-VLN\nchallenges, we present the Expert-Supervised Cross-Modal (VLN-CM) and\nNon-Expert-Supervised Decision Transformer (VLN-DT) agents, utilizing\ncross-modal fusion and diverse training strategies for effective navigation in\ndynamic human environments. A comprehensive evaluation, including metrics\nconsidering human activities, and systematic analysis of HA-VLN's unique\nchallenges, underscores the need for further research to enhance HA-VLN agents'\nreal-world robustness and adaptability. Ultimately, this work provides\nbenchmarks and insights for future research on embodied AI and Sim2Real\ntransfer, paving the way for more realistic and applicable VLN systems in\nhuman-populated environments.",
        "chunk-id": 1,
        "chunk": "Vision-and-Language Navigation (VLN) aims to develop embodied agents that\nnavigate based on human instructions. However, current VLN frameworks often\nrely on static environments and optimal expert supervision, limiting their\nreal-world applicability. To address this, we introduce Human-Aware\nVision-and-Language Navigation (HA-VLN), extending traditional VLN by",
        "authors": [
            "Minghan Li",
            "Heng Li",
            "Zhi-Qi Cheng",
            "Yifei Dong",
            "Yuxuan Zhou",
            "Jun-Yan He",
            "Qi Dai",
            "Teruko Mitamura",
            "Alexander G. Hauptmann"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:01:42+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19236v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19236v1",
        "categories": [
            "Artificial Intelligence",
            "Computer Vision and Pattern Recognition",
            "Robotics"
        ]
    },
    {
        "id": 30000066,
        "doi": null,
        "title": "Human-Aware Vision-and-Language Navigation: Bridging Simulation to Reality with Dynamic Human Interactions",
        "abstract": "Vision-and-Language Navigation (VLN) aims to develop embodied agents that\nnavigate based on human instructions. However, current VLN frameworks often\nrely on static environments and optimal expert supervision, limiting their\nreal-world applicability. To address this, we introduce Human-Aware\nVision-and-Language Navigation (HA-VLN), extending traditional VLN by\nincorporating dynamic human activities and relaxing key assumptions. We propose\nthe Human-Aware 3D (HA3D) simulator, which combines dynamic human activities\nwith the Matterport3D dataset, and the Human-Aware Room-to-Room (HA-R2R)\ndataset, extending R2R with human activity descriptions. To tackle HA-VLN\nchallenges, we present the Expert-Supervised Cross-Modal (VLN-CM) and\nNon-Expert-Supervised Decision Transformer (VLN-DT) agents, utilizing\ncross-modal fusion and diverse training strategies for effective navigation in\ndynamic human environments. A comprehensive evaluation, including metrics\nconsidering human activities, and systematic analysis of HA-VLN's unique\nchallenges, underscores the need for further research to enhance HA-VLN agents'\nreal-world robustness and adaptability. Ultimately, this work provides\nbenchmarks and insights for future research on embodied AI and Sim2Real\ntransfer, paving the way for more realistic and applicable VLN systems in\nhuman-populated environments.",
        "chunk-id": 2,
        "chunk": "incorporating dynamic human activities and relaxing key assumptions. We propose\nthe Human-Aware 3D (HA3D) simulator, which combines dynamic human activities\nwith the Matterport3D dataset, and the Human-Aware Room-to-Room (HA-R2R)\ndataset, extending R2R with human activity descriptions. To tackle HA-VLN\nchallenges, we present the Expert-Supervised Cross-Modal (VLN-CM) and",
        "authors": [
            "Minghan Li",
            "Heng Li",
            "Zhi-Qi Cheng",
            "Yifei Dong",
            "Yuxuan Zhou",
            "Jun-Yan He",
            "Qi Dai",
            "Teruko Mitamura",
            "Alexander G. Hauptmann"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:01:42+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19236v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19236v1",
        "categories": [
            "Artificial Intelligence",
            "Computer Vision and Pattern Recognition",
            "Robotics"
        ]
    },
    {
        "id": 30000066,
        "doi": null,
        "title": "Human-Aware Vision-and-Language Navigation: Bridging Simulation to Reality with Dynamic Human Interactions",
        "abstract": "Vision-and-Language Navigation (VLN) aims to develop embodied agents that\nnavigate based on human instructions. However, current VLN frameworks often\nrely on static environments and optimal expert supervision, limiting their\nreal-world applicability. To address this, we introduce Human-Aware\nVision-and-Language Navigation (HA-VLN), extending traditional VLN by\nincorporating dynamic human activities and relaxing key assumptions. We propose\nthe Human-Aware 3D (HA3D) simulator, which combines dynamic human activities\nwith the Matterport3D dataset, and the Human-Aware Room-to-Room (HA-R2R)\ndataset, extending R2R with human activity descriptions. To tackle HA-VLN\nchallenges, we present the Expert-Supervised Cross-Modal (VLN-CM) and\nNon-Expert-Supervised Decision Transformer (VLN-DT) agents, utilizing\ncross-modal fusion and diverse training strategies for effective navigation in\ndynamic human environments. A comprehensive evaluation, including metrics\nconsidering human activities, and systematic analysis of HA-VLN's unique\nchallenges, underscores the need for further research to enhance HA-VLN agents'\nreal-world robustness and adaptability. Ultimately, this work provides\nbenchmarks and insights for future research on embodied AI and Sim2Real\ntransfer, paving the way for more realistic and applicable VLN systems in\nhuman-populated environments.",
        "chunk-id": 3,
        "chunk": "Non-Expert-Supervised Decision Transformer (VLN-DT) agents, utilizing\ncross-modal fusion and diverse training strategies for effective navigation in\ndynamic human environments. A comprehensive evaluation, including metrics\nconsidering human activities, and systematic analysis of HA-VLN's unique\nchallenges, underscores the need for further research to enhance HA-VLN agents'",
        "authors": [
            "Minghan Li",
            "Heng Li",
            "Zhi-Qi Cheng",
            "Yifei Dong",
            "Yuxuan Zhou",
            "Jun-Yan He",
            "Qi Dai",
            "Teruko Mitamura",
            "Alexander G. Hauptmann"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:01:42+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19236v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19236v1",
        "categories": [
            "Artificial Intelligence",
            "Computer Vision and Pattern Recognition",
            "Robotics"
        ]
    },
    {
        "id": 30000066,
        "doi": null,
        "title": "Human-Aware Vision-and-Language Navigation: Bridging Simulation to Reality with Dynamic Human Interactions",
        "abstract": "Vision-and-Language Navigation (VLN) aims to develop embodied agents that\nnavigate based on human instructions. However, current VLN frameworks often\nrely on static environments and optimal expert supervision, limiting their\nreal-world applicability. To address this, we introduce Human-Aware\nVision-and-Language Navigation (HA-VLN), extending traditional VLN by\nincorporating dynamic human activities and relaxing key assumptions. We propose\nthe Human-Aware 3D (HA3D) simulator, which combines dynamic human activities\nwith the Matterport3D dataset, and the Human-Aware Room-to-Room (HA-R2R)\ndataset, extending R2R with human activity descriptions. To tackle HA-VLN\nchallenges, we present the Expert-Supervised Cross-Modal (VLN-CM) and\nNon-Expert-Supervised Decision Transformer (VLN-DT) agents, utilizing\ncross-modal fusion and diverse training strategies for effective navigation in\ndynamic human environments. A comprehensive evaluation, including metrics\nconsidering human activities, and systematic analysis of HA-VLN's unique\nchallenges, underscores the need for further research to enhance HA-VLN agents'\nreal-world robustness and adaptability. Ultimately, this work provides\nbenchmarks and insights for future research on embodied AI and Sim2Real\ntransfer, paving the way for more realistic and applicable VLN systems in\nhuman-populated environments.",
        "chunk-id": 4,
        "chunk": "real-world robustness and adaptability. Ultimately, this work provides\nbenchmarks and insights for future research on embodied AI and Sim2Real\ntransfer, paving the way for more realistic and applicable VLN systems in\nhuman-populated environments.",
        "authors": [
            "Minghan Li",
            "Heng Li",
            "Zhi-Qi Cheng",
            "Yifei Dong",
            "Yuxuan Zhou",
            "Jun-Yan He",
            "Qi Dai",
            "Teruko Mitamura",
            "Alexander G. Hauptmann"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T15:01:42+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19236v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19236v1",
        "categories": [
            "Artificial Intelligence",
            "Computer Vision and Pattern Recognition",
            "Robotics"
        ]
    },
    {
        "id": 30000067,
        "doi": null,
        "title": "Seeing Is Believing: Black-Box Membership Inference Attacks Against Retrieval Augmented Generation",
        "abstract": "Retrieval-Augmented Generation (RAG) is a state-of-the-art technique that\nenhances Large Language Models (LLMs) by retrieving relevant knowledge from an\nexternal, non-parametric database. This approach aims to mitigate common LLM\nissues such as hallucinations and outdated knowledge. Although existing\nresearch has demonstrated security and privacy vulnerabilities within RAG\nsystems, making them susceptible to attacks like jailbreaks and prompt\ninjections, the security of the RAG system's external databases remains largely\nunderexplored. In this paper, we employ Membership Inference Attacks (MIA) to\ndetermine whether a sample is part of the knowledge database of a RAG system,\nusing only black-box API access. Our core hypothesis posits that if a sample is\na member, it will exhibit significant similarity to the text generated by the\nRAG system. To test this, we compute the cosine similarity and the model's\nperplexity to establish a membership score, thereby building robust features.\nWe then introduce two novel attack strategies: a Threshold-based Attack and a\nMachine Learning-based Attack, designed to accurately identify membership.\nExperimental validation of our methods has achieved a ROC AUC of 82%.",
        "chunk-id": 1,
        "chunk": "Retrieval-Augmented Generation (RAG) is a state-of-the-art technique that\nenhances Large Language Models (LLMs) by retrieving relevant knowledge from an\nexternal, non-parametric database. This approach aims to mitigate common LLM\nissues such as hallucinations and outdated knowledge. Although existing\nresearch has demonstrated security and privacy vulnerabilities within RAG",
        "authors": [
            "Yuying Li",
            "Gaoyang Liu",
            "Yang Yang",
            "Chen Wang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:58:38+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19234v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19234v1",
        "categories": [
            "Cryptography and Security",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 30000067,
        "doi": null,
        "title": "Seeing Is Believing: Black-Box Membership Inference Attacks Against Retrieval Augmented Generation",
        "abstract": "Retrieval-Augmented Generation (RAG) is a state-of-the-art technique that\nenhances Large Language Models (LLMs) by retrieving relevant knowledge from an\nexternal, non-parametric database. This approach aims to mitigate common LLM\nissues such as hallucinations and outdated knowledge. Although existing\nresearch has demonstrated security and privacy vulnerabilities within RAG\nsystems, making them susceptible to attacks like jailbreaks and prompt\ninjections, the security of the RAG system's external databases remains largely\nunderexplored. In this paper, we employ Membership Inference Attacks (MIA) to\ndetermine whether a sample is part of the knowledge database of a RAG system,\nusing only black-box API access. Our core hypothesis posits that if a sample is\na member, it will exhibit significant similarity to the text generated by the\nRAG system. To test this, we compute the cosine similarity and the model's\nperplexity to establish a membership score, thereby building robust features.\nWe then introduce two novel attack strategies: a Threshold-based Attack and a\nMachine Learning-based Attack, designed to accurately identify membership.\nExperimental validation of our methods has achieved a ROC AUC of 82%.",
        "chunk-id": 2,
        "chunk": "systems, making them susceptible to attacks like jailbreaks and prompt\ninjections, the security of the RAG system's external databases remains largely\nunderexplored. In this paper, we employ Membership Inference Attacks (MIA) to\ndetermine whether a sample is part of the knowledge database of a RAG system,",
        "authors": [
            "Yuying Li",
            "Gaoyang Liu",
            "Yang Yang",
            "Chen Wang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:58:38+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19234v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19234v1",
        "categories": [
            "Cryptography and Security",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 30000067,
        "doi": null,
        "title": "Seeing Is Believing: Black-Box Membership Inference Attacks Against Retrieval Augmented Generation",
        "abstract": "Retrieval-Augmented Generation (RAG) is a state-of-the-art technique that\nenhances Large Language Models (LLMs) by retrieving relevant knowledge from an\nexternal, non-parametric database. This approach aims to mitigate common LLM\nissues such as hallucinations and outdated knowledge. Although existing\nresearch has demonstrated security and privacy vulnerabilities within RAG\nsystems, making them susceptible to attacks like jailbreaks and prompt\ninjections, the security of the RAG system's external databases remains largely\nunderexplored. In this paper, we employ Membership Inference Attacks (MIA) to\ndetermine whether a sample is part of the knowledge database of a RAG system,\nusing only black-box API access. Our core hypothesis posits that if a sample is\na member, it will exhibit significant similarity to the text generated by the\nRAG system. To test this, we compute the cosine similarity and the model's\nperplexity to establish a membership score, thereby building robust features.\nWe then introduce two novel attack strategies: a Threshold-based Attack and a\nMachine Learning-based Attack, designed to accurately identify membership.\nExperimental validation of our methods has achieved a ROC AUC of 82%.",
        "chunk-id": 3,
        "chunk": "using only black-box API access. Our core hypothesis posits that if a sample is\na member, it will exhibit significant similarity to the text generated by the\nRAG system. To test this, we compute the cosine similarity and the model's\nperplexity to establish a membership score, thereby building robust features.",
        "authors": [
            "Yuying Li",
            "Gaoyang Liu",
            "Yang Yang",
            "Chen Wang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:58:38+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19234v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19234v1",
        "categories": [
            "Cryptography and Security",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 30000067,
        "doi": null,
        "title": "Seeing Is Believing: Black-Box Membership Inference Attacks Against Retrieval Augmented Generation",
        "abstract": "Retrieval-Augmented Generation (RAG) is a state-of-the-art technique that\nenhances Large Language Models (LLMs) by retrieving relevant knowledge from an\nexternal, non-parametric database. This approach aims to mitigate common LLM\nissues such as hallucinations and outdated knowledge. Although existing\nresearch has demonstrated security and privacy vulnerabilities within RAG\nsystems, making them susceptible to attacks like jailbreaks and prompt\ninjections, the security of the RAG system's external databases remains largely\nunderexplored. In this paper, we employ Membership Inference Attacks (MIA) to\ndetermine whether a sample is part of the knowledge database of a RAG system,\nusing only black-box API access. Our core hypothesis posits that if a sample is\na member, it will exhibit significant similarity to the text generated by the\nRAG system. To test this, we compute the cosine similarity and the model's\nperplexity to establish a membership score, thereby building robust features.\nWe then introduce two novel attack strategies: a Threshold-based Attack and a\nMachine Learning-based Attack, designed to accurately identify membership.\nExperimental validation of our methods has achieved a ROC AUC of 82%.",
        "chunk-id": 4,
        "chunk": "We then introduce two novel attack strategies: a Threshold-based Attack and a\nMachine Learning-based Attack, designed to accurately identify membership.\nExperimental validation of our methods has achieved a ROC AUC of 82%.",
        "authors": [
            "Yuying Li",
            "Gaoyang Liu",
            "Yang Yang",
            "Chen Wang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:58:38+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19234v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19234v1",
        "categories": [
            "Cryptography and Security",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 30000068,
        "doi": null,
        "title": "RuBLiMP: Russian Benchmark of Linguistic Minimal Pairs",
        "abstract": "Minimal pairs are a well-established approach to evaluating the grammatical\nknowledge of language models. However, existing resources for minimal pairs\naddress a limited number of languages and lack diversity of language-specific\ngrammatical phenomena. This paper introduces the Russian Benchmark of\nLinguistic Minimal Pairs (RuBLiMP), which includes 45k pairs of sentences that\ndiffer in grammaticality and isolate a morphological, syntactic, or semantic\nphenomenon. In contrast to existing benchmarks of linguistic minimal pairs,\nRuBLiMP is created by applying linguistic perturbations to automatically\nannotated sentences from open text corpora and carefully curating test data. We\ndescribe the data collection protocol and present the results of evaluating 25\nlanguage models in various scenarios. We find that the widely used language\nmodels for Russian are sensitive to morphological and agreement-oriented\ncontrasts but fall behind humans on phenomena requiring understanding of\nstructural relations, negation, transitivity, and tense. RuBLiMP, the codebase,\nand other materials are publicly available.",
        "chunk-id": 1,
        "chunk": "Minimal pairs are a well-established approach to evaluating the grammatical\nknowledge of language models. However, existing resources for minimal pairs\naddress a limited number of languages and lack diversity of language-specific\ngrammatical phenomena. This paper introduces the Russian Benchmark of\nLinguistic Minimal Pairs (RuBLiMP), which includes 45k pairs of sentences that",
        "authors": [
            "Ekaterina Taktasheva",
            "Maxim Bazhukov",
            "Kirill Koncha",
            "Alena Fenogenova",
            "Ekaterina Artemova"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:55:19+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19232v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19232v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 30000068,
        "doi": null,
        "title": "RuBLiMP: Russian Benchmark of Linguistic Minimal Pairs",
        "abstract": "Minimal pairs are a well-established approach to evaluating the grammatical\nknowledge of language models. However, existing resources for minimal pairs\naddress a limited number of languages and lack diversity of language-specific\ngrammatical phenomena. This paper introduces the Russian Benchmark of\nLinguistic Minimal Pairs (RuBLiMP), which includes 45k pairs of sentences that\ndiffer in grammaticality and isolate a morphological, syntactic, or semantic\nphenomenon. In contrast to existing benchmarks of linguistic minimal pairs,\nRuBLiMP is created by applying linguistic perturbations to automatically\nannotated sentences from open text corpora and carefully curating test data. We\ndescribe the data collection protocol and present the results of evaluating 25\nlanguage models in various scenarios. We find that the widely used language\nmodels for Russian are sensitive to morphological and agreement-oriented\ncontrasts but fall behind humans on phenomena requiring understanding of\nstructural relations, negation, transitivity, and tense. RuBLiMP, the codebase,\nand other materials are publicly available.",
        "chunk-id": 2,
        "chunk": "differ in grammaticality and isolate a morphological, syntactic, or semantic\nphenomenon. In contrast to existing benchmarks of linguistic minimal pairs,\nRuBLiMP is created by applying linguistic perturbations to automatically\nannotated sentences from open text corpora and carefully curating test data. We\ndescribe the data collection protocol and present the results of evaluating 25",
        "authors": [
            "Ekaterina Taktasheva",
            "Maxim Bazhukov",
            "Kirill Koncha",
            "Alena Fenogenova",
            "Ekaterina Artemova"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:55:19+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19232v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19232v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 30000068,
        "doi": null,
        "title": "RuBLiMP: Russian Benchmark of Linguistic Minimal Pairs",
        "abstract": "Minimal pairs are a well-established approach to evaluating the grammatical\nknowledge of language models. However, existing resources for minimal pairs\naddress a limited number of languages and lack diversity of language-specific\ngrammatical phenomena. This paper introduces the Russian Benchmark of\nLinguistic Minimal Pairs (RuBLiMP), which includes 45k pairs of sentences that\ndiffer in grammaticality and isolate a morphological, syntactic, or semantic\nphenomenon. In contrast to existing benchmarks of linguistic minimal pairs,\nRuBLiMP is created by applying linguistic perturbations to automatically\nannotated sentences from open text corpora and carefully curating test data. We\ndescribe the data collection protocol and present the results of evaluating 25\nlanguage models in various scenarios. We find that the widely used language\nmodels for Russian are sensitive to morphological and agreement-oriented\ncontrasts but fall behind humans on phenomena requiring understanding of\nstructural relations, negation, transitivity, and tense. RuBLiMP, the codebase,\nand other materials are publicly available.",
        "chunk-id": 3,
        "chunk": "language models in various scenarios. We find that the widely used language\nmodels for Russian are sensitive to morphological and agreement-oriented\ncontrasts but fall behind humans on phenomena requiring understanding of\nstructural relations, negation, transitivity, and tense. RuBLiMP, the codebase,\nand other materials are publicly available.",
        "authors": [
            "Ekaterina Taktasheva",
            "Maxim Bazhukov",
            "Kirill Koncha",
            "Alena Fenogenova",
            "Ekaterina Artemova"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:55:19+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19232v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19232v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 30000069,
        "doi": null,
        "title": "Spiking Convolutional Neural Networks for Text Classification",
        "abstract": "Spiking neural networks (SNNs) offer a promising pathway to implement deep\nneural networks (DNNs) in a more energy-efficient manner since their neurons\nare sparsely activated and inferences are event-driven. However, there have\nbeen very few works that have demonstrated the efficacy of SNNs in language\ntasks partially because it is non-trivial to represent words in the forms of\nspikes and to deal with variable-length texts by SNNs. This work presents a\n\"conversion + fine-tuning\" two-step method for training SNNs for text\nclassification and proposes a simple but effective way to encode pre-trained\nword embeddings as spike trains. We show empirically that after fine-tuning\nwith surrogate gradients, the converted SNNs achieve comparable results to\ntheir DNN counterparts with much less energy consumption across multiple\ndatasets for both English and Chinese. We also show that such SNNs are more\nrobust to adversarial attacks than DNNs.",
        "chunk-id": 1,
        "chunk": "Spiking neural networks (SNNs) offer a promising pathway to implement deep\nneural networks (DNNs) in a more energy-efficient manner since their neurons\nare sparsely activated and inferences are event-driven. However, there have\nbeen very few works that have demonstrated the efficacy of SNNs in language\ntasks partially because it is non-trivial to represent words in the forms of",
        "authors": [
            "Changze Lv",
            "Jianhan Xu",
            "Xiaoqing Zheng"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:54:27+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19230v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19230v1",
        "categories": [
            "Neural and Evolutionary Computing",
            "Computation and Language"
        ]
    },
    {
        "id": 30000069,
        "doi": null,
        "title": "Spiking Convolutional Neural Networks for Text Classification",
        "abstract": "Spiking neural networks (SNNs) offer a promising pathway to implement deep\nneural networks (DNNs) in a more energy-efficient manner since their neurons\nare sparsely activated and inferences are event-driven. However, there have\nbeen very few works that have demonstrated the efficacy of SNNs in language\ntasks partially because it is non-trivial to represent words in the forms of\nspikes and to deal with variable-length texts by SNNs. This work presents a\n\"conversion + fine-tuning\" two-step method for training SNNs for text\nclassification and proposes a simple but effective way to encode pre-trained\nword embeddings as spike trains. We show empirically that after fine-tuning\nwith surrogate gradients, the converted SNNs achieve comparable results to\ntheir DNN counterparts with much less energy consumption across multiple\ndatasets for both English and Chinese. We also show that such SNNs are more\nrobust to adversarial attacks than DNNs.",
        "chunk-id": 2,
        "chunk": "spikes and to deal with variable-length texts by SNNs. This work presents a\n\"conversion + fine-tuning\" two-step method for training SNNs for text\nclassification and proposes a simple but effective way to encode pre-trained\nword embeddings as spike trains. We show empirically that after fine-tuning\nwith surrogate gradients, the converted SNNs achieve comparable results to",
        "authors": [
            "Changze Lv",
            "Jianhan Xu",
            "Xiaoqing Zheng"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:54:27+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19230v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19230v1",
        "categories": [
            "Neural and Evolutionary Computing",
            "Computation and Language"
        ]
    },
    {
        "id": 30000069,
        "doi": null,
        "title": "Spiking Convolutional Neural Networks for Text Classification",
        "abstract": "Spiking neural networks (SNNs) offer a promising pathway to implement deep\nneural networks (DNNs) in a more energy-efficient manner since their neurons\nare sparsely activated and inferences are event-driven. However, there have\nbeen very few works that have demonstrated the efficacy of SNNs in language\ntasks partially because it is non-trivial to represent words in the forms of\nspikes and to deal with variable-length texts by SNNs. This work presents a\n\"conversion + fine-tuning\" two-step method for training SNNs for text\nclassification and proposes a simple but effective way to encode pre-trained\nword embeddings as spike trains. We show empirically that after fine-tuning\nwith surrogate gradients, the converted SNNs achieve comparable results to\ntheir DNN counterparts with much less energy consumption across multiple\ndatasets for both English and Chinese. We also show that such SNNs are more\nrobust to adversarial attacks than DNNs.",
        "chunk-id": 3,
        "chunk": "their DNN counterparts with much less energy consumption across multiple\ndatasets for both English and Chinese. We also show that such SNNs are more\nrobust to adversarial attacks than DNNs.",
        "authors": [
            "Changze Lv",
            "Jianhan Xu",
            "Xiaoqing Zheng"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:54:27+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19230v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19230v1",
        "categories": [
            "Neural and Evolutionary Computing",
            "Computation and Language"
        ]
    },
    {
        "id": 30000070,
        "doi": null,
        "title": "Tools Fail: Detecting Silent Errors in Faulty Tools",
        "abstract": "Tools have become a mainstay of LLMs, allowing them to retrieve knowledge not\nin their weights, to perform tasks on the web, and even to control robots.\nHowever, most ontologies and surveys of tool-use have assumed the core\nchallenge for LLMs is choosing the tool. Instead, we introduce a framework for\ntools more broadly which guides us to explore a model's ability to detect\n\"silent\" tool errors, and reflect on how to plan. This more directly aligns\nwith the increasingly popular use of models as tools. We provide an initial\napproach to failure recovery with promising results both on a controlled\ncalculator setting and embodied agent planning.",
        "chunk-id": 1,
        "chunk": "Tools have become a mainstay of LLMs, allowing them to retrieve knowledge not\nin their weights, to perform tasks on the web, and even to control robots.\nHowever, most ontologies and surveys of tool-use have assumed the core\nchallenge for LLMs is choosing the tool. Instead, we introduce a framework for\ntools more broadly which guides us to explore a model's ability to detect",
        "authors": [
            "Jimin Sun",
            "So Yeon Min",
            "Yingshan Chang",
            "Yonatan Bisk"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:52:34+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19228v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19228v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Machine Learning"
        ]
    },
    {
        "id": 30000070,
        "doi": null,
        "title": "Tools Fail: Detecting Silent Errors in Faulty Tools",
        "abstract": "Tools have become a mainstay of LLMs, allowing them to retrieve knowledge not\nin their weights, to perform tasks on the web, and even to control robots.\nHowever, most ontologies and surveys of tool-use have assumed the core\nchallenge for LLMs is choosing the tool. Instead, we introduce a framework for\ntools more broadly which guides us to explore a model's ability to detect\n\"silent\" tool errors, and reflect on how to plan. This more directly aligns\nwith the increasingly popular use of models as tools. We provide an initial\napproach to failure recovery with promising results both on a controlled\ncalculator setting and embodied agent planning.",
        "chunk-id": 2,
        "chunk": "\"silent\" tool errors, and reflect on how to plan. This more directly aligns\nwith the increasingly popular use of models as tools. We provide an initial\napproach to failure recovery with promising results both on a controlled\ncalculator setting and embodied agent planning.",
        "authors": [
            "Jimin Sun",
            "So Yeon Min",
            "Yingshan Chang",
            "Yonatan Bisk"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:52:34+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19228v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19228v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Machine Learning"
        ]
    },
    {
        "id": 30000071,
        "doi": null,
        "title": "Aligning Teacher with Student Preferences for Tailored Training Data Generation",
        "abstract": "Large Language Models (LLMs) have shown significant promise as copilots in\nvarious tasks. Local deployment of LLMs on edge devices is necessary when\nhandling privacy-sensitive data or latency-sensitive tasks. The computational\nconstraints of such devices make direct deployment of powerful large-scale LLMs\nimpractical, necessitating the Knowledge Distillation from large-scale models\nto lightweight models. Lots of work has been done to elicit diversity and\nquality training examples from LLMs, but little attention has been paid to\naligning teacher instructional content based on student preferences, akin to\n\"responsive teaching\" in pedagogy. Thus, we propose ARTE, dubbed Aligning\nTeacheR with StudenT PreferencEs, a framework that aligns the teacher model\nwith student preferences to generate tailored training examples for Knowledge\nDistillation. Specifically, we elicit draft questions and rationales from the\nteacher model, then collect student preferences on these questions and\nrationales using students' performance with in-context learning as a proxy, and\nfinally align the teacher model with student preferences. In the end, we repeat\nthe first step with the aligned teacher model to elicit tailored training\nexamples for the student model on the target task. Extensive experiments on\nacademic benchmarks demonstrate the superiority of ARTE over existing\ninstruction-tuning datasets distilled from powerful LLMs. Moreover, we\nthoroughly investigate the generalization of ARTE, including the generalization\nof fine-tuned student models in reasoning ability and the generalization of\naligned teacher models to generate tailored training data across tasks and\nstudents. In summary, our contributions lie in proposing a novel framework for\ntailored training example generation, demonstrating its efficacy in\nexperiments, and investigating the generalization of both student & aligned\nteacher models in ARTE.",
        "chunk-id": 1,
        "chunk": "Large Language Models (LLMs) have shown significant promise as copilots in\nvarious tasks. Local deployment of LLMs on edge devices is necessary when\nhandling privacy-sensitive data or latency-sensitive tasks. The computational\nconstraints of such devices make direct deployment of powerful large-scale LLMs\nimpractical, necessitating the Knowledge Distillation from large-scale models",
        "authors": [
            "Yantao Liu",
            "Zhao Zhang",
            "Zijun Yao",
            "Shulin Cao",
            "Lei Hou",
            "Juanzi Li"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:51:17+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19227v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19227v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 30000071,
        "doi": null,
        "title": "Aligning Teacher with Student Preferences for Tailored Training Data Generation",
        "abstract": "Large Language Models (LLMs) have shown significant promise as copilots in\nvarious tasks. Local deployment of LLMs on edge devices is necessary when\nhandling privacy-sensitive data or latency-sensitive tasks. The computational\nconstraints of such devices make direct deployment of powerful large-scale LLMs\nimpractical, necessitating the Knowledge Distillation from large-scale models\nto lightweight models. Lots of work has been done to elicit diversity and\nquality training examples from LLMs, but little attention has been paid to\naligning teacher instructional content based on student preferences, akin to\n\"responsive teaching\" in pedagogy. Thus, we propose ARTE, dubbed Aligning\nTeacheR with StudenT PreferencEs, a framework that aligns the teacher model\nwith student preferences to generate tailored training examples for Knowledge\nDistillation. Specifically, we elicit draft questions and rationales from the\nteacher model, then collect student preferences on these questions and\nrationales using students' performance with in-context learning as a proxy, and\nfinally align the teacher model with student preferences. In the end, we repeat\nthe first step with the aligned teacher model to elicit tailored training\nexamples for the student model on the target task. Extensive experiments on\nacademic benchmarks demonstrate the superiority of ARTE over existing\ninstruction-tuning datasets distilled from powerful LLMs. Moreover, we\nthoroughly investigate the generalization of ARTE, including the generalization\nof fine-tuned student models in reasoning ability and the generalization of\naligned teacher models to generate tailored training data across tasks and\nstudents. In summary, our contributions lie in proposing a novel framework for\ntailored training example generation, demonstrating its efficacy in\nexperiments, and investigating the generalization of both student & aligned\nteacher models in ARTE.",
        "chunk-id": 2,
        "chunk": "to lightweight models. Lots of work has been done to elicit diversity and\nquality training examples from LLMs, but little attention has been paid to\naligning teacher instructional content based on student preferences, akin to\n\"responsive teaching\" in pedagogy. Thus, we propose ARTE, dubbed Aligning\nTeacheR with StudenT PreferencEs, a framework that aligns the teacher model",
        "authors": [
            "Yantao Liu",
            "Zhao Zhang",
            "Zijun Yao",
            "Shulin Cao",
            "Lei Hou",
            "Juanzi Li"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:51:17+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19227v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19227v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 30000071,
        "doi": null,
        "title": "Aligning Teacher with Student Preferences for Tailored Training Data Generation",
        "abstract": "Large Language Models (LLMs) have shown significant promise as copilots in\nvarious tasks. Local deployment of LLMs on edge devices is necessary when\nhandling privacy-sensitive data or latency-sensitive tasks. The computational\nconstraints of such devices make direct deployment of powerful large-scale LLMs\nimpractical, necessitating the Knowledge Distillation from large-scale models\nto lightweight models. Lots of work has been done to elicit diversity and\nquality training examples from LLMs, but little attention has been paid to\naligning teacher instructional content based on student preferences, akin to\n\"responsive teaching\" in pedagogy. Thus, we propose ARTE, dubbed Aligning\nTeacheR with StudenT PreferencEs, a framework that aligns the teacher model\nwith student preferences to generate tailored training examples for Knowledge\nDistillation. Specifically, we elicit draft questions and rationales from the\nteacher model, then collect student preferences on these questions and\nrationales using students' performance with in-context learning as a proxy, and\nfinally align the teacher model with student preferences. In the end, we repeat\nthe first step with the aligned teacher model to elicit tailored training\nexamples for the student model on the target task. Extensive experiments on\nacademic benchmarks demonstrate the superiority of ARTE over existing\ninstruction-tuning datasets distilled from powerful LLMs. Moreover, we\nthoroughly investigate the generalization of ARTE, including the generalization\nof fine-tuned student models in reasoning ability and the generalization of\naligned teacher models to generate tailored training data across tasks and\nstudents. In summary, our contributions lie in proposing a novel framework for\ntailored training example generation, demonstrating its efficacy in\nexperiments, and investigating the generalization of both student & aligned\nteacher models in ARTE.",
        "chunk-id": 3,
        "chunk": "with student preferences to generate tailored training examples for Knowledge\nDistillation. Specifically, we elicit draft questions and rationales from the\nteacher model, then collect student preferences on these questions and\nrationales using students' performance with in-context learning as a proxy, and",
        "authors": [
            "Yantao Liu",
            "Zhao Zhang",
            "Zijun Yao",
            "Shulin Cao",
            "Lei Hou",
            "Juanzi Li"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:51:17+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19227v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19227v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 30000071,
        "doi": null,
        "title": "Aligning Teacher with Student Preferences for Tailored Training Data Generation",
        "abstract": "Large Language Models (LLMs) have shown significant promise as copilots in\nvarious tasks. Local deployment of LLMs on edge devices is necessary when\nhandling privacy-sensitive data or latency-sensitive tasks. The computational\nconstraints of such devices make direct deployment of powerful large-scale LLMs\nimpractical, necessitating the Knowledge Distillation from large-scale models\nto lightweight models. Lots of work has been done to elicit diversity and\nquality training examples from LLMs, but little attention has been paid to\naligning teacher instructional content based on student preferences, akin to\n\"responsive teaching\" in pedagogy. Thus, we propose ARTE, dubbed Aligning\nTeacheR with StudenT PreferencEs, a framework that aligns the teacher model\nwith student preferences to generate tailored training examples for Knowledge\nDistillation. Specifically, we elicit draft questions and rationales from the\nteacher model, then collect student preferences on these questions and\nrationales using students' performance with in-context learning as a proxy, and\nfinally align the teacher model with student preferences. In the end, we repeat\nthe first step with the aligned teacher model to elicit tailored training\nexamples for the student model on the target task. Extensive experiments on\nacademic benchmarks demonstrate the superiority of ARTE over existing\ninstruction-tuning datasets distilled from powerful LLMs. Moreover, we\nthoroughly investigate the generalization of ARTE, including the generalization\nof fine-tuned student models in reasoning ability and the generalization of\naligned teacher models to generate tailored training data across tasks and\nstudents. In summary, our contributions lie in proposing a novel framework for\ntailored training example generation, demonstrating its efficacy in\nexperiments, and investigating the generalization of both student & aligned\nteacher models in ARTE.",
        "chunk-id": 4,
        "chunk": "finally align the teacher model with student preferences. In the end, we repeat\nthe first step with the aligned teacher model to elicit tailored training\nexamples for the student model on the target task. Extensive experiments on\nacademic benchmarks demonstrate the superiority of ARTE over existing\ninstruction-tuning datasets distilled from powerful LLMs. Moreover, we",
        "authors": [
            "Yantao Liu",
            "Zhao Zhang",
            "Zijun Yao",
            "Shulin Cao",
            "Lei Hou",
            "Juanzi Li"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:51:17+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19227v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19227v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 30000071,
        "doi": null,
        "title": "Aligning Teacher with Student Preferences for Tailored Training Data Generation",
        "abstract": "Large Language Models (LLMs) have shown significant promise as copilots in\nvarious tasks. Local deployment of LLMs on edge devices is necessary when\nhandling privacy-sensitive data or latency-sensitive tasks. The computational\nconstraints of such devices make direct deployment of powerful large-scale LLMs\nimpractical, necessitating the Knowledge Distillation from large-scale models\nto lightweight models. Lots of work has been done to elicit diversity and\nquality training examples from LLMs, but little attention has been paid to\naligning teacher instructional content based on student preferences, akin to\n\"responsive teaching\" in pedagogy. Thus, we propose ARTE, dubbed Aligning\nTeacheR with StudenT PreferencEs, a framework that aligns the teacher model\nwith student preferences to generate tailored training examples for Knowledge\nDistillation. Specifically, we elicit draft questions and rationales from the\nteacher model, then collect student preferences on these questions and\nrationales using students' performance with in-context learning as a proxy, and\nfinally align the teacher model with student preferences. In the end, we repeat\nthe first step with the aligned teacher model to elicit tailored training\nexamples for the student model on the target task. Extensive experiments on\nacademic benchmarks demonstrate the superiority of ARTE over existing\ninstruction-tuning datasets distilled from powerful LLMs. Moreover, we\nthoroughly investigate the generalization of ARTE, including the generalization\nof fine-tuned student models in reasoning ability and the generalization of\naligned teacher models to generate tailored training data across tasks and\nstudents. In summary, our contributions lie in proposing a novel framework for\ntailored training example generation, demonstrating its efficacy in\nexperiments, and investigating the generalization of both student & aligned\nteacher models in ARTE.",
        "chunk-id": 5,
        "chunk": "thoroughly investigate the generalization of ARTE, including the generalization\nof fine-tuned student models in reasoning ability and the generalization of\naligned teacher models to generate tailored training data across tasks and\nstudents. In summary, our contributions lie in proposing a novel framework for\ntailored training example generation, demonstrating its efficacy in",
        "authors": [
            "Yantao Liu",
            "Zhao Zhang",
            "Zijun Yao",
            "Shulin Cao",
            "Lei Hou",
            "Juanzi Li"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:51:17+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19227v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19227v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 30000071,
        "doi": null,
        "title": "Aligning Teacher with Student Preferences for Tailored Training Data Generation",
        "abstract": "Large Language Models (LLMs) have shown significant promise as copilots in\nvarious tasks. Local deployment of LLMs on edge devices is necessary when\nhandling privacy-sensitive data or latency-sensitive tasks. The computational\nconstraints of such devices make direct deployment of powerful large-scale LLMs\nimpractical, necessitating the Knowledge Distillation from large-scale models\nto lightweight models. Lots of work has been done to elicit diversity and\nquality training examples from LLMs, but little attention has been paid to\naligning teacher instructional content based on student preferences, akin to\n\"responsive teaching\" in pedagogy. Thus, we propose ARTE, dubbed Aligning\nTeacheR with StudenT PreferencEs, a framework that aligns the teacher model\nwith student preferences to generate tailored training examples for Knowledge\nDistillation. Specifically, we elicit draft questions and rationales from the\nteacher model, then collect student preferences on these questions and\nrationales using students' performance with in-context learning as a proxy, and\nfinally align the teacher model with student preferences. In the end, we repeat\nthe first step with the aligned teacher model to elicit tailored training\nexamples for the student model on the target task. Extensive experiments on\nacademic benchmarks demonstrate the superiority of ARTE over existing\ninstruction-tuning datasets distilled from powerful LLMs. Moreover, we\nthoroughly investigate the generalization of ARTE, including the generalization\nof fine-tuned student models in reasoning ability and the generalization of\naligned teacher models to generate tailored training data across tasks and\nstudents. In summary, our contributions lie in proposing a novel framework for\ntailored training example generation, demonstrating its efficacy in\nexperiments, and investigating the generalization of both student & aligned\nteacher models in ARTE.",
        "chunk-id": 6,
        "chunk": "experiments, and investigating the generalization of both student & aligned\nteacher models in ARTE.",
        "authors": [
            "Yantao Liu",
            "Zhao Zhang",
            "Zijun Yao",
            "Shulin Cao",
            "Lei Hou",
            "Juanzi Li"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:51:17+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19227v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19227v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 30000072,
        "doi": null,
        "title": "Simulating Classroom Education with LLM-Empowered Agents",
        "abstract": "Large language models (LLMs) have been employed in various intelligent\neducational tasks to assist teaching. While preliminary explorations have\nfocused on independent LLM-empowered agents for specific educational tasks, the\npotential for LLMs within a multi-agent collaborative framework to simulate a\nclassroom with real user participation remains unexplored. In this work, we\npropose SimClass, a multi-agent classroom simulation framework involving user\nparticipation. We recognize representative class roles and introduce a novel\nclass control mechanism for automatic classroom teaching, and conduct user\nexperiments in two real-world courses. Utilizing the Flanders Interactive\nAnalysis System and Community of Inquiry theoretical frame works from\neducational analysis, we demonstrate that LLMs can simulate traditional\nclassroom interaction patterns effectively while enhancing user's experience.\nWe also observe emergent group behaviors among agents in SimClass, where agents\ncollaborate to create enlivening interactions in classrooms to improve user\nlearning process. We hope this work pioneers the application of LLM-empowered\nmulti-agent systems in virtual classroom teaching.",
        "chunk-id": 1,
        "chunk": "Large language models (LLMs) have been employed in various intelligent\neducational tasks to assist teaching. While preliminary explorations have\nfocused on independent LLM-empowered agents for specific educational tasks, the\npotential for LLMs within a multi-agent collaborative framework to simulate a\nclassroom with real user participation remains unexplored. In this work, we",
        "authors": [
            "Zheyuan Zhang",
            "Daniel Zhang-Li",
            "Jifan Yu",
            "Linlu Gong",
            "Jinchang Zhou",
            "Zhiyuan Liu",
            "Lei Hou",
            "Juanzi Li"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:51:07+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19226v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19226v1",
        "categories": [
            "Computation and Language",
            "Human-Computer Interaction"
        ]
    },
    {
        "id": 30000072,
        "doi": null,
        "title": "Simulating Classroom Education with LLM-Empowered Agents",
        "abstract": "Large language models (LLMs) have been employed in various intelligent\neducational tasks to assist teaching. While preliminary explorations have\nfocused on independent LLM-empowered agents for specific educational tasks, the\npotential for LLMs within a multi-agent collaborative framework to simulate a\nclassroom with real user participation remains unexplored. In this work, we\npropose SimClass, a multi-agent classroom simulation framework involving user\nparticipation. We recognize representative class roles and introduce a novel\nclass control mechanism for automatic classroom teaching, and conduct user\nexperiments in two real-world courses. Utilizing the Flanders Interactive\nAnalysis System and Community of Inquiry theoretical frame works from\neducational analysis, we demonstrate that LLMs can simulate traditional\nclassroom interaction patterns effectively while enhancing user's experience.\nWe also observe emergent group behaviors among agents in SimClass, where agents\ncollaborate to create enlivening interactions in classrooms to improve user\nlearning process. We hope this work pioneers the application of LLM-empowered\nmulti-agent systems in virtual classroom teaching.",
        "chunk-id": 2,
        "chunk": "propose SimClass, a multi-agent classroom simulation framework involving user\nparticipation. We recognize representative class roles and introduce a novel\nclass control mechanism for automatic classroom teaching, and conduct user\nexperiments in two real-world courses. Utilizing the Flanders Interactive\nAnalysis System and Community of Inquiry theoretical frame works from",
        "authors": [
            "Zheyuan Zhang",
            "Daniel Zhang-Li",
            "Jifan Yu",
            "Linlu Gong",
            "Jinchang Zhou",
            "Zhiyuan Liu",
            "Lei Hou",
            "Juanzi Li"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:51:07+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19226v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19226v1",
        "categories": [
            "Computation and Language",
            "Human-Computer Interaction"
        ]
    },
    {
        "id": 30000072,
        "doi": null,
        "title": "Simulating Classroom Education with LLM-Empowered Agents",
        "abstract": "Large language models (LLMs) have been employed in various intelligent\neducational tasks to assist teaching. While preliminary explorations have\nfocused on independent LLM-empowered agents for specific educational tasks, the\npotential for LLMs within a multi-agent collaborative framework to simulate a\nclassroom with real user participation remains unexplored. In this work, we\npropose SimClass, a multi-agent classroom simulation framework involving user\nparticipation. We recognize representative class roles and introduce a novel\nclass control mechanism for automatic classroom teaching, and conduct user\nexperiments in two real-world courses. Utilizing the Flanders Interactive\nAnalysis System and Community of Inquiry theoretical frame works from\neducational analysis, we demonstrate that LLMs can simulate traditional\nclassroom interaction patterns effectively while enhancing user's experience.\nWe also observe emergent group behaviors among agents in SimClass, where agents\ncollaborate to create enlivening interactions in classrooms to improve user\nlearning process. We hope this work pioneers the application of LLM-empowered\nmulti-agent systems in virtual classroom teaching.",
        "chunk-id": 3,
        "chunk": "educational analysis, we demonstrate that LLMs can simulate traditional\nclassroom interaction patterns effectively while enhancing user's experience.\nWe also observe emergent group behaviors among agents in SimClass, where agents\ncollaborate to create enlivening interactions in classrooms to improve user\nlearning process. We hope this work pioneers the application of LLM-empowered",
        "authors": [
            "Zheyuan Zhang",
            "Daniel Zhang-Li",
            "Jifan Yu",
            "Linlu Gong",
            "Jinchang Zhou",
            "Zhiyuan Liu",
            "Lei Hou",
            "Juanzi Li"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:51:07+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19226v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19226v1",
        "categories": [
            "Computation and Language",
            "Human-Computer Interaction"
        ]
    },
    {
        "id": 30000072,
        "doi": null,
        "title": "Simulating Classroom Education with LLM-Empowered Agents",
        "abstract": "Large language models (LLMs) have been employed in various intelligent\neducational tasks to assist teaching. While preliminary explorations have\nfocused on independent LLM-empowered agents for specific educational tasks, the\npotential for LLMs within a multi-agent collaborative framework to simulate a\nclassroom with real user participation remains unexplored. In this work, we\npropose SimClass, a multi-agent classroom simulation framework involving user\nparticipation. We recognize representative class roles and introduce a novel\nclass control mechanism for automatic classroom teaching, and conduct user\nexperiments in two real-world courses. Utilizing the Flanders Interactive\nAnalysis System and Community of Inquiry theoretical frame works from\neducational analysis, we demonstrate that LLMs can simulate traditional\nclassroom interaction patterns effectively while enhancing user's experience.\nWe also observe emergent group behaviors among agents in SimClass, where agents\ncollaborate to create enlivening interactions in classrooms to improve user\nlearning process. We hope this work pioneers the application of LLM-empowered\nmulti-agent systems in virtual classroom teaching.",
        "chunk-id": 4,
        "chunk": "multi-agent systems in virtual classroom teaching.",
        "authors": [
            "Zheyuan Zhang",
            "Daniel Zhang-Li",
            "Jifan Yu",
            "Linlu Gong",
            "Jinchang Zhou",
            "Zhiyuan Liu",
            "Lei Hou",
            "Juanzi Li"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:51:07+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19226v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19226v1",
        "categories": [
            "Computation and Language",
            "Human-Computer Interaction"
        ]
    },
    {
        "id": 30000073,
        "doi": null,
        "title": "ProtoGMM: Multi-prototype Gaussian-Mixture-based Domain Adaptation Model for Semantic Segmentation",
        "abstract": "Domain adaptive semantic segmentation aims to generate accurate and dense\npredictions for an unlabeled target domain by leveraging a supervised model\ntrained on a labeled source domain. The prevalent self-training approach\ninvolves retraining the dense discriminative classifier of $p(class|pixel\nfeature)$ using the pseudo-labels from the target domain. While many methods\nfocus on mitigating the issue of noisy pseudo-labels, they often overlook the\nunderlying data distribution p(pixel feature|class) in both the source and\ntarget domains. To address this limitation, we propose the multi-prototype\nGaussian-Mixture-based (ProtoGMM) model, which incorporates the GMM into\ncontrastive losses to perform guided contrastive learning. Contrastive losses\nare commonly executed in the literature using memory banks, which can lead to\nclass biases due to underrepresented classes. Furthermore, memory banks often\nhave fixed capacities, potentially restricting the model's ability to capture\ndiverse representations of the target/source domains. An alternative approach\nis to use global class prototypes (i.e. averaged features per category).\nHowever, the global prototypes are based on the unimodal distribution\nassumption per class, disregarding within-class variation. To address these\nchallenges, we propose the ProtoGMM model. This novel approach involves\nestimating the underlying multi-prototype source distribution by utilizing the\nGMM on the feature space of the source samples. The components of the GMM model\nact as representative prototypes. To achieve increased intra-class semantic\nsimilarity, decreased inter-class similarity, and domain alignment between the\nsource and target domains, we employ multi-prototype contrastive learning\nbetween source distribution and target samples. The experiments show the\neffectiveness of our method on UDA benchmarks.",
        "chunk-id": 1,
        "chunk": "Domain adaptive semantic segmentation aims to generate accurate and dense\npredictions for an unlabeled target domain by leveraging a supervised model\ntrained on a labeled source domain. The prevalent self-training approach\ninvolves retraining the dense discriminative classifier of $p(class|pixel\nfeature)$ using the pseudo-labels from the target domain. While many methods",
        "authors": [
            "Nazanin Moradinasab",
            "Laura S. Shankman",
            "Rebecca A. Deaton",
            "Gary K. Owens",
            "Donald E. Brown"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:50:50+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19225v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19225v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000073,
        "doi": null,
        "title": "ProtoGMM: Multi-prototype Gaussian-Mixture-based Domain Adaptation Model for Semantic Segmentation",
        "abstract": "Domain adaptive semantic segmentation aims to generate accurate and dense\npredictions for an unlabeled target domain by leveraging a supervised model\ntrained on a labeled source domain. The prevalent self-training approach\ninvolves retraining the dense discriminative classifier of $p(class|pixel\nfeature)$ using the pseudo-labels from the target domain. While many methods\nfocus on mitigating the issue of noisy pseudo-labels, they often overlook the\nunderlying data distribution p(pixel feature|class) in both the source and\ntarget domains. To address this limitation, we propose the multi-prototype\nGaussian-Mixture-based (ProtoGMM) model, which incorporates the GMM into\ncontrastive losses to perform guided contrastive learning. Contrastive losses\nare commonly executed in the literature using memory banks, which can lead to\nclass biases due to underrepresented classes. Furthermore, memory banks often\nhave fixed capacities, potentially restricting the model's ability to capture\ndiverse representations of the target/source domains. An alternative approach\nis to use global class prototypes (i.e. averaged features per category).\nHowever, the global prototypes are based on the unimodal distribution\nassumption per class, disregarding within-class variation. To address these\nchallenges, we propose the ProtoGMM model. This novel approach involves\nestimating the underlying multi-prototype source distribution by utilizing the\nGMM on the feature space of the source samples. The components of the GMM model\nact as representative prototypes. To achieve increased intra-class semantic\nsimilarity, decreased inter-class similarity, and domain alignment between the\nsource and target domains, we employ multi-prototype contrastive learning\nbetween source distribution and target samples. The experiments show the\neffectiveness of our method on UDA benchmarks.",
        "chunk-id": 2,
        "chunk": "focus on mitigating the issue of noisy pseudo-labels, they often overlook the\nunderlying data distribution p(pixel feature|class) in both the source and\ntarget domains. To address this limitation, we propose the multi-prototype\nGaussian-Mixture-based (ProtoGMM) model, which incorporates the GMM into\ncontrastive losses to perform guided contrastive learning. Contrastive losses",
        "authors": [
            "Nazanin Moradinasab",
            "Laura S. Shankman",
            "Rebecca A. Deaton",
            "Gary K. Owens",
            "Donald E. Brown"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:50:50+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19225v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19225v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000073,
        "doi": null,
        "title": "ProtoGMM: Multi-prototype Gaussian-Mixture-based Domain Adaptation Model for Semantic Segmentation",
        "abstract": "Domain adaptive semantic segmentation aims to generate accurate and dense\npredictions for an unlabeled target domain by leveraging a supervised model\ntrained on a labeled source domain. The prevalent self-training approach\ninvolves retraining the dense discriminative classifier of $p(class|pixel\nfeature)$ using the pseudo-labels from the target domain. While many methods\nfocus on mitigating the issue of noisy pseudo-labels, they often overlook the\nunderlying data distribution p(pixel feature|class) in both the source and\ntarget domains. To address this limitation, we propose the multi-prototype\nGaussian-Mixture-based (ProtoGMM) model, which incorporates the GMM into\ncontrastive losses to perform guided contrastive learning. Contrastive losses\nare commonly executed in the literature using memory banks, which can lead to\nclass biases due to underrepresented classes. Furthermore, memory banks often\nhave fixed capacities, potentially restricting the model's ability to capture\ndiverse representations of the target/source domains. An alternative approach\nis to use global class prototypes (i.e. averaged features per category).\nHowever, the global prototypes are based on the unimodal distribution\nassumption per class, disregarding within-class variation. To address these\nchallenges, we propose the ProtoGMM model. This novel approach involves\nestimating the underlying multi-prototype source distribution by utilizing the\nGMM on the feature space of the source samples. The components of the GMM model\nact as representative prototypes. To achieve increased intra-class semantic\nsimilarity, decreased inter-class similarity, and domain alignment between the\nsource and target domains, we employ multi-prototype contrastive learning\nbetween source distribution and target samples. The experiments show the\neffectiveness of our method on UDA benchmarks.",
        "chunk-id": 3,
        "chunk": "are commonly executed in the literature using memory banks, which can lead to\nclass biases due to underrepresented classes. Furthermore, memory banks often\nhave fixed capacities, potentially restricting the model's ability to capture\ndiverse representations of the target/source domains. An alternative approach\nis to use global class prototypes (i.e. averaged features per category).",
        "authors": [
            "Nazanin Moradinasab",
            "Laura S. Shankman",
            "Rebecca A. Deaton",
            "Gary K. Owens",
            "Donald E. Brown"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:50:50+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19225v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19225v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000073,
        "doi": null,
        "title": "ProtoGMM: Multi-prototype Gaussian-Mixture-based Domain Adaptation Model for Semantic Segmentation",
        "abstract": "Domain adaptive semantic segmentation aims to generate accurate and dense\npredictions for an unlabeled target domain by leveraging a supervised model\ntrained on a labeled source domain. The prevalent self-training approach\ninvolves retraining the dense discriminative classifier of $p(class|pixel\nfeature)$ using the pseudo-labels from the target domain. While many methods\nfocus on mitigating the issue of noisy pseudo-labels, they often overlook the\nunderlying data distribution p(pixel feature|class) in both the source and\ntarget domains. To address this limitation, we propose the multi-prototype\nGaussian-Mixture-based (ProtoGMM) model, which incorporates the GMM into\ncontrastive losses to perform guided contrastive learning. Contrastive losses\nare commonly executed in the literature using memory banks, which can lead to\nclass biases due to underrepresented classes. Furthermore, memory banks often\nhave fixed capacities, potentially restricting the model's ability to capture\ndiverse representations of the target/source domains. An alternative approach\nis to use global class prototypes (i.e. averaged features per category).\nHowever, the global prototypes are based on the unimodal distribution\nassumption per class, disregarding within-class variation. To address these\nchallenges, we propose the ProtoGMM model. This novel approach involves\nestimating the underlying multi-prototype source distribution by utilizing the\nGMM on the feature space of the source samples. The components of the GMM model\nact as representative prototypes. To achieve increased intra-class semantic\nsimilarity, decreased inter-class similarity, and domain alignment between the\nsource and target domains, we employ multi-prototype contrastive learning\nbetween source distribution and target samples. The experiments show the\neffectiveness of our method on UDA benchmarks.",
        "chunk-id": 4,
        "chunk": "However, the global prototypes are based on the unimodal distribution\nassumption per class, disregarding within-class variation. To address these\nchallenges, we propose the ProtoGMM model. This novel approach involves\nestimating the underlying multi-prototype source distribution by utilizing the\nGMM on the feature space of the source samples. The components of the GMM model",
        "authors": [
            "Nazanin Moradinasab",
            "Laura S. Shankman",
            "Rebecca A. Deaton",
            "Gary K. Owens",
            "Donald E. Brown"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:50:50+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19225v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19225v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000073,
        "doi": null,
        "title": "ProtoGMM: Multi-prototype Gaussian-Mixture-based Domain Adaptation Model for Semantic Segmentation",
        "abstract": "Domain adaptive semantic segmentation aims to generate accurate and dense\npredictions for an unlabeled target domain by leveraging a supervised model\ntrained on a labeled source domain. The prevalent self-training approach\ninvolves retraining the dense discriminative classifier of $p(class|pixel\nfeature)$ using the pseudo-labels from the target domain. While many methods\nfocus on mitigating the issue of noisy pseudo-labels, they often overlook the\nunderlying data distribution p(pixel feature|class) in both the source and\ntarget domains. To address this limitation, we propose the multi-prototype\nGaussian-Mixture-based (ProtoGMM) model, which incorporates the GMM into\ncontrastive losses to perform guided contrastive learning. Contrastive losses\nare commonly executed in the literature using memory banks, which can lead to\nclass biases due to underrepresented classes. Furthermore, memory banks often\nhave fixed capacities, potentially restricting the model's ability to capture\ndiverse representations of the target/source domains. An alternative approach\nis to use global class prototypes (i.e. averaged features per category).\nHowever, the global prototypes are based on the unimodal distribution\nassumption per class, disregarding within-class variation. To address these\nchallenges, we propose the ProtoGMM model. This novel approach involves\nestimating the underlying multi-prototype source distribution by utilizing the\nGMM on the feature space of the source samples. The components of the GMM model\nact as representative prototypes. To achieve increased intra-class semantic\nsimilarity, decreased inter-class similarity, and domain alignment between the\nsource and target domains, we employ multi-prototype contrastive learning\nbetween source distribution and target samples. The experiments show the\neffectiveness of our method on UDA benchmarks.",
        "chunk-id": 5,
        "chunk": "act as representative prototypes. To achieve increased intra-class semantic\nsimilarity, decreased inter-class similarity, and domain alignment between the\nsource and target domains, we employ multi-prototype contrastive learning\nbetween source distribution and target samples. The experiments show the\neffectiveness of our method on UDA benchmarks.",
        "authors": [
            "Nazanin Moradinasab",
            "Laura S. Shankman",
            "Rebecca A. Deaton",
            "Gary K. Owens",
            "Donald E. Brown"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:50:50+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19225v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19225v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000074,
        "doi": null,
        "title": "T-FREE: Tokenizer-Free Generative LLMs via Sparse Representations for Memory-Efficient Embeddings",
        "abstract": "Tokenizers are crucial for encoding information in Large Language Models, but\ntheir development has recently stagnated, and they contain inherent weaknesses.\nMajor limitations include computational overhead, ineffective vocabulary use,\nand unnecessarily large embedding and head layers. Additionally, their\nperformance is biased towards a reference corpus, leading to reduced\neffectiveness for underrepresented languages.\n  To remedy these issues, we propose T-FREE, which directly embeds words\nthrough sparse activation patterns over character triplets, and does not\nrequire a reference corpus. T-FREE inherently exploits morphological\nsimilarities and allows for strong compression of embedding layers. In our\nexhaustive experimental evaluation, we achieve competitive downstream\nperformance with a parameter reduction of more than 85% on these layers.\nFurther, T-FREE shows significant improvements in cross-lingual transfer\nlearning.",
        "chunk-id": 1,
        "chunk": "Tokenizers are crucial for encoding information in Large Language Models, but\ntheir development has recently stagnated, and they contain inherent weaknesses.\nMajor limitations include computational overhead, ineffective vocabulary use,\nand unnecessarily large embedding and head layers. Additionally, their\nperformance is biased towards a reference corpus, leading to reduced",
        "authors": [
            "Bj\u00f6rn Deiseroth",
            "Manuel Brack",
            "Patrick Schramowski",
            "Kristian Kersting",
            "Samuel Weinbach"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:49:08+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19223v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19223v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Machine Learning"
        ]
    },
    {
        "id": 30000074,
        "doi": null,
        "title": "T-FREE: Tokenizer-Free Generative LLMs via Sparse Representations for Memory-Efficient Embeddings",
        "abstract": "Tokenizers are crucial for encoding information in Large Language Models, but\ntheir development has recently stagnated, and they contain inherent weaknesses.\nMajor limitations include computational overhead, ineffective vocabulary use,\nand unnecessarily large embedding and head layers. Additionally, their\nperformance is biased towards a reference corpus, leading to reduced\neffectiveness for underrepresented languages.\n  To remedy these issues, we propose T-FREE, which directly embeds words\nthrough sparse activation patterns over character triplets, and does not\nrequire a reference corpus. T-FREE inherently exploits morphological\nsimilarities and allows for strong compression of embedding layers. In our\nexhaustive experimental evaluation, we achieve competitive downstream\nperformance with a parameter reduction of more than 85% on these layers.\nFurther, T-FREE shows significant improvements in cross-lingual transfer\nlearning.",
        "chunk-id": 2,
        "chunk": "effectiveness for underrepresented languages.\n  To remedy these issues, we propose T-FREE, which directly embeds words\nthrough sparse activation patterns over character triplets, and does not\nrequire a reference corpus. T-FREE inherently exploits morphological\nsimilarities and allows for strong compression of embedding layers. In our",
        "authors": [
            "Bj\u00f6rn Deiseroth",
            "Manuel Brack",
            "Patrick Schramowski",
            "Kristian Kersting",
            "Samuel Weinbach"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:49:08+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19223v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19223v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Machine Learning"
        ]
    },
    {
        "id": 30000074,
        "doi": null,
        "title": "T-FREE: Tokenizer-Free Generative LLMs via Sparse Representations for Memory-Efficient Embeddings",
        "abstract": "Tokenizers are crucial for encoding information in Large Language Models, but\ntheir development has recently stagnated, and they contain inherent weaknesses.\nMajor limitations include computational overhead, ineffective vocabulary use,\nand unnecessarily large embedding and head layers. Additionally, their\nperformance is biased towards a reference corpus, leading to reduced\neffectiveness for underrepresented languages.\n  To remedy these issues, we propose T-FREE, which directly embeds words\nthrough sparse activation patterns over character triplets, and does not\nrequire a reference corpus. T-FREE inherently exploits morphological\nsimilarities and allows for strong compression of embedding layers. In our\nexhaustive experimental evaluation, we achieve competitive downstream\nperformance with a parameter reduction of more than 85% on these layers.\nFurther, T-FREE shows significant improvements in cross-lingual transfer\nlearning.",
        "chunk-id": 3,
        "chunk": "exhaustive experimental evaluation, we achieve competitive downstream\nperformance with a parameter reduction of more than 85% on these layers.\nFurther, T-FREE shows significant improvements in cross-lingual transfer\nlearning.",
        "authors": [
            "Bj\u00f6rn Deiseroth",
            "Manuel Brack",
            "Patrick Schramowski",
            "Kristian Kersting",
            "Samuel Weinbach"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:49:08+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19223v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19223v1",
        "categories": [
            "Computation and Language",
            "Artificial Intelligence",
            "Machine Learning"
        ]
    },
    {
        "id": 30000075,
        "doi": null,
        "title": "Hack Me If You Can: Aggregating AutoEncoders for Countering Persistent Access Threats Within Highly Imbalanced Data",
        "abstract": "Advanced Persistent Threats (APTs) are sophisticated, targeted cyberattacks\ndesigned to gain unauthorized access to systems and remain undetected for\nextended periods. To evade detection, APT cyberattacks deceive defense layers\nwith breaches and exploits, thereby complicating exposure by traditional\nanomaly detection-based security methods. The challenge of detecting APTs with\nmachine learning is compounded by the rarity of relevant datasets and the\nsignificant imbalance in the data, which makes the detection process highly\nburdensome. We present AE-APT, a deep learning-based tool for APT detection\nthat features a family of AutoEncoder methods ranging from a basic one to a\nTransformer-based one. We evaluated our tool on a suite of provenance trace\ndatabases produced by the DARPA Transparent Computing program, where APT-like\nattacks constitute as little as 0.004% of the data. The datasets span multiple\noperating systems, including Android, Linux, BSD, and Windows, and cover two\nattack scenarios. The outcomes showed that AE-APT has significantly higher\ndetection rates compared to its competitors, indicating superior performance in\ndetecting and ranking anomalies.",
        "chunk-id": 1,
        "chunk": "Advanced Persistent Threats (APTs) are sophisticated, targeted cyberattacks\ndesigned to gain unauthorized access to systems and remain undetected for\nextended periods. To evade detection, APT cyberattacks deceive defense layers\nwith breaches and exploits, thereby complicating exposure by traditional\nanomaly detection-based security methods. The challenge of detecting APTs with",
        "authors": [
            "Sidahmed Benabderrahmane",
            "Ngoc Hoang",
            "Petko Valtchev",
            "James Cheney",
            "Talal Rahwan"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:45:38+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19220v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19220v1",
        "categories": [
            "Cryptography and Security",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 30000075,
        "doi": null,
        "title": "Hack Me If You Can: Aggregating AutoEncoders for Countering Persistent Access Threats Within Highly Imbalanced Data",
        "abstract": "Advanced Persistent Threats (APTs) are sophisticated, targeted cyberattacks\ndesigned to gain unauthorized access to systems and remain undetected for\nextended periods. To evade detection, APT cyberattacks deceive defense layers\nwith breaches and exploits, thereby complicating exposure by traditional\nanomaly detection-based security methods. The challenge of detecting APTs with\nmachine learning is compounded by the rarity of relevant datasets and the\nsignificant imbalance in the data, which makes the detection process highly\nburdensome. We present AE-APT, a deep learning-based tool for APT detection\nthat features a family of AutoEncoder methods ranging from a basic one to a\nTransformer-based one. We evaluated our tool on a suite of provenance trace\ndatabases produced by the DARPA Transparent Computing program, where APT-like\nattacks constitute as little as 0.004% of the data. The datasets span multiple\noperating systems, including Android, Linux, BSD, and Windows, and cover two\nattack scenarios. The outcomes showed that AE-APT has significantly higher\ndetection rates compared to its competitors, indicating superior performance in\ndetecting and ranking anomalies.",
        "chunk-id": 2,
        "chunk": "machine learning is compounded by the rarity of relevant datasets and the\nsignificant imbalance in the data, which makes the detection process highly\nburdensome. We present AE-APT, a deep learning-based tool for APT detection\nthat features a family of AutoEncoder methods ranging from a basic one to a\nTransformer-based one. We evaluated our tool on a suite of provenance trace",
        "authors": [
            "Sidahmed Benabderrahmane",
            "Ngoc Hoang",
            "Petko Valtchev",
            "James Cheney",
            "Talal Rahwan"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:45:38+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19220v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19220v1",
        "categories": [
            "Cryptography and Security",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 30000075,
        "doi": null,
        "title": "Hack Me If You Can: Aggregating AutoEncoders for Countering Persistent Access Threats Within Highly Imbalanced Data",
        "abstract": "Advanced Persistent Threats (APTs) are sophisticated, targeted cyberattacks\ndesigned to gain unauthorized access to systems and remain undetected for\nextended periods. To evade detection, APT cyberattacks deceive defense layers\nwith breaches and exploits, thereby complicating exposure by traditional\nanomaly detection-based security methods. The challenge of detecting APTs with\nmachine learning is compounded by the rarity of relevant datasets and the\nsignificant imbalance in the data, which makes the detection process highly\nburdensome. We present AE-APT, a deep learning-based tool for APT detection\nthat features a family of AutoEncoder methods ranging from a basic one to a\nTransformer-based one. We evaluated our tool on a suite of provenance trace\ndatabases produced by the DARPA Transparent Computing program, where APT-like\nattacks constitute as little as 0.004% of the data. The datasets span multiple\noperating systems, including Android, Linux, BSD, and Windows, and cover two\nattack scenarios. The outcomes showed that AE-APT has significantly higher\ndetection rates compared to its competitors, indicating superior performance in\ndetecting and ranking anomalies.",
        "chunk-id": 3,
        "chunk": "databases produced by the DARPA Transparent Computing program, where APT-like\nattacks constitute as little as 0.004% of the data. The datasets span multiple\noperating systems, including Android, Linux, BSD, and Windows, and cover two\nattack scenarios. The outcomes showed that AE-APT has significantly higher",
        "authors": [
            "Sidahmed Benabderrahmane",
            "Ngoc Hoang",
            "Petko Valtchev",
            "James Cheney",
            "Talal Rahwan"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:45:38+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19220v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19220v1",
        "categories": [
            "Cryptography and Security",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 30000075,
        "doi": null,
        "title": "Hack Me If You Can: Aggregating AutoEncoders for Countering Persistent Access Threats Within Highly Imbalanced Data",
        "abstract": "Advanced Persistent Threats (APTs) are sophisticated, targeted cyberattacks\ndesigned to gain unauthorized access to systems and remain undetected for\nextended periods. To evade detection, APT cyberattacks deceive defense layers\nwith breaches and exploits, thereby complicating exposure by traditional\nanomaly detection-based security methods. The challenge of detecting APTs with\nmachine learning is compounded by the rarity of relevant datasets and the\nsignificant imbalance in the data, which makes the detection process highly\nburdensome. We present AE-APT, a deep learning-based tool for APT detection\nthat features a family of AutoEncoder methods ranging from a basic one to a\nTransformer-based one. We evaluated our tool on a suite of provenance trace\ndatabases produced by the DARPA Transparent Computing program, where APT-like\nattacks constitute as little as 0.004% of the data. The datasets span multiple\noperating systems, including Android, Linux, BSD, and Windows, and cover two\nattack scenarios. The outcomes showed that AE-APT has significantly higher\ndetection rates compared to its competitors, indicating superior performance in\ndetecting and ranking anomalies.",
        "chunk-id": 4,
        "chunk": "detection rates compared to its competitors, indicating superior performance in\ndetecting and ranking anomalies.",
        "authors": [
            "Sidahmed Benabderrahmane",
            "Ngoc Hoang",
            "Petko Valtchev",
            "James Cheney",
            "Talal Rahwan"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:45:38+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19220v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19220v1",
        "categories": [
            "Cryptography and Security",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 30000076,
        "doi": null,
        "title": "Think Step by Step: Chain-of-Gesture Prompting for Error Detection in Robotic Surgical Videos",
        "abstract": "Despite significant advancements in robotic systems and surgical data\nscience, ensuring safe and optimal execution in robot-assisted minimally\ninvasive surgery (RMIS) remains a complex challenge. Current surgical error\ndetection methods involve two parts: identifying surgical gestures and then\ndetecting errors within each gesture clip. These methods seldom consider the\nrich contextual and semantic information inherent in surgical videos, limiting\ntheir performance due to reliance on accurate gesture identification. Motivated\nby the chain-of-thought prompting in natural language processing, this letter\npresents a novel and real-time end-to-end error detection framework,\nChain-of-Thought (COG) prompting, leveraging contextual information from\nsurgical videos. This encompasses two reasoning modules designed to mimic the\ndecision-making processes of expert surgeons. Concretely, we first design a\nGestural-Visual Reasoning module, which utilizes transformer and attention\narchitectures for gesture prompting, while the second, a Multi-Scale Temporal\nReasoning module, employs a multi-stage temporal convolutional network with\nboth slow and fast paths for temporal information extraction. We extensively\nvalidate our method on the public benchmark RMIS dataset JIGSAWS. Our method\nencapsulates the reasoning processes inherent to surgical activities enabling\nit to outperform the state-of-the-art by 4.6% in F1 score, 4.6% in Accuracy,\nand 5.9% in Jaccard index while processing each frame in 6.69 milliseconds on\naverage, demonstrating the great potential of our approach in enhancing the\nsafety and efficacy of RMIS procedures and surgical education. The code will be\navailable.",
        "chunk-id": 1,
        "chunk": "Despite significant advancements in robotic systems and surgical data\nscience, ensuring safe and optimal execution in robot-assisted minimally\ninvasive surgery (RMIS) remains a complex challenge. Current surgical error\ndetection methods involve two parts: identifying surgical gestures and then\ndetecting errors within each gesture clip. These methods seldom consider the",
        "authors": [
            "Zhimin Shao",
            "Jialang Xu",
            "Danail Stoyanov",
            "Evangelos B. Mazomenos",
            "Yueming Jin"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:43:50+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19217v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19217v1",
        "categories": [
            "Computer Vision and Pattern Recognition",
            "Artificial Intelligence",
            "Robotics"
        ]
    },
    {
        "id": 30000076,
        "doi": null,
        "title": "Think Step by Step: Chain-of-Gesture Prompting for Error Detection in Robotic Surgical Videos",
        "abstract": "Despite significant advancements in robotic systems and surgical data\nscience, ensuring safe and optimal execution in robot-assisted minimally\ninvasive surgery (RMIS) remains a complex challenge. Current surgical error\ndetection methods involve two parts: identifying surgical gestures and then\ndetecting errors within each gesture clip. These methods seldom consider the\nrich contextual and semantic information inherent in surgical videos, limiting\ntheir performance due to reliance on accurate gesture identification. Motivated\nby the chain-of-thought prompting in natural language processing, this letter\npresents a novel and real-time end-to-end error detection framework,\nChain-of-Thought (COG) prompting, leveraging contextual information from\nsurgical videos. This encompasses two reasoning modules designed to mimic the\ndecision-making processes of expert surgeons. Concretely, we first design a\nGestural-Visual Reasoning module, which utilizes transformer and attention\narchitectures for gesture prompting, while the second, a Multi-Scale Temporal\nReasoning module, employs a multi-stage temporal convolutional network with\nboth slow and fast paths for temporal information extraction. We extensively\nvalidate our method on the public benchmark RMIS dataset JIGSAWS. Our method\nencapsulates the reasoning processes inherent to surgical activities enabling\nit to outperform the state-of-the-art by 4.6% in F1 score, 4.6% in Accuracy,\nand 5.9% in Jaccard index while processing each frame in 6.69 milliseconds on\naverage, demonstrating the great potential of our approach in enhancing the\nsafety and efficacy of RMIS procedures and surgical education. The code will be\navailable.",
        "chunk-id": 2,
        "chunk": "rich contextual and semantic information inherent in surgical videos, limiting\ntheir performance due to reliance on accurate gesture identification. Motivated\nby the chain-of-thought prompting in natural language processing, this letter\npresents a novel and real-time end-to-end error detection framework,\nChain-of-Thought (COG) prompting, leveraging contextual information from",
        "authors": [
            "Zhimin Shao",
            "Jialang Xu",
            "Danail Stoyanov",
            "Evangelos B. Mazomenos",
            "Yueming Jin"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:43:50+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19217v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19217v1",
        "categories": [
            "Computer Vision and Pattern Recognition",
            "Artificial Intelligence",
            "Robotics"
        ]
    },
    {
        "id": 30000076,
        "doi": null,
        "title": "Think Step by Step: Chain-of-Gesture Prompting for Error Detection in Robotic Surgical Videos",
        "abstract": "Despite significant advancements in robotic systems and surgical data\nscience, ensuring safe and optimal execution in robot-assisted minimally\ninvasive surgery (RMIS) remains a complex challenge. Current surgical error\ndetection methods involve two parts: identifying surgical gestures and then\ndetecting errors within each gesture clip. These methods seldom consider the\nrich contextual and semantic information inherent in surgical videos, limiting\ntheir performance due to reliance on accurate gesture identification. Motivated\nby the chain-of-thought prompting in natural language processing, this letter\npresents a novel and real-time end-to-end error detection framework,\nChain-of-Thought (COG) prompting, leveraging contextual information from\nsurgical videos. This encompasses two reasoning modules designed to mimic the\ndecision-making processes of expert surgeons. Concretely, we first design a\nGestural-Visual Reasoning module, which utilizes transformer and attention\narchitectures for gesture prompting, while the second, a Multi-Scale Temporal\nReasoning module, employs a multi-stage temporal convolutional network with\nboth slow and fast paths for temporal information extraction. We extensively\nvalidate our method on the public benchmark RMIS dataset JIGSAWS. Our method\nencapsulates the reasoning processes inherent to surgical activities enabling\nit to outperform the state-of-the-art by 4.6% in F1 score, 4.6% in Accuracy,\nand 5.9% in Jaccard index while processing each frame in 6.69 milliseconds on\naverage, demonstrating the great potential of our approach in enhancing the\nsafety and efficacy of RMIS procedures and surgical education. The code will be\navailable.",
        "chunk-id": 3,
        "chunk": "surgical videos. This encompasses two reasoning modules designed to mimic the\ndecision-making processes of expert surgeons. Concretely, we first design a\nGestural-Visual Reasoning module, which utilizes transformer and attention\narchitectures for gesture prompting, while the second, a Multi-Scale Temporal\nReasoning module, employs a multi-stage temporal convolutional network with",
        "authors": [
            "Zhimin Shao",
            "Jialang Xu",
            "Danail Stoyanov",
            "Evangelos B. Mazomenos",
            "Yueming Jin"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:43:50+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19217v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19217v1",
        "categories": [
            "Computer Vision and Pattern Recognition",
            "Artificial Intelligence",
            "Robotics"
        ]
    },
    {
        "id": 30000076,
        "doi": null,
        "title": "Think Step by Step: Chain-of-Gesture Prompting for Error Detection in Robotic Surgical Videos",
        "abstract": "Despite significant advancements in robotic systems and surgical data\nscience, ensuring safe and optimal execution in robot-assisted minimally\ninvasive surgery (RMIS) remains a complex challenge. Current surgical error\ndetection methods involve two parts: identifying surgical gestures and then\ndetecting errors within each gesture clip. These methods seldom consider the\nrich contextual and semantic information inherent in surgical videos, limiting\ntheir performance due to reliance on accurate gesture identification. Motivated\nby the chain-of-thought prompting in natural language processing, this letter\npresents a novel and real-time end-to-end error detection framework,\nChain-of-Thought (COG) prompting, leveraging contextual information from\nsurgical videos. This encompasses two reasoning modules designed to mimic the\ndecision-making processes of expert surgeons. Concretely, we first design a\nGestural-Visual Reasoning module, which utilizes transformer and attention\narchitectures for gesture prompting, while the second, a Multi-Scale Temporal\nReasoning module, employs a multi-stage temporal convolutional network with\nboth slow and fast paths for temporal information extraction. We extensively\nvalidate our method on the public benchmark RMIS dataset JIGSAWS. Our method\nencapsulates the reasoning processes inherent to surgical activities enabling\nit to outperform the state-of-the-art by 4.6% in F1 score, 4.6% in Accuracy,\nand 5.9% in Jaccard index while processing each frame in 6.69 milliseconds on\naverage, demonstrating the great potential of our approach in enhancing the\nsafety and efficacy of RMIS procedures and surgical education. The code will be\navailable.",
        "chunk-id": 4,
        "chunk": "both slow and fast paths for temporal information extraction. We extensively\nvalidate our method on the public benchmark RMIS dataset JIGSAWS. Our method\nencapsulates the reasoning processes inherent to surgical activities enabling\nit to outperform the state-of-the-art by 4.6% in F1 score, 4.6% in Accuracy,",
        "authors": [
            "Zhimin Shao",
            "Jialang Xu",
            "Danail Stoyanov",
            "Evangelos B. Mazomenos",
            "Yueming Jin"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:43:50+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19217v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19217v1",
        "categories": [
            "Computer Vision and Pattern Recognition",
            "Artificial Intelligence",
            "Robotics"
        ]
    },
    {
        "id": 30000076,
        "doi": null,
        "title": "Think Step by Step: Chain-of-Gesture Prompting for Error Detection in Robotic Surgical Videos",
        "abstract": "Despite significant advancements in robotic systems and surgical data\nscience, ensuring safe and optimal execution in robot-assisted minimally\ninvasive surgery (RMIS) remains a complex challenge. Current surgical error\ndetection methods involve two parts: identifying surgical gestures and then\ndetecting errors within each gesture clip. These methods seldom consider the\nrich contextual and semantic information inherent in surgical videos, limiting\ntheir performance due to reliance on accurate gesture identification. Motivated\nby the chain-of-thought prompting in natural language processing, this letter\npresents a novel and real-time end-to-end error detection framework,\nChain-of-Thought (COG) prompting, leveraging contextual information from\nsurgical videos. This encompasses two reasoning modules designed to mimic the\ndecision-making processes of expert surgeons. Concretely, we first design a\nGestural-Visual Reasoning module, which utilizes transformer and attention\narchitectures for gesture prompting, while the second, a Multi-Scale Temporal\nReasoning module, employs a multi-stage temporal convolutional network with\nboth slow and fast paths for temporal information extraction. We extensively\nvalidate our method on the public benchmark RMIS dataset JIGSAWS. Our method\nencapsulates the reasoning processes inherent to surgical activities enabling\nit to outperform the state-of-the-art by 4.6% in F1 score, 4.6% in Accuracy,\nand 5.9% in Jaccard index while processing each frame in 6.69 milliseconds on\naverage, demonstrating the great potential of our approach in enhancing the\nsafety and efficacy of RMIS procedures and surgical education. The code will be\navailable.",
        "chunk-id": 5,
        "chunk": "and 5.9% in Jaccard index while processing each frame in 6.69 milliseconds on\naverage, demonstrating the great potential of our approach in enhancing the\nsafety and efficacy of RMIS procedures and surgical education. The code will be\navailable.",
        "authors": [
            "Zhimin Shao",
            "Jialang Xu",
            "Danail Stoyanov",
            "Evangelos B. Mazomenos",
            "Yueming Jin"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:43:50+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19217v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19217v1",
        "categories": [
            "Computer Vision and Pattern Recognition",
            "Artificial Intelligence",
            "Robotics"
        ]
    },
    {
        "id": 30000077,
        "doi": null,
        "title": "SeaKR: Self-aware Knowledge Retrieval for Adaptive Retrieval Augmented Generation",
        "abstract": "This paper introduces Self-aware Knowledge Retrieval (SeaKR), a novel\nadaptive RAG model that extracts self-aware uncertainty of LLMs from their\ninternal states. SeaKR activates retrieval when the LLMs present high\nself-aware uncertainty for generation. To effectively integrate retrieved\nknowledge snippets, SeaKR re-ranks them based on LLM's self-aware uncertainty\nto preserve the snippet that reduces their uncertainty to the utmost. To\nfacilitate solving complex tasks that require multiple retrievals, SeaKR\nutilizes their self-aware uncertainty to choose among different reasoning\nstrategies. Our experiments on both complex and simple Question Answering\ndatasets show that SeaKR outperforms existing adaptive RAG methods. We release\nour code at https://github.com/THU-KEG/SeaKR.",
        "chunk-id": 1,
        "chunk": "This paper introduces Self-aware Knowledge Retrieval (SeaKR), a novel\nadaptive RAG model that extracts self-aware uncertainty of LLMs from their\ninternal states. SeaKR activates retrieval when the LLMs present high\nself-aware uncertainty for generation. To effectively integrate retrieved\nknowledge snippets, SeaKR re-ranks them based on LLM's self-aware uncertainty",
        "authors": [
            "Zijun Yao",
            "Weijian Qi",
            "Liangming Pan",
            "Shulin Cao",
            "Linmei Hu",
            "Weichuan Liu",
            "Lei Hou",
            "Juanzi Li"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:38:33+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19215v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19215v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 30000077,
        "doi": null,
        "title": "SeaKR: Self-aware Knowledge Retrieval for Adaptive Retrieval Augmented Generation",
        "abstract": "This paper introduces Self-aware Knowledge Retrieval (SeaKR), a novel\nadaptive RAG model that extracts self-aware uncertainty of LLMs from their\ninternal states. SeaKR activates retrieval when the LLMs present high\nself-aware uncertainty for generation. To effectively integrate retrieved\nknowledge snippets, SeaKR re-ranks them based on LLM's self-aware uncertainty\nto preserve the snippet that reduces their uncertainty to the utmost. To\nfacilitate solving complex tasks that require multiple retrievals, SeaKR\nutilizes their self-aware uncertainty to choose among different reasoning\nstrategies. Our experiments on both complex and simple Question Answering\ndatasets show that SeaKR outperforms existing adaptive RAG methods. We release\nour code at https://github.com/THU-KEG/SeaKR.",
        "chunk-id": 2,
        "chunk": "to preserve the snippet that reduces their uncertainty to the utmost. To\nfacilitate solving complex tasks that require multiple retrievals, SeaKR\nutilizes their self-aware uncertainty to choose among different reasoning\nstrategies. Our experiments on both complex and simple Question Answering\ndatasets show that SeaKR outperforms existing adaptive RAG methods. We release",
        "authors": [
            "Zijun Yao",
            "Weijian Qi",
            "Liangming Pan",
            "Shulin Cao",
            "Linmei Hu",
            "Weichuan Liu",
            "Lei Hou",
            "Juanzi Li"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:38:33+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19215v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19215v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 30000077,
        "doi": null,
        "title": "SeaKR: Self-aware Knowledge Retrieval for Adaptive Retrieval Augmented Generation",
        "abstract": "This paper introduces Self-aware Knowledge Retrieval (SeaKR), a novel\nadaptive RAG model that extracts self-aware uncertainty of LLMs from their\ninternal states. SeaKR activates retrieval when the LLMs present high\nself-aware uncertainty for generation. To effectively integrate retrieved\nknowledge snippets, SeaKR re-ranks them based on LLM's self-aware uncertainty\nto preserve the snippet that reduces their uncertainty to the utmost. To\nfacilitate solving complex tasks that require multiple retrievals, SeaKR\nutilizes their self-aware uncertainty to choose among different reasoning\nstrategies. Our experiments on both complex and simple Question Answering\ndatasets show that SeaKR outperforms existing adaptive RAG methods. We release\nour code at https://github.com/THU-KEG/SeaKR.",
        "chunk-id": 3,
        "chunk": "our code at https://github.com/THU-KEG/SeaKR.",
        "authors": [
            "Zijun Yao",
            "Weijian Qi",
            "Liangming Pan",
            "Shulin Cao",
            "Linmei Hu",
            "Weichuan Liu",
            "Lei Hou",
            "Juanzi Li"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:38:33+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19215v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19215v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 30000078,
        "doi": null,
        "title": "Bessel Models for Representations of $GSp(4, q)$",
        "abstract": "We compute the Bessel models of irreducible representations of the finite\ngroup $GSp(4, q)$.",
        "chunk-id": 1,
        "chunk": "We compute the Bessel models of irreducible representations of the finite\ngroup $GSp(4, q)$.",
        "authors": [
            "Jonathan Cohen"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:22:56+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19203v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19203v1",
        "categories": [
            "Representation Theory",
            "Number Theory",
            "Representations of finite groups of Lie type"
        ]
    },
    {
        "id": 30000079,
        "doi": null,
        "title": "Evolving reservoir computers reveals bidirectional coupling between predictive power and emergent dynamics",
        "abstract": "Biological neural networks can perform complex computations to predict their\nenvironment, far above the limited predictive capabilities of individual\nneurons. While conventional approaches to understanding these computations\noften focus on isolating the contributions of single neurons, here we argue\nthat a deeper understanding requires considering emergent dynamics - dynamics\nthat make the whole system \"more than the sum of its parts\". Specifically, we\nexamine the relationship between prediction performance and emergence by\nleveraging recent quantitative metrics of emergence, derived from Partial\nInformation Decomposition, and by modelling the prediction of environmental\ndynamics in a bio-inspired computational framework known as reservoir\ncomputing. Notably, we reveal a bidirectional coupling between prediction\nperformance and emergence, which generalises across task environments and\nreservoir network topologies, and is recapitulated by three key results: 1)\nOptimising hyperparameters for performance enhances emergent dynamics, and vice\nversa; 2) Emergent dynamics represent a near sufficient criterion for\nprediction success in all task environments, and an almost necessary criterion\nin most environments; 3) Training reservoir computers on larger datasets\nresults in stronger emergent dynamics, which contain task-relevant information\ncrucial for performance. Overall, our study points to a pivotal role of\nemergence in facilitating environmental predictions in a bio-inspired\ncomputational architecture.",
        "chunk-id": 1,
        "chunk": "Biological neural networks can perform complex computations to predict their\nenvironment, far above the limited predictive capabilities of individual\nneurons. While conventional approaches to understanding these computations\noften focus on isolating the contributions of single neurons, here we argue\nthat a deeper understanding requires considering emergent dynamics - dynamics",
        "authors": [
            "Hanna M. Tolle",
            "Andrea I Luppi",
            "Anil K. Seth",
            "Pedro A. M. Mediano"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:19:30+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19201v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19201v1",
        "categories": [
            "Neurons and Cognition"
        ]
    },
    {
        "id": 30000079,
        "doi": null,
        "title": "Evolving reservoir computers reveals bidirectional coupling between predictive power and emergent dynamics",
        "abstract": "Biological neural networks can perform complex computations to predict their\nenvironment, far above the limited predictive capabilities of individual\nneurons. While conventional approaches to understanding these computations\noften focus on isolating the contributions of single neurons, here we argue\nthat a deeper understanding requires considering emergent dynamics - dynamics\nthat make the whole system \"more than the sum of its parts\". Specifically, we\nexamine the relationship between prediction performance and emergence by\nleveraging recent quantitative metrics of emergence, derived from Partial\nInformation Decomposition, and by modelling the prediction of environmental\ndynamics in a bio-inspired computational framework known as reservoir\ncomputing. Notably, we reveal a bidirectional coupling between prediction\nperformance and emergence, which generalises across task environments and\nreservoir network topologies, and is recapitulated by three key results: 1)\nOptimising hyperparameters for performance enhances emergent dynamics, and vice\nversa; 2) Emergent dynamics represent a near sufficient criterion for\nprediction success in all task environments, and an almost necessary criterion\nin most environments; 3) Training reservoir computers on larger datasets\nresults in stronger emergent dynamics, which contain task-relevant information\ncrucial for performance. Overall, our study points to a pivotal role of\nemergence in facilitating environmental predictions in a bio-inspired\ncomputational architecture.",
        "chunk-id": 2,
        "chunk": "that make the whole system \"more than the sum of its parts\". Specifically, we\nexamine the relationship between prediction performance and emergence by\nleveraging recent quantitative metrics of emergence, derived from Partial\nInformation Decomposition, and by modelling the prediction of environmental\ndynamics in a bio-inspired computational framework known as reservoir",
        "authors": [
            "Hanna M. Tolle",
            "Andrea I Luppi",
            "Anil K. Seth",
            "Pedro A. M. Mediano"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:19:30+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19201v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19201v1",
        "categories": [
            "Neurons and Cognition"
        ]
    },
    {
        "id": 30000079,
        "doi": null,
        "title": "Evolving reservoir computers reveals bidirectional coupling between predictive power and emergent dynamics",
        "abstract": "Biological neural networks can perform complex computations to predict their\nenvironment, far above the limited predictive capabilities of individual\nneurons. While conventional approaches to understanding these computations\noften focus on isolating the contributions of single neurons, here we argue\nthat a deeper understanding requires considering emergent dynamics - dynamics\nthat make the whole system \"more than the sum of its parts\". Specifically, we\nexamine the relationship between prediction performance and emergence by\nleveraging recent quantitative metrics of emergence, derived from Partial\nInformation Decomposition, and by modelling the prediction of environmental\ndynamics in a bio-inspired computational framework known as reservoir\ncomputing. Notably, we reveal a bidirectional coupling between prediction\nperformance and emergence, which generalises across task environments and\nreservoir network topologies, and is recapitulated by three key results: 1)\nOptimising hyperparameters for performance enhances emergent dynamics, and vice\nversa; 2) Emergent dynamics represent a near sufficient criterion for\nprediction success in all task environments, and an almost necessary criterion\nin most environments; 3) Training reservoir computers on larger datasets\nresults in stronger emergent dynamics, which contain task-relevant information\ncrucial for performance. Overall, our study points to a pivotal role of\nemergence in facilitating environmental predictions in a bio-inspired\ncomputational architecture.",
        "chunk-id": 3,
        "chunk": "computing. Notably, we reveal a bidirectional coupling between prediction\nperformance and emergence, which generalises across task environments and\nreservoir network topologies, and is recapitulated by three key results: 1)\nOptimising hyperparameters for performance enhances emergent dynamics, and vice\nversa; 2) Emergent dynamics represent a near sufficient criterion for",
        "authors": [
            "Hanna M. Tolle",
            "Andrea I Luppi",
            "Anil K. Seth",
            "Pedro A. M. Mediano"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:19:30+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19201v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19201v1",
        "categories": [
            "Neurons and Cognition"
        ]
    },
    {
        "id": 30000079,
        "doi": null,
        "title": "Evolving reservoir computers reveals bidirectional coupling between predictive power and emergent dynamics",
        "abstract": "Biological neural networks can perform complex computations to predict their\nenvironment, far above the limited predictive capabilities of individual\nneurons. While conventional approaches to understanding these computations\noften focus on isolating the contributions of single neurons, here we argue\nthat a deeper understanding requires considering emergent dynamics - dynamics\nthat make the whole system \"more than the sum of its parts\". Specifically, we\nexamine the relationship between prediction performance and emergence by\nleveraging recent quantitative metrics of emergence, derived from Partial\nInformation Decomposition, and by modelling the prediction of environmental\ndynamics in a bio-inspired computational framework known as reservoir\ncomputing. Notably, we reveal a bidirectional coupling between prediction\nperformance and emergence, which generalises across task environments and\nreservoir network topologies, and is recapitulated by three key results: 1)\nOptimising hyperparameters for performance enhances emergent dynamics, and vice\nversa; 2) Emergent dynamics represent a near sufficient criterion for\nprediction success in all task environments, and an almost necessary criterion\nin most environments; 3) Training reservoir computers on larger datasets\nresults in stronger emergent dynamics, which contain task-relevant information\ncrucial for performance. Overall, our study points to a pivotal role of\nemergence in facilitating environmental predictions in a bio-inspired\ncomputational architecture.",
        "chunk-id": 4,
        "chunk": "prediction success in all task environments, and an almost necessary criterion\nin most environments; 3) Training reservoir computers on larger datasets\nresults in stronger emergent dynamics, which contain task-relevant information\ncrucial for performance. Overall, our study points to a pivotal role of\nemergence in facilitating environmental predictions in a bio-inspired",
        "authors": [
            "Hanna M. Tolle",
            "Andrea I Luppi",
            "Anil K. Seth",
            "Pedro A. M. Mediano"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:19:30+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19201v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19201v1",
        "categories": [
            "Neurons and Cognition"
        ]
    },
    {
        "id": 30000079,
        "doi": null,
        "title": "Evolving reservoir computers reveals bidirectional coupling between predictive power and emergent dynamics",
        "abstract": "Biological neural networks can perform complex computations to predict their\nenvironment, far above the limited predictive capabilities of individual\nneurons. While conventional approaches to understanding these computations\noften focus on isolating the contributions of single neurons, here we argue\nthat a deeper understanding requires considering emergent dynamics - dynamics\nthat make the whole system \"more than the sum of its parts\". Specifically, we\nexamine the relationship between prediction performance and emergence by\nleveraging recent quantitative metrics of emergence, derived from Partial\nInformation Decomposition, and by modelling the prediction of environmental\ndynamics in a bio-inspired computational framework known as reservoir\ncomputing. Notably, we reveal a bidirectional coupling between prediction\nperformance and emergence, which generalises across task environments and\nreservoir network topologies, and is recapitulated by three key results: 1)\nOptimising hyperparameters for performance enhances emergent dynamics, and vice\nversa; 2) Emergent dynamics represent a near sufficient criterion for\nprediction success in all task environments, and an almost necessary criterion\nin most environments; 3) Training reservoir computers on larger datasets\nresults in stronger emergent dynamics, which contain task-relevant information\ncrucial for performance. Overall, our study points to a pivotal role of\nemergence in facilitating environmental predictions in a bio-inspired\ncomputational architecture.",
        "chunk-id": 5,
        "chunk": "computational architecture.",
        "authors": [
            "Hanna M. Tolle",
            "Andrea I Luppi",
            "Anil K. Seth",
            "Pedro A. M. Mediano"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:19:30+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19201v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19201v1",
        "categories": [
            "Neurons and Cognition"
        ]
    },
    {
        "id": 30000080,
        "doi": null,
        "title": "Light and strange vector resonances from lattice QCD at physical quark masses",
        "abstract": "We present the first ab initio calculation at physical quark masses of\nscattering amplitudes describing the lightest pseudoscalar mesons interacting\nvia the strong force in the vector channel. Using lattice quantum\nchromodynamics, we postdict the defining parameters for two short-lived\nresonances, the $\\rho(770)$ and $K^*(892)$, which manifest as complex energy\npoles in $\\pi \\pi$ and $K \\pi$ scattering amplitudes, respectively. The\ncalculation proceeds by first computing the finite-volume energy spectrum of\nthe two-hadron systems, and then determining the amplitudes from the energies\nusing the L\\\"uscher formalism. The error budget includes a data-driven\nsystematic error, obtained by scanning possible fit ranges and fit models to\nextract the spectrum from Euclidean correlators, as well as the scattering\namplitudes from the latter. The final results, obtained by analytically\ncontinuing multiple parameterizations into the complex energy plane, are\n$M_\\rho = 796(5)(50)~\\mathrm{MeV}$, $\\Gamma_\\rho = 192(10)(31)~\\mathrm{MeV}$,\n$M_{K^*} = 893(2)(54)~\\mathrm{MeV}$ and $\\Gamma_{K^*} =\n51(2)(11)~\\mathrm{MeV}$, where the subscript indicates the resonance and $M$\nand $\\Gamma$ stand for the mass and width, respectively, and where the first\nbracket indicates the statistical and the second bracket the systematic\nuncertainty.",
        "chunk-id": 1,
        "chunk": "We present the first ab initio calculation at physical quark masses of\nscattering amplitudes describing the lightest pseudoscalar mesons interacting\nvia the strong force in the vector channel. Using lattice quantum\nchromodynamics, we postdict the defining parameters for two short-lived\nresonances, the $\\rho(770)$ and $K^*(892)$, which manifest as complex energy",
        "authors": [
            "Peter Boyle",
            "Felix Erben",
            "Vera G\u00fclpers",
            "Maxwell T. Hansen",
            "Fabian Joswig",
            "Nelson Pitanga Lachini",
            "Michael Marshall",
            "Antonin Portelli"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:13:31+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19194v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19194v1",
        "categories": [
            "High Energy Physics - Lattice",
            "High Energy Physics - Phenomenology"
        ]
    },
    {
        "id": 30000080,
        "doi": null,
        "title": "Light and strange vector resonances from lattice QCD at physical quark masses",
        "abstract": "We present the first ab initio calculation at physical quark masses of\nscattering amplitudes describing the lightest pseudoscalar mesons interacting\nvia the strong force in the vector channel. Using lattice quantum\nchromodynamics, we postdict the defining parameters for two short-lived\nresonances, the $\\rho(770)$ and $K^*(892)$, which manifest as complex energy\npoles in $\\pi \\pi$ and $K \\pi$ scattering amplitudes, respectively. The\ncalculation proceeds by first computing the finite-volume energy spectrum of\nthe two-hadron systems, and then determining the amplitudes from the energies\nusing the L\\\"uscher formalism. The error budget includes a data-driven\nsystematic error, obtained by scanning possible fit ranges and fit models to\nextract the spectrum from Euclidean correlators, as well as the scattering\namplitudes from the latter. The final results, obtained by analytically\ncontinuing multiple parameterizations into the complex energy plane, are\n$M_\\rho = 796(5)(50)~\\mathrm{MeV}$, $\\Gamma_\\rho = 192(10)(31)~\\mathrm{MeV}$,\n$M_{K^*} = 893(2)(54)~\\mathrm{MeV}$ and $\\Gamma_{K^*} =\n51(2)(11)~\\mathrm{MeV}$, where the subscript indicates the resonance and $M$\nand $\\Gamma$ stand for the mass and width, respectively, and where the first\nbracket indicates the statistical and the second bracket the systematic\nuncertainty.",
        "chunk-id": 2,
        "chunk": "poles in $\\pi \\pi$ and $K \\pi$ scattering amplitudes, respectively. The\ncalculation proceeds by first computing the finite-volume energy spectrum of\nthe two-hadron systems, and then determining the amplitudes from the energies\nusing the L\\\"uscher formalism. The error budget includes a data-driven\nsystematic error, obtained by scanning possible fit ranges and fit models to",
        "authors": [
            "Peter Boyle",
            "Felix Erben",
            "Vera G\u00fclpers",
            "Maxwell T. Hansen",
            "Fabian Joswig",
            "Nelson Pitanga Lachini",
            "Michael Marshall",
            "Antonin Portelli"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:13:31+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19194v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19194v1",
        "categories": [
            "High Energy Physics - Lattice",
            "High Energy Physics - Phenomenology"
        ]
    },
    {
        "id": 30000080,
        "doi": null,
        "title": "Light and strange vector resonances from lattice QCD at physical quark masses",
        "abstract": "We present the first ab initio calculation at physical quark masses of\nscattering amplitudes describing the lightest pseudoscalar mesons interacting\nvia the strong force in the vector channel. Using lattice quantum\nchromodynamics, we postdict the defining parameters for two short-lived\nresonances, the $\\rho(770)$ and $K^*(892)$, which manifest as complex energy\npoles in $\\pi \\pi$ and $K \\pi$ scattering amplitudes, respectively. The\ncalculation proceeds by first computing the finite-volume energy spectrum of\nthe two-hadron systems, and then determining the amplitudes from the energies\nusing the L\\\"uscher formalism. The error budget includes a data-driven\nsystematic error, obtained by scanning possible fit ranges and fit models to\nextract the spectrum from Euclidean correlators, as well as the scattering\namplitudes from the latter. The final results, obtained by analytically\ncontinuing multiple parameterizations into the complex energy plane, are\n$M_\\rho = 796(5)(50)~\\mathrm{MeV}$, $\\Gamma_\\rho = 192(10)(31)~\\mathrm{MeV}$,\n$M_{K^*} = 893(2)(54)~\\mathrm{MeV}$ and $\\Gamma_{K^*} =\n51(2)(11)~\\mathrm{MeV}$, where the subscript indicates the resonance and $M$\nand $\\Gamma$ stand for the mass and width, respectively, and where the first\nbracket indicates the statistical and the second bracket the systematic\nuncertainty.",
        "chunk-id": 3,
        "chunk": "extract the spectrum from Euclidean correlators, as well as the scattering\namplitudes from the latter. The final results, obtained by analytically\ncontinuing multiple parameterizations into the complex energy plane, are\n$M_\\rho = 796(5)(50)~\\mathrm{MeV}$, $\\Gamma_\\rho = 192(10)(31)~\\mathrm{MeV}$,\n$M_{K^*} = 893(2)(54)~\\mathrm{MeV}$ and $\\Gamma_{K^*} =",
        "authors": [
            "Peter Boyle",
            "Felix Erben",
            "Vera G\u00fclpers",
            "Maxwell T. Hansen",
            "Fabian Joswig",
            "Nelson Pitanga Lachini",
            "Michael Marshall",
            "Antonin Portelli"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:13:31+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19194v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19194v1",
        "categories": [
            "High Energy Physics - Lattice",
            "High Energy Physics - Phenomenology"
        ]
    },
    {
        "id": 30000080,
        "doi": null,
        "title": "Light and strange vector resonances from lattice QCD at physical quark masses",
        "abstract": "We present the first ab initio calculation at physical quark masses of\nscattering amplitudes describing the lightest pseudoscalar mesons interacting\nvia the strong force in the vector channel. Using lattice quantum\nchromodynamics, we postdict the defining parameters for two short-lived\nresonances, the $\\rho(770)$ and $K^*(892)$, which manifest as complex energy\npoles in $\\pi \\pi$ and $K \\pi$ scattering amplitudes, respectively. The\ncalculation proceeds by first computing the finite-volume energy spectrum of\nthe two-hadron systems, and then determining the amplitudes from the energies\nusing the L\\\"uscher formalism. The error budget includes a data-driven\nsystematic error, obtained by scanning possible fit ranges and fit models to\nextract the spectrum from Euclidean correlators, as well as the scattering\namplitudes from the latter. The final results, obtained by analytically\ncontinuing multiple parameterizations into the complex energy plane, are\n$M_\\rho = 796(5)(50)~\\mathrm{MeV}$, $\\Gamma_\\rho = 192(10)(31)~\\mathrm{MeV}$,\n$M_{K^*} = 893(2)(54)~\\mathrm{MeV}$ and $\\Gamma_{K^*} =\n51(2)(11)~\\mathrm{MeV}$, where the subscript indicates the resonance and $M$\nand $\\Gamma$ stand for the mass and width, respectively, and where the first\nbracket indicates the statistical and the second bracket the systematic\nuncertainty.",
        "chunk-id": 4,
        "chunk": "51(2)(11)~\\mathrm{MeV}$, where the subscript indicates the resonance and $M$\nand $\\Gamma$ stand for the mass and width, respectively, and where the first\nbracket indicates the statistical and the second bracket the systematic\nuncertainty.",
        "authors": [
            "Peter Boyle",
            "Felix Erben",
            "Vera G\u00fclpers",
            "Maxwell T. Hansen",
            "Fabian Joswig",
            "Nelson Pitanga Lachini",
            "Michael Marshall",
            "Antonin Portelli"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:13:31+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19194v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19194v1",
        "categories": [
            "High Energy Physics - Lattice",
            "High Energy Physics - Phenomenology"
        ]
    },
    {
        "id": 30000081,
        "doi": null,
        "title": "Physical-mass calculation of $\u03c1(770)$ and $K^*(892)$ resonance parameters via $\u03c0\u03c0$ and $K \u03c0$ scattering amplitudes from lattice QCD",
        "abstract": "We present our study of the $\\rho(770)$ and $K^*(892)$ resonances from\nlattice quantum chromodynamics (QCD) employing domain-wall fermions at physical\nquark masses. We determine the finite-volume energy spectrum in various\nmomentum frames and obtain phase-shift parameterizations via the L\\\"uscher\nformalism, and as a final step the complex resonance poles of the $\\pi \\pi$ and\n$K \\pi$ elastic scattering amplitudes via an analytical continuation of the\nmodels. By sampling a large number of representative sets of underlying\nenergy-level fits, we also assign a systematic uncertainty to our final\nresults. This is a significant extension to data-driven analysis methods that\nhave been used in lattice QCD to date, due to the two-step nature of the\nformalism. Our final pole positions, $M+i\\Gamma/2$, with all statistical and\nsystematic errors exposed, are $M_{K^{*}} = 893(2)(8)(54)(2)~\\mathrm{MeV}$ and\n$\\Gamma_{K^{*}} = 51(2)(11)(3)(0)~\\mathrm{MeV}$ for the $K^*(892)$ resonance\nand $M_{\\rho} = 796(5)(15)(48)(2)~\\mathrm{MeV}$ and $\\Gamma_{\\rho} =\n192(10)(28)(12)(0)~\\mathrm{MeV}$ for the $\\rho(770)$ resonance. The four\ndifferently grouped sources of uncertainties are, in the order of occurrence:\nstatistical, data-driven systematic, an estimation of systematic effects beyond\nour computation (dominated by the fact that we employ a single lattice\nspacing), and the error from the scale-setting uncertainty on our ensemble.",
        "chunk-id": 1,
        "chunk": "We present our study of the $\\rho(770)$ and $K^*(892)$ resonances from\nlattice quantum chromodynamics (QCD) employing domain-wall fermions at physical\nquark masses. We determine the finite-volume energy spectrum in various\nmomentum frames and obtain phase-shift parameterizations via the L\\\"uscher\nformalism, and as a final step the complex resonance poles of the $\\pi \\pi$ and",
        "authors": [
            "Peter Boyle",
            "Felix Erben",
            "Vera G\u00fclpers",
            "Maxwell T. Hansen",
            "Fabian Joswig",
            "Nelson Pitanga Lachini",
            "Michael Marshall",
            "Antonin Portelli"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:12:11+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19193v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19193v1",
        "categories": [
            "High Energy Physics - Lattice",
            "High Energy Physics - Phenomenology"
        ]
    },
    {
        "id": 30000081,
        "doi": null,
        "title": "Physical-mass calculation of $\u03c1(770)$ and $K^*(892)$ resonance parameters via $\u03c0\u03c0$ and $K \u03c0$ scattering amplitudes from lattice QCD",
        "abstract": "We present our study of the $\\rho(770)$ and $K^*(892)$ resonances from\nlattice quantum chromodynamics (QCD) employing domain-wall fermions at physical\nquark masses. We determine the finite-volume energy spectrum in various\nmomentum frames and obtain phase-shift parameterizations via the L\\\"uscher\nformalism, and as a final step the complex resonance poles of the $\\pi \\pi$ and\n$K \\pi$ elastic scattering amplitudes via an analytical continuation of the\nmodels. By sampling a large number of representative sets of underlying\nenergy-level fits, we also assign a systematic uncertainty to our final\nresults. This is a significant extension to data-driven analysis methods that\nhave been used in lattice QCD to date, due to the two-step nature of the\nformalism. Our final pole positions, $M+i\\Gamma/2$, with all statistical and\nsystematic errors exposed, are $M_{K^{*}} = 893(2)(8)(54)(2)~\\mathrm{MeV}$ and\n$\\Gamma_{K^{*}} = 51(2)(11)(3)(0)~\\mathrm{MeV}$ for the $K^*(892)$ resonance\nand $M_{\\rho} = 796(5)(15)(48)(2)~\\mathrm{MeV}$ and $\\Gamma_{\\rho} =\n192(10)(28)(12)(0)~\\mathrm{MeV}$ for the $\\rho(770)$ resonance. The four\ndifferently grouped sources of uncertainties are, in the order of occurrence:\nstatistical, data-driven systematic, an estimation of systematic effects beyond\nour computation (dominated by the fact that we employ a single lattice\nspacing), and the error from the scale-setting uncertainty on our ensemble.",
        "chunk-id": 2,
        "chunk": "$K \\pi$ elastic scattering amplitudes via an analytical continuation of the\nmodels. By sampling a large number of representative sets of underlying\nenergy-level fits, we also assign a systematic uncertainty to our final\nresults. This is a significant extension to data-driven analysis methods that\nhave been used in lattice QCD to date, due to the two-step nature of the",
        "authors": [
            "Peter Boyle",
            "Felix Erben",
            "Vera G\u00fclpers",
            "Maxwell T. Hansen",
            "Fabian Joswig",
            "Nelson Pitanga Lachini",
            "Michael Marshall",
            "Antonin Portelli"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:12:11+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19193v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19193v1",
        "categories": [
            "High Energy Physics - Lattice",
            "High Energy Physics - Phenomenology"
        ]
    },
    {
        "id": 30000081,
        "doi": null,
        "title": "Physical-mass calculation of $\u03c1(770)$ and $K^*(892)$ resonance parameters via $\u03c0\u03c0$ and $K \u03c0$ scattering amplitudes from lattice QCD",
        "abstract": "We present our study of the $\\rho(770)$ and $K^*(892)$ resonances from\nlattice quantum chromodynamics (QCD) employing domain-wall fermions at physical\nquark masses. We determine the finite-volume energy spectrum in various\nmomentum frames and obtain phase-shift parameterizations via the L\\\"uscher\nformalism, and as a final step the complex resonance poles of the $\\pi \\pi$ and\n$K \\pi$ elastic scattering amplitudes via an analytical continuation of the\nmodels. By sampling a large number of representative sets of underlying\nenergy-level fits, we also assign a systematic uncertainty to our final\nresults. This is a significant extension to data-driven analysis methods that\nhave been used in lattice QCD to date, due to the two-step nature of the\nformalism. Our final pole positions, $M+i\\Gamma/2$, with all statistical and\nsystematic errors exposed, are $M_{K^{*}} = 893(2)(8)(54)(2)~\\mathrm{MeV}$ and\n$\\Gamma_{K^{*}} = 51(2)(11)(3)(0)~\\mathrm{MeV}$ for the $K^*(892)$ resonance\nand $M_{\\rho} = 796(5)(15)(48)(2)~\\mathrm{MeV}$ and $\\Gamma_{\\rho} =\n192(10)(28)(12)(0)~\\mathrm{MeV}$ for the $\\rho(770)$ resonance. The four\ndifferently grouped sources of uncertainties are, in the order of occurrence:\nstatistical, data-driven systematic, an estimation of systematic effects beyond\nour computation (dominated by the fact that we employ a single lattice\nspacing), and the error from the scale-setting uncertainty on our ensemble.",
        "chunk-id": 3,
        "chunk": "formalism. Our final pole positions, $M+i\\Gamma/2$, with all statistical and\nsystematic errors exposed, are $M_{K^{*}} = 893(2)(8)(54)(2)~\\mathrm{MeV}$ and\n$\\Gamma_{K^{*}} = 51(2)(11)(3)(0)~\\mathrm{MeV}$ for the $K^*(892)$ resonance\nand $M_{\\rho} = 796(5)(15)(48)(2)~\\mathrm{MeV}$ and $\\Gamma_{\\rho} =\n192(10)(28)(12)(0)~\\mathrm{MeV}$ for the $\\rho(770)$ resonance. The four",
        "authors": [
            "Peter Boyle",
            "Felix Erben",
            "Vera G\u00fclpers",
            "Maxwell T. Hansen",
            "Fabian Joswig",
            "Nelson Pitanga Lachini",
            "Michael Marshall",
            "Antonin Portelli"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:12:11+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19193v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19193v1",
        "categories": [
            "High Energy Physics - Lattice",
            "High Energy Physics - Phenomenology"
        ]
    },
    {
        "id": 30000081,
        "doi": null,
        "title": "Physical-mass calculation of $\u03c1(770)$ and $K^*(892)$ resonance parameters via $\u03c0\u03c0$ and $K \u03c0$ scattering amplitudes from lattice QCD",
        "abstract": "We present our study of the $\\rho(770)$ and $K^*(892)$ resonances from\nlattice quantum chromodynamics (QCD) employing domain-wall fermions at physical\nquark masses. We determine the finite-volume energy spectrum in various\nmomentum frames and obtain phase-shift parameterizations via the L\\\"uscher\nformalism, and as a final step the complex resonance poles of the $\\pi \\pi$ and\n$K \\pi$ elastic scattering amplitudes via an analytical continuation of the\nmodels. By sampling a large number of representative sets of underlying\nenergy-level fits, we also assign a systematic uncertainty to our final\nresults. This is a significant extension to data-driven analysis methods that\nhave been used in lattice QCD to date, due to the two-step nature of the\nformalism. Our final pole positions, $M+i\\Gamma/2$, with all statistical and\nsystematic errors exposed, are $M_{K^{*}} = 893(2)(8)(54)(2)~\\mathrm{MeV}$ and\n$\\Gamma_{K^{*}} = 51(2)(11)(3)(0)~\\mathrm{MeV}$ for the $K^*(892)$ resonance\nand $M_{\\rho} = 796(5)(15)(48)(2)~\\mathrm{MeV}$ and $\\Gamma_{\\rho} =\n192(10)(28)(12)(0)~\\mathrm{MeV}$ for the $\\rho(770)$ resonance. The four\ndifferently grouped sources of uncertainties are, in the order of occurrence:\nstatistical, data-driven systematic, an estimation of systematic effects beyond\nour computation (dominated by the fact that we employ a single lattice\nspacing), and the error from the scale-setting uncertainty on our ensemble.",
        "chunk-id": 4,
        "chunk": "differently grouped sources of uncertainties are, in the order of occurrence:\nstatistical, data-driven systematic, an estimation of systematic effects beyond\nour computation (dominated by the fact that we employ a single lattice\nspacing), and the error from the scale-setting uncertainty on our ensemble.",
        "authors": [
            "Peter Boyle",
            "Felix Erben",
            "Vera G\u00fclpers",
            "Maxwell T. Hansen",
            "Fabian Joswig",
            "Nelson Pitanga Lachini",
            "Michael Marshall",
            "Antonin Portelli"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:12:11+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19193v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19193v1",
        "categories": [
            "High Energy Physics - Lattice",
            "High Energy Physics - Phenomenology"
        ]
    },
    {
        "id": 30000082,
        "doi": null,
        "title": "Averaging log-likelihoods in direct alignment",
        "abstract": "To better align Large Language Models (LLMs) with human judgment,\nReinforcement Learning from Human Feedback (RLHF) learns a reward model and\nthen optimizes it using regularized RL. Recently, direct alignment methods were\nintroduced to learn such a fine-tuned model directly from a preference dataset\nwithout computing a proxy reward function. These methods are built upon\ncontrastive losses involving the log-likelihood of (dis)preferred completions\naccording to the trained model. However, completions have various lengths, and\nthe log-likelihood is not length-invariant. On the other side, the\ncross-entropy loss used in supervised training is length-invariant, as batches\nare typically averaged token-wise. To reconcile these approaches, we introduce\na principled approach for making direct alignment length-invariant. Formally,\nwe introduce a new averaging operator, to be composed with the optimality\noperator giving the best policy for the underlying RL problem. It translates\ninto averaging the log-likelihood within the loss. We empirically study the\neffect of such averaging, observing a trade-off between the length of\ngenerations and their scores.",
        "chunk-id": 1,
        "chunk": "To better align Large Language Models (LLMs) with human judgment,\nReinforcement Learning from Human Feedback (RLHF) learns a reward model and\nthen optimizes it using regularized RL. Recently, direct alignment methods were\nintroduced to learn such a fine-tuned model directly from a preference dataset\nwithout computing a proxy reward function. These methods are built upon",
        "authors": [
            "Nathan Grinsztajn",
            "Yannis Flet-Berliac",
            "Mohammad Gheshlaghi Azar",
            "Florian Strub",
            "Bill Wu",
            "Eugene Choi",
            "Chris Cremer",
            "Arash Ahmadian",
            "Yash Chandak",
            "Olivier Pietquin",
            "Matthieu Geist"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:07:38+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19188v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19188v1",
        "categories": [
            "Machine Learning"
        ]
    },
    {
        "id": 30000082,
        "doi": null,
        "title": "Averaging log-likelihoods in direct alignment",
        "abstract": "To better align Large Language Models (LLMs) with human judgment,\nReinforcement Learning from Human Feedback (RLHF) learns a reward model and\nthen optimizes it using regularized RL. Recently, direct alignment methods were\nintroduced to learn such a fine-tuned model directly from a preference dataset\nwithout computing a proxy reward function. These methods are built upon\ncontrastive losses involving the log-likelihood of (dis)preferred completions\naccording to the trained model. However, completions have various lengths, and\nthe log-likelihood is not length-invariant. On the other side, the\ncross-entropy loss used in supervised training is length-invariant, as batches\nare typically averaged token-wise. To reconcile these approaches, we introduce\na principled approach for making direct alignment length-invariant. Formally,\nwe introduce a new averaging operator, to be composed with the optimality\noperator giving the best policy for the underlying RL problem. It translates\ninto averaging the log-likelihood within the loss. We empirically study the\neffect of such averaging, observing a trade-off between the length of\ngenerations and their scores.",
        "chunk-id": 2,
        "chunk": "contrastive losses involving the log-likelihood of (dis)preferred completions\naccording to the trained model. However, completions have various lengths, and\nthe log-likelihood is not length-invariant. On the other side, the\ncross-entropy loss used in supervised training is length-invariant, as batches\nare typically averaged token-wise. To reconcile these approaches, we introduce",
        "authors": [
            "Nathan Grinsztajn",
            "Yannis Flet-Berliac",
            "Mohammad Gheshlaghi Azar",
            "Florian Strub",
            "Bill Wu",
            "Eugene Choi",
            "Chris Cremer",
            "Arash Ahmadian",
            "Yash Chandak",
            "Olivier Pietquin",
            "Matthieu Geist"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:07:38+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19188v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19188v1",
        "categories": [
            "Machine Learning"
        ]
    },
    {
        "id": 30000082,
        "doi": null,
        "title": "Averaging log-likelihoods in direct alignment",
        "abstract": "To better align Large Language Models (LLMs) with human judgment,\nReinforcement Learning from Human Feedback (RLHF) learns a reward model and\nthen optimizes it using regularized RL. Recently, direct alignment methods were\nintroduced to learn such a fine-tuned model directly from a preference dataset\nwithout computing a proxy reward function. These methods are built upon\ncontrastive losses involving the log-likelihood of (dis)preferred completions\naccording to the trained model. However, completions have various lengths, and\nthe log-likelihood is not length-invariant. On the other side, the\ncross-entropy loss used in supervised training is length-invariant, as batches\nare typically averaged token-wise. To reconcile these approaches, we introduce\na principled approach for making direct alignment length-invariant. Formally,\nwe introduce a new averaging operator, to be composed with the optimality\noperator giving the best policy for the underlying RL problem. It translates\ninto averaging the log-likelihood within the loss. We empirically study the\neffect of such averaging, observing a trade-off between the length of\ngenerations and their scores.",
        "chunk-id": 3,
        "chunk": "a principled approach for making direct alignment length-invariant. Formally,\nwe introduce a new averaging operator, to be composed with the optimality\noperator giving the best policy for the underlying RL problem. It translates\ninto averaging the log-likelihood within the loss. We empirically study the\neffect of such averaging, observing a trade-off between the length of",
        "authors": [
            "Nathan Grinsztajn",
            "Yannis Flet-Berliac",
            "Mohammad Gheshlaghi Azar",
            "Florian Strub",
            "Bill Wu",
            "Eugene Choi",
            "Chris Cremer",
            "Arash Ahmadian",
            "Yash Chandak",
            "Olivier Pietquin",
            "Matthieu Geist"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:07:38+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19188v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19188v1",
        "categories": [
            "Machine Learning"
        ]
    },
    {
        "id": 30000082,
        "doi": null,
        "title": "Averaging log-likelihoods in direct alignment",
        "abstract": "To better align Large Language Models (LLMs) with human judgment,\nReinforcement Learning from Human Feedback (RLHF) learns a reward model and\nthen optimizes it using regularized RL. Recently, direct alignment methods were\nintroduced to learn such a fine-tuned model directly from a preference dataset\nwithout computing a proxy reward function. These methods are built upon\ncontrastive losses involving the log-likelihood of (dis)preferred completions\naccording to the trained model. However, completions have various lengths, and\nthe log-likelihood is not length-invariant. On the other side, the\ncross-entropy loss used in supervised training is length-invariant, as batches\nare typically averaged token-wise. To reconcile these approaches, we introduce\na principled approach for making direct alignment length-invariant. Formally,\nwe introduce a new averaging operator, to be composed with the optimality\noperator giving the best policy for the underlying RL problem. It translates\ninto averaging the log-likelihood within the loss. We empirically study the\neffect of such averaging, observing a trade-off between the length of\ngenerations and their scores.",
        "chunk-id": 4,
        "chunk": "generations and their scores.",
        "authors": [
            "Nathan Grinsztajn",
            "Yannis Flet-Berliac",
            "Mohammad Gheshlaghi Azar",
            "Florian Strub",
            "Bill Wu",
            "Eugene Choi",
            "Chris Cremer",
            "Arash Ahmadian",
            "Yash Chandak",
            "Olivier Pietquin",
            "Matthieu Geist"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:07:38+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19188v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19188v1",
        "categories": [
            "Machine Learning"
        ]
    },
    {
        "id": 30000083,
        "doi": null,
        "title": "On asymptotic independence in higher dimensions",
        "abstract": "In the study of extremes, the presence of asymptotic independence signifies\nthat extreme events across multiple variables are probably less likely to occur\ntogether. Although well-understood in a bivariate context, the concept remains\nrelatively unexplored when addressing the nuances of joint occurrence of\nextremes in higher dimensions. In this paper, we propose a notion of mutual\nasymptotic independence to capture the behavior of joint extremes in dimensions\nlarger than two and contrast it with the classical notion of (pairwise)\nasymptotic independence. Furthermore, we define $k$-wise asymptotic\nindependence which lies in between pairwise and mutual asymptotic independence.\nThe concepts are compared using examples of Archimedean, Gaussian and\nMarshall-Olkin copulas among others. Notably, for the popular Gaussian copula,\nwe provide explicit conditions on the correlation matrix for mutual asymptotic\nindependence to hold; moreover, we are able to compute exact tail orders for\nvarious tail events.",
        "chunk-id": 1,
        "chunk": "In the study of extremes, the presence of asymptotic independence signifies\nthat extreme events across multiple variables are probably less likely to occur\ntogether. Although well-understood in a bivariate context, the concept remains\nrelatively unexplored when addressing the nuances of joint occurrence of\nextremes in higher dimensions. In this paper, we propose a notion of mutual",
        "authors": [
            "Bikramjit Das",
            "Vicky Fasen-Hartmann"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:05:36+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19186v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19186v1",
        "categories": [
            "Statistics Theory",
            "Statistics Theory",
            "Primary 62H05, 62H20, 62G32, Secondary 60E05"
        ]
    },
    {
        "id": 30000083,
        "doi": null,
        "title": "On asymptotic independence in higher dimensions",
        "abstract": "In the study of extremes, the presence of asymptotic independence signifies\nthat extreme events across multiple variables are probably less likely to occur\ntogether. Although well-understood in a bivariate context, the concept remains\nrelatively unexplored when addressing the nuances of joint occurrence of\nextremes in higher dimensions. In this paper, we propose a notion of mutual\nasymptotic independence to capture the behavior of joint extremes in dimensions\nlarger than two and contrast it with the classical notion of (pairwise)\nasymptotic independence. Furthermore, we define $k$-wise asymptotic\nindependence which lies in between pairwise and mutual asymptotic independence.\nThe concepts are compared using examples of Archimedean, Gaussian and\nMarshall-Olkin copulas among others. Notably, for the popular Gaussian copula,\nwe provide explicit conditions on the correlation matrix for mutual asymptotic\nindependence to hold; moreover, we are able to compute exact tail orders for\nvarious tail events.",
        "chunk-id": 2,
        "chunk": "asymptotic independence to capture the behavior of joint extremes in dimensions\nlarger than two and contrast it with the classical notion of (pairwise)\nasymptotic independence. Furthermore, we define $k$-wise asymptotic\nindependence which lies in between pairwise and mutual asymptotic independence.\nThe concepts are compared using examples of Archimedean, Gaussian and",
        "authors": [
            "Bikramjit Das",
            "Vicky Fasen-Hartmann"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:05:36+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19186v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19186v1",
        "categories": [
            "Statistics Theory",
            "Statistics Theory",
            "Primary 62H05, 62H20, 62G32, Secondary 60E05"
        ]
    },
    {
        "id": 30000083,
        "doi": null,
        "title": "On asymptotic independence in higher dimensions",
        "abstract": "In the study of extremes, the presence of asymptotic independence signifies\nthat extreme events across multiple variables are probably less likely to occur\ntogether. Although well-understood in a bivariate context, the concept remains\nrelatively unexplored when addressing the nuances of joint occurrence of\nextremes in higher dimensions. In this paper, we propose a notion of mutual\nasymptotic independence to capture the behavior of joint extremes in dimensions\nlarger than two and contrast it with the classical notion of (pairwise)\nasymptotic independence. Furthermore, we define $k$-wise asymptotic\nindependence which lies in between pairwise and mutual asymptotic independence.\nThe concepts are compared using examples of Archimedean, Gaussian and\nMarshall-Olkin copulas among others. Notably, for the popular Gaussian copula,\nwe provide explicit conditions on the correlation matrix for mutual asymptotic\nindependence to hold; moreover, we are able to compute exact tail orders for\nvarious tail events.",
        "chunk-id": 3,
        "chunk": "Marshall-Olkin copulas among others. Notably, for the popular Gaussian copula,\nwe provide explicit conditions on the correlation matrix for mutual asymptotic\nindependence to hold; moreover, we are able to compute exact tail orders for\nvarious tail events.",
        "authors": [
            "Bikramjit Das",
            "Vicky Fasen-Hartmann"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T14:05:36+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19186v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19186v1",
        "categories": [
            "Statistics Theory",
            "Statistics Theory",
            "Primary 62H05, 62H20, 62G32, Secondary 60E05"
        ]
    },
    {
        "id": 30000084,
        "doi": null,
        "title": "Towards Reducing Data Acquisition and Labeling for Defect Detection using Simulated Data",
        "abstract": "In many manufacturing settings, annotating data for machine learning and\ncomputer vision is costly, but synthetic data can be generated at significantly\nlower cost. Substituting the real-world data with synthetic data is therefore\nappealing for many machine learning applications that require large amounts of\ntraining data. However, relying solely on synthetic data is frequently\ninadequate for effectively training models that perform well on real-world\ndata, primarily due to domain shifts between the synthetic and real-world data.\nWe discuss approaches for dealing with such a domain shift when detecting\ndefects in X-ray scans of aluminium wheels. Using both simulated and real-world\nX-ray images, we train an object detection model with different strategies to\nidentify the training approach that generates the best detection results while\nminimising the demand for annotated real-world training samples. Our\npreliminary findings suggest that the sim-2-real domain adaptation approach is\nmore cost-efficient than a fully supervised oracle - if the total number of\navailable annotated samples is fixed. Given a certain number of labeled\nreal-world samples, training on a mix of synthetic and unlabeled real-world\ndata achieved comparable or even better detection results at significantly\nlower cost. We argue that future research into the cost-efficiency of different\ntraining strategies is important for a better understanding of how to allocate\nbudget in applied machine learning projects.",
        "chunk-id": 1,
        "chunk": "In many manufacturing settings, annotating data for machine learning and\ncomputer vision is costly, but synthetic data can be generated at significantly\nlower cost. Substituting the real-world data with synthetic data is therefore\nappealing for many machine learning applications that require large amounts of\ntraining data. However, relying solely on synthetic data is frequently",
        "authors": [
            "Lukas Malte Kemeter",
            "Rasmus Hvingelby",
            "Paulina Sierak",
            "Tobias Sch\u00f6n",
            "Bishwajit Gosswam"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:51:53+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19175v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19175v1",
        "categories": [
            "Machine Learning",
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000084,
        "doi": null,
        "title": "Towards Reducing Data Acquisition and Labeling for Defect Detection using Simulated Data",
        "abstract": "In many manufacturing settings, annotating data for machine learning and\ncomputer vision is costly, but synthetic data can be generated at significantly\nlower cost. Substituting the real-world data with synthetic data is therefore\nappealing for many machine learning applications that require large amounts of\ntraining data. However, relying solely on synthetic data is frequently\ninadequate for effectively training models that perform well on real-world\ndata, primarily due to domain shifts between the synthetic and real-world data.\nWe discuss approaches for dealing with such a domain shift when detecting\ndefects in X-ray scans of aluminium wheels. Using both simulated and real-world\nX-ray images, we train an object detection model with different strategies to\nidentify the training approach that generates the best detection results while\nminimising the demand for annotated real-world training samples. Our\npreliminary findings suggest that the sim-2-real domain adaptation approach is\nmore cost-efficient than a fully supervised oracle - if the total number of\navailable annotated samples is fixed. Given a certain number of labeled\nreal-world samples, training on a mix of synthetic and unlabeled real-world\ndata achieved comparable or even better detection results at significantly\nlower cost. We argue that future research into the cost-efficiency of different\ntraining strategies is important for a better understanding of how to allocate\nbudget in applied machine learning projects.",
        "chunk-id": 2,
        "chunk": "inadequate for effectively training models that perform well on real-world\ndata, primarily due to domain shifts between the synthetic and real-world data.\nWe discuss approaches for dealing with such a domain shift when detecting\ndefects in X-ray scans of aluminium wheels. Using both simulated and real-world",
        "authors": [
            "Lukas Malte Kemeter",
            "Rasmus Hvingelby",
            "Paulina Sierak",
            "Tobias Sch\u00f6n",
            "Bishwajit Gosswam"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:51:53+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19175v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19175v1",
        "categories": [
            "Machine Learning",
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000084,
        "doi": null,
        "title": "Towards Reducing Data Acquisition and Labeling for Defect Detection using Simulated Data",
        "abstract": "In many manufacturing settings, annotating data for machine learning and\ncomputer vision is costly, but synthetic data can be generated at significantly\nlower cost. Substituting the real-world data with synthetic data is therefore\nappealing for many machine learning applications that require large amounts of\ntraining data. However, relying solely on synthetic data is frequently\ninadequate for effectively training models that perform well on real-world\ndata, primarily due to domain shifts between the synthetic and real-world data.\nWe discuss approaches for dealing with such a domain shift when detecting\ndefects in X-ray scans of aluminium wheels. Using both simulated and real-world\nX-ray images, we train an object detection model with different strategies to\nidentify the training approach that generates the best detection results while\nminimising the demand for annotated real-world training samples. Our\npreliminary findings suggest that the sim-2-real domain adaptation approach is\nmore cost-efficient than a fully supervised oracle - if the total number of\navailable annotated samples is fixed. Given a certain number of labeled\nreal-world samples, training on a mix of synthetic and unlabeled real-world\ndata achieved comparable or even better detection results at significantly\nlower cost. We argue that future research into the cost-efficiency of different\ntraining strategies is important for a better understanding of how to allocate\nbudget in applied machine learning projects.",
        "chunk-id": 3,
        "chunk": "X-ray images, we train an object detection model with different strategies to\nidentify the training approach that generates the best detection results while\nminimising the demand for annotated real-world training samples. Our\npreliminary findings suggest that the sim-2-real domain adaptation approach is\nmore cost-efficient than a fully supervised oracle - if the total number of",
        "authors": [
            "Lukas Malte Kemeter",
            "Rasmus Hvingelby",
            "Paulina Sierak",
            "Tobias Sch\u00f6n",
            "Bishwajit Gosswam"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:51:53+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19175v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19175v1",
        "categories": [
            "Machine Learning",
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000084,
        "doi": null,
        "title": "Towards Reducing Data Acquisition and Labeling for Defect Detection using Simulated Data",
        "abstract": "In many manufacturing settings, annotating data for machine learning and\ncomputer vision is costly, but synthetic data can be generated at significantly\nlower cost. Substituting the real-world data with synthetic data is therefore\nappealing for many machine learning applications that require large amounts of\ntraining data. However, relying solely on synthetic data is frequently\ninadequate for effectively training models that perform well on real-world\ndata, primarily due to domain shifts between the synthetic and real-world data.\nWe discuss approaches for dealing with such a domain shift when detecting\ndefects in X-ray scans of aluminium wheels. Using both simulated and real-world\nX-ray images, we train an object detection model with different strategies to\nidentify the training approach that generates the best detection results while\nminimising the demand for annotated real-world training samples. Our\npreliminary findings suggest that the sim-2-real domain adaptation approach is\nmore cost-efficient than a fully supervised oracle - if the total number of\navailable annotated samples is fixed. Given a certain number of labeled\nreal-world samples, training on a mix of synthetic and unlabeled real-world\ndata achieved comparable or even better detection results at significantly\nlower cost. We argue that future research into the cost-efficiency of different\ntraining strategies is important for a better understanding of how to allocate\nbudget in applied machine learning projects.",
        "chunk-id": 4,
        "chunk": "available annotated samples is fixed. Given a certain number of labeled\nreal-world samples, training on a mix of synthetic and unlabeled real-world\ndata achieved comparable or even better detection results at significantly\nlower cost. We argue that future research into the cost-efficiency of different\ntraining strategies is important for a better understanding of how to allocate",
        "authors": [
            "Lukas Malte Kemeter",
            "Rasmus Hvingelby",
            "Paulina Sierak",
            "Tobias Sch\u00f6n",
            "Bishwajit Gosswam"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:51:53+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19175v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19175v1",
        "categories": [
            "Machine Learning",
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000084,
        "doi": null,
        "title": "Towards Reducing Data Acquisition and Labeling for Defect Detection using Simulated Data",
        "abstract": "In many manufacturing settings, annotating data for machine learning and\ncomputer vision is costly, but synthetic data can be generated at significantly\nlower cost. Substituting the real-world data with synthetic data is therefore\nappealing for many machine learning applications that require large amounts of\ntraining data. However, relying solely on synthetic data is frequently\ninadequate for effectively training models that perform well on real-world\ndata, primarily due to domain shifts between the synthetic and real-world data.\nWe discuss approaches for dealing with such a domain shift when detecting\ndefects in X-ray scans of aluminium wheels. Using both simulated and real-world\nX-ray images, we train an object detection model with different strategies to\nidentify the training approach that generates the best detection results while\nminimising the demand for annotated real-world training samples. Our\npreliminary findings suggest that the sim-2-real domain adaptation approach is\nmore cost-efficient than a fully supervised oracle - if the total number of\navailable annotated samples is fixed. Given a certain number of labeled\nreal-world samples, training on a mix of synthetic and unlabeled real-world\ndata achieved comparable or even better detection results at significantly\nlower cost. We argue that future research into the cost-efficiency of different\ntraining strategies is important for a better understanding of how to allocate\nbudget in applied machine learning projects.",
        "chunk-id": 5,
        "chunk": "budget in applied machine learning projects.",
        "authors": [
            "Lukas Malte Kemeter",
            "Rasmus Hvingelby",
            "Paulina Sierak",
            "Tobias Sch\u00f6n",
            "Bishwajit Gosswam"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:51:53+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19175v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19175v1",
        "categories": [
            "Machine Learning",
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000085,
        "doi": null,
        "title": "Stark Control of Plexcitonic States in Incoherent Quantum Systems",
        "abstract": "Electro-optic control of quantum dots embedded in the plasmonic nanocavities\nenables active tuning of photonic devices for emerging applications in Quantum\noptics such as quantum information processing, entanglement and ultrafast\noptical switching. Here, we demonstrate the coherent control of plexcitonic\nstates in (i) an off-resonant and (ii) a resonant coupled quantum systems\nthrough optical Stark effect (OSE). We analyze a hybrid plasmon-emitter system\nwhich exhibits tunable Fano resonance, Stark induced transparency (SIT) and\nvacuum Rabi splitting due to quadratic Stark shift in the degenerate states of\nquantum emitter (QE). In addition, a resonantly coupled system shows the\nsignature of double Fano resonance due to Stark-induced splitting in a\ntwo-level QE. Our study shows that Stark tuning of plexcitons not only\nmitigates decoherence in the quantum system but it also stimulates on/off\nswitching of spontaneous photon emission in the visible regime. Such tunable\nsystems can be used to operate photonic integrated circuits (PIC) for\napplications in quantum computing and information processing.",
        "chunk-id": 1,
        "chunk": "Electro-optic control of quantum dots embedded in the plasmonic nanocavities\nenables active tuning of photonic devices for emerging applications in Quantum\noptics such as quantum information processing, entanglement and ultrafast\noptical switching. Here, we demonstrate the coherent control of plexcitonic\nstates in (i) an off-resonant and (ii) a resonant coupled quantum systems",
        "authors": [
            "Hira Asif",
            "Ramazan Sahin"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:49:42+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19173v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19173v1",
        "categories": [
            "Optics",
            "Mesoscale and Nanoscale Physics",
            "Applied Physics",
            "Quantum Physics"
        ]
    },
    {
        "id": 30000085,
        "doi": null,
        "title": "Stark Control of Plexcitonic States in Incoherent Quantum Systems",
        "abstract": "Electro-optic control of quantum dots embedded in the plasmonic nanocavities\nenables active tuning of photonic devices for emerging applications in Quantum\noptics such as quantum information processing, entanglement and ultrafast\noptical switching. Here, we demonstrate the coherent control of plexcitonic\nstates in (i) an off-resonant and (ii) a resonant coupled quantum systems\nthrough optical Stark effect (OSE). We analyze a hybrid plasmon-emitter system\nwhich exhibits tunable Fano resonance, Stark induced transparency (SIT) and\nvacuum Rabi splitting due to quadratic Stark shift in the degenerate states of\nquantum emitter (QE). In addition, a resonantly coupled system shows the\nsignature of double Fano resonance due to Stark-induced splitting in a\ntwo-level QE. Our study shows that Stark tuning of plexcitons not only\nmitigates decoherence in the quantum system but it also stimulates on/off\nswitching of spontaneous photon emission in the visible regime. Such tunable\nsystems can be used to operate photonic integrated circuits (PIC) for\napplications in quantum computing and information processing.",
        "chunk-id": 2,
        "chunk": "through optical Stark effect (OSE). We analyze a hybrid plasmon-emitter system\nwhich exhibits tunable Fano resonance, Stark induced transparency (SIT) and\nvacuum Rabi splitting due to quadratic Stark shift in the degenerate states of\nquantum emitter (QE). In addition, a resonantly coupled system shows the\nsignature of double Fano resonance due to Stark-induced splitting in a",
        "authors": [
            "Hira Asif",
            "Ramazan Sahin"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:49:42+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19173v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19173v1",
        "categories": [
            "Optics",
            "Mesoscale and Nanoscale Physics",
            "Applied Physics",
            "Quantum Physics"
        ]
    },
    {
        "id": 30000085,
        "doi": null,
        "title": "Stark Control of Plexcitonic States in Incoherent Quantum Systems",
        "abstract": "Electro-optic control of quantum dots embedded in the plasmonic nanocavities\nenables active tuning of photonic devices for emerging applications in Quantum\noptics such as quantum information processing, entanglement and ultrafast\noptical switching. Here, we demonstrate the coherent control of plexcitonic\nstates in (i) an off-resonant and (ii) a resonant coupled quantum systems\nthrough optical Stark effect (OSE). We analyze a hybrid plasmon-emitter system\nwhich exhibits tunable Fano resonance, Stark induced transparency (SIT) and\nvacuum Rabi splitting due to quadratic Stark shift in the degenerate states of\nquantum emitter (QE). In addition, a resonantly coupled system shows the\nsignature of double Fano resonance due to Stark-induced splitting in a\ntwo-level QE. Our study shows that Stark tuning of plexcitons not only\nmitigates decoherence in the quantum system but it also stimulates on/off\nswitching of spontaneous photon emission in the visible regime. Such tunable\nsystems can be used to operate photonic integrated circuits (PIC) for\napplications in quantum computing and information processing.",
        "chunk-id": 3,
        "chunk": "two-level QE. Our study shows that Stark tuning of plexcitons not only\nmitigates decoherence in the quantum system but it also stimulates on/off\nswitching of spontaneous photon emission in the visible regime. Such tunable\nsystems can be used to operate photonic integrated circuits (PIC) for\napplications in quantum computing and information processing.",
        "authors": [
            "Hira Asif",
            "Ramazan Sahin"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:49:42+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19173v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19173v1",
        "categories": [
            "Optics",
            "Mesoscale and Nanoscale Physics",
            "Applied Physics",
            "Quantum Physics"
        ]
    },
    {
        "id": 30000086,
        "doi": null,
        "title": "Annotation Errors and NER: A Study with OntoNotes 5.0",
        "abstract": "Named Entity Recognition (NER) is a well-studied problem in NLP. However,\nthere is much less focus on studying NER datasets, compared to developing new\nNER models. In this paper, we employed three simple techniques to detect\nannotation errors in the OntoNotes 5.0 corpus for English NER, which is the\nlargest available NER corpus for English. Our techniques corrected ~10% of the\nsentences in train/dev/test data. In terms of entity mentions, we corrected the\nspan and/or type of ~8% of mentions in the dataset, while\nadding/deleting/splitting/merging a few more. These are large numbers of\nchanges, considering the size of OntoNotes. We used three NER libraries to\ntrain, evaluate and compare the models trained with the original and the\nre-annotated datasets, which showed an average improvement of 1.23% in overall\nF-scores, with large (>10%) improvements for some of the entity types. While\nour annotation error detection methods are not exhaustive and there is some\nmanual annotation effort involved, they are largely language agnostic and can\nbe employed with other NER datasets, and other sequence labelling tasks.",
        "chunk-id": 1,
        "chunk": "Named Entity Recognition (NER) is a well-studied problem in NLP. However,\nthere is much less focus on studying NER datasets, compared to developing new\nNER models. In this paper, we employed three simple techniques to detect\nannotation errors in the OntoNotes 5.0 corpus for English NER, which is the\nlargest available NER corpus for English. Our techniques corrected ~10% of the",
        "authors": [
            "Gabriel Bernier-Colborne",
            "Sowmya Vajjala"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:48:46+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19172v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19172v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 30000086,
        "doi": null,
        "title": "Annotation Errors and NER: A Study with OntoNotes 5.0",
        "abstract": "Named Entity Recognition (NER) is a well-studied problem in NLP. However,\nthere is much less focus on studying NER datasets, compared to developing new\nNER models. In this paper, we employed three simple techniques to detect\nannotation errors in the OntoNotes 5.0 corpus for English NER, which is the\nlargest available NER corpus for English. Our techniques corrected ~10% of the\nsentences in train/dev/test data. In terms of entity mentions, we corrected the\nspan and/or type of ~8% of mentions in the dataset, while\nadding/deleting/splitting/merging a few more. These are large numbers of\nchanges, considering the size of OntoNotes. We used three NER libraries to\ntrain, evaluate and compare the models trained with the original and the\nre-annotated datasets, which showed an average improvement of 1.23% in overall\nF-scores, with large (>10%) improvements for some of the entity types. While\nour annotation error detection methods are not exhaustive and there is some\nmanual annotation effort involved, they are largely language agnostic and can\nbe employed with other NER datasets, and other sequence labelling tasks.",
        "chunk-id": 2,
        "chunk": "sentences in train/dev/test data. In terms of entity mentions, we corrected the\nspan and/or type of ~8% of mentions in the dataset, while\nadding/deleting/splitting/merging a few more. These are large numbers of\nchanges, considering the size of OntoNotes. We used three NER libraries to\ntrain, evaluate and compare the models trained with the original and the",
        "authors": [
            "Gabriel Bernier-Colborne",
            "Sowmya Vajjala"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:48:46+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19172v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19172v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 30000086,
        "doi": null,
        "title": "Annotation Errors and NER: A Study with OntoNotes 5.0",
        "abstract": "Named Entity Recognition (NER) is a well-studied problem in NLP. However,\nthere is much less focus on studying NER datasets, compared to developing new\nNER models. In this paper, we employed three simple techniques to detect\nannotation errors in the OntoNotes 5.0 corpus for English NER, which is the\nlargest available NER corpus for English. Our techniques corrected ~10% of the\nsentences in train/dev/test data. In terms of entity mentions, we corrected the\nspan and/or type of ~8% of mentions in the dataset, while\nadding/deleting/splitting/merging a few more. These are large numbers of\nchanges, considering the size of OntoNotes. We used three NER libraries to\ntrain, evaluate and compare the models trained with the original and the\nre-annotated datasets, which showed an average improvement of 1.23% in overall\nF-scores, with large (>10%) improvements for some of the entity types. While\nour annotation error detection methods are not exhaustive and there is some\nmanual annotation effort involved, they are largely language agnostic and can\nbe employed with other NER datasets, and other sequence labelling tasks.",
        "chunk-id": 3,
        "chunk": "re-annotated datasets, which showed an average improvement of 1.23% in overall\nF-scores, with large (>10%) improvements for some of the entity types. While\nour annotation error detection methods are not exhaustive and there is some\nmanual annotation effort involved, they are largely language agnostic and can\nbe employed with other NER datasets, and other sequence labelling tasks.",
        "authors": [
            "Gabriel Bernier-Colborne",
            "Sowmya Vajjala"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:48:46+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19172v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19172v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 30000087,
        "doi": null,
        "title": "The Illusion of Competence: Evaluating the Effect of Explanations on Users' Mental Models of Visual Question Answering Systems",
        "abstract": "We examine how users perceive the limitations of an AI system when it\nencounters a task that it cannot perform perfectly and whether providing\nexplanations alongside its answers aids users in constructing an appropriate\nmental model of the system's capabilities and limitations. We employ a visual\nquestion answer and explanation task where we control the AI system's\nlimitations by manipulating the visual inputs: during inference, the system\neither processes full-color or grayscale images. Our goal is to determine\nwhether participants can perceive the limitations of the system. We hypothesize\nthat explanations will make limited AI capabilities more transparent to users.\nHowever, our results show that explanations do not have this effect. Instead of\nallowing users to more accurately assess the limitations of the AI system,\nexplanations generally increase users' perceptions of the system's competence -\nregardless of its actual performance.",
        "chunk-id": 1,
        "chunk": "We examine how users perceive the limitations of an AI system when it\nencounters a task that it cannot perform perfectly and whether providing\nexplanations alongside its answers aids users in constructing an appropriate\nmental model of the system's capabilities and limitations. We employ a visual\nquestion answer and explanation task where we control the AI system's",
        "authors": [
            "Judith Sieker",
            "Simeon Junker",
            "Ronja Utescher",
            "Nazia Attari",
            "Heiko Wersing",
            "Hendrik Buschmeier",
            "Sina Zarrie\u00df"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:44:03+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19170v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19170v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 30000087,
        "doi": null,
        "title": "The Illusion of Competence: Evaluating the Effect of Explanations on Users' Mental Models of Visual Question Answering Systems",
        "abstract": "We examine how users perceive the limitations of an AI system when it\nencounters a task that it cannot perform perfectly and whether providing\nexplanations alongside its answers aids users in constructing an appropriate\nmental model of the system's capabilities and limitations. We employ a visual\nquestion answer and explanation task where we control the AI system's\nlimitations by manipulating the visual inputs: during inference, the system\neither processes full-color or grayscale images. Our goal is to determine\nwhether participants can perceive the limitations of the system. We hypothesize\nthat explanations will make limited AI capabilities more transparent to users.\nHowever, our results show that explanations do not have this effect. Instead of\nallowing users to more accurately assess the limitations of the AI system,\nexplanations generally increase users' perceptions of the system's competence -\nregardless of its actual performance.",
        "chunk-id": 2,
        "chunk": "limitations by manipulating the visual inputs: during inference, the system\neither processes full-color or grayscale images. Our goal is to determine\nwhether participants can perceive the limitations of the system. We hypothesize\nthat explanations will make limited AI capabilities more transparent to users.",
        "authors": [
            "Judith Sieker",
            "Simeon Junker",
            "Ronja Utescher",
            "Nazia Attari",
            "Heiko Wersing",
            "Hendrik Buschmeier",
            "Sina Zarrie\u00df"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:44:03+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19170v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19170v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 30000087,
        "doi": null,
        "title": "The Illusion of Competence: Evaluating the Effect of Explanations on Users' Mental Models of Visual Question Answering Systems",
        "abstract": "We examine how users perceive the limitations of an AI system when it\nencounters a task that it cannot perform perfectly and whether providing\nexplanations alongside its answers aids users in constructing an appropriate\nmental model of the system's capabilities and limitations. We employ a visual\nquestion answer and explanation task where we control the AI system's\nlimitations by manipulating the visual inputs: during inference, the system\neither processes full-color or grayscale images. Our goal is to determine\nwhether participants can perceive the limitations of the system. We hypothesize\nthat explanations will make limited AI capabilities more transparent to users.\nHowever, our results show that explanations do not have this effect. Instead of\nallowing users to more accurately assess the limitations of the AI system,\nexplanations generally increase users' perceptions of the system's competence -\nregardless of its actual performance.",
        "chunk-id": 3,
        "chunk": "However, our results show that explanations do not have this effect. Instead of\nallowing users to more accurately assess the limitations of the AI system,\nexplanations generally increase users' perceptions of the system's competence -\nregardless of its actual performance.",
        "authors": [
            "Judith Sieker",
            "Simeon Junker",
            "Ronja Utescher",
            "Nazia Attari",
            "Heiko Wersing",
            "Hendrik Buschmeier",
            "Sina Zarrie\u00df"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:44:03+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19170v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19170v1",
        "categories": [
            "Computation and Language"
        ]
    },
    {
        "id": 30000088,
        "doi": null,
        "title": "Exact Minimum Weight Spanners via Column Generation",
        "abstract": "Given a weighted graph $G$, a minimum weight $\\alpha$-spanner is a\nleast-weight subgraph $H\\subseteq G$ that preserves minimum distances between\nall node pairs up to a factor of $\\alpha$. There are many results on heuristics\nand approximation algorithms, including a recent investigation of their\npractical performance [20]. Exact approaches, in contrast, have long been\ndenounced as impractical: The first exact ILP (integer linear program) method\n[48] from 2004 is based on a model with exponentially many path variables,\nsolved via column generation. A second approach [2], modeling via arc-based\nmulticommodity flow, was presented in 2019. In both cases, only graphs with\n40-100 nodes were reported to be solvable.\n  In this paper, we briefly report on a theoretical comparison between these\ntwo models from a polyhedral point of view, and then concentrate on\nimprovements and engineering aspects. We evaluate their performance in a\nlarge-scale empirical study. We report that our tuned column generation\napproach, based on multicriteria shortest path computations, is able to solve\ninstances with over 16000 nodes within 13 minutes. Furthermore, now knowing\noptimal solutions for larger graphs, we are able to investigate the quality of\nthe strongest known heuristic on reasonably sized instances for the first time.",
        "chunk-id": 1,
        "chunk": "Given a weighted graph $G$, a minimum weight $\\alpha$-spanner is a\nleast-weight subgraph $H\\subseteq G$ that preserves minimum distances between\nall node pairs up to a factor of $\\alpha$. There are many results on heuristics\nand approximation algorithms, including a recent investigation of their\npractical performance [20]. Exact approaches, in contrast, have long been",
        "authors": [
            "Fritz B\u00f6kler",
            "Markus Chimani",
            "Henning Jasper",
            "Mirko H. Wagner"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:32:23+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19164v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19164v1",
        "categories": [
            "Data Structures and Algorithms",
            "Discrete Mathematics",
            "Combinatorics",
            "Graph theory (including graph drawing) in computer science, Graph algorithms (graph-theoretic aspects), Mixed integer programming",
            "Nonnumerical Algorithms and Problems, Combinatorics, Graph Theory"
        ]
    },
    {
        "id": 30000088,
        "doi": null,
        "title": "Exact Minimum Weight Spanners via Column Generation",
        "abstract": "Given a weighted graph $G$, a minimum weight $\\alpha$-spanner is a\nleast-weight subgraph $H\\subseteq G$ that preserves minimum distances between\nall node pairs up to a factor of $\\alpha$. There are many results on heuristics\nand approximation algorithms, including a recent investigation of their\npractical performance [20]. Exact approaches, in contrast, have long been\ndenounced as impractical: The first exact ILP (integer linear program) method\n[48] from 2004 is based on a model with exponentially many path variables,\nsolved via column generation. A second approach [2], modeling via arc-based\nmulticommodity flow, was presented in 2019. In both cases, only graphs with\n40-100 nodes were reported to be solvable.\n  In this paper, we briefly report on a theoretical comparison between these\ntwo models from a polyhedral point of view, and then concentrate on\nimprovements and engineering aspects. We evaluate their performance in a\nlarge-scale empirical study. We report that our tuned column generation\napproach, based on multicriteria shortest path computations, is able to solve\ninstances with over 16000 nodes within 13 minutes. Furthermore, now knowing\noptimal solutions for larger graphs, we are able to investigate the quality of\nthe strongest known heuristic on reasonably sized instances for the first time.",
        "chunk-id": 2,
        "chunk": "denounced as impractical: The first exact ILP (integer linear program) method\n[48] from 2004 is based on a model with exponentially many path variables,\nsolved via column generation. A second approach [2], modeling via arc-based\nmulticommodity flow, was presented in 2019. In both cases, only graphs with\n40-100 nodes were reported to be solvable.",
        "authors": [
            "Fritz B\u00f6kler",
            "Markus Chimani",
            "Henning Jasper",
            "Mirko H. Wagner"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:32:23+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19164v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19164v1",
        "categories": [
            "Data Structures and Algorithms",
            "Discrete Mathematics",
            "Combinatorics",
            "Graph theory (including graph drawing) in computer science, Graph algorithms (graph-theoretic aspects), Mixed integer programming",
            "Nonnumerical Algorithms and Problems, Combinatorics, Graph Theory"
        ]
    },
    {
        "id": 30000088,
        "doi": null,
        "title": "Exact Minimum Weight Spanners via Column Generation",
        "abstract": "Given a weighted graph $G$, a minimum weight $\\alpha$-spanner is a\nleast-weight subgraph $H\\subseteq G$ that preserves minimum distances between\nall node pairs up to a factor of $\\alpha$. There are many results on heuristics\nand approximation algorithms, including a recent investigation of their\npractical performance [20]. Exact approaches, in contrast, have long been\ndenounced as impractical: The first exact ILP (integer linear program) method\n[48] from 2004 is based on a model with exponentially many path variables,\nsolved via column generation. A second approach [2], modeling via arc-based\nmulticommodity flow, was presented in 2019. In both cases, only graphs with\n40-100 nodes were reported to be solvable.\n  In this paper, we briefly report on a theoretical comparison between these\ntwo models from a polyhedral point of view, and then concentrate on\nimprovements and engineering aspects. We evaluate their performance in a\nlarge-scale empirical study. We report that our tuned column generation\napproach, based on multicriteria shortest path computations, is able to solve\ninstances with over 16000 nodes within 13 minutes. Furthermore, now knowing\noptimal solutions for larger graphs, we are able to investigate the quality of\nthe strongest known heuristic on reasonably sized instances for the first time.",
        "chunk-id": 3,
        "chunk": "In this paper, we briefly report on a theoretical comparison between these\ntwo models from a polyhedral point of view, and then concentrate on\nimprovements and engineering aspects. We evaluate their performance in a\nlarge-scale empirical study. We report that our tuned column generation\napproach, based on multicriteria shortest path computations, is able to solve",
        "authors": [
            "Fritz B\u00f6kler",
            "Markus Chimani",
            "Henning Jasper",
            "Mirko H. Wagner"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:32:23+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19164v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19164v1",
        "categories": [
            "Data Structures and Algorithms",
            "Discrete Mathematics",
            "Combinatorics",
            "Graph theory (including graph drawing) in computer science, Graph algorithms (graph-theoretic aspects), Mixed integer programming",
            "Nonnumerical Algorithms and Problems, Combinatorics, Graph Theory"
        ]
    },
    {
        "id": 30000088,
        "doi": null,
        "title": "Exact Minimum Weight Spanners via Column Generation",
        "abstract": "Given a weighted graph $G$, a minimum weight $\\alpha$-spanner is a\nleast-weight subgraph $H\\subseteq G$ that preserves minimum distances between\nall node pairs up to a factor of $\\alpha$. There are many results on heuristics\nand approximation algorithms, including a recent investigation of their\npractical performance [20]. Exact approaches, in contrast, have long been\ndenounced as impractical: The first exact ILP (integer linear program) method\n[48] from 2004 is based on a model with exponentially many path variables,\nsolved via column generation. A second approach [2], modeling via arc-based\nmulticommodity flow, was presented in 2019. In both cases, only graphs with\n40-100 nodes were reported to be solvable.\n  In this paper, we briefly report on a theoretical comparison between these\ntwo models from a polyhedral point of view, and then concentrate on\nimprovements and engineering aspects. We evaluate their performance in a\nlarge-scale empirical study. We report that our tuned column generation\napproach, based on multicriteria shortest path computations, is able to solve\ninstances with over 16000 nodes within 13 minutes. Furthermore, now knowing\noptimal solutions for larger graphs, we are able to investigate the quality of\nthe strongest known heuristic on reasonably sized instances for the first time.",
        "chunk-id": 4,
        "chunk": "instances with over 16000 nodes within 13 minutes. Furthermore, now knowing\noptimal solutions for larger graphs, we are able to investigate the quality of\nthe strongest known heuristic on reasonably sized instances for the first time.",
        "authors": [
            "Fritz B\u00f6kler",
            "Markus Chimani",
            "Henning Jasper",
            "Mirko H. Wagner"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:32:23+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19164v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19164v1",
        "categories": [
            "Data Structures and Algorithms",
            "Discrete Mathematics",
            "Combinatorics",
            "Graph theory (including graph drawing) in computer science, Graph algorithms (graph-theoretic aspects), Mixed integer programming",
            "Nonnumerical Algorithms and Problems, Combinatorics, Graph Theory"
        ]
    },
    {
        "id": 30000089,
        "doi": null,
        "title": "Single Image Estimation of Cell Migration Direction by Deep Circular Regression",
        "abstract": "In this paper we study the problem of estimating the migration direction of\ncells based on a single image. To the best of our knowledge, there is only one\nrelated work that uses a classification CNN for four classes (quadrants). This\napproach does not allow detailed directional resolution. We solve the single\nimage estimation problem using deep circular regression with special attention\nto cycle-sensitive methods. On two databases we achieve an average accuracy of\n$\\sim$17 degrees, which is a significant improvement over the previous work.",
        "chunk-id": 1,
        "chunk": "In this paper we study the problem of estimating the migration direction of\ncells based on a single image. To the best of our knowledge, there is only one\nrelated work that uses a classification CNN for four classes (quadrants). This\napproach does not allow detailed directional resolution. We solve the single",
        "authors": [
            "Lennart Bruns",
            "Lucas Lamparter",
            "Milos Galic",
            "Xiaoyi Jiang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:29:25+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19162v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19162v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000089,
        "doi": null,
        "title": "Single Image Estimation of Cell Migration Direction by Deep Circular Regression",
        "abstract": "In this paper we study the problem of estimating the migration direction of\ncells based on a single image. To the best of our knowledge, there is only one\nrelated work that uses a classification CNN for four classes (quadrants). This\napproach does not allow detailed directional resolution. We solve the single\nimage estimation problem using deep circular regression with special attention\nto cycle-sensitive methods. On two databases we achieve an average accuracy of\n$\\sim$17 degrees, which is a significant improvement over the previous work.",
        "chunk-id": 2,
        "chunk": "image estimation problem using deep circular regression with special attention\nto cycle-sensitive methods. On two databases we achieve an average accuracy of\n$\\sim$17 degrees, which is a significant improvement over the previous work.",
        "authors": [
            "Lennart Bruns",
            "Lucas Lamparter",
            "Milos Galic",
            "Xiaoyi Jiang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:29:25+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19162v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19162v1",
        "categories": [
            "Computer Vision and Pattern Recognition"
        ]
    },
    {
        "id": 30000090,
        "doi": null,
        "title": "Robust Classification of Dynamic Bichromatic point Sets in R2",
        "abstract": "Let $R \\cup B$ be a set of $n$ points in $\\mathbb{R}^2$, and let $k \\in\n1..n$. Our goal is to compute a line that \"best\" separates the \"red\" points $R$\nfrom the \"blue\" points $B$ with at most $k$ outliers. We present an efficient\nsemi-online dynamic data structure that can maintain whether such a separator\nexists. Furthermore, we present efficient exact and approximation algorithms\nthat compute a linear separator that is guaranteed to misclassify at most $k$,\npoints and minimizes the distance to the farthest outlier. Our exact algorithm\nruns in $O(nk + n \\log n)$ time, and our $(1+\\varepsilon)$-approximation\nalgorithm runs in $O(\\varepsilon^{-1/2}((n + k^2) \\log n))$ time. Based on our\n$(1+\\varepsilon)$-approximation algorithm we then also obtain a semi-online\ndata structure to maintain such a separator efficiently.",
        "chunk-id": 1,
        "chunk": "Let $R \\cup B$ be a set of $n$ points in $\\mathbb{R}^2$, and let $k \\in\n1..n$. Our goal is to compute a line that \"best\" separates the \"red\" points $R$\nfrom the \"blue\" points $B$ with at most $k$ outliers. We present an efficient\nsemi-online dynamic data structure that can maintain whether such a separator\nexists. Furthermore, we present efficient exact and approximation algorithms",
        "authors": [
            "Erwin Glazenburg",
            "Frank Staals",
            "Marc van Kreveld"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:29:03+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19161v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19161v1",
        "categories": [
            "Computational Geometry",
            "Analysis of algorithms,Approximation algorithms"
        ]
    },
    {
        "id": 30000090,
        "doi": null,
        "title": "Robust Classification of Dynamic Bichromatic point Sets in R2",
        "abstract": "Let $R \\cup B$ be a set of $n$ points in $\\mathbb{R}^2$, and let $k \\in\n1..n$. Our goal is to compute a line that \"best\" separates the \"red\" points $R$\nfrom the \"blue\" points $B$ with at most $k$ outliers. We present an efficient\nsemi-online dynamic data structure that can maintain whether such a separator\nexists. Furthermore, we present efficient exact and approximation algorithms\nthat compute a linear separator that is guaranteed to misclassify at most $k$,\npoints and minimizes the distance to the farthest outlier. Our exact algorithm\nruns in $O(nk + n \\log n)$ time, and our $(1+\\varepsilon)$-approximation\nalgorithm runs in $O(\\varepsilon^{-1/2}((n + k^2) \\log n))$ time. Based on our\n$(1+\\varepsilon)$-approximation algorithm we then also obtain a semi-online\ndata structure to maintain such a separator efficiently.",
        "chunk-id": 2,
        "chunk": "that compute a linear separator that is guaranteed to misclassify at most $k$,\npoints and minimizes the distance to the farthest outlier. Our exact algorithm\nruns in $O(nk + n \\log n)$ time, and our $(1+\\varepsilon)$-approximation\nalgorithm runs in $O(\\varepsilon^{-1/2}((n + k^2) \\log n))$ time. Based on our",
        "authors": [
            "Erwin Glazenburg",
            "Frank Staals",
            "Marc van Kreveld"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:29:03+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19161v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19161v1",
        "categories": [
            "Computational Geometry",
            "Analysis of algorithms,Approximation algorithms"
        ]
    },
    {
        "id": 30000090,
        "doi": null,
        "title": "Robust Classification of Dynamic Bichromatic point Sets in R2",
        "abstract": "Let $R \\cup B$ be a set of $n$ points in $\\mathbb{R}^2$, and let $k \\in\n1..n$. Our goal is to compute a line that \"best\" separates the \"red\" points $R$\nfrom the \"blue\" points $B$ with at most $k$ outliers. We present an efficient\nsemi-online dynamic data structure that can maintain whether such a separator\nexists. Furthermore, we present efficient exact and approximation algorithms\nthat compute a linear separator that is guaranteed to misclassify at most $k$,\npoints and minimizes the distance to the farthest outlier. Our exact algorithm\nruns in $O(nk + n \\log n)$ time, and our $(1+\\varepsilon)$-approximation\nalgorithm runs in $O(\\varepsilon^{-1/2}((n + k^2) \\log n))$ time. Based on our\n$(1+\\varepsilon)$-approximation algorithm we then also obtain a semi-online\ndata structure to maintain such a separator efficiently.",
        "chunk-id": 3,
        "chunk": "$(1+\\varepsilon)$-approximation algorithm we then also obtain a semi-online\ndata structure to maintain such a separator efficiently.",
        "authors": [
            "Erwin Glazenburg",
            "Frank Staals",
            "Marc van Kreveld"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:29:03+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19161v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19161v1",
        "categories": [
            "Computational Geometry",
            "Analysis of algorithms,Approximation algorithms"
        ]
    },
    {
        "id": 30000091,
        "doi": null,
        "title": "Gravitational waveforms from inspiral compact binaries in Hybrid metric-Palatini gravity",
        "abstract": "In this study, gravitational waveforms emitted by inspiralling compact binary\nsystems on quasicircular orbits in hybrid metric-Palatini gravity are computed\nin the lowest post-Newtonian approximation. By applying the stationary phase\napproximation, Fourier transforms of tensor polarization modes are obtained,\nand correction terms in the amplitude and phase of gravitational waves relative\nto General Relativity results are derived. Moreover, post-Einsteinian\nparameters are identified, and potential constraints on the background value of\nthe scalar field are obtained based on possible observations of the\ngravitational waves by the future ground based gravitational wave detectors.\nAdditionally, constraints on the background value of the scalar field are\nderived using updated observational data from the PSR J0737-3039 system. The\nlast rescrictions are comparable in order of magnitude to the best currently\nexisting constraints, which were derived from observational data within the\nsolar system.",
        "chunk-id": 1,
        "chunk": "In this study, gravitational waveforms emitted by inspiralling compact binary\nsystems on quasicircular orbits in hybrid metric-Palatini gravity are computed\nin the lowest post-Newtonian approximation. By applying the stationary phase\napproximation, Fourier transforms of tensor polarization modes are obtained,",
        "authors": [
            "P. I. Dyadina"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:23:11+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19159v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19159v1",
        "categories": [
            "General Relativity and Quantum Cosmology"
        ]
    },
    {
        "id": 30000091,
        "doi": null,
        "title": "Gravitational waveforms from inspiral compact binaries in Hybrid metric-Palatini gravity",
        "abstract": "In this study, gravitational waveforms emitted by inspiralling compact binary\nsystems on quasicircular orbits in hybrid metric-Palatini gravity are computed\nin the lowest post-Newtonian approximation. By applying the stationary phase\napproximation, Fourier transforms of tensor polarization modes are obtained,\nand correction terms in the amplitude and phase of gravitational waves relative\nto General Relativity results are derived. Moreover, post-Einsteinian\nparameters are identified, and potential constraints on the background value of\nthe scalar field are obtained based on possible observations of the\ngravitational waves by the future ground based gravitational wave detectors.\nAdditionally, constraints on the background value of the scalar field are\nderived using updated observational data from the PSR J0737-3039 system. The\nlast rescrictions are comparable in order of magnitude to the best currently\nexisting constraints, which were derived from observational data within the\nsolar system.",
        "chunk-id": 2,
        "chunk": "and correction terms in the amplitude and phase of gravitational waves relative\nto General Relativity results are derived. Moreover, post-Einsteinian\nparameters are identified, and potential constraints on the background value of\nthe scalar field are obtained based on possible observations of the\ngravitational waves by the future ground based gravitational wave detectors.",
        "authors": [
            "P. I. Dyadina"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:23:11+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19159v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19159v1",
        "categories": [
            "General Relativity and Quantum Cosmology"
        ]
    },
    {
        "id": 30000091,
        "doi": null,
        "title": "Gravitational waveforms from inspiral compact binaries in Hybrid metric-Palatini gravity",
        "abstract": "In this study, gravitational waveforms emitted by inspiralling compact binary\nsystems on quasicircular orbits in hybrid metric-Palatini gravity are computed\nin the lowest post-Newtonian approximation. By applying the stationary phase\napproximation, Fourier transforms of tensor polarization modes are obtained,\nand correction terms in the amplitude and phase of gravitational waves relative\nto General Relativity results are derived. Moreover, post-Einsteinian\nparameters are identified, and potential constraints on the background value of\nthe scalar field are obtained based on possible observations of the\ngravitational waves by the future ground based gravitational wave detectors.\nAdditionally, constraints on the background value of the scalar field are\nderived using updated observational data from the PSR J0737-3039 system. The\nlast rescrictions are comparable in order of magnitude to the best currently\nexisting constraints, which were derived from observational data within the\nsolar system.",
        "chunk-id": 3,
        "chunk": "Additionally, constraints on the background value of the scalar field are\nderived using updated observational data from the PSR J0737-3039 system. The\nlast rescrictions are comparable in order of magnitude to the best currently\nexisting constraints, which were derived from observational data within the\nsolar system.",
        "authors": [
            "P. I. Dyadina"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:23:11+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19159v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19159v1",
        "categories": [
            "General Relativity and Quantum Cosmology"
        ]
    },
    {
        "id": 30000092,
        "doi": null,
        "title": "Heterogeneous Causal Metapath Graph Neural Network for Gene-Microbe-Disease Association Prediction",
        "abstract": "The recent focus on microbes in human medicine highlights their potential\nrole in the genetic framework of diseases. To decode the complex interactions\namong genes, microbes, and diseases, computational predictions of\ngene-microbe-disease (GMD) associations are crucial. Existing methods primarily\naddress gene-disease and microbe-disease associations, but the more intricate\ntriple-wise GMD associations remain less explored. In this paper, we propose a\nHeterogeneous Causal Metapath Graph Neural Network (HCMGNN) to predict GMD\nassociations. HCMGNN constructs a heterogeneous graph linking genes, microbes,\nand diseases through their pairwise associations, and utilizes six predefined\ncausal metapaths to extract directed causal subgraphs, which facilitate the\nmulti-view analysis of causal relations among three entity types. Within each\nsubgraph, we employ a causal semantic sharing message passing network for node\nrepresentation learning, coupled with an attentive fusion method to integrate\nthese representations for predicting GMD associations. Our extensive\nexperiments show that HCMGNN effectively predicts GMD associations and\naddresses association sparsity issue by enhancing the graph's semantics and\nstructure.",
        "chunk-id": 1,
        "chunk": "The recent focus on microbes in human medicine highlights their potential\nrole in the genetic framework of diseases. To decode the complex interactions\namong genes, microbes, and diseases, computational predictions of\ngene-microbe-disease (GMD) associations are crucial. Existing methods primarily\naddress gene-disease and microbe-disease associations, but the more intricate",
        "authors": [
            "Kexin Zhang",
            "Feng Huang",
            "Luotao Liu",
            "Zhankun Xiong",
            "Hongyu Zhang",
            "Yuan Quan",
            "Wen Zhang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:17:33+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19156v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19156v1",
        "categories": [
            "Machine Learning"
        ]
    },
    {
        "id": 30000092,
        "doi": null,
        "title": "Heterogeneous Causal Metapath Graph Neural Network for Gene-Microbe-Disease Association Prediction",
        "abstract": "The recent focus on microbes in human medicine highlights their potential\nrole in the genetic framework of diseases. To decode the complex interactions\namong genes, microbes, and diseases, computational predictions of\ngene-microbe-disease (GMD) associations are crucial. Existing methods primarily\naddress gene-disease and microbe-disease associations, but the more intricate\ntriple-wise GMD associations remain less explored. In this paper, we propose a\nHeterogeneous Causal Metapath Graph Neural Network (HCMGNN) to predict GMD\nassociations. HCMGNN constructs a heterogeneous graph linking genes, microbes,\nand diseases through their pairwise associations, and utilizes six predefined\ncausal metapaths to extract directed causal subgraphs, which facilitate the\nmulti-view analysis of causal relations among three entity types. Within each\nsubgraph, we employ a causal semantic sharing message passing network for node\nrepresentation learning, coupled with an attentive fusion method to integrate\nthese representations for predicting GMD associations. Our extensive\nexperiments show that HCMGNN effectively predicts GMD associations and\naddresses association sparsity issue by enhancing the graph's semantics and\nstructure.",
        "chunk-id": 2,
        "chunk": "triple-wise GMD associations remain less explored. In this paper, we propose a\nHeterogeneous Causal Metapath Graph Neural Network (HCMGNN) to predict GMD\nassociations. HCMGNN constructs a heterogeneous graph linking genes, microbes,\nand diseases through their pairwise associations, and utilizes six predefined",
        "authors": [
            "Kexin Zhang",
            "Feng Huang",
            "Luotao Liu",
            "Zhankun Xiong",
            "Hongyu Zhang",
            "Yuan Quan",
            "Wen Zhang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:17:33+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19156v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19156v1",
        "categories": [
            "Machine Learning"
        ]
    },
    {
        "id": 30000092,
        "doi": null,
        "title": "Heterogeneous Causal Metapath Graph Neural Network for Gene-Microbe-Disease Association Prediction",
        "abstract": "The recent focus on microbes in human medicine highlights their potential\nrole in the genetic framework of diseases. To decode the complex interactions\namong genes, microbes, and diseases, computational predictions of\ngene-microbe-disease (GMD) associations are crucial. Existing methods primarily\naddress gene-disease and microbe-disease associations, but the more intricate\ntriple-wise GMD associations remain less explored. In this paper, we propose a\nHeterogeneous Causal Metapath Graph Neural Network (HCMGNN) to predict GMD\nassociations. HCMGNN constructs a heterogeneous graph linking genes, microbes,\nand diseases through their pairwise associations, and utilizes six predefined\ncausal metapaths to extract directed causal subgraphs, which facilitate the\nmulti-view analysis of causal relations among three entity types. Within each\nsubgraph, we employ a causal semantic sharing message passing network for node\nrepresentation learning, coupled with an attentive fusion method to integrate\nthese representations for predicting GMD associations. Our extensive\nexperiments show that HCMGNN effectively predicts GMD associations and\naddresses association sparsity issue by enhancing the graph's semantics and\nstructure.",
        "chunk-id": 3,
        "chunk": "causal metapaths to extract directed causal subgraphs, which facilitate the\nmulti-view analysis of causal relations among three entity types. Within each\nsubgraph, we employ a causal semantic sharing message passing network for node\nrepresentation learning, coupled with an attentive fusion method to integrate\nthese representations for predicting GMD associations. Our extensive",
        "authors": [
            "Kexin Zhang",
            "Feng Huang",
            "Luotao Liu",
            "Zhankun Xiong",
            "Hongyu Zhang",
            "Yuan Quan",
            "Wen Zhang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:17:33+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19156v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19156v1",
        "categories": [
            "Machine Learning"
        ]
    },
    {
        "id": 30000092,
        "doi": null,
        "title": "Heterogeneous Causal Metapath Graph Neural Network for Gene-Microbe-Disease Association Prediction",
        "abstract": "The recent focus on microbes in human medicine highlights their potential\nrole in the genetic framework of diseases. To decode the complex interactions\namong genes, microbes, and diseases, computational predictions of\ngene-microbe-disease (GMD) associations are crucial. Existing methods primarily\naddress gene-disease and microbe-disease associations, but the more intricate\ntriple-wise GMD associations remain less explored. In this paper, we propose a\nHeterogeneous Causal Metapath Graph Neural Network (HCMGNN) to predict GMD\nassociations. HCMGNN constructs a heterogeneous graph linking genes, microbes,\nand diseases through their pairwise associations, and utilizes six predefined\ncausal metapaths to extract directed causal subgraphs, which facilitate the\nmulti-view analysis of causal relations among three entity types. Within each\nsubgraph, we employ a causal semantic sharing message passing network for node\nrepresentation learning, coupled with an attentive fusion method to integrate\nthese representations for predicting GMD associations. Our extensive\nexperiments show that HCMGNN effectively predicts GMD associations and\naddresses association sparsity issue by enhancing the graph's semantics and\nstructure.",
        "chunk-id": 4,
        "chunk": "experiments show that HCMGNN effectively predicts GMD associations and\naddresses association sparsity issue by enhancing the graph's semantics and\nstructure.",
        "authors": [
            "Kexin Zhang",
            "Feng Huang",
            "Luotao Liu",
            "Zhankun Xiong",
            "Hongyu Zhang",
            "Yuan Quan",
            "Wen Zhang"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:17:33+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19156v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19156v1",
        "categories": [
            "Machine Learning"
        ]
    },
    {
        "id": 30000093,
        "doi": null,
        "title": "Trivariate Bicycle Codes",
        "abstract": "Quantum error correction suppresses noise in quantum systems to allow for\nhigh-precision computations. In this work, we introduce Trivariate Bicycle\nQuantum Low-Density Parity-Check (TB-QLDPC) codes, via an extension of the\nframework developed by Bravyi et al. [Nature, 627, 778-782 (2024)]. Unlike the\nweight-6 codes proposed in their study, our approach also offers weight-4 and\nweight-5 codes, which promises to be more amenable to near-term experimental\nsetups. We show that our TB-QLDPC codes up to weight-6 have a bi-planar\nstructure. Further, most of our new codes can also be arranged in a\ntwo-dimensional toric layout, and have substantially better encoding rates than\ncomparable surface codes while offering comparable error suppression\ncapabilities. For example, we can encode 4 logical qubits with distance 5 into\n30 physical qubits with weight-5 check measurements, while a surface code with\ncomparable parameters requires 100 physical qubits. The high encoding rate and\ncompact layout make our codes highly suitable candidates for near-term hardware\nimplementations, paving the way for a realizable quantum error correction\nprotocol.",
        "chunk-id": 1,
        "chunk": "Quantum error correction suppresses noise in quantum systems to allow for\nhigh-precision computations. In this work, we introduce Trivariate Bicycle\nQuantum Low-Density Parity-Check (TB-QLDPC) codes, via an extension of the\nframework developed by Bravyi et al. [Nature, 627, 778-782 (2024)]. Unlike the\nweight-6 codes proposed in their study, our approach also offers weight-4 and",
        "authors": [
            "Lukas Voss",
            "Sim Jian Xian",
            "Tobias Haug",
            "Kishor Bharti"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:10:37+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19151v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19151v1",
        "categories": [
            "Quantum Physics"
        ]
    },
    {
        "id": 30000093,
        "doi": null,
        "title": "Trivariate Bicycle Codes",
        "abstract": "Quantum error correction suppresses noise in quantum systems to allow for\nhigh-precision computations. In this work, we introduce Trivariate Bicycle\nQuantum Low-Density Parity-Check (TB-QLDPC) codes, via an extension of the\nframework developed by Bravyi et al. [Nature, 627, 778-782 (2024)]. Unlike the\nweight-6 codes proposed in their study, our approach also offers weight-4 and\nweight-5 codes, which promises to be more amenable to near-term experimental\nsetups. We show that our TB-QLDPC codes up to weight-6 have a bi-planar\nstructure. Further, most of our new codes can also be arranged in a\ntwo-dimensional toric layout, and have substantially better encoding rates than\ncomparable surface codes while offering comparable error suppression\ncapabilities. For example, we can encode 4 logical qubits with distance 5 into\n30 physical qubits with weight-5 check measurements, while a surface code with\ncomparable parameters requires 100 physical qubits. The high encoding rate and\ncompact layout make our codes highly suitable candidates for near-term hardware\nimplementations, paving the way for a realizable quantum error correction\nprotocol.",
        "chunk-id": 2,
        "chunk": "weight-5 codes, which promises to be more amenable to near-term experimental\nsetups. We show that our TB-QLDPC codes up to weight-6 have a bi-planar\nstructure. Further, most of our new codes can also be arranged in a\ntwo-dimensional toric layout, and have substantially better encoding rates than\ncomparable surface codes while offering comparable error suppression",
        "authors": [
            "Lukas Voss",
            "Sim Jian Xian",
            "Tobias Haug",
            "Kishor Bharti"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:10:37+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19151v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19151v1",
        "categories": [
            "Quantum Physics"
        ]
    },
    {
        "id": 30000093,
        "doi": null,
        "title": "Trivariate Bicycle Codes",
        "abstract": "Quantum error correction suppresses noise in quantum systems to allow for\nhigh-precision computations. In this work, we introduce Trivariate Bicycle\nQuantum Low-Density Parity-Check (TB-QLDPC) codes, via an extension of the\nframework developed by Bravyi et al. [Nature, 627, 778-782 (2024)]. Unlike the\nweight-6 codes proposed in their study, our approach also offers weight-4 and\nweight-5 codes, which promises to be more amenable to near-term experimental\nsetups. We show that our TB-QLDPC codes up to weight-6 have a bi-planar\nstructure. Further, most of our new codes can also be arranged in a\ntwo-dimensional toric layout, and have substantially better encoding rates than\ncomparable surface codes while offering comparable error suppression\ncapabilities. For example, we can encode 4 logical qubits with distance 5 into\n30 physical qubits with weight-5 check measurements, while a surface code with\ncomparable parameters requires 100 physical qubits. The high encoding rate and\ncompact layout make our codes highly suitable candidates for near-term hardware\nimplementations, paving the way for a realizable quantum error correction\nprotocol.",
        "chunk-id": 3,
        "chunk": "capabilities. For example, we can encode 4 logical qubits with distance 5 into\n30 physical qubits with weight-5 check measurements, while a surface code with\ncomparable parameters requires 100 physical qubits. The high encoding rate and\ncompact layout make our codes highly suitable candidates for near-term hardware",
        "authors": [
            "Lukas Voss",
            "Sim Jian Xian",
            "Tobias Haug",
            "Kishor Bharti"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:10:37+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19151v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19151v1",
        "categories": [
            "Quantum Physics"
        ]
    },
    {
        "id": 30000093,
        "doi": null,
        "title": "Trivariate Bicycle Codes",
        "abstract": "Quantum error correction suppresses noise in quantum systems to allow for\nhigh-precision computations. In this work, we introduce Trivariate Bicycle\nQuantum Low-Density Parity-Check (TB-QLDPC) codes, via an extension of the\nframework developed by Bravyi et al. [Nature, 627, 778-782 (2024)]. Unlike the\nweight-6 codes proposed in their study, our approach also offers weight-4 and\nweight-5 codes, which promises to be more amenable to near-term experimental\nsetups. We show that our TB-QLDPC codes up to weight-6 have a bi-planar\nstructure. Further, most of our new codes can also be arranged in a\ntwo-dimensional toric layout, and have substantially better encoding rates than\ncomparable surface codes while offering comparable error suppression\ncapabilities. For example, we can encode 4 logical qubits with distance 5 into\n30 physical qubits with weight-5 check measurements, while a surface code with\ncomparable parameters requires 100 physical qubits. The high encoding rate and\ncompact layout make our codes highly suitable candidates for near-term hardware\nimplementations, paving the way for a realizable quantum error correction\nprotocol.",
        "chunk-id": 4,
        "chunk": "implementations, paving the way for a realizable quantum error correction\nprotocol.",
        "authors": [
            "Lukas Voss",
            "Sim Jian Xian",
            "Tobias Haug",
            "Kishor Bharti"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:10:37+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19151v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19151v1",
        "categories": [
            "Quantum Physics"
        ]
    },
    {
        "id": 30000094,
        "doi": null,
        "title": "RAVEN: Multitask Retrieval Augmented Vision-Language Learning",
        "abstract": "The scaling of large language models to encode all the world's knowledge in\nmodel parameters is unsustainable and has exacerbated resource barriers.\nRetrieval-Augmented Generation (RAG) presents a potential solution, yet its\napplication to vision-language models (VLMs) is under explored. Existing\nmethods focus on models designed for single tasks. Furthermore, they're limited\nby the need for resource intensive pre training, additional parameter\nrequirements, unaddressed modality prioritization and lack of clear benefit\nover non-retrieval baselines. This paper introduces RAVEN, a multitask\nretrieval augmented VLM framework that enhances base VLMs through efficient,\ntask specific fine-tuning. By integrating retrieval augmented samples without\nthe need for additional retrieval-specific parameters, we show that the model\nacquires retrieval properties that are effective across multiple tasks. Our\nresults and extensive ablations across retrieved modalities for the image\ncaptioning and VQA tasks indicate significant performance improvements compared\nto non retrieved baselines +1 CIDEr on MSCOCO, +4 CIDEr on NoCaps and nearly a\n+3\\% accuracy on specific VQA question types. This underscores the efficacy of\napplying RAG approaches to VLMs, marking a stride toward more efficient and\naccessible multimodal learning.",
        "chunk-id": 1,
        "chunk": "The scaling of large language models to encode all the world's knowledge in\nmodel parameters is unsustainable and has exacerbated resource barriers.\nRetrieval-Augmented Generation (RAG) presents a potential solution, yet its\napplication to vision-language models (VLMs) is under explored. Existing\nmethods focus on models designed for single tasks. Furthermore, they're limited",
        "authors": [
            "Varun Nagaraj Rao",
            "Siddharth Choudhary",
            "Aditya Deshpande",
            "Ravi Kumar Satzoda",
            "Srikar Appalaraju"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:08:35+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19150v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19150v1",
        "categories": [
            "Computer Vision and Pattern Recognition",
            "Artificial Intelligence",
            "Information Retrieval"
        ]
    },
    {
        "id": 30000094,
        "doi": null,
        "title": "RAVEN: Multitask Retrieval Augmented Vision-Language Learning",
        "abstract": "The scaling of large language models to encode all the world's knowledge in\nmodel parameters is unsustainable and has exacerbated resource barriers.\nRetrieval-Augmented Generation (RAG) presents a potential solution, yet its\napplication to vision-language models (VLMs) is under explored. Existing\nmethods focus on models designed for single tasks. Furthermore, they're limited\nby the need for resource intensive pre training, additional parameter\nrequirements, unaddressed modality prioritization and lack of clear benefit\nover non-retrieval baselines. This paper introduces RAVEN, a multitask\nretrieval augmented VLM framework that enhances base VLMs through efficient,\ntask specific fine-tuning. By integrating retrieval augmented samples without\nthe need for additional retrieval-specific parameters, we show that the model\nacquires retrieval properties that are effective across multiple tasks. Our\nresults and extensive ablations across retrieved modalities for the image\ncaptioning and VQA tasks indicate significant performance improvements compared\nto non retrieved baselines +1 CIDEr on MSCOCO, +4 CIDEr on NoCaps and nearly a\n+3\\% accuracy on specific VQA question types. This underscores the efficacy of\napplying RAG approaches to VLMs, marking a stride toward more efficient and\naccessible multimodal learning.",
        "chunk-id": 2,
        "chunk": "by the need for resource intensive pre training, additional parameter\nrequirements, unaddressed modality prioritization and lack of clear benefit\nover non-retrieval baselines. This paper introduces RAVEN, a multitask\nretrieval augmented VLM framework that enhances base VLMs through efficient,\ntask specific fine-tuning. By integrating retrieval augmented samples without",
        "authors": [
            "Varun Nagaraj Rao",
            "Siddharth Choudhary",
            "Aditya Deshpande",
            "Ravi Kumar Satzoda",
            "Srikar Appalaraju"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:08:35+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19150v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19150v1",
        "categories": [
            "Computer Vision and Pattern Recognition",
            "Artificial Intelligence",
            "Information Retrieval"
        ]
    },
    {
        "id": 30000094,
        "doi": null,
        "title": "RAVEN: Multitask Retrieval Augmented Vision-Language Learning",
        "abstract": "The scaling of large language models to encode all the world's knowledge in\nmodel parameters is unsustainable and has exacerbated resource barriers.\nRetrieval-Augmented Generation (RAG) presents a potential solution, yet its\napplication to vision-language models (VLMs) is under explored. Existing\nmethods focus on models designed for single tasks. Furthermore, they're limited\nby the need for resource intensive pre training, additional parameter\nrequirements, unaddressed modality prioritization and lack of clear benefit\nover non-retrieval baselines. This paper introduces RAVEN, a multitask\nretrieval augmented VLM framework that enhances base VLMs through efficient,\ntask specific fine-tuning. By integrating retrieval augmented samples without\nthe need for additional retrieval-specific parameters, we show that the model\nacquires retrieval properties that are effective across multiple tasks. Our\nresults and extensive ablations across retrieved modalities for the image\ncaptioning and VQA tasks indicate significant performance improvements compared\nto non retrieved baselines +1 CIDEr on MSCOCO, +4 CIDEr on NoCaps and nearly a\n+3\\% accuracy on specific VQA question types. This underscores the efficacy of\napplying RAG approaches to VLMs, marking a stride toward more efficient and\naccessible multimodal learning.",
        "chunk-id": 3,
        "chunk": "the need for additional retrieval-specific parameters, we show that the model\nacquires retrieval properties that are effective across multiple tasks. Our\nresults and extensive ablations across retrieved modalities for the image\ncaptioning and VQA tasks indicate significant performance improvements compared",
        "authors": [
            "Varun Nagaraj Rao",
            "Siddharth Choudhary",
            "Aditya Deshpande",
            "Ravi Kumar Satzoda",
            "Srikar Appalaraju"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:08:35+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19150v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19150v1",
        "categories": [
            "Computer Vision and Pattern Recognition",
            "Artificial Intelligence",
            "Information Retrieval"
        ]
    },
    {
        "id": 30000094,
        "doi": null,
        "title": "RAVEN: Multitask Retrieval Augmented Vision-Language Learning",
        "abstract": "The scaling of large language models to encode all the world's knowledge in\nmodel parameters is unsustainable and has exacerbated resource barriers.\nRetrieval-Augmented Generation (RAG) presents a potential solution, yet its\napplication to vision-language models (VLMs) is under explored. Existing\nmethods focus on models designed for single tasks. Furthermore, they're limited\nby the need for resource intensive pre training, additional parameter\nrequirements, unaddressed modality prioritization and lack of clear benefit\nover non-retrieval baselines. This paper introduces RAVEN, a multitask\nretrieval augmented VLM framework that enhances base VLMs through efficient,\ntask specific fine-tuning. By integrating retrieval augmented samples without\nthe need for additional retrieval-specific parameters, we show that the model\nacquires retrieval properties that are effective across multiple tasks. Our\nresults and extensive ablations across retrieved modalities for the image\ncaptioning and VQA tasks indicate significant performance improvements compared\nto non retrieved baselines +1 CIDEr on MSCOCO, +4 CIDEr on NoCaps and nearly a\n+3\\% accuracy on specific VQA question types. This underscores the efficacy of\napplying RAG approaches to VLMs, marking a stride toward more efficient and\naccessible multimodal learning.",
        "chunk-id": 4,
        "chunk": "to non retrieved baselines +1 CIDEr on MSCOCO, +4 CIDEr on NoCaps and nearly a\n+3\\% accuracy on specific VQA question types. This underscores the efficacy of\napplying RAG approaches to VLMs, marking a stride toward more efficient and\naccessible multimodal learning.",
        "authors": [
            "Varun Nagaraj Rao",
            "Siddharth Choudhary",
            "Aditya Deshpande",
            "Ravi Kumar Satzoda",
            "Srikar Appalaraju"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:08:35+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19150v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19150v1",
        "categories": [
            "Computer Vision and Pattern Recognition",
            "Artificial Intelligence",
            "Information Retrieval"
        ]
    },
    {
        "id": 30000095,
        "doi": null,
        "title": "BackMix: Mitigating Shortcut Learning in Echocardiography with Minimal Supervision",
        "abstract": "Neural networks can learn spurious correlations that lead to the correct\nprediction in a validation set, but generalise poorly because the predictions\nare right for the wrong reason. This undesired learning of naive shortcuts\n(Clever Hans effect) can happen for example in echocardiogram view\nclassification when background cues (e.g. metadata) are biased towards a class\nand the model learns to focus on those background features instead of on the\nimage content. We propose a simple, yet effective random background\naugmentation method called BackMix, which samples random backgrounds from other\nexamples in the training set. By enforcing the background to be uncorrelated\nwith the outcome, the model learns to focus on the data within the ultrasound\nsector and becomes invariant to the regions outside this. We extend our method\nin a semi-supervised setting, finding that the positive effects of BackMix are\nmaintained with as few as 5% of segmentation labels. A loss weighting\nmechanism, wBackMix, is also proposed to increase the contribution of the\naugmented examples. We validate our method on both in-distribution and\nout-of-distribution datasets, demonstrating significant improvements in\nclassification accuracy, region focus and generalisability. Our source code is\navailable at: https://github.com/kitbransby/BackMix",
        "chunk-id": 1,
        "chunk": "Neural networks can learn spurious correlations that lead to the correct\nprediction in a validation set, but generalise poorly because the predictions\nare right for the wrong reason. This undesired learning of naive shortcuts\n(Clever Hans effect) can happen for example in echocardiogram view\nclassification when background cues (e.g. metadata) are biased towards a class",
        "authors": [
            "Kit Mills Bransby",
            "Arian Beqiri",
            "Woo-Jin Cho Kim",
            "Jorge Oliveira",
            "Agisilaos Chartsias",
            "Alberto Gomez"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:06:47+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19148v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19148v1",
        "categories": [
            "Computer Vision and Pattern Recognition",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 30000095,
        "doi": null,
        "title": "BackMix: Mitigating Shortcut Learning in Echocardiography with Minimal Supervision",
        "abstract": "Neural networks can learn spurious correlations that lead to the correct\nprediction in a validation set, but generalise poorly because the predictions\nare right for the wrong reason. This undesired learning of naive shortcuts\n(Clever Hans effect) can happen for example in echocardiogram view\nclassification when background cues (e.g. metadata) are biased towards a class\nand the model learns to focus on those background features instead of on the\nimage content. We propose a simple, yet effective random background\naugmentation method called BackMix, which samples random backgrounds from other\nexamples in the training set. By enforcing the background to be uncorrelated\nwith the outcome, the model learns to focus on the data within the ultrasound\nsector and becomes invariant to the regions outside this. We extend our method\nin a semi-supervised setting, finding that the positive effects of BackMix are\nmaintained with as few as 5% of segmentation labels. A loss weighting\nmechanism, wBackMix, is also proposed to increase the contribution of the\naugmented examples. We validate our method on both in-distribution and\nout-of-distribution datasets, demonstrating significant improvements in\nclassification accuracy, region focus and generalisability. Our source code is\navailable at: https://github.com/kitbransby/BackMix",
        "chunk-id": 2,
        "chunk": "and the model learns to focus on those background features instead of on the\nimage content. We propose a simple, yet effective random background\naugmentation method called BackMix, which samples random backgrounds from other\nexamples in the training set. By enforcing the background to be uncorrelated\nwith the outcome, the model learns to focus on the data within the ultrasound",
        "authors": [
            "Kit Mills Bransby",
            "Arian Beqiri",
            "Woo-Jin Cho Kim",
            "Jorge Oliveira",
            "Agisilaos Chartsias",
            "Alberto Gomez"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:06:47+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19148v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19148v1",
        "categories": [
            "Computer Vision and Pattern Recognition",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 30000095,
        "doi": null,
        "title": "BackMix: Mitigating Shortcut Learning in Echocardiography with Minimal Supervision",
        "abstract": "Neural networks can learn spurious correlations that lead to the correct\nprediction in a validation set, but generalise poorly because the predictions\nare right for the wrong reason. This undesired learning of naive shortcuts\n(Clever Hans effect) can happen for example in echocardiogram view\nclassification when background cues (e.g. metadata) are biased towards a class\nand the model learns to focus on those background features instead of on the\nimage content. We propose a simple, yet effective random background\naugmentation method called BackMix, which samples random backgrounds from other\nexamples in the training set. By enforcing the background to be uncorrelated\nwith the outcome, the model learns to focus on the data within the ultrasound\nsector and becomes invariant to the regions outside this. We extend our method\nin a semi-supervised setting, finding that the positive effects of BackMix are\nmaintained with as few as 5% of segmentation labels. A loss weighting\nmechanism, wBackMix, is also proposed to increase the contribution of the\naugmented examples. We validate our method on both in-distribution and\nout-of-distribution datasets, demonstrating significant improvements in\nclassification accuracy, region focus and generalisability. Our source code is\navailable at: https://github.com/kitbransby/BackMix",
        "chunk-id": 3,
        "chunk": "sector and becomes invariant to the regions outside this. We extend our method\nin a semi-supervised setting, finding that the positive effects of BackMix are\nmaintained with as few as 5% of segmentation labels. A loss weighting\nmechanism, wBackMix, is also proposed to increase the contribution of the\naugmented examples. We validate our method on both in-distribution and",
        "authors": [
            "Kit Mills Bransby",
            "Arian Beqiri",
            "Woo-Jin Cho Kim",
            "Jorge Oliveira",
            "Agisilaos Chartsias",
            "Alberto Gomez"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:06:47+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19148v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19148v1",
        "categories": [
            "Computer Vision and Pattern Recognition",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 30000095,
        "doi": null,
        "title": "BackMix: Mitigating Shortcut Learning in Echocardiography with Minimal Supervision",
        "abstract": "Neural networks can learn spurious correlations that lead to the correct\nprediction in a validation set, but generalise poorly because the predictions\nare right for the wrong reason. This undesired learning of naive shortcuts\n(Clever Hans effect) can happen for example in echocardiogram view\nclassification when background cues (e.g. metadata) are biased towards a class\nand the model learns to focus on those background features instead of on the\nimage content. We propose a simple, yet effective random background\naugmentation method called BackMix, which samples random backgrounds from other\nexamples in the training set. By enforcing the background to be uncorrelated\nwith the outcome, the model learns to focus on the data within the ultrasound\nsector and becomes invariant to the regions outside this. We extend our method\nin a semi-supervised setting, finding that the positive effects of BackMix are\nmaintained with as few as 5% of segmentation labels. A loss weighting\nmechanism, wBackMix, is also proposed to increase the contribution of the\naugmented examples. We validate our method on both in-distribution and\nout-of-distribution datasets, demonstrating significant improvements in\nclassification accuracy, region focus and generalisability. Our source code is\navailable at: https://github.com/kitbransby/BackMix",
        "chunk-id": 4,
        "chunk": "out-of-distribution datasets, demonstrating significant improvements in\nclassification accuracy, region focus and generalisability. Our source code is\navailable at: https://github.com/kitbransby/BackMix",
        "authors": [
            "Kit Mills Bransby",
            "Arian Beqiri",
            "Woo-Jin Cho Kim",
            "Jorge Oliveira",
            "Agisilaos Chartsias",
            "Alberto Gomez"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:06:47+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19148v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19148v1",
        "categories": [
            "Computer Vision and Pattern Recognition",
            "Artificial Intelligence"
        ]
    },
    {
        "id": 30000096,
        "doi": null,
        "title": "Resolving Discrepancies in Compute-Optimal Scaling of Language Models",
        "abstract": "Kaplan et al. and Hoffmann et al. developed influential scaling laws for the\noptimal model size as a function of the compute budget, but these laws yield\nsubstantially different predictions. We explain the discrepancy by reproducing\nthe Kaplan scaling law on two datasets (OpenWebText2 and RefinedWeb) and\nidentifying three factors causing the difference: last layer computational\ncost, warmup duration, and scale-dependent optimizer tuning. With these factors\ncorrected, we obtain excellent agreement with the Hoffmann et al. (i.e.,\n\"Chinchilla\") scaling law. Counter to a hypothesis of Hoffmann et al., we find\nthat careful learning rate decay is not essential for the validity of their\nscaling law. As a secondary result, we derive scaling laws for the optimal\nlearning rate and batch size, finding that tuning the AdamW $\\beta_2$ parameter\nis essential at lower batch sizes.",
        "chunk-id": 1,
        "chunk": "Kaplan et al. and Hoffmann et al. developed influential scaling laws for the\noptimal model size as a function of the compute budget, but these laws yield\nsubstantially different predictions. We explain the discrepancy by reproducing\nthe Kaplan scaling law on two datasets (OpenWebText2 and RefinedWeb) and\nidentifying three factors causing the difference: last layer computational",
        "authors": [
            "Tomer Porian",
            "Mitchell Wortsman",
            "Jenia Jitsev",
            "Ludwig Schmidt",
            "Yair Carmon"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:02:43+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19146v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19146v1",
        "categories": [
            "Machine Learning",
            "Computation and Language"
        ]
    },
    {
        "id": 30000096,
        "doi": null,
        "title": "Resolving Discrepancies in Compute-Optimal Scaling of Language Models",
        "abstract": "Kaplan et al. and Hoffmann et al. developed influential scaling laws for the\noptimal model size as a function of the compute budget, but these laws yield\nsubstantially different predictions. We explain the discrepancy by reproducing\nthe Kaplan scaling law on two datasets (OpenWebText2 and RefinedWeb) and\nidentifying three factors causing the difference: last layer computational\ncost, warmup duration, and scale-dependent optimizer tuning. With these factors\ncorrected, we obtain excellent agreement with the Hoffmann et al. (i.e.,\n\"Chinchilla\") scaling law. Counter to a hypothesis of Hoffmann et al., we find\nthat careful learning rate decay is not essential for the validity of their\nscaling law. As a secondary result, we derive scaling laws for the optimal\nlearning rate and batch size, finding that tuning the AdamW $\\beta_2$ parameter\nis essential at lower batch sizes.",
        "chunk-id": 2,
        "chunk": "cost, warmup duration, and scale-dependent optimizer tuning. With these factors\ncorrected, we obtain excellent agreement with the Hoffmann et al. (i.e.,\n\"Chinchilla\") scaling law. Counter to a hypothesis of Hoffmann et al., we find\nthat careful learning rate decay is not essential for the validity of their\nscaling law. As a secondary result, we derive scaling laws for the optimal",
        "authors": [
            "Tomer Porian",
            "Mitchell Wortsman",
            "Jenia Jitsev",
            "Ludwig Schmidt",
            "Yair Carmon"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:02:43+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19146v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19146v1",
        "categories": [
            "Machine Learning",
            "Computation and Language"
        ]
    },
    {
        "id": 30000096,
        "doi": null,
        "title": "Resolving Discrepancies in Compute-Optimal Scaling of Language Models",
        "abstract": "Kaplan et al. and Hoffmann et al. developed influential scaling laws for the\noptimal model size as a function of the compute budget, but these laws yield\nsubstantially different predictions. We explain the discrepancy by reproducing\nthe Kaplan scaling law on two datasets (OpenWebText2 and RefinedWeb) and\nidentifying three factors causing the difference: last layer computational\ncost, warmup duration, and scale-dependent optimizer tuning. With these factors\ncorrected, we obtain excellent agreement with the Hoffmann et al. (i.e.,\n\"Chinchilla\") scaling law. Counter to a hypothesis of Hoffmann et al., we find\nthat careful learning rate decay is not essential for the validity of their\nscaling law. As a secondary result, we derive scaling laws for the optimal\nlearning rate and batch size, finding that tuning the AdamW $\\beta_2$ parameter\nis essential at lower batch sizes.",
        "chunk-id": 3,
        "chunk": "learning rate and batch size, finding that tuning the AdamW $\\beta_2$ parameter\nis essential at lower batch sizes.",
        "authors": [
            "Tomer Porian",
            "Mitchell Wortsman",
            "Jenia Jitsev",
            "Ludwig Schmidt",
            "Yair Carmon"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T13:02:43+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19146v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19146v1",
        "categories": [
            "Machine Learning",
            "Computation and Language"
        ]
    },
    {
        "id": 30000097,
        "doi": null,
        "title": "QSketch: An Efficient Sketch for Weighted Cardinality Estimation in Streams",
        "abstract": "Estimating cardinality, i.e., the number of distinct elements, of a data\nstream is a fundamental problem in areas like databases, computer networks, and\ninformation retrieval. This study delves into a broader scenario where each\nelement carries a positive weight. Unlike traditional cardinality estimation,\nlimited research exists on weighted cardinality, with current methods requiring\nsubstantial memory and computational resources, challenging for devices with\nlimited capabilities and real-time applications like anomaly detection. To\naddress these issues, we propose QSketch, a memory-efficient sketch method for\nestimating weighted cardinality in streams. QSketch uses a quantization\ntechnique to condense continuous variables into a compact set of integer\nvariables, with each variable requiring only 8 bits, making it 8 times smaller\nthan previous methods. Furthermore, we leverage dynamic properties during\nQSketch generation to significantly enhance estimation accuracy and achieve a\nlower time complexity of $O(1)$ for updating estimations upon encountering a\nnew element. Experimental results on synthetic and real-world datasets show\nthat QSketch is approximately 30\\% more accurate and two orders of magnitude\nfaster than the state-of-the-art, using only $1/8$ of the memory.",
        "chunk-id": 1,
        "chunk": "Estimating cardinality, i.e., the number of distinct elements, of a data\nstream is a fundamental problem in areas like databases, computer networks, and\ninformation retrieval. This study delves into a broader scenario where each\nelement carries a positive weight. Unlike traditional cardinality estimation,",
        "authors": [
            "Yiyan Qi",
            "Rundong Li",
            "Pinghui Wang",
            "Yufang Sun",
            "Rui Xing"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T12:55:42+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19143v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19143v1",
        "categories": [
            "Databases",
            "Data Structures and Algorithms"
        ]
    },
    {
        "id": 30000097,
        "doi": null,
        "title": "QSketch: An Efficient Sketch for Weighted Cardinality Estimation in Streams",
        "abstract": "Estimating cardinality, i.e., the number of distinct elements, of a data\nstream is a fundamental problem in areas like databases, computer networks, and\ninformation retrieval. This study delves into a broader scenario where each\nelement carries a positive weight. Unlike traditional cardinality estimation,\nlimited research exists on weighted cardinality, with current methods requiring\nsubstantial memory and computational resources, challenging for devices with\nlimited capabilities and real-time applications like anomaly detection. To\naddress these issues, we propose QSketch, a memory-efficient sketch method for\nestimating weighted cardinality in streams. QSketch uses a quantization\ntechnique to condense continuous variables into a compact set of integer\nvariables, with each variable requiring only 8 bits, making it 8 times smaller\nthan previous methods. Furthermore, we leverage dynamic properties during\nQSketch generation to significantly enhance estimation accuracy and achieve a\nlower time complexity of $O(1)$ for updating estimations upon encountering a\nnew element. Experimental results on synthetic and real-world datasets show\nthat QSketch is approximately 30\\% more accurate and two orders of magnitude\nfaster than the state-of-the-art, using only $1/8$ of the memory.",
        "chunk-id": 2,
        "chunk": "limited research exists on weighted cardinality, with current methods requiring\nsubstantial memory and computational resources, challenging for devices with\nlimited capabilities and real-time applications like anomaly detection. To\naddress these issues, we propose QSketch, a memory-efficient sketch method for\nestimating weighted cardinality in streams. QSketch uses a quantization",
        "authors": [
            "Yiyan Qi",
            "Rundong Li",
            "Pinghui Wang",
            "Yufang Sun",
            "Rui Xing"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T12:55:42+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19143v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19143v1",
        "categories": [
            "Databases",
            "Data Structures and Algorithms"
        ]
    },
    {
        "id": 30000097,
        "doi": null,
        "title": "QSketch: An Efficient Sketch for Weighted Cardinality Estimation in Streams",
        "abstract": "Estimating cardinality, i.e., the number of distinct elements, of a data\nstream is a fundamental problem in areas like databases, computer networks, and\ninformation retrieval. This study delves into a broader scenario where each\nelement carries a positive weight. Unlike traditional cardinality estimation,\nlimited research exists on weighted cardinality, with current methods requiring\nsubstantial memory and computational resources, challenging for devices with\nlimited capabilities and real-time applications like anomaly detection. To\naddress these issues, we propose QSketch, a memory-efficient sketch method for\nestimating weighted cardinality in streams. QSketch uses a quantization\ntechnique to condense continuous variables into a compact set of integer\nvariables, with each variable requiring only 8 bits, making it 8 times smaller\nthan previous methods. Furthermore, we leverage dynamic properties during\nQSketch generation to significantly enhance estimation accuracy and achieve a\nlower time complexity of $O(1)$ for updating estimations upon encountering a\nnew element. Experimental results on synthetic and real-world datasets show\nthat QSketch is approximately 30\\% more accurate and two orders of magnitude\nfaster than the state-of-the-art, using only $1/8$ of the memory.",
        "chunk-id": 3,
        "chunk": "technique to condense continuous variables into a compact set of integer\nvariables, with each variable requiring only 8 bits, making it 8 times smaller\nthan previous methods. Furthermore, we leverage dynamic properties during\nQSketch generation to significantly enhance estimation accuracy and achieve a\nlower time complexity of $O(1)$ for updating estimations upon encountering a",
        "authors": [
            "Yiyan Qi",
            "Rundong Li",
            "Pinghui Wang",
            "Yufang Sun",
            "Rui Xing"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T12:55:42+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19143v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19143v1",
        "categories": [
            "Databases",
            "Data Structures and Algorithms"
        ]
    },
    {
        "id": 30000097,
        "doi": null,
        "title": "QSketch: An Efficient Sketch for Weighted Cardinality Estimation in Streams",
        "abstract": "Estimating cardinality, i.e., the number of distinct elements, of a data\nstream is a fundamental problem in areas like databases, computer networks, and\ninformation retrieval. This study delves into a broader scenario where each\nelement carries a positive weight. Unlike traditional cardinality estimation,\nlimited research exists on weighted cardinality, with current methods requiring\nsubstantial memory and computational resources, challenging for devices with\nlimited capabilities and real-time applications like anomaly detection. To\naddress these issues, we propose QSketch, a memory-efficient sketch method for\nestimating weighted cardinality in streams. QSketch uses a quantization\ntechnique to condense continuous variables into a compact set of integer\nvariables, with each variable requiring only 8 bits, making it 8 times smaller\nthan previous methods. Furthermore, we leverage dynamic properties during\nQSketch generation to significantly enhance estimation accuracy and achieve a\nlower time complexity of $O(1)$ for updating estimations upon encountering a\nnew element. Experimental results on synthetic and real-world datasets show\nthat QSketch is approximately 30\\% more accurate and two orders of magnitude\nfaster than the state-of-the-art, using only $1/8$ of the memory.",
        "chunk-id": 4,
        "chunk": "new element. Experimental results on synthetic and real-world datasets show\nthat QSketch is approximately 30\\% more accurate and two orders of magnitude\nfaster than the state-of-the-art, using only $1/8$ of the memory.",
        "authors": [
            "Yiyan Qi",
            "Rundong Li",
            "Pinghui Wang",
            "Yufang Sun",
            "Rui Xing"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T12:55:42+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19143v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19143v1",
        "categories": [
            "Databases",
            "Data Structures and Algorithms"
        ]
    },
    {
        "id": 30000098,
        "doi": null,
        "title": "Exact confidence intervals for functions of parameters in the k-sample multinomial problem",
        "abstract": "When the target of inference is a real-valued function of probability\nparameters in the k-sample multinomial problem, variance estimation may be\nchallenging. In small samples, methods like the nonparametric bootstrap or\ndelta method may perform poorly. We propose a novel general method in this\nsetting for computing exact p-values and confidence intervals which means that\ntype I error rates are correctly bounded and confidence intervals have at least\nnominal coverage at all sample sizes. Our method is applicable to any\nreal-valued function of multinomial probabilities, accommodating an arbitrary\nnumber of samples with varying category counts. We describe the method and\nprovide an implementation of it in R, with some computational optimization to\nensure broad applicability. Simulations demonstrate our method's ability to\nmaintain correct coverage rates in settings where the nonparametric bootstrap\nfails.",
        "chunk-id": 1,
        "chunk": "When the target of inference is a real-valued function of probability\nparameters in the k-sample multinomial problem, variance estimation may be\nchallenging. In small samples, methods like the nonparametric bootstrap or\ndelta method may perform poorly. We propose a novel general method in this\nsetting for computing exact p-values and confidence intervals which means that",
        "authors": [
            "Michael C Sachs",
            "Erin E Gabriel",
            "Michael P Fay"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T12:48:35+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19141v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19141v1",
        "categories": [
            "Computation"
        ]
    },
    {
        "id": 30000098,
        "doi": null,
        "title": "Exact confidence intervals for functions of parameters in the k-sample multinomial problem",
        "abstract": "When the target of inference is a real-valued function of probability\nparameters in the k-sample multinomial problem, variance estimation may be\nchallenging. In small samples, methods like the nonparametric bootstrap or\ndelta method may perform poorly. We propose a novel general method in this\nsetting for computing exact p-values and confidence intervals which means that\ntype I error rates are correctly bounded and confidence intervals have at least\nnominal coverage at all sample sizes. Our method is applicable to any\nreal-valued function of multinomial probabilities, accommodating an arbitrary\nnumber of samples with varying category counts. We describe the method and\nprovide an implementation of it in R, with some computational optimization to\nensure broad applicability. Simulations demonstrate our method's ability to\nmaintain correct coverage rates in settings where the nonparametric bootstrap\nfails.",
        "chunk-id": 2,
        "chunk": "type I error rates are correctly bounded and confidence intervals have at least\nnominal coverage at all sample sizes. Our method is applicable to any\nreal-valued function of multinomial probabilities, accommodating an arbitrary\nnumber of samples with varying category counts. We describe the method and\nprovide an implementation of it in R, with some computational optimization to",
        "authors": [
            "Michael C Sachs",
            "Erin E Gabriel",
            "Michael P Fay"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T12:48:35+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19141v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19141v1",
        "categories": [
            "Computation"
        ]
    },
    {
        "id": 30000098,
        "doi": null,
        "title": "Exact confidence intervals for functions of parameters in the k-sample multinomial problem",
        "abstract": "When the target of inference is a real-valued function of probability\nparameters in the k-sample multinomial problem, variance estimation may be\nchallenging. In small samples, methods like the nonparametric bootstrap or\ndelta method may perform poorly. We propose a novel general method in this\nsetting for computing exact p-values and confidence intervals which means that\ntype I error rates are correctly bounded and confidence intervals have at least\nnominal coverage at all sample sizes. Our method is applicable to any\nreal-valued function of multinomial probabilities, accommodating an arbitrary\nnumber of samples with varying category counts. We describe the method and\nprovide an implementation of it in R, with some computational optimization to\nensure broad applicability. Simulations demonstrate our method's ability to\nmaintain correct coverage rates in settings where the nonparametric bootstrap\nfails.",
        "chunk-id": 3,
        "chunk": "ensure broad applicability. Simulations demonstrate our method's ability to\nmaintain correct coverage rates in settings where the nonparametric bootstrap\nfails.",
        "authors": [
            "Michael C Sachs",
            "Erin E Gabriel",
            "Michael P Fay"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T12:48:35+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19141v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19141v1",
        "categories": [
            "Computation"
        ]
    },
    {
        "id": 30000099,
        "doi": null,
        "title": "Topological Dynamics and Correspondences in Composite Exceptional Rings",
        "abstract": "The exploration of novel phases and the elucidation of correspondences\nbetween topological invariants and their intriguing properties are pivotal in\nthe realm of topological physics. Here, we investigate a complex exceptional\nstructure, termed the composite exceptional ring (CER), composed of a\nthird-order exceptional ring and multiple Weyl exceptional rings. We establish\na direct correspondence between Chern numbers and the distinctive behaviors\nexhibited by these exceptional structures. Notably, we demonstrate that band\nbraiding during quasistatic encircling processes correlates with bands\npossessing nontrivial Chern numbers, leading to triple (double) periodic\nspectra for cases with topologically nontrivial (trivial) middle bands.\nMoreover, the Chern numbers predict mode transfer behaviors during dynamical\nencircling process. We propose experimental schemes to realize CER in cold\natoms, emphasizing the critical role of Chern numbers as both a measurable\nquantity and a descriptor of the exceptional physics inherent to dissipative\nsystems. The discovery of CER opens significant avenues for expanding the scope\nof topological classifications in non-Hermitian systems, with promising\napplications in quantum computing and metrology.",
        "chunk-id": 1,
        "chunk": "The exploration of novel phases and the elucidation of correspondences\nbetween topological invariants and their intriguing properties are pivotal in\nthe realm of topological physics. Here, we investigate a complex exceptional\nstructure, termed the composite exceptional ring (CER), composed of a\nthird-order exceptional ring and multiple Weyl exceptional rings. We establish",
        "authors": [
            "Zhoutao Lei",
            "Yuangang Deng"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T12:41:20+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19137v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19137v1",
        "categories": [
            "Quantum Gases"
        ]
    },
    {
        "id": 30000099,
        "doi": null,
        "title": "Topological Dynamics and Correspondences in Composite Exceptional Rings",
        "abstract": "The exploration of novel phases and the elucidation of correspondences\nbetween topological invariants and their intriguing properties are pivotal in\nthe realm of topological physics. Here, we investigate a complex exceptional\nstructure, termed the composite exceptional ring (CER), composed of a\nthird-order exceptional ring and multiple Weyl exceptional rings. We establish\na direct correspondence between Chern numbers and the distinctive behaviors\nexhibited by these exceptional structures. Notably, we demonstrate that band\nbraiding during quasistatic encircling processes correlates with bands\npossessing nontrivial Chern numbers, leading to triple (double) periodic\nspectra for cases with topologically nontrivial (trivial) middle bands.\nMoreover, the Chern numbers predict mode transfer behaviors during dynamical\nencircling process. We propose experimental schemes to realize CER in cold\natoms, emphasizing the critical role of Chern numbers as both a measurable\nquantity and a descriptor of the exceptional physics inherent to dissipative\nsystems. The discovery of CER opens significant avenues for expanding the scope\nof topological classifications in non-Hermitian systems, with promising\napplications in quantum computing and metrology.",
        "chunk-id": 2,
        "chunk": "a direct correspondence between Chern numbers and the distinctive behaviors\nexhibited by these exceptional structures. Notably, we demonstrate that band\nbraiding during quasistatic encircling processes correlates with bands\npossessing nontrivial Chern numbers, leading to triple (double) periodic\nspectra for cases with topologically nontrivial (trivial) middle bands.",
        "authors": [
            "Zhoutao Lei",
            "Yuangang Deng"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T12:41:20+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19137v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19137v1",
        "categories": [
            "Quantum Gases"
        ]
    },
    {
        "id": 30000099,
        "doi": null,
        "title": "Topological Dynamics and Correspondences in Composite Exceptional Rings",
        "abstract": "The exploration of novel phases and the elucidation of correspondences\nbetween topological invariants and their intriguing properties are pivotal in\nthe realm of topological physics. Here, we investigate a complex exceptional\nstructure, termed the composite exceptional ring (CER), composed of a\nthird-order exceptional ring and multiple Weyl exceptional rings. We establish\na direct correspondence between Chern numbers and the distinctive behaviors\nexhibited by these exceptional structures. Notably, we demonstrate that band\nbraiding during quasistatic encircling processes correlates with bands\npossessing nontrivial Chern numbers, leading to triple (double) periodic\nspectra for cases with topologically nontrivial (trivial) middle bands.\nMoreover, the Chern numbers predict mode transfer behaviors during dynamical\nencircling process. We propose experimental schemes to realize CER in cold\natoms, emphasizing the critical role of Chern numbers as both a measurable\nquantity and a descriptor of the exceptional physics inherent to dissipative\nsystems. The discovery of CER opens significant avenues for expanding the scope\nof topological classifications in non-Hermitian systems, with promising\napplications in quantum computing and metrology.",
        "chunk-id": 3,
        "chunk": "Moreover, the Chern numbers predict mode transfer behaviors during dynamical\nencircling process. We propose experimental schemes to realize CER in cold\natoms, emphasizing the critical role of Chern numbers as both a measurable\nquantity and a descriptor of the exceptional physics inherent to dissipative\nsystems. The discovery of CER opens significant avenues for expanding the scope",
        "authors": [
            "Zhoutao Lei",
            "Yuangang Deng"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T12:41:20+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19137v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19137v1",
        "categories": [
            "Quantum Gases"
        ]
    },
    {
        "id": 30000099,
        "doi": null,
        "title": "Topological Dynamics and Correspondences in Composite Exceptional Rings",
        "abstract": "The exploration of novel phases and the elucidation of correspondences\nbetween topological invariants and their intriguing properties are pivotal in\nthe realm of topological physics. Here, we investigate a complex exceptional\nstructure, termed the composite exceptional ring (CER), composed of a\nthird-order exceptional ring and multiple Weyl exceptional rings. We establish\na direct correspondence between Chern numbers and the distinctive behaviors\nexhibited by these exceptional structures. Notably, we demonstrate that band\nbraiding during quasistatic encircling processes correlates with bands\npossessing nontrivial Chern numbers, leading to triple (double) periodic\nspectra for cases with topologically nontrivial (trivial) middle bands.\nMoreover, the Chern numbers predict mode transfer behaviors during dynamical\nencircling process. We propose experimental schemes to realize CER in cold\natoms, emphasizing the critical role of Chern numbers as both a measurable\nquantity and a descriptor of the exceptional physics inherent to dissipative\nsystems. The discovery of CER opens significant avenues for expanding the scope\nof topological classifications in non-Hermitian systems, with promising\napplications in quantum computing and metrology.",
        "chunk-id": 4,
        "chunk": "of topological classifications in non-Hermitian systems, with promising\napplications in quantum computing and metrology.",
        "authors": [
            "Zhoutao Lei",
            "Yuangang Deng"
        ],
        "journal_ref": "N/A",
        "published": "2024-06-27T12:41:20+00:00",
        "pdf_link": "http://arxiv.org/pdf/2406.19137v1",
        "arxiv_link": "http://arxiv.org/abs/2406.19137v1",
        "categories": [
            "Quantum Gases"
        ]
    }
]